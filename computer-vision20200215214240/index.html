<!-- build time:Thu Mar 05 2020 20:26:19 GMT+0800 (GMT+08:00) --><!DOCTYPE html><html class="theme-next gemini use-motion" lang="zh-Hans"><head><meta name="generator" content="Hexo 3.9.0"><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width,initial-scale=1,maximum-scale=1"><meta name="theme-color" content="#222"><meta name="google-site-verification" content="YV24rdmIIf8GuLLOBH5IYEWm0Z3TGAqiLS-LLlspD7w"><script src="/lib/pace/pace.min.js?v=1.0.2"></script><link href="/lib/pace/pace-theme-big-counter.min.css?v=1.0.2" rel="stylesheet"><meta http-equiv="Cache-Control" content="no-transform"><meta http-equiv="Cache-Control" content="no-siteapp"><meta name="baidu-site-verification" content="true"><link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css"><link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css"><link href="/css/main.css?v=5.1.4" rel="stylesheet" type="text/css"><link rel="apple-touch-icon" sizes="180x180" href="/images/haizei.ico?v=5.1.4"><link rel="icon" type="image/png" sizes="32x32" href="/images/haizei.ico?v=5.1.4"><link rel="icon" type="image/png" sizes="16x16" href="/images/haizei.ico?v=5.1.4"><link rel="mask-icon" href="/images/haizei.ico?v=5.1.4" color="#222"><meta name="keywords" content="论文分享,计算机视觉,目标跟踪,"><link rel="alternate" href="/atom.xml" title="高深远的博客" type="application/atom+xml"><meta name="description" content="看了一寒假的目标跟踪，一直想将自己学习到的内容整理归纳一下，迟迟没动笔（其实是打字，但说迟迟没打字比较难听哈哈）。如今快要开学了，决定还是积累一下。本文主要是把目标跟踪中的一些相关要点做一个总结，并且按具体问题对一些主流的或者我阅读过觉得有价值的算法做一个简单概述，方法层出不穷，码字不易，无法涵盖所有的方法，也可能会存在错误，在今后的学习过程中会一直保持更新，这将是一篇LTS的文章哈哈。注：本文重"><meta name="keywords" content="论文分享,计算机视觉,目标跟踪"><meta property="og:type" content="article"><meta property="og:title" content="computer vision笔记：目标跟踪的小总结"><meta property="og:url" content="https://gsy00517.github.io/computer-vision20200215214240/index.html"><meta property="og:site_name" content="高深远的博客"><meta property="og:description" content="看了一寒假的目标跟踪，一直想将自己学习到的内容整理归纳一下，迟迟没动笔（其实是打字，但说迟迟没打字比较难听哈哈）。如今快要开学了，决定还是积累一下。本文主要是把目标跟踪中的一些相关要点做一个总结，并且按具体问题对一些主流的或者我阅读过觉得有价值的算法做一个简单概述，方法层出不穷，码字不易，无法涵盖所有的方法，也可能会存在错误，在今后的学习过程中会一直保持更新，这将是一篇LTS的文章哈哈。注：本文重"><meta property="og:locale" content="zh-Hans"><meta property="og:image" content="https://gsy00517.github.io/computer-vision20200215214240/目标跟踪.png"><meta property="og:image" content="https://gsy00517.github.io/computer-vision20200215214240/目标跟踪伪代码.png"><meta property="og:image" content="https://gsy00517.github.io/computer-vision20200215214240/问题.png"><meta property="og:image" content="https://gsy00517.github.io/computer-vision20200215214240/OTB对问题的划分.png"><meta property="og:image" content="https://gsy00517.github.io/computer-vision20200215214240/生成式模型.png"><meta property="og:image" content="https://gsy00517.github.io/computer-vision20200215214240/判别式模型.png"><meta property="og:image" content="https://gsy00517.github.io/computer-vision20200215214240/论文中的导图.png"><meta property="og:image" content="https://gsy00517.github.io/computer-vision20200215214240/王强的导图.png"><meta property="og:image" content="https://gsy00517.github.io/computer-vision20200215214240/王蒙蒙的导图.png"><meta property="og:image" content="https://gsy00517.github.io/computer-vision20200215214240/光流.gif"><meta property="og:image" content="https://gsy00517.github.io/computer-vision20200215214240/画出光流.png"><meta property="og:image" content="https://gsy00517.github.io/computer-vision20200215214240/难例挖掘.png"><meta property="og:image" content="https://gsy00517.github.io/computer-vision20200215214240/热力图.jpg"><meta property="og:image" content="https://gsy00517.github.io/computer-vision20200215214240/STRCF和SRDCF对比.png"><meta property="og:image" content="https://gsy00517.github.io/computer-vision20200215214240/MCCT特征.png"><meta property="og:image" content="https://gsy00517.github.io/computer-vision20200215214240/三种样本选取方法.png"><meta property="og:image" content="https://gsy00517.github.io/computer-vision20200215214240/发廊旋转柱.png"><meta property="og:updated_time" content="2020-03-05T12:25:37.197Z"><meta name="twitter:card" content="summary"><meta name="twitter:title" content="computer vision笔记：目标跟踪的小总结"><meta name="twitter:description" content="看了一寒假的目标跟踪，一直想将自己学习到的内容整理归纳一下，迟迟没动笔（其实是打字，但说迟迟没打字比较难听哈哈）。如今快要开学了，决定还是积累一下。本文主要是把目标跟踪中的一些相关要点做一个总结，并且按具体问题对一些主流的或者我阅读过觉得有价值的算法做一个简单概述，方法层出不穷，码字不易，无法涵盖所有的方法，也可能会存在错误，在今后的学习过程中会一直保持更新，这将是一篇LTS的文章哈哈。注：本文重"><meta name="twitter:image" content="https://gsy00517.github.io/computer-vision20200215214240/目标跟踪.png"><script type="text/javascript" id="hexo.configurations">var NexT=window.NexT||{},CONFIG={root:"/",scheme:"Gemini",version:"5.1.4",sidebar:{position:"left",display:"post",offset:12,b2t:!1,scrollpercent:!0,onmobile:!1},fancybox:!0,tabs:!0,motion:{enable:!0,async:!1,transition:{post_block:"fadeIn",post_header:"slideDownIn",post_body:"slideDownIn",coll_header:"slideLeftIn",sidebar:"slideUpIn"}},duoshuo:{userId:"0",author:"博主"},algolia:{applicationID:"",apiKey:"",indexName:"",hits:{per_page:10},labels:{input_placeholder:"Search for Posts",hits_empty:"We didn't find any results for the search: ${query}",hits_stats:"${hits} results found in ${time} ms"}}}</script><link rel="canonical" href="https://gsy00517.github.io/computer-vision20200215214240/"><meta name="baidu-site-verification" content="o5QfpvLBz5"><title>computer vision笔记：目标跟踪的小总结 | 高深远的博客</title></head><body itemscope itemtype="http://schema.org/WebPage" lang="zh-Hans"><div class="container sidebar-position-left page-post-detail"><div class="headband"></div><header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="header-inner"><div class="site-brand-wrapper"><div class="site-meta"><div class="custom-logo-site-title"><a href="/" class="brand" rel="start"><span class="logo-line-before"><i></i></span> <span class="site-title">高深远的博客</span> <span class="logo-line-after"><i></i></span></a></div><h1 class="site-subtitle" itemprop="description"></h1></div><div class="site-nav-toggle"><button><span class="btn-bar"></span> <span class="btn-bar"></span> <span class="btn-bar"></span></button></div></div><nav class="site-nav"><ul id="menu" class="menu"><li class="menu-item menu-item-home"><a href="/" rel="section"><i class="menu-item-icon fa fa-fw fa-home"></i><br>首页</a></li><li class="menu-item menu-item-about"><a href="/about/" rel="section"><i class="menu-item-icon fa fa-fw fa-user"></i><br>关于</a></li><li class="menu-item menu-item-tags"><a href="/tags/" rel="section"><i class="menu-item-icon fa fa-fw fa-tags"></i><br>标签</a></li><li class="menu-item menu-item-categories"><a href="/categories/" rel="section"><i class="menu-item-icon fa fa-fw fa-th"></i><br>分类</a></li><li class="menu-item menu-item-archives"><a href="/archives/" rel="section"><i class="menu-item-icon fa fa-fw fa-archive"></i><br>最新发布</a></li><li class="menu-item menu-item-new"><a href="/new/" rel="section"><i class="menu-item-icon fa fa-fw fa-history"></i><br>最近阅读</a></li><li class="menu-item menu-item-rank"><a href="/rank/" rel="section"><i class="menu-item-icon fa fa-fw fa-signal"></i><br>热度排名</a></li><li class="menu-item menu-item-search"><a href="javascript:;" class="popup-trigger"><i class="menu-item-icon fa fa-search fa-fw"></i><br>站内搜索</a></li></ul><div class="site-search"><div class="popup search-popup local-search-popup"><div class="local-search-header clearfix"><span class="search-icon"><i class="fa fa-search"></i> </span><span class="popup-btn-close"><i class="fa fa-times-circle"></i></span><div class="local-search-input-wrapper"><input autocomplete="off" placeholder="搜索..." spellcheck="false" type="text" id="local-search-input"></div></div><div id="local-search-result"></div></div></div><div style="Text-align:center;width:100%"><div style="margin:0 auto"><canvas id="canvas" style="width:60%">当前浏览器不支持canvas，请更换浏览器后再试</canvas></div><script>!function(){function t(t,e){for(var a=0;a<l[e].length;a++)for(var n=0;n<l[e][a].length;n++)1==l[e][a][n]&&(h.beginPath(),h.arc(14*(g+2)*t+2*n*(g+1)+(g+1),2*a*(g+1)+(g+1),g,0,2*Math.PI),h.closePath(),h.fill())}function e(){var t=[],e=/(\d)(\d):(\d)(\d):(\d)(\d)/.exec(new Date),a=[];a.push(e[1],e[2],10,e[3],e[4],10,e[5],e[6]);for(var r=c.length-1;r>=0;r--)a[r]!==c[r]&&t.push(r+"_"+(Number(c[r])+1)%10);for(var r=0;r<t.length;r++)n.apply(this,t[r].split("_"));c=a.concat()}function a(){for(var t=0;t<d.length;t++)d[t].stepY+=d[t].disY,d[t].x+=d[t].stepX,d[t].y+=d[t].stepY,(d[t].x>i+g||d[t].y>f+g)&&(d.splice(t,1),t--)}function n(t,e){for(var a=[1,2,3],n=["#3BE","#09C","#A6C","#93C","#9C0","#690","#FB3","#F80","#F44","#C00"],r=0;r<l[e].length;r++)for(var o=0;o<l[e][r].length;o++)if(1==l[e][r][o]){var h={x:14*(g+2)*t+2*o*(g+1)+(g+1),y:2*r*(g+1)+(g+1),stepX:Math.floor(4*Math.random()-2),stepY:-2*a[Math.floor(Math.random()*a.length)],color:n[Math.floor(Math.random()*n.length)],disY:1};d.push(h)}}function r(){o.height=100;for(var e=0;e<c.length;e++)t(e,c[e]);for(var e=0;e<d.length;e++)h.beginPath(),h.arc(d[e].x,d[e].y,g,0,2*Math.PI),h.fillStyle=d[e].color,h.closePath(),h.fill()}var l=[[[0,0,1,1,1,0,0],[0,1,1,0,1,1,0],[1,1,0,0,0,1,1],[1,1,0,0,0,1,1],[1,1,0,0,0,1,1],[1,1,0,0,0,1,1],[1,1,0,0,0,1,1],[1,1,0,0,0,1,1],[0,1,1,0,1,1,0],[0,0,1,1,1,0,0]],[[0,0,0,1,1,0,0],[0,1,1,1,1,0,0],[0,0,0,1,1,0,0],[0,0,0,1,1,0,0],[0,0,0,1,1,0,0],[0,0,0,1,1,0,0],[0,0,0,1,1,0,0],[0,0,0,1,1,0,0],[0,0,0,1,1,0,0],[1,1,1,1,1,1,1]],[[0,1,1,1,1,1,0],[1,1,0,0,0,1,1],[0,0,0,0,0,1,1],[0,0,0,0,1,1,0],[0,0,0,1,1,0,0],[0,0,1,1,0,0,0],[0,1,1,0,0,0,0],[1,1,0,0,0,0,0],[1,1,0,0,0,1,1],[1,1,1,1,1,1,1]],[[1,1,1,1,1,1,1],[0,0,0,0,0,1,1],[0,0,0,0,1,1,0],[0,0,0,1,1,0,0],[0,0,1,1,1,0,0],[0,0,0,0,1,1,0],[0,0,0,0,0,1,1],[0,0,0,0,0,1,1],[1,1,0,0,0,1,1],[0,1,1,1,1,1,0]],[[0,0,0,0,1,1,0],[0,0,0,1,1,1,0],[0,0,1,1,1,1,0],[0,1,1,0,1,1,0],[1,1,0,0,1,1,0],[1,1,1,1,1,1,1],[0,0,0,0,1,1,0],[0,0,0,0,1,1,0],[0,0,0,0,1,1,0],[0,0,0,1,1,1,1]],[[1,1,1,1,1,1,1],[1,1,0,0,0,0,0],[1,1,0,0,0,0,0],[1,1,1,1,1,1,0],[0,0,0,0,0,1,1],[0,0,0,0,0,1,1],[0,0,0,0,0,1,1],[0,0,0,0,0,1,1],[1,1,0,0,0,1,1],[0,1,1,1,1,1,0]],[[0,0,0,0,1,1,0],[0,0,1,1,0,0,0],[0,1,1,0,0,0,0],[1,1,0,0,0,0,0],[1,1,0,1,1,1,0],[1,1,0,0,0,1,1],[1,1,0,0,0,1,1],[1,1,0,0,0,1,1],[1,1,0,0,0,1,1],[0,1,1,1,1,1,0]],[[1,1,1,1,1,1,1],[1,1,0,0,0,1,1],[0,0,0,0,1,1,0],[0,0,0,0,1,1,0],[0,0,0,1,1,0,0],[0,0,0,1,1,0,0],[0,0,1,1,0,0,0],[0,0,1,1,0,0,0],[0,0,1,1,0,0,0],[0,0,1,1,0,0,0]],[[0,1,1,1,1,1,0],[1,1,0,0,0,1,1],[1,1,0,0,0,1,1],[1,1,0,0,0,1,1],[0,1,1,1,1,1,0],[1,1,0,0,0,1,1],[1,1,0,0,0,1,1],[1,1,0,0,0,1,1],[1,1,0,0,0,1,1],[0,1,1,1,1,1,0]],[[0,1,1,1,1,1,0],[1,1,0,0,0,1,1],[1,1,0,0,0,1,1],[1,1,0,0,0,1,1],[0,1,1,1,0,1,1],[0,0,0,0,0,1,1],[0,0,0,0,0,1,1],[0,0,0,0,1,1,0],[0,0,0,1,1,0,0],[0,1,1,0,0,0,0]],[[0,0,0,0,0,0,0],[0,0,1,1,1,0,0],[0,0,1,1,1,0,0],[0,0,1,1,1,0,0],[0,0,0,0,0,0,0],[0,0,0,0,0,0,0],[0,0,1,1,1,0,0],[0,0,1,1,1,0,0],[0,0,1,1,1,0,0],[0,0,0,0,0,0,0]]],o=document.getElementById("canvas");if(o.getContext){var h=o.getContext("2d"),f=100,i=700;o.height=f,o.width=i,h.fillStyle="#f00",h.fillRect(10,10,50,50);var c=[],d=[],g=o.height/20-1;!function(){var t=/(\d)(\d):(\d)(\d):(\d)(\d)/.exec(new Date);c.push(t[1],t[2],10,t[3],t[4],10,t[5],t[6])}(),clearInterval(v);var v=setInterval(function(){e(),a(),r()},50)}}()</script></div></nav></div></header><main id="main" class="main"><div class="main-inner"><div class="content-wrap"><div id="content" class="content"><div id="posts" class="posts-expand"><article class="post post-type-normal" itemscope itemtype="http://schema.org/Article"><div class="post-block"><link itemprop="mainEntityOfPage" href="https://gsy00517.github.io/computer-vision20200215214240/"><span hidden itemprop="author" itemscope itemtype="http://schema.org/Person"><meta itemprop="name" content="高深远"><meta itemprop="description" content><meta itemprop="image" content="/images/lufei.jpg"></span><span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization"><meta itemprop="name" content="高深远的博客"></span><header class="post-header"><h2 class="post-title" itemprop="name headline">computer vision笔记：目标跟踪的小总结</h2><div class="post-meta"><span class="post-time"><span class="post-meta-item-icon"><i class="fa fa-calendar-o"></i> </span><span class="post-meta-item-text">发表于</span> <time title="创建于" itemprop="dateCreated datePublished" datetime="2020-02-15T21:42:40+08:00">2020-02-15 </time><span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-calendar-check-o"></i> </span><span class="post-meta-item-text">更新于&#58;</span> <time title="更新于" itemprop="dateModified" datetime="2020-03-05T20:25:37+08:00">2020-03-05 </time></span><span class="post-category"><span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-folder-o"></i> </span><span class="post-meta-item-text">分类于</span> <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/人工智能/" itemprop="url" rel="index"><span itemprop="name">人工智能</span> </a></span></span><span class="post-comments-count"><span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-comment-o"></i> </span><a href="/computer-vision20200215214240/#comments" itemprop="discussionUrl"><span class="post-comments-count valine-comment-count" data-xid="/computer-vision20200215214240/" itemprop="commentCount"></span> </a></span><span id="/computer-vision20200215214240/" class="leancloud_visitors" data-flag-title="computer vision笔记：目标跟踪的小总结"><span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-eye"></i> </span><span class="post-meta-item-text">阅读次数&#58;</span> <span class="leancloud-visitors-count"></span></span><div class="post-wordcount"><span class="post-meta-item-icon"><i class="fa fa-file-word-o"></i> </span><span class="post-meta-item-text">字数统计&#58;</span> <span title="字数统计">12.7k字 </span><span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-clock-o"></i> </span><span class="post-meta-item-text">阅读时长 &asymp;</span> <span title="阅读时长">47分钟</span></div></div></header><div class="post-body" itemprop="articleBody"><p>看了一寒假的目标跟踪，一直想将自己学习到的内容整理归纳一下，迟迟没动笔（其实是打字，但说迟迟没打字比较难听哈哈）。如今快要开学了，决定还是积累一下。<br>本文主要是把目标跟踪中的一些相关要点做一个总结，并且按具体问题对一些主流的或者我阅读过觉得有价值的算法做一个简单概述，方法层出不穷，码字不易，无法涵盖所有的方法，也可能会存在错误，在今后的学习过程中会一直保持更新，这将是一篇LTS的文章哈哈。</p><blockquote><p>注：本文重点关注单目标跟踪。其中各个算法的年份以参加benchmark的年份或者论文发表年份为依据，可能会存在1年的区别，但影响不大。</p></blockquote><p><strong>References</strong>：</p><p>参考文献：<br>[1]统计学习方法（第2版）<br>[2]Understanding and Diagnosing Visual Tracking Systems<br>[3]Survey of Visual Object Tracking Algorithms Based on Deep Learning<br>[4]Handcrafted and Deep Trackers: Recent Visual Object Tracking Approaches and Trends<br>[5]Review of visual object tracking technology<br>[6]A Review of Visual Trackers and Analysis of its Application to Mobile Robot<br>[7]Deep Learning for Visual Tracking: A Comprehensive Survey<br>[8]Object Tracking Benchmark<br>[9]Visual Object Tracking using Adaptive Correlation Filters<br>[10]Adaptive Color Attributes for Real-Time Visual Tracking<br>[11]Discriminative Scale Space Tracking<br>[12]High-Speed Tracking with Kernelized Correlation Filters<br>[13]Learning Multi-Domain Convolutional Neural Networks for Visual Tracking<br>[14]Transferring Rich Feature Hierarchies for Robust Visual Tracking<br>[15]Learning Spatially Regularized Correlation Filters for Visual Tracking<br>[16]Adaptive Decontamination of the Training Set: A Unified Formulation for Discriminative Visual Tracking<br>[17]Beyond Correlation Filters: Learning Continuous Convolution Operators for Visual Tracking<br>[18]Attentional Correlation Filter Network for Adaptive Visual Tracking<br>[19]Action-Decision Networks for Visual Tracking with Deep Reinforcement Learning<br>[20]Superpixel-based Tracking-by-Segmentation using Markov Chains<br>[21]Learning Background-Aware Correlation Filters for Visual Tracking<br>[22]BranchOut: Regularization for Online Ensemble Tracking with Convolutional Neural Networks<br>[23]Context-Aware Correlation Filter Tracking<br>[24]End-to-end representation learning for Correlation Filter based tracking<br>[25]Correlation Filters with Weighted Convolution Responses<br>[26]CREST: Convolutional Residual Learning for Visual Tracking<br>[27]Discriminative Correlation Filter with Channel and Spatial Reliability<br>[28]Learning Dynamic Siamese Network for Visual Object Tracking<br>[29]Learning Policies for Adaptive Tracking with Deep Feature Cascades<br>[30]ECO: Efficient Convolution Operators for Tracking<br>[31]Hierarchical Attentive Recurrent Tracking<br>[32]Integrating Boundary and Center Correlation Filters for Visual Tracking with Aspect Ratio Variation<br>[33]Large Margin Object Tracking with Circulant Feature Maps<br>[34]Multi-task Correlation Particle Filter for Robust Object Tracking<br>[35]The Benefits of Evaluating Tracker Performance using Pixel-wise Segmentations<br>[36]Parallel Tracking and Verifying: A Framework for Real-Time and High Accuracy Visual Tracking<br>[37]Tracking as Online Decision-Making: Learning a Policy from Streaming Videos with Reinforcement Learning<br>[38]Robust Visual Tracking Using Oblique Random Forests<br>[39]Recurrent Filter Learning for Visual Tracking<br>[40]SANet: Structure-Aware Network for Visual Tracking<br>[41]Non-Rigid Object Tracking via Deformable Patches using Shape-Preserved KCF and Level Sets<br>[42]Robust Object Tracking based on Temporal and Spatial Deep Networks<br>[43]UCT: Learning Uniﬁed Convolutional Networks for Real-time Visual Tracking<br>[44]Real-time ‘Actor-Critic’ Tracking<br>[45]Distractor-aware Siamese Networks for Visual Object Tracking<br>[46]Deep Attentive Tracking via Reciprocative Learning<br>[47]Efﬁcient Diverse Ensemble for Discriminative Co-Tracking<br>[48]Detect or Track: Towards Cost-Effective Video Object Detection/Tracking<br>[49]Deep Reinforcement Learning with Iterative Shift for Visual Tracking<br>[50]Correlation Tracking via Joint Discrimination and Reliability Learning<br>[51]Deep Regression Tracking with Shrinkage Loss<br>[52]End-to-end Flow Correlation Tracking with Spatial-temporal Attention<br>[53]Hyperparameter Optimization for Tracking with Continuous Deep Q-Learning<br>[54]Learning Adaptive Discriminative Correlation Filters via Temporal Consistency Preserving Spatial Feature Selection for Robust Visual Object Tracking<br>[55]Learning Spatial-Aware Regressions for Visual Tracking<br>[56]Multi-Cue Correlation Filters for Robust Visual Tracking<br>[57]Learning Dynamic Memory Networks for Object Tracking<br>[58]Meta-Tracker: Fast and Robust Online Adaptation for Visual Object Trackers<br>[59]High-speed Tracking with Multi-kernel Correlation Filters<br>[60]Learning Attentions: Residual Attentional Siamese Network for High Performance Online Visual Tracking<br>[61]Joint Representation and Truncated Inference Learning for Correlation Filter based Tracking<br>[62]Real-Time MDNet<br>[63]Visual Tracking via Spatially Aligned Correlation Filters Network<br>[64]A Twofold Siamese Network for Real-Time Object Tracking<br>[65]Triplet Loss in Siamese Network for Object Tracking<br>[66]High Performance Visual Tracking with Siamese Region Proposal Network<br>[67]SINT++: Robust Visual Tracking via Adversarial Positive Instance Generation<br>[68]Learning Spatial-Temporal Regularized Correlation Filters for Visual Tracking<br>[69]Structured Siamese Network for Real-Time Visual Tracking<br>[70]Context-aware Deep Feature Compression for High-speed Visual Tracking<br>[71]Unveiling the Power of Deep Tracking<br>[72]VITAL: VIsual Tracking via Adversarial Learning<br>[73]A Generative Appearance Model for End-to-end Video Object Segmentation<br>[74]Bridging the Gap Between Detection and Tracking: A Uniﬁed Approach<br>[75]Learning Aberrance Repressed Correlation Filters for Real-Time UAV Tracking<br>[76]Visual Tracking via Adaptive Spatially-Regularized Correlation Filters<br>[77]ATOM: Accurate Tracking by Overlap Maximization<br>[78]Siamese Cascaded Region Proposal Networks for Real-Time Visual Tracking<br>[79]D3S – A Discriminative Single Shot Segmentation Tracker<br>[80]Learning Discriminative Model Prediction for Tracking<br>[81]Graph Convolutional Tracking<br>[82]Joint Group Feature Selection and Discriminative Filter Learning for Robust Visual Object Tracking<br>[83]GradNet: Gradient-Guided Network for Visual Object Tracking<br>[84]Robust Estimation of Similarity Transformation for Visual Object Tracking<br>[85]Deep Meta Learning for Real-Time Target-Aware Visual Tracking<br>[86]Object Tracking by Reconstruction with View-Speciﬁc Discriminative Correlation Filters<br>[87]Physical Adversarial Textures That Fool Visual Object Tracking<br>[88]ROI Pooled Correlation Filters for Visual Tracking<br>[89]SiamR-CNN: Visual Tracking by Re-Detection<br>[90]Deeper and Wider Siamese Networks for Real-Time Visual Tracking<br>[91]Fast Online Object Tracking and Segmentation: A Unifying Approach<br>[92]SiamRPN++: Evolution of Siamese Visual Tracking with Very Deep Networks<br>[93]‘Skimming-Perusal’ Tracking: A Framework for Real-Time and Robust Long-term Tracking<br>[94]SPM-Tracker: Series-Parallel Matching for Real-Time Visual Object Tracking<br>[95]Target-Aware Deep Tracking<br>[96]Unsupervised Deep Tracking<br>[97]Learning the Model Update for Siamese Trackers</p><hr><h1 id="简介与要求"><a href="#简介与要求" class="headerlink" title="简介与要求"></a>简介与要求</h1><p>目标跟踪是利用一个视频或图像序列的上下文信息，对目标的外观和运动信息进行建模，从而对目标运动状态进行预测并标定目标位置的一种技术。一般是在第一帧给出一个框，框中的物体就是我们需要在后续帧中用算法进行跟踪的对象。就目前的单目标跟踪而言，一般有如下要求：<br><strong>monocular</strong>：我们的视频或者图片序列是仅从一个摄像头中获得的，也就是不考虑比如在城市道路场景中跨摄像头对目标跟踪的复杂应用。<br><strong>model-free</strong>：没有任何先验，也就是在获取第一帧的框之前我们并不知道会框出什么物体，也不需要在之前对初始框中的物体进行建模。<br><strong>single-target</strong>：只追踪第一帧框出的那一个物体，也就是除了那个物体之外所有的物体都是back ground。<br><strong>casual/real-time</strong>：目标跟踪是一个在线过程，也就是不能提前获取未来的框对目标进行跟踪。<br><strong>short-term</strong>：没有重检测，也就是目标跟丢了就丢了。<br><strong>long-term</strong>：可以在跟丢之后重检测，这类算法一般除了跟踪之外还需要有检测的功能。<br><img src="/computer-vision20200215214240/目标跟踪.png" title="目标跟踪"><br>下面是目标跟踪流程的伪代码表示（不一定普适，比如有些算法不在线更新，但符合基本的过程）。<br><img src="/computer-vision20200215214240/目标跟踪伪代码.png" title="目标跟踪伪代码"></p><hr><h1 id="问题及挑战"><a href="#问题及挑战" class="headerlink" title="问题及挑战"></a>问题及挑战</h1><p>通俗来讲，目标跟踪的最终目标就是要又快又准。“快”主要表现在计算量小和所需的存储空间小，“准”就是预测出的bounding box要尽可能地接近ground truth。除了上面两个基本需求（也可以说是为了更好地达到这两个基本需求），近年来的算法主要针对目标跟踪中的一些挑战进行突破，从而更好地解决某些问题之后达到更好的整体效果。<br>总的来说，目标跟踪的主要问题有如下这些：遮挡（occlusion）、背景干扰（background clutter）、光照变化（illumination changes）、尺度变化（scale variation）、低分辨率（low resolution）、快速移动（fast motion）、超出画面（out of view）、运动模糊（motion blur）、形变（deformation）、旋转（rotation）等。<br><img src="/computer-vision20200215214240/问题.png" title="问题"><br>OTB数据集依据各种问题对其中的序列进行了一个划分，这对之后针对性的研究提供了重要的参考。<br><img src="/computer-vision20200215214240/OTB对问题的划分.png" title="OTB对问题的划分"></p><hr><h1 id="生成式与判别式"><a href="#生成式与判别式" class="headerlink" title="生成式与判别式"></a>生成式与判别式</h1><p>利用特征判断候选样本是否为跟踪目标，可将目标跟踪的模型分为生成式模型和判别式模型，本小节就介绍一下什么是生成式模型和判别式模型。</p><h2 id="机器学习"><a href="#机器学习" class="headerlink" title="机器学习"></a>机器学习</h2><p>我们首先看看在机器学习中生成式模型和判别式模型定义的一般区分。<br>一般而言，机器学习的任务就是学习一个模型，应用这一个模型，对给定的输入预测相应的输出。输出的一般形式可以是决策函数，也可以是条件概率分布。<br>对于生成式模型，我们需要通过数据学习输入X与输出Y之间的生成关系（比如联合概率分布），也就是认为X和Y都是随机变量。典型的生成式模型有朴素贝叶斯模型、隐马尔可夫模型（HMM）、高斯混合模型（GMM）等。<br>对于判别式模型，我们只需要直接学习决策函数或者条件概率分布，只关心对给定的输入X我们需要输出怎么样的Y，也就是不考虑X是否是随机变量。典型的判别式模型包括k近邻、感知机、决策树、逻辑斯蒂回归模型、最大熵模型、支持向量机（SVM）、提升方法和条件随机场等。此外神经网络也属于判别式模型。<br>相较而言，生成式模型体现了更多的信息，不过这还是因条件而异的，不同情况不同任务两种方法各有优缺点。</p><h2 id="目标跟踪"><a href="#目标跟踪" class="headerlink" title="目标跟踪"></a>目标跟踪</h2><p>在目标跟踪领域，生成式模型通过提取目标特征来构建表观模型，然后在图像中搜索与模型最匹配的区域作为跟踪结果。不论采用全局特征还是局部特征，生成式模型的本质是在目标表示的高维空间中，找到与目标模型最相邻的候选目标作为当前估计。此类方法的缺陷在于只关注目标信息，而忽略了背景信息。<br><img src="/computer-vision20200215214240/生成式模型.png" title="生成式模型"><br>与生成式模型不同的是，判别式模型同时考虑了目标和背景信息。它将跟踪问题看做分类或者回归问题，其目的是寻找一个判别函数，将目标从背景中分离出来，从而实现对目标的跟踪。<br><img src="/computer-vision20200215214240/判别式模型.png" title="判别式模型"><br>一般来说，在目标跟踪领域，判别式充分利用了目标前景和背景信息，能更加有效地区分出目标，比单单运用目标区域特征进行模板匹配的生成式模型在复杂环境中的鲁棒性更强。</p><hr><h1 id="算法导图"><a href="#算法导图" class="headerlink" title="算法导图"></a>算法导图</h1><p>首先是参考文献[6]中的一个树状导图。<br><img src="/computer-vision20200215214240/论文中的导图.png" title="论文中的导图"><br>下图是中科院博士王强（github名为foolwood…呃不得不说这名字取得真谦虚）在<a href="https://github.com/foolwood/benchmark_results" target="_blank">github</a>上总结的历年各大benchmark的优秀成果的一个思维导图，同一个链接下还包括了各项成果的paper及code，值得收藏一下。<br><img src="/computer-vision20200215214240/王强的导图.png" title="王强的导图"><br>下图是浙大硕士王蒙蒙极市平台做分享的时候所用的一张思维导图，归纳得也比较清晰。<br><img src="/computer-vision20200215214240/王蒙蒙的导图.png" title="王蒙蒙的导图"></p><blockquote><p>注：后两张导图中都把历年benchmark的冠军工作作了标注。</p></blockquote><p>对比几张思维导图可以发现，他们都把主流算法分成了相关滤波、深度学习两个分支（或者说是基于handcrafted特征的算法和基于CNN提取特征的算法，其实近年已有所融合），此外还有一些基于强化学习、结构化SVM的模型。其实，目标跟踪算得上是计算机视觉领域中深度学习涉足较晚的一个方向，其主要原因是目标跟踪相关数据集的标注花费较大。此外，相关滤波的速度优势，也就是实时性是十分引人注目的，但在应付当前目标跟踪中的各种挑战、问题时，相关滤波的鲁棒性还是落后于深度学习方法的。<br>在下一节，我将结合上面几张导图，对历年尤其是近几年的算法做一个简单的整理，以方便日后的学习与研究。</p><hr><h1 id="各类算法的梳理与简述"><a href="#各类算法的梳理与简述" class="headerlink" title="各类算法的梳理与简述"></a>各类算法的梳理与简述</h1><blockquote><p>注意：如果你对计算机视觉或者说目标跟踪方面的一些基础方法、概念和经典算法已经有些了解，可以跳过本条建议。<br>考虑到在后文频繁地插入链接不太好，我就在此先推荐一下我博客的几个标签<a href="https://gsy00517.github.io/tags/%E7%9B%AE%E6%A0%87%E8%B7%9F%E8%B8%AA/" target="_blank">目标跟踪</a>、<a href="https://gsy00517.github.io/tags/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/" target="_blank">计算机视觉</a>、<a href="https://gsy00517.github.io/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/" target="_blank">深度学习</a>、<a href="https://gsy00517.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" target="_blank">机器学习</a>以及<a href="https://gsy00517.github.io/tags/%E7%BA%BF%E6%80%A7%E4%BB%A3%E6%95%B0/" target="_blank">线性代数</a>，其中的文章包含了一部分接下来要提到的概念和算法，可以事先浏览一下。当你在阅读时对相关概念、方法感到迷惑或者想进一步了解，博客内置的搜索功能或许能够为你提供帮助。</p></blockquote><h2 id="LK-Tracker（1981）"><a href="#LK-Tracker（1981）" class="headerlink" title="LK Tracker（1981）"></a>LK Tracker（1981）</h2><p>LK Tracker应该是最早的目标跟踪工作，它使用了光流的概念，如下图所示，不同颜色表示光流不同的方向，颜色的深浅表示运动的速度。<br><img src="/computer-vision20200215214240/光流.gif" title="光流"><br>LK Tracker假定目标灰度在短时间内保持不变，同时目标邻域内的速度向量场变化缓慢。由于光流方程包含坐标x，y和时间t共三个未知数，其中时间变化dt已知而坐标变化dx和dy未知，一个方程两个未知数无法求解，因此作者假定相邻的点它们的光流是一致的，这样就可以求解方程组了。下图是求解之后的光流向量，其中绿色箭头的方向表示运动方向，线段长度表示运动速度的大小。<br><img src="/computer-vision20200215214240/画出光流.png" title="画出光流"><br>光流的计算非常简单也非常快，而且由于提出得很早，各种库都有实现好的轮子可以轻松调用，但是它的鲁棒性不好，基本上只能对平移且外观不变的物体进行跟踪。</p><h2 id="KLT（1994）"><a href="#KLT（1994）" class="headerlink" title="KLT（1994）"></a>KLT（1994）</h2><p>KLT是一种生成式方法，也是使用了光流特征。在此基础上，作者使用了匹配角点的方法，也就是寻找边角处、纹理处等易辨识的地方计算光流来进行追踪。</p><h2 id="Mean-Shift（2002）"><a href="#Mean-Shift（2002）" class="headerlink" title="Mean Shift（2002）"></a>Mean Shift（2002）</h2><p>Mean Shift采用均值漂移作为搜索策略，这是一种无参概率估计方法，该方法通过迭代沿着概率密度函数的梯度方向，搜索函数局部最大值。在当时成为了常用的视觉跟踪系统的目标搜索方法，简单易实现，但鲁棒性较低。</p><h2 id="IVT（2008）"><a href="#IVT（2008）" class="headerlink" title="IVT（2008）"></a>IVT（2008）</h2><p>IVT在线更新特征空间的基，直接将以前检测到的目标作为样本在线学习而无需大量的标注样本。</p><h2 id="MOSSE（2010）"><a href="#MOSSE（2010）" class="headerlink" title="MOSSE（2010）"></a>MOSSE（2010）</h2><p>MOSSE（Minimum Output Sum of Squared Error）使用相关滤波来做目标跟踪（不是第一个，但可以看作前期的一个代表），其速度能够达到600多帧每秒，但是效果一般，这主要是因为它只使用了简单的raw pixel特征。<br>相比之前的算法，MOSSE能够形成更加明确的峰值，减少了漂移；此外，MOSSE可以在线更新，同时还采用了PSR来检测遮挡或者跟丢的情况，从而决定是否需要停止更新。<br>值得一提的是，MOSSE在做相关操作之前，对每张图都进行了减去平均值的处理，这有利于淡化背景对相关操作的影响。另外假如发生光照变化的话，减去均值也有利于减小这种变化的影响。</p><h2 id="TLD（2010）"><a href="#TLD（2010）" class="headerlink" title="TLD（2010）"></a>TLD（2010）</h2><p>TLD（Tracking Learning Detection）主要针对long-term tracking，在跟踪的同时全局检测。它由三部分组成：跟踪模块、检测模块、学习模块。<br>跟踪模块观察帧与帧之间的目标的动向。作者采用了光流来跟踪，此外还提出了一种判断跟踪失效的算法，由于光流跟踪时选取的若干特征点，当其中某一个特征点的位移与所有特征点位移的中值之差过大时，也就是某个特征点离跟踪模块认为的目标中心位置很远时，就认为跟踪失效。作者还通过相似度和错误匹配度来对特征点进行筛选。<br>检测模块把每张图看成独立的，然后对单张图片进行目标检测定位。作者使用了方差检测器、随机森林和最近邻分类器来对目标做检测。<br>学习模块对根据跟踪模块的结果对检测模块的错误进行评估，当置信度较低时，重新组织正负样本对随机深林的后验概率和最近邻分类器的在线模板进行更新，从而避免以后出现类似错误。<br>TLD与传统跟踪算法的显著区别在于将传统的跟踪算法和传统的检测算法相结合来解决被跟踪目标在被跟踪过程中发生的形变、部分遮挡等问题。同时，通过一种改进的在线学习机制不断更新跟踪模块的“显著特征点”和检测模块的目标模型及相关参数，从而使得跟踪能够自适应，效果较之前更加稳定、可靠。</p><h2 id="L1-Tracker（2011）"><a href="#L1-Tracker（2011）" class="headerlink" title="L1 Tracker（2011）"></a>L1 Tracker（2011）</h2><p>L1 Tracker是第一个将稀疏编码引入目标跟踪问题中的算法。它把跟踪看做一个稀疏近似问题，主要是用第一帧和最近几帧得到的图像（特征）作为字典，通过求解L1范数最小化问题，实现对目标的跟踪。</p><h2 id="Struck（2011）"><a href="#Struck（2011）" class="headerlink" title="Struck（2011）"></a>Struck（2011）</h2><p>Struck的主要贡献是引入了结构化SVM。考虑到传统的跟踪算法将跟踪问题转化为一个分类问题，并通过在线学习技术更新目标模型。然而，为了达到更新的目的，通常需要将一些预估计的目标位置作为已知类别的训练样本，这些分类样本并不一定与实际目标一致，因此难以实现最佳的分类效果。<br>结合上述考虑，Struck利用了结构化SVM直接输出跟踪结果，避免了中间分类环节，这使得在当时效果有明显的提升。同时，为了保证实时性，Struck还引入了阈值机制，防止跟踪过程中支持向量的过增长。</p><h2 id="CSK（2012）"><a href="#CSK（2012）" class="headerlink" title="CSK（2012）"></a>CSK（2012）</h2><p>CSK也称为核相关滤波算法，它针对MOSSE做出了一些改进，采用循环移位进行密集采样，并通过核函数将低维线性空间映射到高维空间，提高了相关滤波器的鲁棒性。随后的工作主要从特征选择、尺度估计、正则化等方面对该算法进行改进和提高。</p><h2 id="DLT（2013）"><a href="#DLT（2013）" class="headerlink" title="DLT（2013）"></a>DLT（2013）</h2><p>DLT是最早的基于深度学习的算法（当时AlexNet刚刚被提出），它采用了堆叠去噪自编码器网络，把跟踪视为一个分类问题，直接利用80 Million Tiny Images数据集上的预训练模型提取深度特征，这种强行task转换的训练方法存在缺陷，但在当时是个进步。</p><h2 id="KCF（2014）"><a href="#KCF（2014）" class="headerlink" title="KCF（2014）"></a>KCF（2014）</h2><p>KCF跟CSK是同一个团队提出的，它跟CSK的区别是就是作者对循环性质进行了完整的理论推导，引入HOG特征并提供了一种把多通道特征融合进相关滤波框架的方法，对CSK作了进一步的完善，是一个具有里程碑意义的工作。算法的详解和一些数学理论可以看看我之前的文章。</p><h2 id="DCF（2014）"><a href="#DCF（2014）" class="headerlink" title="DCF（2014）"></a>DCF（2014）</h2><p>DCF与KCF出自同一篇paper，不同的是KCF使用的是高斯核，DCF使用的是线性核。注意这里的DCF（Dual Correlation Filter）和之后一些文章中提到的DCF（Discriminative Correlation Filter）是两个不同的概念，请注意，别搞错了。</p><h2 id="CN（2014）"><a href="#CN（2014）" class="headerlink" title="CN（2014）"></a>CN（2014）</h2><p>CN（Color Name）考虑到在遇到光照变化、形变、部分遮挡、背景干扰等问题时，颜色特征相比灰度特征能提供更丰富的信息以取得更好的效果，引入了颜色特征来扩展CSK，它将目标RGB（红绿蓝）颜色特征映射为黑、蓝、棕、灰、绿、橙、粉、紫、红、白和黄11种颜色特征的多通道颜色特征，后又降维至2维以保证实时性。Color Name较RGB三原色特征对目标的表征能力更强，而且具有一定的光学不变性。</p><h2 id="DSST（2014）"><a href="#DSST（2014）" class="headerlink" title="DSST（2014）"></a>DSST（2014）</h2><p>DSST主要考虑了尺度缩放的问题。它将目标跟踪看成位置变化和尺度变化两个独立问题，提出了一个高、宽、尺度数三维的滤波器，先计算平移位置再聚集尺度，即训练了两个滤波器，首先训练位置平移相关滤波器以检测目标中心平移，然后训练尺度相关滤波器来检测目标的尺度变化。</p><h2 id="SO-DLT（2015）"><a href="#SO-DLT（2015）" class="headerlink" title="SO-DLT（2015）"></a>SO-DLT（2015）</h2><p>SO-DLT针对DLT的缺陷进行改进，使得CNN更加适用于目标跟踪。由于目标跟踪的目的是将物体从背景中分离出来而不是全图识别，DLT的训练方法和标签化输出就不是很合适了。<br>但是，由于当时跟踪方向标注数据的匮乏，作者还是不得不使用ImageNet图像检测数据集来进行预训练，事实证明这是有效的，因为目标检测和目标跟踪两个不同的task中存在一样的共性信息。不同于标签化的单个数值输出，SO-DLT输出的是一个50x50的像素级的概率图。<br>由于上述训练方法训练的是CNN从非物体中提取出物体的能力，因此在实际跟踪接收到第一帧时，还需根据目标对网络进行微调，否则会跟踪出视频或者图像序列中所有的无论是目标与否的物体。<br>类似DSST，SO-DLT采用的是先预测目标中心位置，然后再从小到大确定尺度的策略，如若扩展到预设的最大尺度来检测的概率图依旧达不到要求，则认为已经跟丢目标。<br>此外，为了提升鲁棒性，作者采用了两个CNN网络共同决策而以不同方式更新的策略。两个网络分别针对的是short-term和long-term。针对short-term的网络在负样本的概率图响应和超过一定阈值时进行更新，为的是防止负样本与目标响应近似而导致漂移；针对long-term的网络在当前帧预测结果的置信度达到一定水平以上时才进行更新，因为此时可认为框出的目标较为可信。<br>每次更新时需要采集正负样本，SO-DLT对正负样本的提取方法比较简单，在目标位置及周围形成一个类似九宫格的区域，在中间格用四种尺度提取正样本，对周围的8格采集负样本。</p><h2 id="MDNet（2015）"><a href="#MDNet（2015）" class="headerlink" title="MDNet（2015）"></a>MDNet（2015）</h2><p>MDNet设计了一个轻量级的小型网络学习卷积特征表示目标。作者提出了一个多域的网络框架，将一个视频序列视为一个域，其中共享的部分用来学习目标的特征表达，独立的全连接层则用于学习针对特定视频序列的softmax分类。<br>在离线训练时，针对每个视频序列构建一个新的检测分支进行训练，而特征提取网络是共享的。这样特征提取网络可以学习到通用性更强的与域无关的特征。<br>在跟踪时，保留并固定特征提取网络，针对跟踪序列构建一个新的分支检测部分，用第1帧样本在线训练检测部分之后再利用跟踪结果生成正负样本来微调检测分支。<br>此外，MDNet在训练时还采用了难例挖掘技术，随着训练的进行增大样本的分类难度。<br><img src="/computer-vision20200215214240/难例挖掘.png" title="难例挖掘"></p><h2 id="SRDCF（2015）"><a href="#SRDCF（2015）" class="headerlink" title="SRDCF（2015）"></a>SRDCF（2015）</h2><p>SRDCF主要考虑到若仅使用单纯的相关滤波，可能会存在边界效应，也就是相关滤波采用循环移位采样导致除了中心样本以外的其他样本中都会存在边界，这就导致了大部分样本都不合理，同时也会限制检测区域。<br>于是，作者采用了大的检测区域，在滤波器系数上加入权重约束（类似于惩罚项）：越靠近边缘权重越大，越靠近中心权重越小。这就使得滤波器系数主要集中在中心区域，从而让边界的影响没有那么明显。SRDCF最终的效果不错，但是速度比较缓慢。</p><h2 id="DeepSRDCF（2015）"><a href="#DeepSRDCF（2015）" class="headerlink" title="DeepSRDCF（2015）"></a>DeepSRDCF（2015）</h2><p>DeepSRDCF在SRDCF的基础上，将handcrafted的特征换为CNN的特征，关注点也在解决边界效应。作者还对不同的特征进行了实验，说明了CNN特征在解决跟踪的问题采取底层的特征效果会比较好（文中仅用了PCA降维处理的第一层），说明了跟踪问题并不需要太高的语义信息。</p><h2 id="HCF（2015）"><a href="#HCF（2015）" class="headerlink" title="HCF（2015）"></a>HCF（2015）</h2><p>HCF的主要贡献是把相关滤波中的HOG特征换成了深度特征，它使用的是VGG的3、4、5三个层来提取特征，针对每层CNN训练一个过滤器，并且按照从深到浅的顺序使用相关滤波，然后利用深层得到的结果来引导浅层从而减少搜索空间。</p><h2 id="FCNT（2015）"><a href="#FCNT（2015）" class="headerlink" title="FCNT（2015）"></a>FCNT（2015）</h2><p>FCNT较早地利用CNN网络底层和顶层不同的表达效果来做跟踪。不同于以往的工作把CNN看成一个黑盒而不关注不同层的表现，FCNT关注了不同层的功能，即发现：顶层的CNN layer编码了更多的关于语义特征的信息并且可以作为类别检测器；而底层的CNN layer关注了更多局部特征，这有助于将目标从目标中分离出来。这个发现在之后的许多工作中也得到了应用和体现。如下图所示，这里的a图表示的是ground truth，b图表示的是使用VGG的conv4-3，也就是第10层产生的热力图，c图是通过conv5-3也就是第13层产生的热力图。<br><img src="/computer-vision20200215214240/热力图.jpg" title="热力图"><br>可以看到，较低维的CNN layer（conv4-3）能够更精准地表示目标的细粒度信息，而较高维的CNN layer（conv5-3）热力图显示较模糊，但对同类别的人也做出了响应。这就是说，顶层缺少类内特征区分，对类间识别比较好，更适合作语义分割；底层则反之，能够更好地表达目标的类内特征和位置信息。<br>基于不同层（顶层和底层）之间提取特征的不同，作者提出了一种新的tracking方法，利用两种特征相互补充辅助，来处理剧烈的外观变化（顶层特征发挥的作用）和区分目标本身（底层特征发挥的作用）。由于feature map本身是有内在结构的，有很多的feature map对目标的表达其实并没有起到作用，因此作者设计了一种方法来自动选择高维CNN（GNet）或者低维CNN（SNet）上的feature map，同时忽略另一个feature map和噪声。在线跟踪时，两个网络一起跟踪，采用不同的更新策略，并在不同的情况下选择不同的网络输出来进行预测。<br>顺便提一下，为了简化学习任务，降低模型复杂度，作者采用了稀疏表示的方法。<br>关于FCNT的一些相关概念和具体按步骤的细节实现，可以参考一下我之前写的文章。</p><h2 id="LCT（2015）"><a href="#LCT（2015）" class="headerlink" title="LCT（2015）"></a>LCT（2015）</h2><p>LCT主要针对的是long-term tracking的问题。作者配置了一个detector，用于跟丢之后快速重检测。LCT用了两个滤波器，一个是用于平移估算的$R_{c}$，使用padding并施加汉宁窗（一种余弦窗），结合了FHOG和一些其他的特征；另一个是用于尺度估计的$R_{t}$，不使用padding和汉宁窗，使用HOG特征，此外$R_{t}$还用于检测置信度，用来决定是否更新模型和是否重检测。</p><h2 id="DLSSVM（2016）"><a href="#DLSSVM（2016）" class="headerlink" title="DLSSVM（2016）"></a>DLSSVM（2016）</h2><p>DLSSVM延续之前的Struck，利用结构化SVM，在优化的阶段做了一些改进进行提速。其实结构化SVM分类器非常强大，但是因为它求解优化的过程比较复杂以及使用稠密采样（粒子滤波或者滑窗采样）比较耗时，使得结构化SVM的速度成为一个瓶颈，因此不如一些使用相关滤波的SOTA的算法。</p><h2 id="C-COT（2016）"><a href="#C-COT（2016）" class="headerlink" title="C-COT（2016）"></a>C-COT（2016）</h2><p>C-COT（连续空间域卷积操作）考虑到单一分辨率的输出结果存在扰动，仅使用单一分辨率的特征图是限制之前DCF系列算法效果的重要因素，因此作者将浅层表观信息和深层语义信息结合起来，利用不同空间分辨率（包含0.96，0.98，1.00，1.02，1.04五种缩放倍率）的响应，在频域进行插值得到连续空间分辨率的响应图，最后通过迭代求解最佳位置和尺度。<br>文中提出了一种使得各个分辨率通道的特征自然融合至相同分辨率的方法，这里相同分辨率可理解为最后的各个响应图在空间上拥有相同的样本点数。作者首先用一个插值运算符对各个不同分辨率的通道进行插值，然后使用对应的滤波器在连续的空间域内卷积，最后将响应求和得到最终的置信度方程。<br>C-COT使用的是类似SRDCF的框架，也引入了空间正则项，当远离目标中心是施加较大的惩罚，这使得能够通过控制滤波器的大小来学习任意大小的图像区域。C-COT在每一帧也会采集一个训练样本，根据过去帧数的远近来设置每个采集样本的重要性权重（每次都做归一化），并且设置了最大的样本容量，当超出容量时删去重要性权值最小的样本。不同于SRDCF使用Gauss-Seidel迭代法，C-COT使用Conjugate Gradient方法来提高效率。<br>得益于浅层特征的高分辨率，C-COT能够达到sub-pixel的精度，也就是仅次于像素级别的精确度。位置细化的过程就是上面所说的用共轭梯度法迭代的过程，在C-COT的代码中有一个迭代次数设置，被设置为1，即就使用一步迭代优化后的位置。换句话说，在当前长时跟踪算法本身误差之下，更精细的位置意义不大。</p><h2 id="LMCF（2017）"><a href="#LMCF（2017）" class="headerlink" title="LMCF（2017）"></a>LMCF（2017）</h2><p>LMCF借鉴了KCF的循环特征图、Struck的结构化SVM，使用相关滤波来解决之前结构化SVM系列算法（Struck、DLSSVM）的速度问题。<br>在前向追踪时，LMCF考虑到画面中相似物体的干扰，提出了一种多峰值的目标跟踪算法（Multimodal Target Tracking），即对高于某一阈值的响应峰值做二次检测，把response map和一个用于筛选的二值矩阵作点乘，相当于把不是峰值的位置滤为0。<br>在模型更新时，LMCF提出了一种高置信度的更新策略（High-confidence Update），由于LMCF主要关注的是实时性，所以希望在算法简单的情况下能够减少失误。在传统的方法中，一般是当最大响应的峰值高于某一个阈值时（认为没跟丢目标），就对模型进行更新；否则若没有响应值超过峰值，就不对模型进行更新。而该工作的实验发现，当目标被遮挡时，响应图会震荡得非常厉害（存在多个较大的峰值），但同时最大响应的峰值仍旧会很高，这就会指导模型进行错误的更新并导致最后跟丢目标。于是作者提出了一个APCE值，定义如下。</p><script type="math/tex;mode=display">APCE=\frac{\left | F_{max}-F_{min} \right |^{2}}{mean(\underset{w,h}{\sum }(F_{w,h}-F_{min})^{2})}</script><p>只有当最大响应的峰值比较明确，即远超response map中的其他的响应时，APCE值才会比较大，此时允许对模型进行更新。</p><h2 id="DeepLMCF（2017）"><a href="#DeepLMCF（2017）" class="headerlink" title="DeepLMCF（2017）"></a>DeepLMCF（2017）</h2><p>同LMCF，不同之处是使用了CNN特征。</p><h2 id="STRCF（2018）"><a href="#STRCF（2018）" class="headerlink" title="STRCF（2018）"></a>STRCF（2018）</h2><p>STRCF（时空正则相关滤波器）主要针对SRDCF的速度做出改进，同时在精度上也有很好的提高。作者发现SRDCF速度很慢的两个原因是：每次对多张图片进行训练打破了循环矩阵的结构，从而无法发挥循环矩阵的计算优势；巨大的线性方程组和Gauss-Seidel迭代法没有闭式解，效率较低。对此，STRCF提出了引入时间正则和ADMM算法。<br>受online Passive-Aggressive learning的启发，STRCF在SRDCF空间正则的基础上引入了时间正则。我们可以对比两者的回归求解公式具体来看一下。<br>SRDCF：</p><script type="math/tex;mode=display">arg\underset{f}{min}\sum_{k=1}^{T}a_{k}\left \| \sum_{d=1}^{D}x_{k}^{d}\ast f^{d}-y_{k} \right \|^{2}+\sum_{d=1}^{D}\left \| w\cdot f^{d} \right \|^{2}</script><p>STRCF：</p><script type="math/tex;mode=display">arg\underset{f}{min}\frac{1}{2}\left \| \sum_{d=1}^{D}x_{t}^{d}\ast f^{d}-y \right \|^{2}+\frac{1}{2}\sum_{d=1}^{D}\left \| w\cdot f^{d} \right \|^{2}+\frac{\mu }{2}\left \| f-f_{t-1} \right \|^{2}</script><p>这里的$w$表示空间正则化矩阵，越靠近边缘值越大；$f$表示相关滤波器，$f_{t-1}$表示的是$t-1$帧时的滤波器。<br>忽略每项之前的常熟系数，我们可以看到两式的第二项是一样的，也就是STRCF保留了SRDCF的空间正则来抑制边界效应；在第一项中，STRCF没有对过去的每一帧进行求和来训练，这就减小了计算量；同时STRCF加入了第三项时间正则，使得新得到的滤波器与之前的滤波器之间的变化尽可能小，相当于保留了之前的信息。<br>这么做有两点好处：首先，STRCF可以看作SRDCF的一个合理近似，能很好地发挥后者同样的作用；此外，由于时间正则的引入，使得STRCF不易于在当前帧上过拟合，在遇到遮挡或者超出画面等问题时，STRCF能很好地保持与之前滤波器的相似度从而降低了跟踪器完全跟丢到另一个物体上去的可能，这一定程度上提高了STRCF的精度。<br><img src="/computer-vision20200215214240/STRCF和SRDCF对比.png" title="STRCF和SRDCF对比"><br>此外，ADMM算法的引入使得最优化求解问题有了闭式解，这比Gauss-Seidel迭代法用稀疏矩阵求解要快得多。得益于SRDCF的凸性，ADMM也能收敛到全局最优点。</p><h2 id="UPDT（2018）"><a href="#UPDT（2018）" class="headerlink" title="UPDT（2018）"></a>UPDT（2018）</h2><p>UPDT区别对待深度特征和浅层特征，主要考虑的是缺少数据和深层卷积在增加语义的同时降低分辨率这两个问题。作者分析了数据增强（flip，rotation，shift，blur，dropout）和鲁棒性训练（也就是tracker应对各种复杂场景和恢复的能力，可以通过扩大正样本的采样范围来训练）对deep feature和shallow feature分别的影响，发现deep feature能通过数据增强来提升效果，同时deep feature主打的是鲁棒性而不是精度；相反，shallow feature经数据增强后反而降低了效果，但同时它能够很好地保证精度。因此，作者得出了深度模型和浅层模型应该独立训练，最后再融合的方案。<br>作者在文中还定义了Prediction Quality Measure，考虑了精度和鲁棒性，精度用响应分数的锋利程度（sharpness）来体现，鲁棒性则用响应值的幅度来表示，幅度越高表明tracker越确信跟踪的目标，也就是鲁棒性越高。关于具体公式的推导和分析，以及Prediction Quality Measure在预测过程中的具体使用可以看一看原文。</p><h2 id="ACT（2018）"><a href="#ACT（2018）" class="headerlink" title="ACT（2018）"></a>ACT（2018）</h2><p>ACT使用了强化学习，构建了由Actor和Critic组成的学习框架。离线训练时，通过Critic指导Actor进行强化学习；在线跟踪时，使用Actor来定位，Critic进行验证使得tracker更加鲁棒。不同于之前的搜索方案（随机采样或者通过一系列分离的action来定位），ACT希望的是搜索一步到位。这步最优的action也就是离线强化学习所关注的行动，而强化学习的状态由输入到网络中bounding box中框出的图片定义，奖励值根据IoU来定义。<br>在训练的过程中，由于action space比较大，因此要获得一个正奖励比较困难（随机采取action的话IoU恰好高于阈值的可能性较小）。因此作者利用了第一帧的信息来初始化Actor以适应新的环境。同样的，由于巨大的action space，原本DDPG方法中的噪声引入就不适合跟踪任务了，因此在训练前期，Actor采取的行动以某种概率被一种专家决策所替代。随着训练的进行，Actor越来越强大，这时就逐渐减弱专家决策的指导作用。<br>在跟踪的过程中，若Critic的给分大于0，则采用Actor的输出一步到位地预测下一帧的目标；否则，使用Critic在上一帧周围采集的样本中选出最优作为目标，完成重定向。此外，可以认为Actor在离线训练时已经比较稳定了，因此在跟踪过程中只对Critic进行更新，且仅在Critic给分小于0（认为Critic没能很好地适应目标的变化）时，取前十帧的样本来更新Critic。</p><h2 id="DRT（2018）"><a href="#DRT（2018）" class="headerlink" title="DRT（2018）"></a>DRT（2018）</h2><p>DRT引入了稳定性的概念，考虑到空间正则、掩膜等抑制边界效应的方法都不能抑制bounding box内部的背景信息，同时这些方法会导致feature map上的响应集中在某些较小的区域，作者认为这是不利于目标跟踪的（容易被误导）。为此，作者提出了DRT，它主要是将滤波器分成了一个base filter和一个reliability term的element-wise product：</p><script type="math/tex;mode=display">w_{d}=h_{d}\odot v_{d}</script><p>这里base filter是用于区分目标和背景的；reliability term用于决定每片区域的reliability，由目标区域每一个patch加权求和决定：</p><script type="math/tex;mode=display">v_{d}=\sum_{m=1}^{M}\beta _{m}P_{d}^{m}</script><p>这里的$\beta_{m}$有上下界的限定，目的就是为了降低feature map中响应不平衡的影响，防止由于响应的集中而导致仅有一小块区域被关注。</p><h2 id="MCCT（2018）"><a href="#MCCT（2018）" class="headerlink" title="MCCT（2018）"></a>MCCT（2018）</h2><p>MCCT使用了多特征集成学习，在跟踪时对每一帧分别选用最合适的特征来做出决策。为了应对不同的场景，MCCT选择了low，middle，high三个层级的特征，并通过排列组合得出7种expert。尽管有些特征的鲁棒性明显差于三类特征的组合，但是它们提供的多样性对集成学习是至关重要的。<br><img src="/computer-vision20200215214240/MCCT特征.png" title="MCCT特征"><br>为了评估每个expert在每一帧的好坏以决定具体选用哪一个，作者提出了Expert Pair-Evaluation和Expert Self-Evaluation。<br>Expert Pair-Evaluation分为两项：在第一项中，作者认为一个expert的好坏可以通过它与其他expert的整体一致性来体现，于是首先计算了每个expert相对于其他6个expert在当前帧预测结果的一致性（通过重叠率来衡量）之和；此外，作者认为一个好的expert还必须是temporal stable的，因此他又计算了每个expert相对于其他6个expert在前几帧内预测趋势的一致性，这就可以防止因为在当前帧碰巧预测一致而导致之前一项的分值很好的情况，也保证了expert的可信度。最后两项结合得到Expert Pair-Evaluation。<br>在Expert Self-Evaluation中，作者认为路径的顺滑程度一定程度上能够体现每个expert的可靠程度。<br>最后将Expert Pair-Evaluation和Expert Self-Evaluation加权求和选出每帧最好的expert做出决策。<br>MCCT提出了一种peak-to-sidelobe ratio和鲁棒性的置信度分数来进行模型更新：</p><script type="math/tex;mode=display">S^{t}=P_{mean}^{t}\cdot R_{mean}^{t}</script><p>其中，$P_{mean}^{t}$是每个expert响应图peak-to-sidelobe ratio的平均，$R_{mean}^{t}$亦然。当$R_{mean}^{t}$比较低时，认为采集到了不可靠的样本（比如遮挡问题等）。为此，作者的模型更新策略是，当置信度分数$S^{t}$大于之前置信度均值时，采用正常学习率，否则，根据置信度算出一个较小的学习率以在一定程度上维持模型。<br>为了提升速度，每个expert之间共享了样本和RoI，最后MCCT的速度为7.8FPS，MCCT-H（没采用深度特征）的速度为44.8FPS。（作为参考，ECO的速度为15FPS）<br>顺便提一下，MCCT使用了汉宁窗来抑制边界效应。</p><h2 id="LSART（2018）"><a href="#LSART（2018）" class="headerlink" title="LSART（2018）"></a>LSART（2018）</h2><p>LSART分析了深度特征中的空间信息，提出了两种互补的回归方式来使得跟踪更加鲁棒。<br>作者首先对比了CNN-based和KRR-based（核岭回归）两类tracker，认为它们各有利弊且是互补的。由于KRR的循环采样，目标的结构特征会被打破，对形变和而CNN则能够很好地提取位置信息；相反，CNN庞大的参数量使得它容易过拟合，而KRR-based tracker就不会出现这样的问题。因此，若将两者结合（将热力图加权求和），就可以让KRR关注全局而让CNN关注较小、较精确的目标，进而达到更好的效果。<br>对于KRR，作者引入cross-patch similarity，将参数看作训练样本的加权求和，将响应项拆分成三项，这就方便把原本的迭代求解的方式设计成神经网络来求解了。<br>对于CNN，考虑到形变和遮挡等问题会使得目标的一部分比其他区域更加重要，不同于以往在feature map上做文章，作者对卷积层的滤波器施加掩膜，使得各个滤波器关注于不同的区域，在跟踪的过程中，这些掩膜不做变化。此外，作者还提出了距离变换池化层用于评判输入feature map的可靠性。另外，作者设计了一种two-stream的训练网络，将空间正则的卷积层和距离变换池化层分开训练以防止过拟合，能够比较好的处理旋转问题。</p><h2 id="DaSiamRPN（2018）"><a href="#DaSiamRPN（2018）" class="headerlink" title="DaSiamRPN（2018）"></a>DaSiamRPN（2018）</h2><p>DaSiamRPN在之前的孪生网络系列的基础上增加了distractor-aware，这里的distractor指的是在判别式方法中，不同于无语义信息易判别的背景，而存在一定的语义并对前景分割存在干扰的背景。这其中的一大原因是之前的训练集仅从同一个视频序列的不同帧中采样，造成了non-semantic的背景样本具有较大的比重而semantic的背景样本较少，这就弱化了模型准确判别前景的能力。此外，之前的孪生网络系列还存在不能在线更新和不进行全局搜索这两个问题。<br>首先，作者提出了三类样本选取方法来弥补传统采样的不足。考虑到视频数据集中类别缺乏和标注的难度，作者引入了ImageNet和COCO图像检测两个数据集，并把样本分成三类对tracker进行训练。<br><img src="/computer-vision20200215214240/三种样本选取方法.png" title="三种样本选取方法"><br>对于正样本对，其作用是提升tracker的泛化能力和回归精度；对于来自同一类别的样本对，其作用是让tracker更注重细粒度的表达方式，提升判别能力；对于来自不同类别的样本对，其作用是让tracker在遮挡、超出视野等情况下拥有更好的鲁棒性。<br>值得一提，作者发现motion pattern能很好地被浅层网络建模，因此在数据增强时还引入了运动模糊。<br>DaSiamRPN通过上述方法对数据做了增强，可是在跟踪特定目标时，还是很难将一般模型转化为特定视频域所用。考虑到上下文信息和时域信息可以提供特定目标的信息以增加tracker的判别能力，作者提出了一个distractor-aware module。具体来说，在上一帧中选择出的proposal中，通过非极大抑制处理，剩下的proposal中最大的就是目标，剩下的就是会产生误导的distractor；在当前帧，为了抑制这些distractor的干扰，可以减去这些distractor之前响应的加权和，减去之后还是最大的proposal就是我们要找的目标，其基本思想如下公式所示：</p><script type="math/tex;mode=display">q=\underset{p_{k}\in P}{argmax}f\left ( z,p_{k} \right )-\frac{\widehat{\alpha }\sum_{i=1}^{n}\alpha _{i}f\left ( d_{i},p_{k} \right )}{\sum_{i=1}^{n}a_{i}}</script><p>这里的$\alpha$是控制distractor影响大小的权重系数，作者又对上式进行调整，通过引入学习率使得该分类器在线可学习，这就无需利用反向传播更新网络参数，而通过微调一个分类器弥补了传统基于孪生网络的tracker不能在线更新的缺点。<br>此外，当认为目标跟丢时，DaSiamRPN会匀速扩大搜索范围，并且通过高效的bounding box回归来代替图像金字塔，这就通过一个简单的方法在应对长时跟踪目标消失问题时较之前基于孪生网络的tracker取得了一个进步。</p><h2 id="Meta-Tracker（2018）"><a href="#Meta-Tracker（2018）" class="headerlink" title="Meta-Tracker（2018）"></a>Meta-Tracker（2018）</h2><p>Meta-Tracker将元学习运用在了目标模型的初始化上。作者认为结合深度特征和在线学习的模型有两大困难，一是训练的样本不容易获得，二是大多数SOTA的tracker在训练阶段都需要花费大量的时间在初始化上面。<br>针对上面的难题，作者提出了一种在未来的帧上训练目标模型的思路，采用了基于预测梯度的策略学习方法获得普适性的初始化模型，使得跟踪模型自适应于后续帧特征的最佳梯度方向，从而在接收到第一帧时仅需一步迭代就能使参数快速收敛到合适的位置。这样做有三点好处，一是能使模型更加关注对后续的帧更有价值的特征，二是避免了在当前帧上过拟合，三是能够使初始化更快速。总而言之，就是能保证精度和鲁棒性。<br>考虑到上述方法在长序列或者目标在帧与帧之间变化不大时表现不佳（会偏离目标），这是因为Meta-Tracker一步到位的思想使得学习率会偏大。因此作者仅在模型初始化时采用学习到的学习率，在之后的跟踪过程中仍旧沿用原来版本的方式进行更新。这里原来的版本指的是CREST和MDNet，作者在这两个tracker的基础上改进出了MetaCREST和MetaSDNet。具体的改进和处理可以看一看我之前写过关于Meta-Tracker的文章。</p><h2 id="DorT（2018）"><a href="#DorT（2018）" class="headerlink" title="DorT（2018）"></a>DorT（2018）</h2><p>DorT（Detect or Track）把跟踪看作一个连续决策的过程，它结合目标检测和目标跟踪两个领域内SOTA的结果，在孪生网络输出的结果上再添加一个小型的CNN网络作为scheduler来判断在下一帧是作检测还是作跟踪。</p><hr><h1 id="研究趋势"><a href="#研究趋势" class="headerlink" title="研究趋势"></a>研究趋势</h1><p>以下是我对近几年来目标跟踪领域各种算法主流的研究趋势和发展方向的一个浅析，个人思考，多多指教。</p><h2 id="深度特征（2013-）"><a href="#深度特征（2013-）" class="headerlink" title="深度特征（2013-）"></a>深度特征（2013-）</h2><p>早期的目标跟踪算法主要在handcrafted特征方面进行探索和改进，以2012年AlexNet问世为节点，深度特征开始被引入目标跟踪领域。<br>我们知道，在现实场景中，物体是在三维的运动场中移动的。而视频或图像序列都是二维的信息，这其实是一些难题的根本原因之一。一个比较极端的例子就是理发店门前经常会出现的旋转柱，如果单纯地从二维角度来看，柱子是向上运动的，可在实际的运动场中柱子是横向运动的，观测和实际的运动方向是完全垂直的。<br><img src="/computer-vision20200215214240/发廊旋转柱.png" title="发廊旋转柱"><br>因此，为了能够更好地跟踪目标，我们需要提取尽可能好的特征，此外最好能从视频或图像序列中学到更多丰富的信息（尤其是含语义的）。值得注意的一点是，在港科大王乃岩博士2015年所做的ablation experiments中（详见参考文献[2]），发现特征提取是影响tracker效果最重要的因素。<br>考虑到精度是保证目标跟踪鲁棒性的重要因素，不同于一些其他的计算机视觉任务，目标跟踪领域的深度算法比较强调结合与充分利用浅层网络的高分辨率信息。</p><h2 id="孪生网络（2016-）"><a href="#孪生网络（2016-）" class="headerlink" title="孪生网络（2016-）"></a>孪生网络（2016-）</h2><p>孪生网络采用的是匹配学习的思想。</p><h2 id="样本的有效性和平衡性（2015-）"><a href="#样本的有效性和平衡性（2015-）" class="headerlink" title="样本的有效性和平衡性（2015-）"></a>样本的有效性和平衡性（2015-）</h2><h2 id="结合语义分割的多任务学习（2017-）"><a href="#结合语义分割的多任务学习（2017-）" class="headerlink" title="结合语义分割的多任务学习（2017-）"></a>结合语义分割的多任务学习（2017-）</h2><p>由于bounding box粗糙的对目标的标注表示使得有不少冗余的背景信息进入template，此外bounding box对平面内旋转等场景不鲁棒。由于跟踪的目标是有具体形状的且是一起运动的，同时判别式方法正是要分离除目标以外的物体。因此近年来有不少算法结合语义分割，通过多任务学习来进一步提高tracker的效果。具体来说，就是引入掩模（mask）来识别预测物体，最后在转化成bounding box作为结果。</p><h2 id="元学习（2018-）"><a href="#元学习（2018-）" class="headerlink" title="元学习（2018-）"></a>元学习（2018-）</h2><h2 id="强化学习（2017-）"><a href="#强化学习（2017-）" class="headerlink" title="强化学习（2017-）"></a>强化学习（2017-）</h2><h2 id="对抗网络（2018-）"><a href="#对抗网络（2018-）" class="headerlink" title="对抗网络（2018-）"></a>对抗网络（2018-）</h2><h2 id="时域和空间域结合（）"><a href="#时域和空间域结合（）" class="headerlink" title="时域和空间域结合（）"></a>时域和空间域结合（）</h2><h2 id="背景信息（）"><a href="#背景信息（）" class="headerlink" title="背景信息（）"></a>背景信息（）</h2><h2 id="搜索机制（）"><a href="#搜索机制（）" class="headerlink" title="搜索机制（）"></a>搜索机制（）</h2><h2 id="专家系统（）"><a href="#专家系统（）" class="headerlink" title="专家系统（）"></a>专家系统（）</h2><blockquote><p>注：其实近几年还出现了一些其他的关注方向，由于不是主流、目前关注较少、本人学识不够等原因，在此不做列举。</p></blockquote><script type="text/javascript" src="/js/src/bai.js"></script></div><div><div><hr style="FILTER:progid:DXImageTransform.Microsoft.Shadow(color:#987cb9,direction:145,strength:15)" width="100%" color="#987cb9" size="1"><div style="text-align:center;color:#555;font-size:16px"><i class="fa fa-hand-o-up" aria-hidden="true"></i> 碰到底线咯 <i class="fa fa-handshake-o" aria-hidden="true"></i> 后面没有啦 <i class="fa fa-hand-o-down" aria-hidden="true"></i></div></div></div><div><div class="my_post_copyright"><script src="//cdn.bootcss.com/clipboard.js/1.5.10/clipboard.min.js"></script><script src="https://cdn.bootcss.com/jquery/2.0.0/jquery.min.js"></script><script src="https://unpkg.com/sweetalert/dist/sweetalert.min.js"></script><p><span>本文标题:</span><a href="/computer-vision20200215214240/">computer vision笔记：目标跟踪的小总结</a></p><p><span>文章作者:</span><a href="/" title="访问 高深远 的个人博客">高深远</a></p><p><span>发布时间:</span>2020年02月15日 - 21:42</p><p><span>最后更新:</span>2020年03月05日 - 20:25</p><p><span>原始链接:</span><a href="/computer-vision20200215214240/" title="computer vision笔记：目标跟踪的小总结">https://gsy00517.github.io/computer-vision20200215214240/</a> <span class="copy-path" title="点击复制文章链接"><i class="fa fa-clipboard" data-clipboard-text="https://gsy00517.github.io/computer-vision20200215214240/" aria-label="复制成功！"></i></span></p><p><span>许可协议:</span><i class="fa fa-creative-commons"></i> <a rel="license" href="https://creativecommons.org/licenses/by-nc-nd/4.0/" target="_blank" title="Attribution-NonCommercial-NoDerivatives 4.0 International (CC BY-NC-ND 4.0)">署名-非商业性使用-禁止演绎 4.0 国际</a> 转载请保留原文链接及作者。</p></div><script>var clipboard=new Clipboard(".fa-clipboard");$(".fa-clipboard").click(function(){clipboard.on("success",function(){swal({title:"复制成功",text:"感谢您的阅读与参考！欢迎留下任何建议噢！",icon:"success",showConfirmButton:!0})})})</script></div><footer class="post-footer"><div class="post-tags"><a href="/tags/论文分享/" rel="tag"><i class="fa fa-tag"></i> 论文分享</a> <a href="/tags/计算机视觉/" rel="tag"><i class="fa fa-tag"></i> 计算机视觉</a> <a href="/tags/目标跟踪/" rel="tag"><i class="fa fa-tag"></i> 目标跟踪</a></div><div class="post-widgets"><div class="wp_rating"><div id="wpac-rating"></div></div></div><div class="post-nav"><div class="post-nav-next post-nav-item"><a href="/vot-toolkit20200215185238/" rel="next" title="vot toolkit笔记：解决无法连接TraX支持的问题"><i class="fa fa-chevron-left"></i> vot toolkit笔记：解决无法连接TraX支持的问题</a></div><span class="post-nav-divider"></span><div class="post-nav-prev post-nav-item"><a href="/anaconda20200222000018/" rel="prev" title="anaconda笔记：解决conda无法下载pytorch的问题">anaconda笔记：解决conda无法下载pytorch的问题 <i class="fa fa-chevron-right"></i></a></div></div></footer></div></article><div class="post-spread"></div></div></div><div class="comments" id="comments"></div></div><div class="sidebar-toggle"><div class="sidebar-toggle-line-wrap"><span class="sidebar-toggle-line sidebar-toggle-line-first"></span> <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span> <span class="sidebar-toggle-line sidebar-toggle-line-last"></span></div></div><aside id="sidebar" class="sidebar"><div class="sidebar-inner"><ul class="sidebar-nav motion-element"><li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap">文章目录</li><li class="sidebar-nav-overview" data-target="site-overview-wrap">站点概览</li></ul><section class="site-overview-wrap sidebar-panel"><div class="site-overview"><div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person"><img class="site-author-image" itemprop="image" src="/images/lufei.jpg" alt="高深远"><p class="site-author-name" itemprop="name">高深远</p><p class="site-description motion-element" itemprop="description"></p></div><nav class="site-state motion-element"><div class="site-state-item site-state-posts"><a href="/archives/"><span class="site-state-item-count">84</span> <span class="site-state-item-name">日志</span></a></div><div class="site-state-item site-state-categories"><a href="/categories/index.html"><span class="site-state-item-count">7</span> <span class="site-state-item-name">分类</span></a></div><div class="site-state-item site-state-tags"><a href="/tags/index.html"><span class="site-state-item-count">33</span> <span class="site-state-item-name">标签</span></a></div></nav><div class="feed-link motion-element"><a href="/atom.xml" rel="alternate"><i class="fa fa-rss"></i> RSS</a></div><div class="links-of-blogroll motion-element links-of-blogroll-inline"><div class="links-of-blogroll-title"><i class="fa fa-fw fa-link"></i> 网站收藏</div><ul class="links-of-blogroll-list"><li class="links-of-blogroll-item"><a href="https://www.runoob.com/" title="菜鸟教程" target="_blank">菜鸟教程</a></li><li class="links-of-blogroll-item"><a href="https://paperswithcode.com/" title="PaperWithCode" target="_blank">PaperWithCode</a></li><li class="links-of-blogroll-item"><a href="https://www.jiqizhixin.com/sota" title="机器之心" target="_blank">机器之心</a></li><li class="links-of-blogroll-item"><a href="http://pytorch123.com/" title="Pytorch中文文档" target="_blank">Pytorch中文文档</a></li></ul></div></div></section><section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active"><div class="post-toc"><div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#简介与要求"><span class="nav-number">1.</span> <span class="nav-text">简介与要求</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#问题及挑战"><span class="nav-number">2.</span> <span class="nav-text">问题及挑战</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#生成式与判别式"><span class="nav-number">3.</span> <span class="nav-text">生成式与判别式</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#机器学习"><span class="nav-number">3.1.</span> <span class="nav-text">机器学习</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#目标跟踪"><span class="nav-number">3.2.</span> <span class="nav-text">目标跟踪</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#算法导图"><span class="nav-number">4.</span> <span class="nav-text">算法导图</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#各类算法的梳理与简述"><span class="nav-number">5.</span> <span class="nav-text">各类算法的梳理与简述</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#LK-Tracker（1981）"><span class="nav-number">5.1.</span> <span class="nav-text">LK Tracker（1981）</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#KLT（1994）"><span class="nav-number">5.2.</span> <span class="nav-text">KLT（1994）</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Mean-Shift（2002）"><span class="nav-number">5.3.</span> <span class="nav-text">Mean Shift（2002）</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#IVT（2008）"><span class="nav-number">5.4.</span> <span class="nav-text">IVT（2008）</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#MOSSE（2010）"><span class="nav-number">5.5.</span> <span class="nav-text">MOSSE（2010）</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#TLD（2010）"><span class="nav-number">5.6.</span> <span class="nav-text">TLD（2010）</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#L1-Tracker（2011）"><span class="nav-number">5.7.</span> <span class="nav-text">L1 Tracker（2011）</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Struck（2011）"><span class="nav-number">5.8.</span> <span class="nav-text">Struck（2011）</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#CSK（2012）"><span class="nav-number">5.9.</span> <span class="nav-text">CSK（2012）</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#DLT（2013）"><span class="nav-number">5.10.</span> <span class="nav-text">DLT（2013）</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#KCF（2014）"><span class="nav-number">5.11.</span> <span class="nav-text">KCF（2014）</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#DCF（2014）"><span class="nav-number">5.12.</span> <span class="nav-text">DCF（2014）</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#CN（2014）"><span class="nav-number">5.13.</span> <span class="nav-text">CN（2014）</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#DSST（2014）"><span class="nav-number">5.14.</span> <span class="nav-text">DSST（2014）</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#SO-DLT（2015）"><span class="nav-number">5.15.</span> <span class="nav-text">SO-DLT（2015）</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#MDNet（2015）"><span class="nav-number">5.16.</span> <span class="nav-text">MDNet（2015）</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#SRDCF（2015）"><span class="nav-number">5.17.</span> <span class="nav-text">SRDCF（2015）</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#DeepSRDCF（2015）"><span class="nav-number">5.18.</span> <span class="nav-text">DeepSRDCF（2015）</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#HCF（2015）"><span class="nav-number">5.19.</span> <span class="nav-text">HCF（2015）</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#FCNT（2015）"><span class="nav-number">5.20.</span> <span class="nav-text">FCNT（2015）</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#LCT（2015）"><span class="nav-number">5.21.</span> <span class="nav-text">LCT（2015）</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#DLSSVM（2016）"><span class="nav-number">5.22.</span> <span class="nav-text">DLSSVM（2016）</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#C-COT（2016）"><span class="nav-number">5.23.</span> <span class="nav-text">C-COT（2016）</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#LMCF（2017）"><span class="nav-number">5.24.</span> <span class="nav-text">LMCF（2017）</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#DeepLMCF（2017）"><span class="nav-number">5.25.</span> <span class="nav-text">DeepLMCF（2017）</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#STRCF（2018）"><span class="nav-number">5.26.</span> <span class="nav-text">STRCF（2018）</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#UPDT（2018）"><span class="nav-number">5.27.</span> <span class="nav-text">UPDT（2018）</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#ACT（2018）"><span class="nav-number">5.28.</span> <span class="nav-text">ACT（2018）</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#DRT（2018）"><span class="nav-number">5.29.</span> <span class="nav-text">DRT（2018）</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#MCCT（2018）"><span class="nav-number">5.30.</span> <span class="nav-text">MCCT（2018）</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#LSART（2018）"><span class="nav-number">5.31.</span> <span class="nav-text">LSART（2018）</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#DaSiamRPN（2018）"><span class="nav-number">5.32.</span> <span class="nav-text">DaSiamRPN（2018）</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Meta-Tracker（2018）"><span class="nav-number">5.33.</span> <span class="nav-text">Meta-Tracker（2018）</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#DorT（2018）"><span class="nav-number">5.34.</span> <span class="nav-text">DorT（2018）</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#研究趋势"><span class="nav-number">6.</span> <span class="nav-text">研究趋势</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#深度特征（2013-）"><span class="nav-number">6.1.</span> <span class="nav-text">深度特征（2013-）</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#孪生网络（2016-）"><span class="nav-number">6.2.</span> <span class="nav-text">孪生网络（2016-）</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#样本的有效性和平衡性（2015-）"><span class="nav-number">6.3.</span> <span class="nav-text">样本的有效性和平衡性（2015-）</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#结合语义分割的多任务学习（2017-）"><span class="nav-number">6.4.</span> <span class="nav-text">结合语义分割的多任务学习（2017-）</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#元学习（2018-）"><span class="nav-number">6.5.</span> <span class="nav-text">元学习（2018-）</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#强化学习（2017-）"><span class="nav-number">6.6.</span> <span class="nav-text">强化学习（2017-）</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#对抗网络（2018-）"><span class="nav-number">6.7.</span> <span class="nav-text">对抗网络（2018-）</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#时域和空间域结合（）"><span class="nav-number">6.8.</span> <span class="nav-text">时域和空间域结合（）</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#背景信息（）"><span class="nav-number">6.9.</span> <span class="nav-text">背景信息（）</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#搜索机制（）"><span class="nav-number">6.10.</span> <span class="nav-text">搜索机制（）</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#专家系统（）"><span class="nav-number">6.11.</span> <span class="nav-text">专家系统（）</span></a></li></ol></li></ol></div></div></section></div></aside></div></main><footer id="footer" class="footer"><div class="footer-inner"><div class="copyright">&copy; <span itemprop="copyrightYear">2020</span> <span class="with-love"><i class="fa fa-user"></i> </span><span class="author" itemprop="copyrightHolder">高深远</span> <span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-area-chart"></i> </span><span class="post-meta-item-text">Site words total count&#58;</span> <span title="Site words total count">159k</span></div><script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><span id="busuanzi_container_site_pv">访客已留下<span id="busuanzi_value_site_pv"></span>个脚印 </span><span id="busuanzi_container_site_uv">你是第<span id="busuanzi_value_site_uv"></span>位小伙伴</span></div></footer><div class="back-to-top"><i class="fa fa-arrow-up"></i> <span id="scrollpercent"><span>0</span>%</span></div></div><script type="text/javascript">"[object Function]"!==Object.prototype.toString.call(window.Promise)&&(window.Promise=null)</script><script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script><script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script><script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script><script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script><script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script><script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script><script type="text/javascript" src="/lib/three/three.min.js"></script><script type="text/javascript" src="/lib/three/three-waves.min.js"></script><script type="text/javascript" src="/js/src/utils.js?v=5.1.4"></script><script type="text/javascript" src="/js/src/motion.js?v=5.1.4"></script><script type="text/javascript" src="/js/src/affix.js?v=5.1.4"></script><script type="text/javascript" src="/js/src/schemes/pisces.js?v=5.1.4"></script><script type="text/javascript" src="/js/src/scrollspy.js?v=5.1.4"></script><script type="text/javascript" src="/js/src/post-details.js?v=5.1.4"></script><script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.4"></script><script src="//cdn1.lncld.net/static/js/3.0.4/av-min.js"></script><script src="//unpkg.com/valine/dist/Valine.min.js"></script><script type="text/javascript">var GUEST = ['nick','mail','link'];
    var guest = 'nick,mail,link';
    guest = guest.split(',').filter(item=>{
      return GUEST.indexOf(item)>-1;
    });
    new Valine({
        el: '#comments' ,
        verify: false,
        notify: false,
        appId: 'SVKPF4oiwJLMtpySQxvJLnLm-gzGzoHsz',
        appKey: '46OfvvEe65XMeUi79STU895I',
        placeholder: '动动手指，写下您的意见、疑惑或者鼓励吧！留下您的邮箱，这样就可以收到别人的回复啦！',
        avatar:'wavatar',
        guest_info:guest,
        pageSize:'10' || 10,
    });

    var infoEle = document.querySelector('#comments .info');
    if (infoEle && infoEle.childNodes && infoEle.childNodes.length > 0){
    infoEle.childNodes.forEach(function(item) {
    item.parentNode.removeChild(item);
    });
  }</script><script type="text/javascript">function proceedsearch(){$("body").append('<div class="search-popup-overlay local-search-pop-overlay"></div>').css("overflow","hidden"),$(".search-popup-overlay").click(onPopupClose),$(".popup").toggle();var t=$("#local-search-input");t.attr("autocapitalize","none"),t.attr("autocorrect","off"),t.focus()}var isfetched=!1,isXml=!0,search_path="search.xml";0===search_path.length?search_path="search.xml":/json$/i.test(search_path)&&(isXml=!1);var path="/"+search_path,onPopupClose=function(t){$(".popup").hide(),$("#local-search-input").val(""),$(".search-result-list").remove(),$("#no-result").remove(),$(".local-search-pop-overlay").remove(),$("body").css("overflow","")},searchFunc=function(t,e,o){"use strict";$("body").append('<div class="search-popup-overlay local-search-pop-overlay"><div id="search-loading-icon"><i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i></div></div>').css("overflow","hidden"),$("#search-loading-icon").css("margin","20% auto 0 auto").css("text-align","center"),$.ajax({url:t,dataType:isXml?"xml":"json",async:!0,success:function(t){isfetched=!0,$(".popup").detach().appendTo(".header-inner");var n=isXml?$("entry",t).map(function(){return{title:$("title",this).text(),content:$("content",this).text(),url:$("url",this).text()}}).get():t,r=document.getElementById(e),s=document.getElementById(o),a=function(){var t=r.value.trim().toLowerCase(),e=t.split(/[\s\-]+/);e.length>1&&e.push(t);var o=[];if(t.length>0&&n.forEach(function(n){function r(e,o,n,r){for(var s=r[r.length-1],a=s.position,i=s.word,l=[],h=0;a+i.length<=n&&0!=r.length;){i===t&&h++,l.push({position:a,length:i.length});var p=a+i.length;for(r.pop();0!=r.length&&(s=r[r.length-1],a=s.position,i=s.word,p>a);)r.pop()}return c+=h,{hits:l,start:o,end:n,searchTextCount:h}}function s(t,e){var o="",n=e.start;return e.hits.forEach(function(e){o+=t.substring(n,e.position);var r=e.position+e.length;o+='<b class="search-keyword">'+t.substring(e.position,r)+"</b>",n=r}),o+=t.substring(n,e.end)}var a=!1,i=0,c=0,l=n.title.trim(),h=l.toLowerCase(),p=n.content.trim().replace(/<[^>]+>/g,""),u=p.toLowerCase(),f=decodeURIComponent(n.url),d=[],g=[];if(""!=l&&(e.forEach(function(t){function e(t,e,o){var n=t.length;if(0===n)return[];var r=0,s=[],a=[];for(o||(e=e.toLowerCase(),t=t.toLowerCase());(s=e.indexOf(t,r))>-1;)a.push({position:s,word:t}),r=s+n;return a}d=d.concat(e(t,h,!1)),g=g.concat(e(t,u,!1))}),(d.length>0||g.length>0)&&(a=!0,i=d.length+g.length)),a){[d,g].forEach(function(t){t.sort(function(t,e){return e.position!==t.position?e.position-t.position:t.word.length-e.word.length})});var v=[];0!=d.length&&v.push(r(l,0,l.length,d));for(var $=[];0!=g.length;){var C=g[g.length-1],m=C.position,x=C.word,w=m-20,y=m+80;0>w&&(w=0),y<m+x.length&&(y=m+x.length),y>p.length&&(y=p.length),$.push(r(p,w,y,g))}$.sort(function(t,e){return t.searchTextCount!==e.searchTextCount?e.searchTextCount-t.searchTextCount:t.hits.length!==e.hits.length?e.hits.length-t.hits.length:t.start-e.start});var T=parseInt("1");T>=0&&($=$.slice(0,T));var b="";b+=0!=v.length?"<li><a href='"+f+"' class='search-result-title'>"+s(l,v[0])+"</a>":"<li><a href='"+f+"' class='search-result-title'>"+l+"</a>",$.forEach(function(t){b+="<a href='"+f+'\'><p class="search-result">'+s(p,t)+"...</p></a>"}),b+="</li>",o.push({item:b,searchTextCount:c,hitCount:i,id:o.length})}}),1===e.length&&""===e[0])s.innerHTML='<div id="no-result"><i class="fa fa-search fa-5x" /></div>';else if(0===o.length)s.innerHTML='<div id="no-result"><i class="fa fa-frown-o fa-5x" /></div>';else{o.sort(function(t,e){return t.searchTextCount!==e.searchTextCount?e.searchTextCount-t.searchTextCount:t.hitCount!==e.hitCount?e.hitCount-t.hitCount:e.id-t.id});var a='<ul class="search-result-list">';o.forEach(function(t){a+=t.item}),a+="</ul>",s.innerHTML=a}};r.addEventListener("input",a),$(".local-search-pop-overlay").remove(),$("body").css("overflow",""),proceedsearch()}})};$(".popup-trigger").click(function(t){t.stopPropagation(),isfetched===!1?searchFunc(path,"local-search-input","local-search-result"):proceedsearch()}),$(".popup-btn-close").click(onPopupClose),$(".popup").click(function(t){t.stopPropagation()}),$(document).on("keyup",function(t){var e=27===t.which&&$(".search-popup").is(":visible");e&&onPopupClose()})</script><script src="https://cdn1.lncld.net/static/js/av-core-mini-0.6.4.js"></script><script>AV.initialize("SVKPF4oiwJLMtpySQxvJLnLm-gzGzoHsz","46OfvvEe65XMeUi79STU895I")</script><script>function showTime(e){var t=new AV.Query(e),n=[],o=$(".leancloud_visitors");o.each(function(){n.push($(this).attr("id").trim())}),t.containedIn("url",n),t.find().done(function(e){var t=".leancloud-visitors-count";if(0===e.length)return void o.find(t).text(0);for(var i=0;i<e.length;i++){var r=e[i],s=r.get("url"),l=r.get("time"),c=document.getElementById(s);$(c).find(t).text(l)}for(var i=0;i<n.length;i++){var s=n[i],c=document.getElementById(s),u=$(c).find(t);""==u.text()&&u.text(0)}}).fail(function(e,t){console.log("Error: "+t.code+" "+t.message)})}function addCount(e){var t=$(".leancloud_visitors"),n=t.attr("id").trim(),o=t.attr("data-flag-title").trim(),i=new AV.Query(e);i.equalTo("url",n),i.find({success:function(t){if(t.length>0){var i=t[0];i.fetchWhenSave(!0),i.increment("time"),i.save(null,{success:function(e){var t=$(document.getElementById(n));t.find(".leancloud-visitors-count").text(e.get("time"))},error:function(e,t){console.log("Failed to save Visitor num, with error message: "+t.message)}})}else{var r=new e,s=new AV.ACL;s.setPublicReadAccess(!0),s.setPublicWriteAccess(!0),r.setACL(s),r.set("title",o),r.set("url",n),r.set("time",1),r.save(null,{success:function(e){var t=$(document.getElementById(n));t.find(".leancloud-visitors-count").text(e.get("time"))},error:function(e,t){console.log("Failed to create")}})}},error:function(e){console.log("Error:"+e.code+" "+e.message)}})}$(function(){var e=AV.Object.extend("Counter");1==$(".leancloud_visitors").length?addCount(e):$(".post-title-link").length>1&&showTime(e)})</script><script>!function(){var t=document.createElement("script"),s=window.location.protocol.split(":")[0];"https"===s?t.src="https://zz.bdstatic.com/linksubmit/push.js":t.src="http://push.zhanzhang.baidu.com/push.js";var e=document.getElementsByTagName("script")[0];e.parentNode.insertBefore(t,e)}()</script><script type="text/javascript">wpac_init=window.wpac_init||[],wpac_init.push({widget:"Rating",id:21228,el:"wpac-rating",color:"fc6423"}),function(){if(!("WIDGETPACK_LOADED"in window)){WIDGETPACK_LOADED=!0;var e=document.createElement("script");e.type="text/javascript",e.async=!0,e.src="//embed.widgetpack.com/widget.js";var t=document.getElementsByTagName("script")[0];t.parentNode.insertBefore(e,t.nextSibling)}}()</script><script type="text/x-mathjax-config">MathJax.Hub.Config({
        tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
          processEscapes: true,
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
        }
      });</script><script type="text/x-mathjax-config">MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for (i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
        }
      });</script><script type="text/javascript" src="//cdn.bootcss.com/mathjax/2.7.1/latest.js?config=TeX-AMS-MML_HTMLorMML"></script><canvas class="fireworks" style="position:fixed;left:0;top:0;z-index:1;pointer-events:none"></canvas><script type="text/javascript" src="//cdn.bootcss.com/animejs/2.2.0/anime.min.js"></script><script type="text/javascript" src="/js/src/fireworks.js"></script><script src="/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({pluginRootPath:"live2dw/",pluginJsPath:"lib/",pluginModelPath:"assets/",tagMode:!0,log:!1,model:{jsonPath:"/live2dw/assets/hijiki.model.json"},display:{position:"left",width:250,height:500},mobile:{show:!1},react:{opacity:.7}})</script></body></html><script type="text/javascript" src="/js/src/crash_cheat.js"></script><!-- rebuild by neat -->