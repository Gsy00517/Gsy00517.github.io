<!-- build time:Thu Mar 12 2020 10:20:58 GMT+0800 (GMT+08:00) --><!DOCTYPE html><html class="theme-next gemini use-motion" lang="zh-Hans"><head><meta name="generator" content="Hexo 3.9.0"><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width,initial-scale=1,maximum-scale=1"><meta name="theme-color" content="#222"><meta name="google-site-verification" content="YV24rdmIIf8GuLLOBH5IYEWm0Z3TGAqiLS-LLlspD7w"><script src="/lib/pace/pace.min.js?v=1.0.2"></script><link href="/lib/pace/pace-theme-big-counter.min.css?v=1.0.2" rel="stylesheet"><meta http-equiv="Cache-Control" content="no-transform"><meta http-equiv="Cache-Control" content="no-siteapp"><meta name="baidu-site-verification" content="true"><link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css"><link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css"><link href="/css/main.css?v=5.1.4" rel="stylesheet" type="text/css"><link rel="apple-touch-icon" sizes="180x180" href="/images/haizei.ico?v=5.1.4"><link rel="icon" type="image/png" sizes="32x32" href="/images/haizei.ico?v=5.1.4"><link rel="icon" type="image/png" sizes="16x16" href="/images/haizei.ico?v=5.1.4"><link rel="mask-icon" href="/images/haizei.ico?v=5.1.4" color="#222"><meta name="keywords" content="论文分享,计算机视觉,目标跟踪,"><link rel="alternate" href="/atom.xml" title="高深远的博客" type="application/atom+xml"><meta name="description" content="看了一寒假的目标跟踪，一直想将自己学习到的内容整理归纳一下，迟迟没动笔（其实是打字，但说迟迟没打字比较难听哈哈）。如今快要开学了，决定还是积累一下。本文主要是把目标跟踪中的一些相关要点做一个总结，并且按具体问题对一些主流的或者我阅读过觉得有价值的算法做一个简单概述。近年来各类方法层出不穷，且码字不易，无法涵盖所有的方法。此外，由于包含自己的转述和理解可能会存在错误。在今后的学习过程中会一直保持本文"><meta name="keywords" content="论文分享,计算机视觉,目标跟踪"><meta property="og:type" content="article"><meta property="og:title" content="computer vision笔记：目标跟踪的小总结"><meta property="og:url" content="https://gsy00517.github.io/computer-vision20200215214240/index.html"><meta property="og:site_name" content="高深远的博客"><meta property="og:description" content="看了一寒假的目标跟踪，一直想将自己学习到的内容整理归纳一下，迟迟没动笔（其实是打字，但说迟迟没打字比较难听哈哈）。如今快要开学了，决定还是积累一下。本文主要是把目标跟踪中的一些相关要点做一个总结，并且按具体问题对一些主流的或者我阅读过觉得有价值的算法做一个简单概述。近年来各类方法层出不穷，且码字不易，无法涵盖所有的方法。此外，由于包含自己的转述和理解可能会存在错误。在今后的学习过程中会一直保持本文"><meta property="og:locale" content="zh-Hans"><meta property="og:image" content="https://gsy00517.github.io/computer-vision20200215214240/目标跟踪.png"><meta property="og:image" content="https://gsy00517.github.io/computer-vision20200215214240/目标跟踪伪代码.png"><meta property="og:image" content="https://gsy00517.github.io/computer-vision20200215214240/问题.png"><meta property="og:image" content="https://gsy00517.github.io/computer-vision20200215214240/OTB对问题的划分.png"><meta property="og:image" content="https://gsy00517.github.io/computer-vision20200215214240/生成式模型.png"><meta property="og:image" content="https://gsy00517.github.io/computer-vision20200215214240/判别式模型.png"><meta property="og:image" content="https://gsy00517.github.io/computer-vision20200215214240/论文中的导图.png"><meta property="og:image" content="https://gsy00517.github.io/computer-vision20200215214240/王强的导图.png"><meta property="og:image" content="https://gsy00517.github.io/computer-vision20200215214240/王蒙蒙的导图.png"><meta property="og:image" content="https://gsy00517.github.io/computer-vision20200215214240/光流.gif"><meta property="og:image" content="https://gsy00517.github.io/computer-vision20200215214240/画出光流.png"><meta property="og:image" content="https://gsy00517.github.io/computer-vision20200215214240/VOT2013.png"><meta property="og:image" content="https://gsy00517.github.io/computer-vision20200215214240/STC.png"><meta property="og:image" content="https://gsy00517.github.io/computer-vision20200215214240/VOT2014.png"><meta property="og:image" content="https://gsy00517.github.io/computer-vision20200215214240/VOT2015.png"><meta property="og:image" content="https://gsy00517.github.io/computer-vision20200215214240/难例挖掘.png"><meta property="og:image" content="https://gsy00517.github.io/computer-vision20200215214240/热力图.jpg"><meta property="og:image" content="https://gsy00517.github.io/computer-vision20200215214240/VOT2016.png"><meta property="og:image" content="https://gsy00517.github.io/computer-vision20200215214240/TCNN.jpg"><meta property="og:image" content="https://gsy00517.github.io/computer-vision20200215214240/VOT2017.png"><meta property="og:image" content="https://gsy00517.github.io/computer-vision20200215214240/卷积因式分解大大降低参数量.png"><meta property="og:image" content="https://gsy00517.github.io/computer-vision20200215214240/样本分组.png"><meta property="og:image" content="https://gsy00517.github.io/computer-vision20200215214240/VOT2018.png"><meta property="og:image" content="https://gsy00517.github.io/computer-vision20200215214240/STRCF和SRDCF对比.png"><meta property="og:image" content="https://gsy00517.github.io/computer-vision20200215214240/MCCT特征.png"><meta property="og:image" content="https://gsy00517.github.io/computer-vision20200215214240/三种样本选取方法.png"><meta property="og:image" content="https://gsy00517.github.io/computer-vision20200215214240/VOT2019.png"><meta property="og:image" content="https://gsy00517.github.io/computer-vision20200215214240/考虑三项正则.png"><meta property="og:image" content="https://gsy00517.github.io/computer-vision20200215214240/发廊旋转柱.png"><meta property="og:updated_time" content="2020-03-12T02:16:00.179Z"><meta name="twitter:card" content="summary"><meta name="twitter:title" content="computer vision笔记：目标跟踪的小总结"><meta name="twitter:description" content="看了一寒假的目标跟踪，一直想将自己学习到的内容整理归纳一下，迟迟没动笔（其实是打字，但说迟迟没打字比较难听哈哈）。如今快要开学了，决定还是积累一下。本文主要是把目标跟踪中的一些相关要点做一个总结，并且按具体问题对一些主流的或者我阅读过觉得有价值的算法做一个简单概述。近年来各类方法层出不穷，且码字不易，无法涵盖所有的方法。此外，由于包含自己的转述和理解可能会存在错误。在今后的学习过程中会一直保持本文"><meta name="twitter:image" content="https://gsy00517.github.io/computer-vision20200215214240/目标跟踪.png"><script type="text/javascript" id="hexo.configurations">var NexT=window.NexT||{},CONFIG={root:"/",scheme:"Gemini",version:"5.1.4",sidebar:{position:"left",display:"post",offset:12,b2t:!1,scrollpercent:!0,onmobile:!1},fancybox:!0,tabs:!0,motion:{enable:!0,async:!1,transition:{post_block:"fadeIn",post_header:"slideDownIn",post_body:"slideDownIn",coll_header:"slideLeftIn",sidebar:"slideUpIn"}},duoshuo:{userId:"0",author:"博主"},algolia:{applicationID:"",apiKey:"",indexName:"",hits:{per_page:10},labels:{input_placeholder:"Search for Posts",hits_empty:"We didn't find any results for the search: ${query}",hits_stats:"${hits} results found in ${time} ms"}}}</script><link rel="canonical" href="https://gsy00517.github.io/computer-vision20200215214240/"><meta name="baidu-site-verification" content="o5QfpvLBz5"><title>computer vision笔记：目标跟踪的小总结 | 高深远的博客</title></head><body itemscope itemtype="http://schema.org/WebPage" lang="zh-Hans"><div class="container sidebar-position-left page-post-detail"><div class="headband"></div><header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="header-inner"><div class="site-brand-wrapper"><div class="site-meta"><div class="custom-logo-site-title"><a href="/" class="brand" rel="start"><span class="logo-line-before"><i></i></span> <span class="site-title">高深远的博客</span> <span class="logo-line-after"><i></i></span></a></div><h1 class="site-subtitle" itemprop="description"></h1></div><div class="site-nav-toggle"><button><span class="btn-bar"></span> <span class="btn-bar"></span> <span class="btn-bar"></span></button></div></div><nav class="site-nav"><ul id="menu" class="menu"><li class="menu-item menu-item-home"><a href="/" rel="section"><i class="menu-item-icon fa fa-fw fa-home"></i><br>首页</a></li><li class="menu-item menu-item-about"><a href="/about/" rel="section"><i class="menu-item-icon fa fa-fw fa-user"></i><br>关于</a></li><li class="menu-item menu-item-tags"><a href="/tags/" rel="section"><i class="menu-item-icon fa fa-fw fa-tags"></i><br>标签</a></li><li class="menu-item menu-item-categories"><a href="/categories/" rel="section"><i class="menu-item-icon fa fa-fw fa-th"></i><br>分类</a></li><li class="menu-item menu-item-archives"><a href="/archives/" rel="section"><i class="menu-item-icon fa fa-fw fa-archive"></i><br>最新发布</a></li><li class="menu-item menu-item-new"><a href="/new/" rel="section"><i class="menu-item-icon fa fa-fw fa-history"></i><br>最近阅读</a></li><li class="menu-item menu-item-rank"><a href="/rank/" rel="section"><i class="menu-item-icon fa fa-fw fa-signal"></i><br>热度排名</a></li><li class="menu-item menu-item-search"><a href="javascript:;" class="popup-trigger"><i class="menu-item-icon fa fa-search fa-fw"></i><br>站内搜索</a></li></ul><div class="site-search"><div class="popup search-popup local-search-popup"><div class="local-search-header clearfix"><span class="search-icon"><i class="fa fa-search"></i> </span><span class="popup-btn-close"><i class="fa fa-times-circle"></i></span><div class="local-search-input-wrapper"><input autocomplete="off" placeholder="搜索..." spellcheck="false" type="text" id="local-search-input"></div></div><div id="local-search-result"></div></div></div><div style="Text-align:center;width:100%"><div style="margin:0 auto"><canvas id="canvas" style="width:60%">当前浏览器不支持canvas，请更换浏览器后再试</canvas></div><script>!function(){function t(t,e){for(var a=0;a<l[e].length;a++)for(var n=0;n<l[e][a].length;n++)1==l[e][a][n]&&(h.beginPath(),h.arc(14*(g+2)*t+2*n*(g+1)+(g+1),2*a*(g+1)+(g+1),g,0,2*Math.PI),h.closePath(),h.fill())}function e(){var t=[],e=/(\d)(\d):(\d)(\d):(\d)(\d)/.exec(new Date),a=[];a.push(e[1],e[2],10,e[3],e[4],10,e[5],e[6]);for(var r=c.length-1;r>=0;r--)a[r]!==c[r]&&t.push(r+"_"+(Number(c[r])+1)%10);for(var r=0;r<t.length;r++)n.apply(this,t[r].split("_"));c=a.concat()}function a(){for(var t=0;t<d.length;t++)d[t].stepY+=d[t].disY,d[t].x+=d[t].stepX,d[t].y+=d[t].stepY,(d[t].x>i+g||d[t].y>f+g)&&(d.splice(t,1),t--)}function n(t,e){for(var a=[1,2,3],n=["#3BE","#09C","#A6C","#93C","#9C0","#690","#FB3","#F80","#F44","#C00"],r=0;r<l[e].length;r++)for(var o=0;o<l[e][r].length;o++)if(1==l[e][r][o]){var h={x:14*(g+2)*t+2*o*(g+1)+(g+1),y:2*r*(g+1)+(g+1),stepX:Math.floor(4*Math.random()-2),stepY:-2*a[Math.floor(Math.random()*a.length)],color:n[Math.floor(Math.random()*n.length)],disY:1};d.push(h)}}function r(){o.height=100;for(var e=0;e<c.length;e++)t(e,c[e]);for(var e=0;e<d.length;e++)h.beginPath(),h.arc(d[e].x,d[e].y,g,0,2*Math.PI),h.fillStyle=d[e].color,h.closePath(),h.fill()}var l=[[[0,0,1,1,1,0,0],[0,1,1,0,1,1,0],[1,1,0,0,0,1,1],[1,1,0,0,0,1,1],[1,1,0,0,0,1,1],[1,1,0,0,0,1,1],[1,1,0,0,0,1,1],[1,1,0,0,0,1,1],[0,1,1,0,1,1,0],[0,0,1,1,1,0,0]],[[0,0,0,1,1,0,0],[0,1,1,1,1,0,0],[0,0,0,1,1,0,0],[0,0,0,1,1,0,0],[0,0,0,1,1,0,0],[0,0,0,1,1,0,0],[0,0,0,1,1,0,0],[0,0,0,1,1,0,0],[0,0,0,1,1,0,0],[1,1,1,1,1,1,1]],[[0,1,1,1,1,1,0],[1,1,0,0,0,1,1],[0,0,0,0,0,1,1],[0,0,0,0,1,1,0],[0,0,0,1,1,0,0],[0,0,1,1,0,0,0],[0,1,1,0,0,0,0],[1,1,0,0,0,0,0],[1,1,0,0,0,1,1],[1,1,1,1,1,1,1]],[[1,1,1,1,1,1,1],[0,0,0,0,0,1,1],[0,0,0,0,1,1,0],[0,0,0,1,1,0,0],[0,0,1,1,1,0,0],[0,0,0,0,1,1,0],[0,0,0,0,0,1,1],[0,0,0,0,0,1,1],[1,1,0,0,0,1,1],[0,1,1,1,1,1,0]],[[0,0,0,0,1,1,0],[0,0,0,1,1,1,0],[0,0,1,1,1,1,0],[0,1,1,0,1,1,0],[1,1,0,0,1,1,0],[1,1,1,1,1,1,1],[0,0,0,0,1,1,0],[0,0,0,0,1,1,0],[0,0,0,0,1,1,0],[0,0,0,1,1,1,1]],[[1,1,1,1,1,1,1],[1,1,0,0,0,0,0],[1,1,0,0,0,0,0],[1,1,1,1,1,1,0],[0,0,0,0,0,1,1],[0,0,0,0,0,1,1],[0,0,0,0,0,1,1],[0,0,0,0,0,1,1],[1,1,0,0,0,1,1],[0,1,1,1,1,1,0]],[[0,0,0,0,1,1,0],[0,0,1,1,0,0,0],[0,1,1,0,0,0,0],[1,1,0,0,0,0,0],[1,1,0,1,1,1,0],[1,1,0,0,0,1,1],[1,1,0,0,0,1,1],[1,1,0,0,0,1,1],[1,1,0,0,0,1,1],[0,1,1,1,1,1,0]],[[1,1,1,1,1,1,1],[1,1,0,0,0,1,1],[0,0,0,0,1,1,0],[0,0,0,0,1,1,0],[0,0,0,1,1,0,0],[0,0,0,1,1,0,0],[0,0,1,1,0,0,0],[0,0,1,1,0,0,0],[0,0,1,1,0,0,0],[0,0,1,1,0,0,0]],[[0,1,1,1,1,1,0],[1,1,0,0,0,1,1],[1,1,0,0,0,1,1],[1,1,0,0,0,1,1],[0,1,1,1,1,1,0],[1,1,0,0,0,1,1],[1,1,0,0,0,1,1],[1,1,0,0,0,1,1],[1,1,0,0,0,1,1],[0,1,1,1,1,1,0]],[[0,1,1,1,1,1,0],[1,1,0,0,0,1,1],[1,1,0,0,0,1,1],[1,1,0,0,0,1,1],[0,1,1,1,0,1,1],[0,0,0,0,0,1,1],[0,0,0,0,0,1,1],[0,0,0,0,1,1,0],[0,0,0,1,1,0,0],[0,1,1,0,0,0,0]],[[0,0,0,0,0,0,0],[0,0,1,1,1,0,0],[0,0,1,1,1,0,0],[0,0,1,1,1,0,0],[0,0,0,0,0,0,0],[0,0,0,0,0,0,0],[0,0,1,1,1,0,0],[0,0,1,1,1,0,0],[0,0,1,1,1,0,0],[0,0,0,0,0,0,0]]],o=document.getElementById("canvas");if(o.getContext){var h=o.getContext("2d"),f=100,i=700;o.height=f,o.width=i,h.fillStyle="#f00",h.fillRect(10,10,50,50);var c=[],d=[],g=o.height/20-1;!function(){var t=/(\d)(\d):(\d)(\d):(\d)(\d)/.exec(new Date);c.push(t[1],t[2],10,t[3],t[4],10,t[5],t[6])}(),clearInterval(v);var v=setInterval(function(){e(),a(),r()},50)}}()</script></div></nav></div></header><main id="main" class="main"><div class="main-inner"><div class="content-wrap"><div id="content" class="content"><div id="posts" class="posts-expand"><article class="post post-type-normal" itemscope itemtype="http://schema.org/Article"><div class="post-block"><link itemprop="mainEntityOfPage" href="https://gsy00517.github.io/computer-vision20200215214240/"><span hidden itemprop="author" itemscope itemtype="http://schema.org/Person"><meta itemprop="name" content="高深远"><meta itemprop="description" content><meta itemprop="image" content="/images/lufei.jpg"></span><span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization"><meta itemprop="name" content="高深远的博客"></span><header class="post-header"><h2 class="post-title" itemprop="name headline">computer vision笔记：目标跟踪的小总结</h2><div class="post-meta"><span class="post-time"><span class="post-meta-item-icon"><i class="fa fa-calendar-o"></i> </span><span class="post-meta-item-text">发表于</span> <time title="创建于" itemprop="dateCreated datePublished" datetime="2020-02-15T21:42:40+08:00">2020-02-15 </time><span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-calendar-check-o"></i> </span><span class="post-meta-item-text">更新于&#58;</span> <time title="更新于" itemprop="dateModified" datetime="2020-03-12T10:16:00+08:00">2020-03-12 </time></span><span class="post-category"><span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-folder-o"></i> </span><span class="post-meta-item-text">分类于</span> <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/人工智能/" itemprop="url" rel="index"><span itemprop="name">人工智能</span> </a></span></span><span class="post-comments-count"><span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-comment-o"></i> </span><a href="/computer-vision20200215214240/#comments" itemprop="discussionUrl"><span class="post-comments-count valine-comment-count" data-xid="/computer-vision20200215214240/" itemprop="commentCount"></span> </a></span><span id="/computer-vision20200215214240/" class="leancloud_visitors" data-flag-title="computer vision笔记：目标跟踪的小总结"><span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-eye"></i> </span><span class="post-meta-item-text">阅读次数&#58;</span> <span class="leancloud-visitors-count"></span></span><div class="post-wordcount"><span class="post-meta-item-icon"><i class="fa fa-file-word-o"></i> </span><span class="post-meta-item-text">字数统计&#58;</span> <span title="字数统计">17k字 </span><span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-clock-o"></i> </span><span class="post-meta-item-text">阅读时长 &asymp;</span> <span title="阅读时长">60分钟</span></div></div></header><div class="post-body" itemprop="articleBody"><p>看了一寒假的目标跟踪，一直想将自己学习到的内容整理归纳一下，迟迟没动笔（其实是打字，但说迟迟没打字比较难听哈哈）。如今快要开学了，决定还是积累一下。<br>本文主要是把目标跟踪中的一些相关要点做一个总结，并且按具体问题对一些主流的或者我阅读过觉得有价值的算法做一个简单概述。近年来各类方法层出不穷，且码字不易，无法涵盖所有的方法。此外，由于包含自己的转述和理解可能会存在错误。在今后的学习过程中会一直保持本文的更新，因此这将是一篇LTS的文章哈哈。</p><blockquote><p>注：本文重点关注单目标跟踪。</p></blockquote><p><strong>References</strong>：</p><p>参考文献：<br>[1]统计学习方法（第2版）<br>[2]Understanding and Diagnosing Visual Tracking Systems<br>[3]Survey of Visual Object Tracking Algorithms Based on Deep Learning<br>[4]Handcrafted and Deep Trackers: Recent Visual Object Tracking Approaches and Trends<br>[5]Review of visual object tracking technology<br>[6]A Review of Visual Trackers and Analysis of its Application to Mobile Robot<br>[7]Deep Learning for Visual Tracking: A Comprehensive Survey<br>[8]Video Object Segmentation and Tracking: A Survey<br>[9]Object Tracking Benchmark<br>[10]The Visual Object Tracking VOT2013 challenge results<br>[11]The Visual Object Tracking VOT2014 challenge results<br>[12]The Visual Object Tracking VOT2015 challenge results<br>[13]The Visual Object Tracking VOT2016 challenge results<br>[14]The Visual Object Tracking VOT2017 challenge results<br>[15]The sixth Visual Object Tracking VOT2018 challenge results<br>[16]The Seventh Visual Object Tracking VOT2019 Challenge Results</p><blockquote><p>注：参考文献重新整理中，待补全…</p></blockquote><hr><h1 id="简介与要求"><a href="#简介与要求" class="headerlink" title="简介与要求"></a>简介与要求</h1><p>目标跟踪是利用一个视频或图像序列的上下文信息，对目标的外观和运动信息进行建模，从而对目标运动状态进行预测并标定目标位置的一种技术。一般是在第一帧给出一个框，框中的物体就是我们需要在后续帧中用算法进行跟踪的对象。就目前的单目标跟踪而言，一般有如下要求：<br><strong>monocular</strong>：我们的视频或者图片序列是仅从一个摄像头中获得的，也就是不考虑比如在城市道路场景中跨摄像头对目标跟踪的复杂应用。<br><strong>model-free</strong>：没有任何先验，也就是在获取第一帧的框之前我们并不知道会框出什么物体，也不需要在之前对初始框中的物体进行建模。<br><strong>single-target</strong>：只追踪第一帧框出的那一个物体，也就是除了那个物体之外所有的物体都是back ground。<br><strong>casual/real-time</strong>：目标跟踪是一个在线过程，也就是不能提前获取未来的框对目标进行跟踪。<br><strong>short-term</strong>：没有重检测，也就是目标跟丢了就丢了。<br><strong>long-term</strong>：可以在跟丢之后重检测，这类算法一般除了跟踪之外还需要有检测的功能。<br><img src="/computer-vision20200215214240/目标跟踪.png" title="目标跟踪"><br>下面是目标跟踪流程的伪代码表示（不一定普适，比如有些算法不在线更新，但符合基本的过程）。<br><img src="/computer-vision20200215214240/目标跟踪伪代码.png" title="目标跟踪伪代码"></p><hr><h1 id="问题及挑战"><a href="#问题及挑战" class="headerlink" title="问题及挑战"></a>问题及挑战</h1><p>通俗来讲，目标跟踪的最终目标就是要又快又准。“快”主要表现在计算量小和所需的存储空间小，“准”就是预测出的bounding box要尽可能地接近ground truth。除了上面两个基本需求（也可以说是为了更好地达到这两个基本需求），近年来的算法主要针对目标跟踪中的一些挑战进行突破，从而更好地解决某些问题之后达到更好的整体效果。<br>总的来说，目标跟踪的主要问题有如下这些：遮挡（occlusion）、背景干扰（background clutter）、光照变化（illumination changes）、尺度变化（scale variation）、低分辨率（low resolution）、快速移动（fast motion）、超出画面（out of view）、运动模糊（motion blur）、形变（deformation）、旋转（rotation）等。<br><img src="/computer-vision20200215214240/问题.png" title="问题"><br>OTB数据集依据各种问题对其中的序列进行了一个划分，这对之后针对性的研究提供了重要的参考。<br><img src="/computer-vision20200215214240/OTB对问题的划分.png" title="OTB对问题的划分"></p><hr><h1 id="生成式与判别式"><a href="#生成式与判别式" class="headerlink" title="生成式与判别式"></a>生成式与判别式</h1><p>利用特征判断候选样本是否为跟踪目标，可将目标跟踪的模型分为生成式模型和判别式模型，本小节就介绍一下什么是生成式模型和判别式模型。</p><h2 id="机器学习"><a href="#机器学习" class="headerlink" title="机器学习"></a>机器学习</h2><p>我们首先看看在机器学习中生成式模型和判别式模型定义的一般区分。<br>一般而言，机器学习的任务就是学习一个模型，应用这一个模型，对给定的输入预测相应的输出。输出的一般形式可以是决策函数，也可以是条件概率分布。<br>对于生成式模型，我们需要通过数据学习输入X与输出Y之间的生成关系（比如联合概率分布），也就是认为X和Y都是随机变量。典型的生成式模型有朴素贝叶斯模型、隐马尔可夫模型（HMM）、高斯混合模型（GMM）等。<br>对于判别式模型，我们只需要直接学习决策函数或者条件概率分布，只关心对给定的输入X我们需要输出怎么样的Y，也就是不考虑X是否是随机变量。典型的判别式模型包括k近邻、感知机、决策树、逻辑斯蒂回归模型、最大熵模型、支持向量机（SVM）、提升方法和条件随机场等。此外神经网络也属于判别式模型。<br>相较而言，生成式模型体现了更多的信息，不过这还是因条件而异的，不同情况不同任务两种方法各有优缺点。</p><h2 id="目标跟踪"><a href="#目标跟踪" class="headerlink" title="目标跟踪"></a>目标跟踪</h2><p>在目标跟踪领域，生成式模型通过提取目标特征来构建表观模型，然后在图像中搜索与模型最匹配的区域作为跟踪结果。不论采用全局特征还是局部特征，生成式模型的本质是在目标表示的高维空间中，找到与目标模型最相邻的候选目标作为当前估计。此类方法的缺陷在于只关注目标信息，而忽略了背景信息。<br><img src="/computer-vision20200215214240/生成式模型.png" title="生成式模型"><br>与生成式模型不同的是，判别式模型同时考虑了目标和背景信息。它将跟踪问题看做二分类或者回归问题，其目的是寻找一个判别函数，将目标从背景中分离出来，从而实现对目标的跟踪。<br><img src="/computer-vision20200215214240/判别式模型.png" title="判别式模型"><br>一般来说，在目标跟踪领域，判别式充分利用了目标前景和背景信息，能更加有效地区分出目标，比单单运用目标区域特征进行模板匹配的生成式模型在复杂环境中的鲁棒性更强。</p><hr><h1 id="算法导图"><a href="#算法导图" class="headerlink" title="算法导图"></a>算法导图</h1><p>首先是参考文献[6]中的一个树状导图。<br><img src="/computer-vision20200215214240/论文中的导图.png" title="论文中的导图"><br>下图是中科院博士王强（github名为foolwood…呃不得不说这名字取得真谦虚）在<a href="https://github.com/foolwood/benchmark_results" target="_blank">github</a>上总结的历年各大benchmark的优秀成果的一个思维导图，同一个链接下还包括了各项成果的paper及code，值得收藏一下。<br><img src="/computer-vision20200215214240/王强的导图.png" title="王强的导图"></p><blockquote><p>补充：这里再推荐一个在github上维护的<a href="https://github.com/HEscop/TBCF" target="_blank">Tracking Benchmark for Correlation Filters</a>，按每篇论文针对或者解决的问题来分类，比较清楚，可以收藏一下。但这个仓库似乎在2017年后就没有更新了，可能是深度学习的进入或者说相关滤波系列和深度学习融合使得独立的相关滤波算法不那么突出了。</p></blockquote><p>下图是浙大硕士王蒙蒙极市平台做分享的时候所用的一张思维导图，归纳得也比较清晰。<br><img src="/computer-vision20200215214240/王蒙蒙的导图.png" title="王蒙蒙的导图"></p><blockquote><p>注：后两张导图中都把历年benchmark的冠军工作作了标注。</p></blockquote><p>对比几张思维导图可以发现，他们都把主流算法分成了相关滤波、深度学习两个分支（或者说是基于handcrafted特征的算法和基于CNN提取特征的算法，其实近年已有所融合），此外还有一些基于强化学习、结构化SVM的模型。其实，目标跟踪算得上是计算机视觉领域中深度学习涉足较晚的一个方向，其主要原因是目标跟踪相关数据集的标注花费较大。此外，相关滤波的速度优势，也就是实时性是十分引人注目的，但在应付当前目标跟踪中的各种挑战、问题时，相关滤波的鲁棒性还是落后于深度学习方法的。<br>在下一节，我将结合上面几张导图，对历年尤其是近几年的算法做一个简单的整理，以方便日后的学习与研究。</p><hr><h1 id="各类算法的梳理与简述"><a href="#各类算法的梳理与简述" class="headerlink" title="各类算法的梳理与简述"></a>各类算法的梳理与简述</h1><p>本节按年份顺序对各个算法进行一个简单地梳理，其中各个算法的年份以论文发表的年份或者参加benchmark的年份为依据，可能会存在1年的区别，但影响不大。对2013以后的算法，我拷贝了VOT challenge的结果排名，以供参照。</p><blockquote><p>注意：如果你对计算机视觉或者说目标跟踪方面的一些基础方法、概念和经典算法已经有些了解，可以跳过本条建议。<br>考虑到在后文频繁地插入链接不太好，我就在此先推荐一下我博客的几个标签<a href="https://gsy00517.github.io/tags/%E7%9B%AE%E6%A0%87%E8%B7%9F%E8%B8%AA/" target="_blank">目标跟踪</a>、<a href="https://gsy00517.github.io/tags/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/" target="_blank">计算机视觉</a>、<a href="https://gsy00517.github.io/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/" target="_blank">深度学习</a>、<a href="https://gsy00517.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" target="_blank">机器学习</a>以及<a href="https://gsy00517.github.io/tags/%E7%BA%BF%E6%80%A7%E4%BB%A3%E6%95%B0/" target="_blank">线性代数</a>，其中的文章包含了一部分接下来要提到的概念和算法，可以事先浏览一下。当你在阅读时对相关概念、方法感到迷惑或者想进一步了解，博客内置的搜索功能或许能够为你提供帮助。</p></blockquote><h2 id="1981"><a href="#1981" class="headerlink" title="1981"></a>1981</h2><h3 id="LK-Tracker"><a href="#LK-Tracker" class="headerlink" title="LK Tracker"></a>LK Tracker</h3><p>LK Tracker应该是最早的目标跟踪工作，它使用了光流的概念，如下图所示，不同颜色表示光流不同的方向，颜色的深浅表示运动的速度。<br><img src="/computer-vision20200215214240/光流.gif" title="光流"><br>LK Tracker假定目标灰度在短时间内保持不变，同时目标邻域内的速度向量场变化缓慢。由于光流方程包含坐标x，y和时间t共三个未知数，其中时间变化dt已知而坐标变化dx和dy未知，一个方程两个未知数无法求解，因此作者假定相邻的点它们的光流具有空间一致性，即实际场景中邻近的点投影到图像上也是邻近点，且邻近点速度一致，这样就可以求解方程组了。下图是求解之后的光流向量，其中绿色箭头的方向表示运动方向，线段长度表示运动速度的大小。<br><img src="/computer-vision20200215214240/画出光流.png" title="画出光流"><br>光流的计算非常简单也非常快，而且由于提出得很早，各种库都有实现好的轮子可以轻松调用，但是它的鲁棒性不好，基本上只能对平移且外观不变的物体进行跟踪。</p><h2 id="1994"><a href="#1994" class="headerlink" title="1994"></a>1994</h2><h3 id="KLT"><a href="#KLT" class="headerlink" title="KLT"></a>KLT</h3><p>KLT是一种生成式方法，也是使用了光流特征。在此基础上，作者使用了匹配角点的方法，也就是寻找边角处、纹理处等易辨识的地方计算光流来进行追踪。</p><h2 id="1998"><a href="#1998" class="headerlink" title="1998"></a>1998</h2><h3 id="Condensation"><a href="#Condensation" class="headerlink" title="Condensation"></a>Condensation</h3><p>Condensation（Conditional density propagation）条件密度传播使用了原始的外观作为主要特征来描述目标，采用了粒子滤波，这是一种非参数化滤波方法，属于生成式模型。它定义了一个粒子样本集，该样本集描述了每个粒子的坐标、运动速度、高和宽、尺度变化等状态；此外，通过一个状态转移矩阵和噪声定义系统状态方程。基于蒙特卡洛方法，粒子滤波将贝叶斯滤波方法中的积分运算转化为粒子采样求样本均值问题，通过对状态空间的粒子的随机采样来近似求解后验概率。</p><h2 id="2002"><a href="#2002" class="headerlink" title="2002"></a>2002</h2><h3 id="Mean-Shift"><a href="#Mean-Shift" class="headerlink" title="Mean Shift"></a>Mean Shift</h3><p>Mean Shift采用均值漂移作为搜索策略，这是一种无参概率估计方法，该方法利用图像特征构造概率密度函数，通过沿着概率密度函数的梯度方向迭代，搜索函数局部最大值。在当时成为了常用的视觉跟踪系统的目标搜索方法，简单易实现，但鲁棒性较低。</p><h2 id="2008"><a href="#2008" class="headerlink" title="2008"></a>2008</h2><h3 id="IVT"><a href="#IVT" class="headerlink" title="IVT"></a>IVT</h3><p>IVT在线更新特征空间的基，直接将以前检测到的目标作为样本在线学习而无需大量的标注样本。</p><h2 id="2010"><a href="#2010" class="headerlink" title="2010"></a>2010</h2><h3 id="MOSSE"><a href="#MOSSE" class="headerlink" title="MOSSE"></a>MOSSE</h3><p>MOSSE（Minimum Output Sum of Squared Error）使用相关滤波来做目标跟踪（不是第一个，但可以看作前期的一个代表），其速度能够达到600多帧每秒，但是效果一般，这主要是因为它只使用了简单的raw pixel特征。<br>相比之前的算法，MOSSE能够形成更加明确的峰值，减少了漂移；此外，MOSSE可以在线更新，同时还采用了PSR来检测遮挡或者跟丢的情况，从而决定是否需要停止更新。<br>值得一提的是，MOSSE在做相关操作之前，对每张图都进行了减去平均值的处理，这有利于淡化背景对相关操作的影响。另外假如发生光照变化的话，减去均值也有利于减小这种变化的影响。</p><h3 id="TLD"><a href="#TLD" class="headerlink" title="TLD"></a>TLD</h3><p>TLD（Tracking Learning Detection）主要针对long-term tracking，在跟踪的同时全局检测。它由三部分组成：跟踪模块、检测模块、学习模块。<br>跟踪模块观察帧与帧之间的目标的动向。作者采用了光流来跟踪，此外还提出了一种判断跟踪失效的算法，由于光流跟踪时选取的若干特征点，当其中某一个特征点的位移与所有特征点位移的中值之差过大时，也就是某个特征点离跟踪模块认为的目标中心位置很远时，就认为跟踪失效。作者还通过相似度和错误匹配度来对特征点进行筛选。<br>检测模块把每张图看成独立的，然后对单张图片进行目标检测定位。作者使用了方差检测器、随机森林和最近邻分类器来对目标做检测。<br>学习模块对根据跟踪模块的结果对检测模块的错误进行评估，当置信度较低时，重新组织正负样本对随机深林的后验概率和最近邻分类器的在线模板进行更新，从而避免以后出现类似错误。<br>TLD与传统跟踪算法的显著区别在于将传统的跟踪算法和传统的检测算法相结合来解决被跟踪目标在被跟踪过程中发生的形变、部分遮挡等问题。同时，通过一种改进的在线学习机制不断更新跟踪模块的“显著特征点”和检测模块的目标模型及相关参数，从而使得跟踪能够自适应，效果较之前更加稳定、可靠。</p><h2 id="2011"><a href="#2011" class="headerlink" title="2011"></a>2011</h2><h3 id="L1-Tracker"><a href="#L1-Tracker" class="headerlink" title="L1 Tracker"></a>L1 Tracker</h3><p>L1 Tracker是第一个将稀疏编码引入目标跟踪问题中的算法。它把跟踪看做一个稀疏近似问题，主要是用第一帧和最近几帧得到的图像（特征）作为字典，通过求解L1范数最小化问题，实现对目标的跟踪。</p><h3 id="Struck"><a href="#Struck" class="headerlink" title="Struck"></a>Struck</h3><p>Struck的主要贡献是引入了结构化SVM。考虑到传统的跟踪算法将跟踪问题转化为一个分类问题，并通过在线学习技术更新目标模型。然而，为了达到更新的目的，通常需要将一些预估计的目标位置作为已知类别的训练样本，这些分类样本并不一定与实际目标一致，因此难以实现最佳的分类效果。<br>结合上述考虑，Struck利用了结构化SVM直接输出跟踪结果，避免了中间分类环节，这使得在当时效果有明显的提升。同时，为了保证实时性，Struck还引入了阈值机制，防止跟踪过程中支持向量的过增长。</p><h2 id="2012"><a href="#2012" class="headerlink" title="2012"></a>2012</h2><h3 id="CSK"><a href="#CSK" class="headerlink" title="CSK"></a>CSK</h3><p>CSK也称为核相关滤波算法，作者针对MOSSE做出了一些改进，作者认为循环移位能模拟当前正样本的所有的转换版本（除边界以外），因此采用循环移位进行密集采样，并通过核函数将低维线性空间映射到高维空间，提高了相关滤波器的鲁棒性。随后的工作主要从特征选择、尺度估计、正则化等方面对该算法进行改进和提高。关于循环移位和线性、非线性的核函数计算，我在之前的文章中做了一些分析，感兴趣的话可以看看。</p><h3 id="CT"><a href="#CT" class="headerlink" title="CT"></a>CT</h3><p>CT（Compressive Tracking）是一种基于压缩感知的高效跟踪算法。和一般的判别式模型架构一样，CT首先利用符合压缩感知RIP条件的随机感知矩阵对图像特征进行降维，使得到的低维信号可以完全保持高维信号的特性并可以完全重建，然后在降维后的特征上，在感知空间下采用朴素贝叶斯分类器进行分类。另外，CT在每一帧通过在线学习更新分类器，在线学习的样本来自通过稀疏感知矩阵提取的前景目标和背景的特征。</p><h2 id="2013"><a href="#2013" class="headerlink" title="2013"></a>2013</h2><p>下面是VOT2013的排名结果，其中Experiment 1是在所有序列上使用ground truth初始化的实验结果，Experiment 2使用含噪声（10%的尺寸扰动）的ground truth，Experiment 3使用灰度图像。<br><img src="/computer-vision20200215214240/VOT2013.png" title="VOT2013"></p><h3 id="DLT"><a href="#DLT" class="headerlink" title="DLT"></a>DLT</h3><p>DLT是最早的基于深度学习的算法（当时AlexNet刚刚被提出），它采用了堆叠去噪自编码器网络，把跟踪视为一个分类问题，直接利用80 Million Tiny Images数据集上的预训练模型提取深度特征，这种强行task转换的训练方法存在缺陷，但在当时是个进步。</p><h3 id="STC"><a href="#STC" class="headerlink" title="STC"></a>STC</h3><p>STC（Spatio-Temporal Context）通过贝叶斯框架目标时间、空间的上下文信息来建模，利用得到的关系结合生物视觉系统中的注意力特性来生成confidence map来预测目标位置。由于上下文信息建模和之后的预测都采用了快速傅里叶变换，因此算法的速度很快。<br><img src="/computer-vision20200215214240/STC.png" title="STC"><br>文章主要举了两个例子来说明空间信息的重要性。当目标物体被部分或者完全遮挡时，周围的信息能帮助定位被遮挡的目标（假设摄像头不移动），也就是说可以利用空间的距离信息；此外，如果目标内部的两个部分比较相似（比如人的一对眼睛），就比较容易发生偏移，而如果这时恰好这两个部分的距离信息相似（距离目标中心长度相同），那就需要引入相对位置也就是方向信息来判断。<br>此外，考虑到生物视觉系统中的注意力特性，作者增加了一项权重函数来构成先验，该函数根据距离目标位置的远近来定义。<br>作者还对confidence map的参数进行了讨论，认为置信度在空间上的分布不能太平滑（增加位置模糊不确定性），也不能太尖锐（导致过拟合）。<br>作者认为目标的形态与近几帧有较强的关联，由此设计了时域信息模型。文中提到的时域滤波器可被证明是低通的，也就是可以滤去一定的噪声。此外，STC还设计了尺度更新方法，最终下一帧的尺度是前n帧估计尺度的均值。</p><h2 id="2014"><a href="#2014" class="headerlink" title="2014"></a>2014</h2><p>下面是VOT2014的排名结果，这里的A表示accuracy，R表示robustness。<br><img src="/computer-vision20200215214240/VOT2014.png" title="VOT2014"></p><h3 id="DSST"><a href="#DSST" class="headerlink" title="DSST"></a>DSST</h3><p>DSST主要考虑了尺度缩放的问题。它将目标跟踪看成位置变化和尺度变化两个独立问题，提出了一个高、宽、尺度数三维的滤波器，先计算平移位置再聚集尺度，即训练了两个滤波器，首先训练位置平移相关滤波器以检测目标中心平移，然后训练尺度相关滤波器来检测目标的尺度变化。</p><h3 id="SAMF"><a href="#SAMF" class="headerlink" title="SAMF"></a>SAMF</h3><p>SAMF也考虑了尺度问题，思路比较简单，采用k个尺度去采样，由于核相关操作的点乘需要固定尺度的输入，因此对采集到的样本作双线性插值成为固定尺度，然后再做相关操作。在特征方面，SAMF发现HOG和Color Name有互补作用，考虑到和相关操作仅包含点乘和向量范数的计算使得多通道很容易被引入，因此使用了HOG和Color Name多通道特征。</p><h3 id="KCF"><a href="#KCF" class="headerlink" title="KCF"></a>KCF</h3><p>KCF跟CSK是同一个团队提出的，它跟CSK的区别是就是作者对循环性质进行了完整的理论推导，引入HOG特征并提供了一种把多通道特征融合进相关滤波框架的方法，对CSK作了进一步的完善，是一个具有里程碑意义的工作。算法的详解和一些数学理论可以看看我之前的文章。</p><h3 id="DCF"><a href="#DCF" class="headerlink" title="DCF"></a>DCF</h3><p>DCF与KCF出自同一篇paper，不同的是KCF使用的是高斯核，DCF使用的是线性核。</p><blockquote><p>注意：这里的DCF（Dual Correlation Filter）和之后一些文章中提到的DCF（Discriminative Correlation Filter）是两个不同的概念，请注意，别搞错了。</p></blockquote><h3 id="CN"><a href="#CN" class="headerlink" title="CN"></a>CN</h3><p>CN（Color Name）考虑到在遇到光照变化、形变、部分遮挡、背景干扰等问题时，颜色特征相比灰度特征能提供更丰富的信息以取得更好的效果，引入了颜色特征来扩展CSK，它将目标RGB（红绿蓝）三维空间的颜色特征映射为黑、蓝、棕、灰、绿、橙、粉、紫、红、白和黄11维空间的颜色特征的多通道颜色特征，后又降维至2维以保证实时性。Color Name较RGB三原色特征更符合人类的感觉，对目标的表征能力更强，而且具有一定的光学不变性。</p><h3 id="FCT"><a href="#FCT" class="headerlink" title="FCT"></a>FCT</h3><p>FCT（Fast Compressive Tracking）和CT一样，也是使用了压缩感知，主打速度。相比之前，FCT提出了一种由粗到精的搜索策略，而不是穷尽搜索。首先在一个较大的搜索半径内选择一个较大的搜索步长，得到一个粗糙的位置，然后以该位置为中心，在一个较小的搜索范围内，以一个较小的搜索步长进行搜索，最后得到跟踪目标的位置，这样就能在不降低最终精度的前提下加速寻找过程。由于可证明CT特征具有尺度不变特性，FCT在采集候选区域时增加了尺度因子，即在同一位置采集三个尺度的候选区域，从而得到当前帧的尺度。此外，作者采用了每5帧更新一次尺度的策略。</p><h2 id="2015"><a href="#2015" class="headerlink" title="2015"></a>2015</h2><p>下面是VOT2015的排名结果。<br><img src="/computer-vision20200215214240/VOT2015.png" title="VOT2015"></p><h3 id="SO-DLT"><a href="#SO-DLT" class="headerlink" title="SO-DLT"></a>SO-DLT</h3><p>SO-DLT针对DLT的缺陷进行改进，使得CNN更加适用于目标跟踪。由于目标跟踪的目的是将物体从背景中分离出来而不是全图识别，DLT的训练方法和标签化输出就不是很合适了。<br>但是，由于当时跟踪方向标注数据的匮乏，作者还是不得不使用ImageNet图像检测数据集来进行预训练，事实证明这是有效的，因为目标检测和目标跟踪两个不同的task中存在一样的共性信息。不同于标签化的单个数值输出，SO-DLT输出的是一个50x50的像素级的概率图。<br>由于上述训练方法训练的是CNN从非物体中提取出物体的能力，因此在实际跟踪接收到第一帧时，还需根据目标对网络进行微调，否则会跟踪出视频或者图像序列中所有的无论是目标与否的物体。<br>类似DSST，SO-DLT采用的是先预测目标中心位置，然后再从小到大确定尺度的策略，如若扩展到预设的最大尺度来检测的概率图依旧达不到要求，则认为已经跟丢目标。<br>此外，为了提升鲁棒性，作者采用了两个CNN网络共同决策而以不同方式更新的策略。两个网络分别针对的是short-term和long-term。针对short-term的网络在负样本的概率图响应和超过一定阈值时进行更新，为的是防止负样本与目标响应近似而导致漂移；针对long-term的网络在当前帧预测结果的置信度达到一定水平以上时才进行更新，因为此时可认为框出的目标较为可信。<br>每次更新时需要采集正负样本，SO-DLT对正负样本的提取方法比较简单，在目标位置及周围形成一个类似九宫格的区域，在中间格用四种尺度提取正样本，对周围的8格采集负样本。</p><h3 id="MDNet"><a href="#MDNet" class="headerlink" title="MDNet"></a>MDNet</h3><p>MDNet设计了一个轻量级的小型网络学习卷积特征表示目标。作者提出了一个多域的网络框架，将一个视频序列视为一个域，其中共享的部分用来学习目标的特征表达，独立的全连接层则用于学习针对特定视频序列的softmax分类。<br>在离线训练时，针对每个视频序列构建一个新的检测分支进行训练，而特征提取网络是共享的。这样特征提取网络可以学习到通用性更强的与域无关的特征。<br>在跟踪时，保留并固定特征提取网络，针对跟踪序列构建一个新的分支检测部分，用第1帧样本在线训练检测部分之后再利用跟踪结果生成正负样本来微调检测分支。<br>此外，MDNet在训练时还采用了难例挖掘技术，随着训练的进行增大样本的分类难度。<br><img src="/computer-vision20200215214240/难例挖掘.png" title="难例挖掘"></p><h3 id="SRDCF"><a href="#SRDCF" class="headerlink" title="SRDCF"></a>SRDCF</h3><p>SRDCF主要考虑到若仅使用单纯的相关滤波，可能会存在边界效应，也就是相关滤波采用循环移位采样导致除了中心样本以外的其他样本中都会存在边界，这就导致了大部分样本都不合理，同时也会限制检测区域。<br>于是，作者采用了大的检测区域，在滤波器系数上加入权重约束（类似于惩罚项）：越靠近边缘权重越大，越靠近中心权重越小。这就使得滤波器系数主要集中在中心区域，从而让边界的影响没有那么明显。SRDCF最终的效果不错，但是速度比较缓慢。</p><h3 id="DeepSRDCF"><a href="#DeepSRDCF" class="headerlink" title="DeepSRDCF"></a>DeepSRDCF</h3><p>DeepSRDCF在SRDCF的基础上，将handcrafted的特征换为CNN的特征，关注点也在解决边界效应。作者还对不同的特征进行了实验，说明了CNN特征在解决跟踪的问题采取底层的特征效果会比较好（文中仅用了PCA降维处理的第一层），说明了跟踪问题并不需要太高的语义信息。</p><h3 id="HCF"><a href="#HCF" class="headerlink" title="HCF"></a>HCF</h3><p>HCF的主要贡献是把相关滤波中的HOG特征换成了深度特征，它使用的是VGG的3、4、5三个层来提取特征，针对每层CNN训练一个过滤器，并且按照从深到浅的顺序使用相关滤波，然后利用深层得到的结果来引导浅层从而减少搜索空间。</p><h3 id="FCNT"><a href="#FCNT" class="headerlink" title="FCNT"></a>FCNT</h3><p>FCNT较早地利用CNN网络底层和顶层不同的表达效果来做跟踪。不同于以往的工作把CNN看成一个黑盒而不关注不同层的表现，FCNT关注了不同层的功能，即发现：顶层的CNN layer编码了更多的关于语义特征的信息并且可以作为类别检测器；而底层的CNN layer关注了更多局部特征，这有助于将目标从目标中分离出来。这个发现在之后的许多工作中也得到了应用和体现。如下图所示，这里的a图表示的是ground truth，b图表示的是使用VGG的conv4-3，也就是第10层产生的热力图，c图是通过conv5-3也就是第13层产生的热力图。<br><img src="/computer-vision20200215214240/热力图.jpg" title="热力图"><br>可以看到，较低维的CNN layer（conv4-3）能够更精准地表示目标的细粒度信息，而较高维的CNN layer（conv5-3）热力图显示较模糊，但对同类别的人也做出了响应。这就是说，顶层缺少类内特征区分，对类间识别比较好，更适合作语义分割；底层则反之，能够更好地表达目标的类内特征和位置信息。<br>基于不同层（顶层和底层）之间提取特征的不同，作者提出了一种新的tracking方法，利用两种特征相互补充辅助，来处理剧烈的外观变化（顶层特征发挥的作用）和区分目标本身（底层特征发挥的作用）。由于feature map本身是有内在结构的，有很多的feature map对目标的表达其实并没有起到作用，因此作者设计了一种方法来自动选择高维CNN（GNet）或者低维CNN（SNet）上的feature map，同时忽略另一个feature map和噪声。在线跟踪时，两个网络一起跟踪，采用不同的更新策略，并在不同的情况下选择不同的网络输出来进行预测。<br>顺便提一下，为了简化学习任务，降低模型复杂度，作者采用了稀疏表示的方法。<br>关于FCNT的一些相关概念和具体按步骤的细节实现，可以参考一下我之前写的文章。</p><h3 id="LCT"><a href="#LCT" class="headerlink" title="LCT"></a>LCT</h3><p>LCT主要针对的是long-term tracking的问题。作者配置了一个detector，用于跟丢之后快速重检测。LCT用了两个滤波器，一个是用于平移估算的$R_{c}$，使用padding并施加汉宁窗（一种余弦窗），结合了FHOG和一些其他的特征；另一个是用于尺度估计的$R_{t}$，不使用padding和汉宁窗，使用HOG特征，此外$R_{t}$还用于检测置信度，用来决定是否更新模型和是否重检测。</p><h3 id="CCT"><a href="#CCT" class="headerlink" title="CCT"></a>CCT</h3><p>CCT借鉴KCF中kernel trick的特性和DSST中将定位和尺度估计两步分离的思想，对DSST只利用本来的特征空间表征目标的不足进行改进，在核特征空间对目标进行表征并用尺度因子扩展KCF的核相关滤波器，也就是为了保证计算效率和连贯性，利用尺度因子将每一帧目标尺度都统一为初始帧的目标尺度，然后使用核相关滤波器进行位置估计。然后，通过和DSST一样的方式再进行尺度估计。<br>为了应对漂移的问题，CCT使用了一个在线CUR滤波器。CUR矩阵分解可以近似的表示原矩阵A，其中C是A的列而R是A的行，两者通过一种固定的方式从A中随机采样形成，这既保证了A的内在结构，可以反映A的低秩属性，也可以看作一种映射，即将过去的目标表征矩阵投影到一个可被证明具有误差上界的子空间。此外，作者还引入了一个基于失败检测的自适应学习率调整方法。</p><h3 id="CFLB"><a href="#CFLB" class="headerlink" title="CFLB"></a>CFLB</h3><p>CFLB讨论了循环移位带来的边界效应的问题，提出了在目标外围扩大尺寸来进行循环移位，使得有效样本的比例大大提高。此外，由于扩大尺寸后部分参数要在空间域而不是频域计算会导致效率降低，作者利用了增广拉格朗日方法（即在拉格朗日方法的基础上添加了二次惩罚项，从而使得转换后的问题能够更容易求解，不至于因为条件数变大不好求）来解决这个问题。</p><h2 id="2016"><a href="#2016" class="headerlink" title="2016"></a>2016</h2><p>下面是VOT2016的排名结果。<br><img src="/computer-vision20200215214240/VOT2016.png" title="VOT2016"></p><h3 id="DLSSVM"><a href="#DLSSVM" class="headerlink" title="DLSSVM"></a>DLSSVM</h3><p>DLSSVM延续之前的Struck，利用结构化SVM，在优化的阶段做了一些改进进行提速。其实结构化SVM分类器非常强大，但是因为它求解优化的过程比较复杂以及使用稠密采样（粒子滤波或者滑窗采样）比较耗时，使得结构化SVM的速度成为一个瓶颈，因此不如一些使用相关滤波的SOTA的算法。</p><h3 id="C-COT"><a href="#C-COT" class="headerlink" title="C-COT"></a>C-COT</h3><p>C-COT（连续空间域卷积操作）考虑到单一分辨率的输出结果存在扰动，仅使用单一分辨率的特征图是限制之前DCF系列算法效果的重要因素，因此作者将浅层表观信息和深层语义信息结合起来，利用不同空间分辨率（包含0.96，0.98，1.00，1.02，1.04五种缩放倍率）的响应，在频域进行插值得到连续空间分辨率的响应图，最后通过迭代求解最佳位置和尺度。<br>文中提出了一种使得各个分辨率通道的特征自然融合至相同分辨率的方法，这里相同分辨率可理解为最后的各个响应图在空间上拥有相同的样本点数。作者首先用一个插值运算符对各个不同分辨率的通道进行插值，然后使用对应的滤波器在连续的空间域内卷积，最后将响应求和得到最终的置信度方程。<br>C-COT使用的是类似SRDCF的框架，也引入了空间正则项，当远离目标中心是施加较大的惩罚，这使得能够通过控制滤波器的大小来学习任意大小的图像区域。C-COT在每一帧也会采集一个训练样本，根据过去帧数的远近来设置每个采集样本的重要性权重（每次都做归一化），并且设置了最大的样本容量，当超出容量时删去重要性权值最小的样本。不同于SRDCF使用Gauss-Seidel迭代法，C-COT使用Conjugate Gradient方法来提高效率。<br>得益于浅层特征的高分辨率，C-COT能够达到sub-pixel的精度，也就是仅次于像素级别的精确度。位置细化的过程就是上面所说的用共轭梯度法迭代的过程，在C-COT的代码中有一个迭代次数设置，被设置为1，即就使用一步迭代优化后的位置。换句话说，在当前长时跟踪算法本身误差之下，更精细的位置意义不大。</p><h3 id="SRDCFdecon"><a href="#SRDCFdecon" class="headerlink" title="SRDCFdecon"></a>SRDCFdecon</h3><p>SRDCFdecon针对在线跟踪时采集的样本中有一部分质量不佳的问题，不同于之前把采样和样本的选择作为一个独立的模块，作者提出了一种将样本权重统一到模型参数的损失函数。<br>不同于之前“加入训练集or舍弃”这样二选一的样本选取方式，SRDCFdecon使得样本的重要性权重连续，同时在跟踪的过程中能够完成权重的重新分配和先验的动态变化。</p><h3 id="Staple"><a href="#Staple" class="headerlink" title="Staple"></a>Staple</h3><p>Staple提出了一种互补的方式。考虑到HOG特征对形变和运动模糊比较敏感，但是对颜色变化能够达到很好的跟踪效果，color特征对颜色比较敏感，但是对形变和运动模糊能够有很好的跟踪效果，因此作者认为若能将两者互补就能够解决跟踪过程当中遇到的一些主要问题。于是，Staple使用HOG-KCF与color-KCF结合算法对目标进行跟踪，速度很快，效果也很好。</p><h3 id="SINT"><a href="#SINT" class="headerlink" title="SINT"></a>SINT</h3><p>SINT运用匹配学习的思想，最早地把孪生网络（Siamese Network）应用于目标跟踪。它通过孪生网络直接学习目标模板和候选目标的匹配函数，并且在online tracking的过程中只用初始帧的目标作为模板来实现跟踪。</p><h3 id="TCNN"><a href="#TCNN" class="headerlink" title="TCNN"></a>TCNN</h3><p>TCNN使用一个树形的结构来处理CNN特征。作者利用可靠性来分配预测目标的权重，采用的更新策略是每10帧删除最前的节点，同时创建一个新的CNN节点，选择能够使新节点的可靠性最高的节点作为其父节点。这样一直保持一个active set，里面是10个最新更新的CNN模型，用这个active set来做跟踪。TCNN效果较之前有一定提升，但是速度比较慢，而且比较消耗存储空间。<br><img src="/computer-vision20200215214240/TCNN.jpg" title="TCNN"></p><h2 id="2017"><a href="#2017" class="headerlink" title="2017"></a>2017</h2><p>下面是VOT2017在隐藏数据集上的排名结果。<br><img src="/computer-vision20200215214240/VOT2017.png" title="VOT2017"></p><h3 id="ECO"><a href="#ECO" class="headerlink" title="ECO"></a>ECO</h3><p>ECO（高效卷积算子）主要是为了解决C-COT速度慢的问题。通过卷积因式分解操作、样本分组和更新策略对其改进，在不影响算法精确度的同时，将算法速度提高了一个数量级。<br>为了减少模型参数，作者提出了一个因子化的卷积算子，其效果类似PCA，用PCA初始化，然后仅在第一帧有监督地优化这个降维矩阵，在之后的帧中直接使用，相比C-COT模型参数量大大降低，降低了过拟合地风险。<br><img src="/computer-vision20200215214240/卷积因式分解大大降低参数量.png" title="卷积因式分解大大降低参数量"><br>为了减少样本数量，作者提出了一个紧凑的样本空间生成模型，采用高斯混合模型（GMM，可理解为当有多个聚类时用多个不同的高斯模型来表示更好）来合并相似样本，建立更具代表性和多样性的样本集，既保持样本之间的差异性，也减少了存储的样本数量。<br><img src="/computer-vision20200215214240/样本分组.png" title="样本分组"><br>此外，作者还提出了一种稀疏的更新策略，即每隔N帧（实验发现5帧左右最好）才更新一次参数。这样做不但提高了算法速度，而且提高了算法在目标物体突变、遮挡等情况下的稳定性。由于样本集是每帧更新的，这种稀疏更新策略并不会错过间隔期的样本变化信息。</p><h3 id="CREST"><a href="#CREST" class="headerlink" title="CREST"></a>CREST</h3><p>CREST提出了将DCF构建成一层卷积神经网络，并且引入了残差学习来应对目标外观变化带来的模型退化。<br>考虑到之前的DCF系列没有发挥端到端训练的优势和空间卷积与相关滤波中循环输入点乘的相似性，作者用一层卷积神经网络来代替DCF的作用，这不仅使得模型能够通过反向传播训练，同时还避免了边界效应。<br>由于上述一层网络难以达成在多种情况下网络输出和ground truth的一致（模型复杂度较低易受干扰），而若使用多层网络很可能会导致模型退化（我理解为过拟合导致的），作者引入空间残差和时间残差。设我们希望最佳的输出为$H(x)$，而上述单层网络的输出是$F_{B}(x)$，为了补足某些时候（尤其是复杂情况下）单层网络的输出与希望最佳的输出之间的差距，引入残差项$F_{R}(x)=H(x)-F_{B}(x)$。在训练时，$F_{B}(x)$和$F_{R}(x)$中的参数一起训练，使得遇到特殊情况（遮挡、运动模糊等）时，$F_{R}(x)$能够补足纠正$F_{B}(x)$不稳定的响应结果。<br>空间残差和单层网络都是利用当前帧作为输入，考虑到空间残差有时候也会失效，作者又引入了把初始帧作为输入的时间残差，最终表达式如下：</p><script type="math/tex;mode=display">F(X_{t})=F_{B}(X_{t})+F_{SR}(X_{t})+F_{TR}(X_{1})</script><blockquote><p>注意：论文中第一项为$F_{R}(X_{t})$，可能有误，为此我作了修改。</p></blockquote><p>另外，CREST采用当前帧最大响应尺度和上一帧尺度加权求和的方法来决定当前帧的最终预测尺度，从而使尺度能够平滑地变化。</p><h3 id="LMCF"><a href="#LMCF" class="headerlink" title="LMCF"></a>LMCF</h3><p>LMCF借鉴了KCF的循环特征图、Struck的结构化SVM，使用相关滤波来解决之前结构化SVM系列算法（Struck、DLSSVM）的速度问题。<br>在前向追踪时，LMCF考虑到画面中相似物体的干扰，提出了一种多峰值的目标跟踪算法（Multimodal Target Tracking），即对高于某一阈值的响应峰值做二次检测，把response map和一个用于筛选的二值矩阵作点乘，相当于把不是峰值的位置滤为0。<br>在模型更新时，LMCF提出了一种高置信度的更新策略（High-confidence Update），由于LMCF主要关注的是实时性，所以希望在算法简单的情况下能够减少失误。在传统的方法中，一般是当最大响应的峰值高于某一个阈值时（认为没跟丢目标），就对模型进行更新；否则若没有响应值超过峰值，就不对模型进行更新。而该工作的实验发现，当目标被遮挡时，响应图会震荡得非常厉害（存在多个较大的峰值），但同时最大响应的峰值仍旧会很高，这就会指导模型进行错误的更新并导致最后跟丢目标。于是作者提出了一个APCE值，定义如下。</p><script type="math/tex;mode=display">APCE=\frac{\left | F_{max}-F_{min} \right |^{2}}{mean(\underset{w,h}{\sum }(F_{w,h}-F_{min})^{2})}</script><p>只有当最大响应的峰值比较明确，即远超response map中的其他的响应时，APCE值才会比较大，此时允许对模型进行更新。</p><h3 id="DeepLMCF"><a href="#DeepLMCF" class="headerlink" title="DeepLMCF"></a>DeepLMCF</h3><p>同LMCF，不同之处是使用了CNN特征。</p><h3 id="MCPF"><a href="#MCPF" class="headerlink" title="MCPF"></a>MCPF</h3><p>MCPF结合多任务相关滤波器（MCF）和粒子滤波器，这里的多任务相关滤波器指的是利用了多种特征滤波器之间的相关性。作者对K种特征，定义了参数$z_{k}$去选择具有判别力的训练样本。作者发现，各个特征中的$z_{k}$往往会选择具有相同循环移位的样本，因此不同的$z_{k}$应该具有相似性和一致性。为此，作者在损失函数中增加了矩阵Z的混和范数。<br>考虑到粒子滤波通过密集采样来覆盖状态空间中的所有状态，这会大大增加计算量，而且并不能保证很好地包括目标物体在一些情况下的状态。因此作者利用MCF对每个采样的粒子进行引导，使其更接近目标的状态分布。<br>算法的流程分为四步：<br>首先，使用转移模型生成粒子并且重采样。<br>然后，使用MCF对粒子进行处理，使其转移到比较合适的位置。<br>接着，利用响应更新MCF的参数。<br>最后，通过求样本均值问题来决定目标的状态，也就是位置等参数。<br>顺便一提，MCPF使用了Accelerated Proximal Gradient来解决这里不可微分的凸优化问题（含有范数）。</p><h3 id="CFNet"><a href="#CFNet" class="headerlink" title="CFNet"></a>CFNet</h3><p>CFNet结合相关滤波的高效性和CNN的判别力，考虑到端到端训练的优势，从理论对相关滤波在CNN中的应用进行了推导，并将相关滤波改写成可微分的神经网络层，将特征提取网络整合到一起以实现端到端优化，从而训练与相关滤波器相匹配的卷积特征。<br>CFNet采用孪生网络的架构，训练样本（这里指用来匹配的模板）和测试样本（搜索的图像区域）通过一个相同的网络，然后只将训练样本做相关滤波操作，形成一个对变化有鲁棒性的模板。为了抑制边界效应，作者施加了余弦窗并在之后又对训练样本进行了裁剪。<br>在对比实验中作者发现仅使用一层卷积层时CFNet相比Baseline+CF效果提升最显著，对此作者的解释是可以把相关滤波层理解为测试时的先验知识编码，当获得足够的数据和容量时（增加CNN层数时），这个先验知识就会变得冗余甚至是过度限制。</p><h2 id="2018"><a href="#2018" class="headerlink" title="2018"></a>2018</h2><p>下面是VOT2018 short-term的排名结果。<br><img src="/computer-vision20200215214240/VOT2018.png" title="VOT2018"></p><h3 id="STRCF"><a href="#STRCF" class="headerlink" title="STRCF"></a>STRCF</h3><p>STRCF（时空正则相关滤波器）主要针对SRDCF的速度做出改进，同时在精度上也有很好的提高。作者发现SRDCF速度很慢的两个原因是：每次对多张图片进行训练打破了循环矩阵的结构，从而无法发挥循环矩阵的计算优势；巨大的线性方程组和Gauss-Seidel迭代法没有闭式解，效率较低。对此，STRCF提出了引入时间正则和ADMM算法。<br>受online Passive-Aggressive learning的启发，STRCF在SRDCF空间正则的基础上引入了时间正则。我们可以对比两者的回归求解公式具体来看一下。<br>SRDCF：</p><script type="math/tex;mode=display">arg\underset{f}{min}\sum_{k=1}^{T}a_{k}\left \| \sum_{d=1}^{D}x_{k}^{d}\ast f^{d}-y_{k} \right \|^{2}+\sum_{d=1}^{D}\left \| w\cdot f^{d} \right \|^{2}</script><p>STRCF：</p><script type="math/tex;mode=display">arg\underset{f}{min}\frac{1}{2}\left \| \sum_{d=1}^{D}x_{t}^{d}\ast f^{d}-y \right \|^{2}+\frac{1}{2}\sum_{d=1}^{D}\left \| w\cdot f^{d} \right \|^{2}+\frac{\mu }{2}\left \| f-f_{t-1} \right \|^{2}</script><p>这里的$w$表示空间正则化矩阵，越靠近边缘值越大；$f$表示相关滤波器，$f_{t-1}$表示的是$t-1$帧时的滤波器。<br>忽略每项之前的常熟系数，我们可以看到两式的第二项是一样的，也就是STRCF保留了SRDCF的空间正则来抑制边界效应；在第一项中，STRCF没有对过去的每一帧进行求和来训练，这就减小了计算量；同时STRCF加入了第三项时间正则，使得新得到的滤波器与之前的滤波器之间的变化尽可能小，相当于保留了之前的信息。<br>这么做有两点好处：首先，STRCF可以看作SRDCF的一个合理近似，能很好地发挥后者同样的作用；此外，由于时间正则的引入，使得STRCF不易于在当前帧上过拟合，在遇到遮挡或者超出画面等问题时，STRCF能很好地保持与之前滤波器的相似度从而降低了跟踪器完全跟丢到另一个物体上去的可能，这一定程度上提高了STRCF的精度。<br><img src="/computer-vision20200215214240/STRCF和SRDCF对比.png" title="STRCF和SRDCF对比"><br>此外，ADMM算法的引入使得最优化求解问题有了闭式解，这比Gauss-Seidel迭代法用稀疏矩阵求解要快得多。得益于SRDCF的凸性，ADMM也能收敛到全局最优点。</p><h3 id="UPDT"><a href="#UPDT" class="headerlink" title="UPDT"></a>UPDT</h3><p>UPDT区别对待深度特征和浅层特征，主要考虑的是缺少数据和深层卷积在增加语义的同时降低分辨率这两个问题。作者分析了数据增强（flip，rotation，shift，blur，dropout）和鲁棒性训练（也就是tracker应对各种复杂场景和恢复的能力，可以通过扩大正样本的采样范围来训练）对deep feature和shallow feature分别的影响，发现deep feature能通过数据增强来提升效果，同时deep feature主打的是鲁棒性而不是精度；相反，shallow feature经数据增强后反而降低了效果，但同时它能够很好地保证精度。因此，作者得出了深度模型和浅层模型应该独立训练，最后再融合的方案。<br>作者在文中还定义了Prediction Quality Measure，考虑了精度和鲁棒性，精度用响应分数的锋利程度（sharpness）来体现，鲁棒性则用响应值的幅度来表示，幅度越高表明tracker越确信跟踪的目标，也就是鲁棒性越高。关于具体公式的推导和分析，以及Prediction Quality Measure在预测过程中的具体使用可以看一看原文。</p><h3 id="ACT"><a href="#ACT" class="headerlink" title="ACT"></a>ACT</h3><p>ACT使用了强化学习，构建了由Actor和Critic组成的学习框架。离线训练时，通过Critic指导Actor进行强化学习；在线跟踪时，使用Actor来定位，Critic进行验证使得tracker更加鲁棒。不同于之前的搜索方案（随机采样或者通过一系列分离的action来定位），ACT希望的是搜索一步到位。这步最优的action也就是离线强化学习所关注的行动，而强化学习的状态由输入到网络中bounding box中框出的图片定义，奖励值根据IoU来定义。<br>在训练的过程中，由于action space比较大，因此要获得一个正奖励比较困难（随机采取action的话IoU恰好高于阈值的可能性较小）。因此作者利用了第一帧的信息来初始化Actor以适应新的环境。同样的，由于巨大的action space，原本DDPG方法中的噪声引入就不适合跟踪任务了，因此在训练前期，Actor采取的行动以某种概率被一种专家决策所替代。随着训练的进行，Actor越来越强大，这时就逐渐减弱专家决策的指导作用。<br>在跟踪的过程中，若Critic的给分大于0，则采用Actor的输出一步到位地预测下一帧的目标；否则，使用Critic在上一帧周围采集的样本中选出最优作为目标，完成重定向。此外，可以认为Actor在离线训练时已经比较稳定了，因此在跟踪过程中只对Critic进行更新，且仅在Critic给分小于0（认为Critic没能很好地适应目标的变化）时，取前十帧的样本来更新Critic。</p><h3 id="DRT"><a href="#DRT" class="headerlink" title="DRT"></a>DRT</h3><p>DRT引入了稳定性的概念，考虑到空间正则、掩模等抑制边界效应的方法都不能抑制bounding box内部的背景信息，同时这些方法会导致feature map上的响应集中在某些较小的区域，作者认为这是不利于目标跟踪的（容易被误导）。为此，作者提出了DRT，它主要是将滤波器分成了一个base filter和一个reliability term的element-wise product：</p><script type="math/tex;mode=display">w_{d}=h_{d}\odot v_{d}</script><p>这里base filter是用于区分目标和背景的；reliability term用于决定每片区域的reliability，由目标区域每一个patch加权求和决定：</p><script type="math/tex;mode=display">v_{d}=\sum_{m=1}^{M}\beta _{m}P_{d}^{m}</script><p>这里的$\beta_{m}$有上下界的限定，目的就是为了降低feature map中响应不平衡的影响，防止由于响应的集中而导致仅有一小块区域被关注。</p><h3 id="MCCT"><a href="#MCCT" class="headerlink" title="MCCT"></a>MCCT</h3><p>MCCT使用了多特征集成学习，在跟踪时对每一帧分别选用最合适的特征来做出决策。为了应对不同的场景，MCCT选择了low，middle，high三个层级的特征，并通过排列组合得出7种expert。尽管有些特征的鲁棒性明显差于三类特征的组合，但是它们提供的多样性对集成学习是至关重要的。<br><img src="/computer-vision20200215214240/MCCT特征.png" title="MCCT特征"><br>为了评估每个expert在每一帧的好坏以决定具体选用哪一个，作者提出了Expert Pair-Evaluation和Expert Self-Evaluation。<br>Expert Pair-Evaluation分为两项：在第一项中，作者认为一个expert的好坏可以通过它与其他expert的整体一致性来体现，于是首先计算了每个expert相对于其他6个expert在当前帧预测结果的一致性（通过重叠率来衡量）之和；此外，作者认为一个好的expert还必须是temporal stable的，因此他又计算了每个expert相对于其他6个expert在前几帧内预测趋势的一致性，这就可以防止因为在当前帧碰巧预测一致而导致之前一项的分值很好的情况，也保证了expert的可信度。最后两项结合得到Expert Pair-Evaluation。<br>在Expert Self-Evaluation中，作者认为路径的顺滑程度一定程度上能够体现每个expert的可靠程度。<br>最后将Expert Pair-Evaluation和Expert Self-Evaluation加权求和选出每帧最好的expert做出决策。<br>MCCT提出了一种peak-to-sidelobe ratio和鲁棒性的置信度分数来进行模型更新：</p><script type="math/tex;mode=display">S^{t}=P_{mean}^{t}\cdot R_{mean}^{t}</script><p>其中，$P_{mean}^{t}$是每个expert响应图peak-to-sidelobe ratio的平均，$R_{mean}^{t}$亦然。当$R_{mean}^{t}$比较低时，认为采集到了不可靠的样本（比如遮挡问题等）。为此，作者的模型更新策略是，当置信度分数$S^{t}$大于之前置信度均值时，采用正常学习率，否则，根据置信度算出一个较小的学习率以在一定程度上维持模型。<br>为了提升速度，每个expert之间共享了样本和RoI，最后MCCT的速度为7.8FPS，MCCT-H（没采用深度特征）的速度为44.8FPS。（作为参考，ECO的速度为15FPS）<br>顺便提一下，MCCT使用了汉宁窗来抑制边界效应。</p><h3 id="LSART"><a href="#LSART" class="headerlink" title="LSART"></a>LSART</h3><p>LSART分析了深度特征中的空间信息，提出了两种互补的回归方式来使得跟踪更加鲁棒。<br>作者首先对比了CNN-based和KRR-based（核岭回归）两类tracker，认为它们各有利弊且是互补的。由于KRR的循环采样，目标的结构特征会被打破，对形变和而CNN则能够很好地提取位置信息；相反，CNN庞大的参数量使得它容易过拟合，而KRR-based tracker就不会出现这样的问题。因此，若将两者结合（将热力图加权求和），就可以让KRR关注全局而让CNN关注较小、较精确的目标，进而达到更好的效果。<br>对于KRR，作者引入cross-patch similarity，将参数看作训练样本的加权求和，将响应项拆分成三项，这就方便把原本的迭代求解的方式设计成神经网络来求解了。<br>对于CNN，考虑到形变和遮挡等问题会使得目标的一部分比其他区域更加重要，不同于以往在feature map上做文章，作者对卷积层的滤波器施加掩模，使得各个滤波器关注于不同的区域，在跟踪的过程中，这些掩模不做变化。此外，作者还提出了距离变换池化层用于评判输入feature map的可靠性。另外，作者设计了一种two-stream的训练网络，将空间正则的卷积层和距离变换池化层分开训练以防止过拟合，能够比较好的处理旋转问题。</p><h3 id="DaSiamRPN"><a href="#DaSiamRPN" class="headerlink" title="DaSiamRPN"></a>DaSiamRPN</h3><p>DaSiamRPN在之前的孪生网络系列的基础上增加了distractor-aware，这里的distractor指的是在判别式方法中，不同于无语义信息易判别的背景，而存在一定的语义并对前景分割存在干扰的背景。这其中的一大原因是之前的训练集仅从同一个视频序列的不同帧中采样，造成了non-semantic的背景样本具有较大的比重而semantic的背景样本较少，这就弱化了模型准确判别前景的能力。此外，之前的孪生网络系列还存在不能在线更新和不进行全局搜索这两个问题。<br>首先，作者提出了三类样本选取方法来弥补传统采样的不足。考虑到视频数据集中类别缺乏和标注的难度，作者引入了ImageNet和COCO图像检测两个数据集，并把样本分成三类对tracker进行训练。<br><img src="/computer-vision20200215214240/三种样本选取方法.png" title="三种样本选取方法"><br>对于正样本对，其作用是提升tracker的泛化能力和回归精度；对于来自同一类别的样本对，其作用是让tracker更注重细粒度的表达方式，提升判别能力；对于来自不同类别的样本对，其作用是让tracker在遮挡、超出视野等情况下拥有更好的鲁棒性。<br>值得一提，作者发现motion pattern能很好地被浅层网络建模，因此在数据增强时还引入了运动模糊。<br>DaSiamRPN通过上述方法对数据做了增强，可是在跟踪特定目标时，还是很难将一般模型转化为特定视频域所用。考虑到上下文信息和时域信息可以提供特定目标的信息以增加tracker的判别能力，作者提出了一个distractor-aware module。具体来说，在上一帧中选择出的proposal中，通过非极大抑制处理，剩下的proposal中最大的就是目标，剩下的就是会产生误导的distractor；在当前帧，为了抑制这些distractor的干扰，可以减去这些distractor之前响应的加权和，减去之后还是最大的proposal就是我们要找的目标，其基本思想如下公式所示：</p><script type="math/tex;mode=display">q=\underset{p_{k}\in P}{argmax}f\left ( z,p_{k} \right )-\frac{\widehat{\alpha }\sum_{i=1}^{n}\alpha _{i}f\left ( d_{i},p_{k} \right )}{\sum_{i=1}^{n}a_{i}}</script><p>这里的$\alpha$是控制distractor影响大小的权重系数，作者又对上式进行调整，通过引入学习率使得该分类器在线可学习，这就无需利用反向传播更新网络参数，而通过微调一个分类器弥补了传统基于孪生网络的tracker不能在线更新的缺点。<br>此外，当认为目标跟丢时，DaSiamRPN会匀速扩大搜索范围，并且通过高效的bounding box回归来代替图像金字塔，这就通过一个简单的方法在应对长时跟踪目标消失问题时较之前基于孪生网络的tracker取得了一个进步。</p><h3 id="Meta-Tracker"><a href="#Meta-Tracker" class="headerlink" title="Meta-Tracker"></a>Meta-Tracker</h3><p>Meta-Tracker将元学习运用在了目标模型的初始化上。作者认为结合深度特征和在线学习的模型有两大困难，一是训练的样本不容易获得，二是大多数SOTA的tracker在训练阶段都需要花费大量的时间在初始化上面。<br>针对上面的难题，作者提出了一种在未来的帧上训练目标模型的思路，采用了基于预测梯度的策略学习方法获得普适性的初始化模型，使得跟踪模型自适应于后续帧特征的最佳梯度方向，从而在接收到第一帧时仅需一步迭代就能使参数快速收敛到合适的位置。这样做有三点好处，一是能使模型更加关注对后续的帧更有价值的特征，二是避免了在当前帧上过拟合，三是能够使初始化更快速。总而言之，就是能保证精度和鲁棒性。<br>考虑到上述方法在长序列或者目标在帧与帧之间变化不大时表现不佳（会偏离目标），这是因为Meta-Tracker一步到位的思想使得学习率会偏大。因此作者仅在模型初始化时采用学习到的学习率，在之后的跟踪过程中仍旧沿用原来版本的方式进行更新。这里原来的版本指的是CREST和MDNet，作者在这两个tracker的基础上改进出了MetaCREST和MetaSDNet。具体的改进和处理可以看一看我之前写过关于Meta-Tracker的文章。</p><h3 id="DorT"><a href="#DorT" class="headerlink" title="DorT"></a>DorT</h3><p>DorT（Detect or Track）把跟踪看作一个连续决策的过程，它结合目标检测和目标跟踪两个领域内SOTA的结果，在孪生网络输出的结果上再添加一个小型的CNN网络作为scheduler来判断在下一帧是作检测还是作跟踪。</p><h3 id="LADCF"><a href="#LADCF" class="headerlink" title="LADCF"></a>LADCF</h3><p>LADCF针对DCF系列的边界效应和模型退化（后者主要是单帧独立学习和模型更新速率固定导致）的问题，提出了一种空间域特征选择和时间域约束结合的方法，并且使其能在低维流形中有效表示。</p><blockquote><p>补充：流形学习的观点认为，我们所能观察到的数据实际上是由一个低维流形映射到高维空间上的。由于数据内部特征的限制，一些高维中的数据会产生维度上的冗余，实际上只需要比较低的维度就能唯一地表示。</p></blockquote><p>掩模策略应用于目标跟踪时，仅将目标区域的参数激活。LADCF也运用了这个思想，对滤波器中的参数$\theta$作降维处理$\theta _{\phi }=diag(\phi )\theta$，这里$\phi$中的元素要么是0、要么是1，即不激活或者激活。不同于PCA和LLE，这种方法在降维的同时也保持了空间特性，不仅能加速求解，也能除去大部分干扰，使滤波器关注于目标部分从而可以使用更大的搜索域。<br>最后的目标函数如下：</p><script type="math/tex;mode=display">arg\underset{\theta }{min}\left \| \theta \circledast x-y \right \|_{2}^{2}+\lambda _{1}\left \| \theta \right \|_{1}+\lambda _{2}\left \| \theta -\theta _{model} \right \|_{2}^{2}</script><p>可以看到，这里还包括与历史模型的正则项，减轻了滤波器退化。作者让$\lambda _{1}&lt; &lt; \lambda _{2}$，也就是让时间域上的一致性更加重要于特征的稀疏选取。</p><h2 id="2019"><a href="#2019" class="headerlink" title="2019"></a>2019</h2><p>下面是VOT2019 short-term的排名结果。<br><img src="/computer-vision20200215214240/VOT2019.png" title="VOT2019"></p><h3 id="GFS-DCF"><a href="#GFS-DCF" class="headerlink" title="GFS-DCF"></a>GFS-DCF</h3><p>GFS-DCF考虑到深度网络的高维通道存在许多冗余的信息，因此作者在时域和空间域之外，还考虑了通道维度的影响，在目标函数中使用了三个正则项。<br><img src="/computer-vision20200215214240/考虑三项正则.png" title="考虑三项正则"></p><blockquote><p>注：建议结合上图来看接下来的分析。</p></blockquote><p>对于空间域，作者将每个通道（也就是每个feature map）的对应点相连接，用范数约束，可以理解为提取那些在绝大多数特征图中都是最重要的特征的位置。</p><script type="math/tex;mode=display">R_{S}(W)=\sum_{i=1}^{N}\sum_{j=1}^{N}\left \| w_{ij:} \right \|_{2}</script><p>从通道角度，作者又把每一个通道作为一项来做约束，可以理解为提取那些特征比较重要的通道。</p><script type="math/tex;mode=display">R_{C}(W)=\sum_{k=1}^{C}\left \| W^{k} \right \|_{F}</script><p>对于时域，作者使用了low-rank约束，这里的rank指的是矩阵的秩而不是排名，low-rank主要用于图像对齐（alignment），在文中的目标是最小化$rank(W_{t})-rank(W_{t-1})$，这里的$W_{t}$表示从1到t每个滤波器向量化后形成的矩阵。下面是作者最后修改后的正则项：</p><script type="math/tex;mode=display">R_{T}(W)=\sum_{k=1}^{C}\left \| W_{t}^{k}-W_{t-1}^{k} \right \|_{F}^{2}</script><blockquote><p>注：上述三项在实际目标函数中还要加上权重。</p></blockquote><p>作者发现，空间正则对使用handcrafted特征的模型效果显著，而对使用CNN的模型（文中是ResNet）效果提升不大；相反，通道正则对使用handcrafted特征的模型效果不明显，而对使用CNN的模型效果显著。作者在文中解释认为由于深层CNN特征表示的语义信息丰富而缺少细粒度的信息，因此相比保留更多空间结构handcrafted特征，对深层CNN特征使用空间正则比较难以判别哪些位置的特征反应了目标位置的信息。此外，由于在训练过程中一些通道的权重下降到很小，也就是说模型本身就不怎么关注这些通道，因此使用通道正则在这里取得了比较明显的效果。</p><hr><h1 id="研究趋势"><a href="#研究趋势" class="headerlink" title="研究趋势"></a>研究趋势</h1><p>以下是我对近几年来目标跟踪领域各种算法主流的研究趋势和发展方向的一个浅析，个人思考，多多指教。</p><blockquote><p>注：本小节施工中…</p></blockquote><h2 id="信息提取"><a href="#信息提取" class="headerlink" title="信息提取"></a>信息提取</h2><h3 id="深度特征（2013-）"><a href="#深度特征（2013-）" class="headerlink" title="深度特征（2013-）"></a>深度特征（2013-）</h3><p>早期的目标跟踪算法主要在handcrafted特征方面进行探索和改进，以2012年AlexNet问世为节点，深度特征开始被引入目标跟踪领域。<br>我们知道，在现实场景中，物体是在三维的运动场中移动的。而视频或图像序列都是二维的信息，这其实是一些难题的根本原因之一。一个比较极端的例子就是理发店门前经常会出现的旋转柱，如果单纯地从二维角度来看，柱子是向上运动的，可在实际的运动场中柱子是横向运动的，观测和实际的运动方向是完全垂直的。<br><img src="/computer-vision20200215214240/发廊旋转柱.png" title="发廊旋转柱"><br>因此，为了能够更好地跟踪目标，我们需要提取尽可能好的特征，此外最好能从视频或图像序列中学到更多丰富的信息（尤其是含语义的）。值得注意的一点是，在港科大王乃岩博士2015年所做的ablation experiments中（详见参考文献[2]），发现特征提取是影响tracker效果最重要的因素。<br>考虑到精度是保证目标跟踪鲁棒性的重要因素，不同于一些其他的计算机视觉任务，目标跟踪领域的深度算法比较强调结合与充分利用浅层网络的高分辨率信息。</p><h3 id="时域和空间域结合（2013-）"><a href="#时域和空间域结合（2013-）" class="headerlink" title="时域和空间域结合（2013-）"></a>时域和空间域结合（2013-）</h3><p>事实上，目标跟踪这一个任务本身就在利用时域信息，因为预测下一帧肯定需要上一帧的信息，然而仅仅利用上一帧的信息往往是不够的，充分的利用时域信息在正则或者辅助记忆方面都可以取得一定的效果。</p><h2 id="学习方式"><a href="#学习方式" class="headerlink" title="学习方式"></a>学习方式</h2><h3 id="孪生网络（2016-）"><a href="#孪生网络（2016-）" class="headerlink" title="孪生网络（2016-）"></a>孪生网络（2016-）</h3><p>孪生网络采用的是匹配学习的思想。</p><h3 id="结合语义分割的多任务学习（2017-）"><a href="#结合语义分割的多任务学习（2017-）" class="headerlink" title="结合语义分割的多任务学习（2017-）"></a>结合语义分割的多任务学习（2017-）</h3><p>由于bounding box粗糙的对目标的标注表示使得有不少冗余的背景信息进入template，此外bounding box对平面内旋转等场景不鲁棒。由于跟踪的目标是有具体形状的且是一起运动的，同时判别式方法正是要分离除目标以外的物体。因此近年来有不少算法结合语义分割，通过多任务学习来进一步提高tracker的效果。具体来说，就是引入掩模（mask）来识别预测物体，最后在转化成bounding box作为结果。</p><h3 id="强化学习（2017-）"><a href="#强化学习（2017-）" class="headerlink" title="强化学习（2017-）"></a>强化学习（2017-）</h3><h3 id="元学习（2018-）"><a href="#元学习（2018-）" class="headerlink" title="元学习（2018-）"></a>元学习（2018-）</h3><h3 id="对抗网络（2018-）"><a href="#对抗网络（2018-）" class="headerlink" title="对抗网络（2018-）"></a>对抗网络（2018-）</h3><h2 id="新关注点"><a href="#新关注点" class="headerlink" title="新关注点"></a>新关注点</h2><h3 id="样本采集（2015-）"><a href="#样本采集（2015-）" class="headerlink" title="样本采集（2015-）"></a>样本采集（2015-）</h3><p>样本采集主要包括样本的数量、样本的有效性、正负样本和难易样本的平衡性。</p><h3 id="防止过拟合（2017-）"><a href="#防止过拟合（2017-）" class="headerlink" title="防止过拟合（2017-）"></a>防止过拟合（2017-）</h3><blockquote><p>注：其实近几年还出现了一些其他的关注方向，由于不是主流、目前关注较少、本人学识不够等原因，在此不做列举。</p></blockquote><script type="text/javascript" src="/js/src/bai.js"></script></div><div><div><hr style="FILTER:progid:DXImageTransform.Microsoft.Shadow(color:#987cb9,direction:145,strength:15)" width="100%" color="#987cb9" size="1"><div style="text-align:center;color:#555;font-size:16px"><i class="fa fa-hand-o-up" aria-hidden="true"></i> 碰到底线咯 <i class="fa fa-handshake-o" aria-hidden="true"></i> 后面没有啦 <i class="fa fa-hand-o-down" aria-hidden="true"></i></div></div></div><div><div class="my_post_copyright"><script src="//cdn.bootcss.com/clipboard.js/1.5.10/clipboard.min.js"></script><script src="https://cdn.bootcss.com/jquery/2.0.0/jquery.min.js"></script><script src="https://unpkg.com/sweetalert/dist/sweetalert.min.js"></script><p><span>本文标题:</span><a href="/computer-vision20200215214240/">computer vision笔记：目标跟踪的小总结</a></p><p><span>文章作者:</span><a href="/" title="访问 高深远 的个人博客">高深远</a></p><p><span>发布时间:</span>2020年02月15日 - 21:42</p><p><span>最后更新:</span>2020年03月12日 - 10:16</p><p><span>原始链接:</span><a href="/computer-vision20200215214240/" title="computer vision笔记：目标跟踪的小总结">https://gsy00517.github.io/computer-vision20200215214240/</a> <span class="copy-path" title="点击复制文章链接"><i class="fa fa-clipboard" data-clipboard-text="https://gsy00517.github.io/computer-vision20200215214240/" aria-label="复制成功！"></i></span></p><p><span>许可协议:</span><i class="fa fa-creative-commons"></i> <a rel="license" href="https://creativecommons.org/licenses/by-nc-nd/4.0/" target="_blank" title="Attribution-NonCommercial-NoDerivatives 4.0 International (CC BY-NC-ND 4.0)">署名-非商业性使用-禁止演绎 4.0 国际</a> 转载请保留原文链接及作者。</p></div><script>var clipboard=new Clipboard(".fa-clipboard");$(".fa-clipboard").click(function(){clipboard.on("success",function(){swal({title:"复制成功",text:"感谢您的阅读与参考！欢迎留下任何建议噢！",icon:"success",showConfirmButton:!0})})})</script></div><footer class="post-footer"><div class="post-tags"><a href="/tags/论文分享/" rel="tag"><i class="fa fa-tag"></i> 论文分享</a> <a href="/tags/计算机视觉/" rel="tag"><i class="fa fa-tag"></i> 计算机视觉</a> <a href="/tags/目标跟踪/" rel="tag"><i class="fa fa-tag"></i> 目标跟踪</a></div><div class="post-widgets"><div class="wp_rating"><div id="wpac-rating"></div></div></div><div class="post-nav"><div class="post-nav-next post-nav-item"><a href="/vot-toolkit20200215185238/" rel="next" title="vot toolkit笔记：解决无法连接TraX支持的问题"><i class="fa fa-chevron-left"></i> vot toolkit笔记：解决无法连接TraX支持的问题</a></div><span class="post-nav-divider"></span><div class="post-nav-prev post-nav-item"><a href="/anaconda20200222000018/" rel="prev" title="anaconda笔记：解决conda无法下载pytorch的问题">anaconda笔记：解决conda无法下载pytorch的问题 <i class="fa fa-chevron-right"></i></a></div></div></footer></div></article><div class="post-spread"></div></div></div><div class="comments" id="comments"></div></div><div class="sidebar-toggle"><div class="sidebar-toggle-line-wrap"><span class="sidebar-toggle-line sidebar-toggle-line-first"></span> <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span> <span class="sidebar-toggle-line sidebar-toggle-line-last"></span></div></div><aside id="sidebar" class="sidebar"><div class="sidebar-inner"><ul class="sidebar-nav motion-element"><li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap">文章目录</li><li class="sidebar-nav-overview" data-target="site-overview-wrap">站点概览</li></ul><section class="site-overview-wrap sidebar-panel"><div class="site-overview"><div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person"><img class="site-author-image" itemprop="image" src="/images/lufei.jpg" alt="高深远"><p class="site-author-name" itemprop="name">高深远</p><p class="site-description motion-element" itemprop="description"></p></div><nav class="site-state motion-element"><div class="site-state-item site-state-posts"><a href="/archives/"><span class="site-state-item-count">84</span> <span class="site-state-item-name">日志</span></a></div><div class="site-state-item site-state-categories"><a href="/categories/index.html"><span class="site-state-item-count">7</span> <span class="site-state-item-name">分类</span></a></div><div class="site-state-item site-state-tags"><a href="/tags/index.html"><span class="site-state-item-count">33</span> <span class="site-state-item-name">标签</span></a></div></nav><div class="feed-link motion-element"><a href="/atom.xml" rel="alternate"><i class="fa fa-rss"></i> RSS</a></div><div class="links-of-blogroll motion-element links-of-blogroll-inline"><div class="links-of-blogroll-title"><i class="fa fa-fw fa-link"></i> 网站收藏</div><ul class="links-of-blogroll-list"><li class="links-of-blogroll-item"><a href="https://www.runoob.com/" title="菜鸟教程" target="_blank">菜鸟教程</a></li><li class="links-of-blogroll-item"><a href="https://paperswithcode.com/" title="PaperWithCode" target="_blank">PaperWithCode</a></li><li class="links-of-blogroll-item"><a href="https://www.jiqizhixin.com/sota" title="机器之心" target="_blank">机器之心</a></li><li class="links-of-blogroll-item"><a href="http://pytorch123.com/" title="Pytorch中文文档" target="_blank">Pytorch中文文档</a></li></ul></div></div></section><section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active"><div class="post-toc"><div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#简介与要求"><span class="nav-number">1.</span> <span class="nav-text">简介与要求</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#问题及挑战"><span class="nav-number">2.</span> <span class="nav-text">问题及挑战</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#生成式与判别式"><span class="nav-number">3.</span> <span class="nav-text">生成式与判别式</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#机器学习"><span class="nav-number">3.1.</span> <span class="nav-text">机器学习</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#目标跟踪"><span class="nav-number">3.2.</span> <span class="nav-text">目标跟踪</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#算法导图"><span class="nav-number">4.</span> <span class="nav-text">算法导图</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#各类算法的梳理与简述"><span class="nav-number">5.</span> <span class="nav-text">各类算法的梳理与简述</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#1981"><span class="nav-number">5.1.</span> <span class="nav-text">1981</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#LK-Tracker"><span class="nav-number">5.1.1.</span> <span class="nav-text">LK Tracker</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#1994"><span class="nav-number">5.2.</span> <span class="nav-text">1994</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#KLT"><span class="nav-number">5.2.1.</span> <span class="nav-text">KLT</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#1998"><span class="nav-number">5.3.</span> <span class="nav-text">1998</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Condensation"><span class="nav-number">5.3.1.</span> <span class="nav-text">Condensation</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#2002"><span class="nav-number">5.4.</span> <span class="nav-text">2002</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Mean-Shift"><span class="nav-number">5.4.1.</span> <span class="nav-text">Mean Shift</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#2008"><span class="nav-number">5.5.</span> <span class="nav-text">2008</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#IVT"><span class="nav-number">5.5.1.</span> <span class="nav-text">IVT</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#2010"><span class="nav-number">5.6.</span> <span class="nav-text">2010</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#MOSSE"><span class="nav-number">5.6.1.</span> <span class="nav-text">MOSSE</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#TLD"><span class="nav-number">5.6.2.</span> <span class="nav-text">TLD</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#2011"><span class="nav-number">5.7.</span> <span class="nav-text">2011</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#L1-Tracker"><span class="nav-number">5.7.1.</span> <span class="nav-text">L1 Tracker</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Struck"><span class="nav-number">5.7.2.</span> <span class="nav-text">Struck</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#2012"><span class="nav-number">5.8.</span> <span class="nav-text">2012</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#CSK"><span class="nav-number">5.8.1.</span> <span class="nav-text">CSK</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#CT"><span class="nav-number">5.8.2.</span> <span class="nav-text">CT</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#2013"><span class="nav-number">5.9.</span> <span class="nav-text">2013</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#DLT"><span class="nav-number">5.9.1.</span> <span class="nav-text">DLT</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#STC"><span class="nav-number">5.9.2.</span> <span class="nav-text">STC</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#2014"><span class="nav-number">5.10.</span> <span class="nav-text">2014</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#DSST"><span class="nav-number">5.10.1.</span> <span class="nav-text">DSST</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#SAMF"><span class="nav-number">5.10.2.</span> <span class="nav-text">SAMF</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#KCF"><span class="nav-number">5.10.3.</span> <span class="nav-text">KCF</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#DCF"><span class="nav-number">5.10.4.</span> <span class="nav-text">DCF</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#CN"><span class="nav-number">5.10.5.</span> <span class="nav-text">CN</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#FCT"><span class="nav-number">5.10.6.</span> <span class="nav-text">FCT</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#2015"><span class="nav-number">5.11.</span> <span class="nav-text">2015</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#SO-DLT"><span class="nav-number">5.11.1.</span> <span class="nav-text">SO-DLT</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#MDNet"><span class="nav-number">5.11.2.</span> <span class="nav-text">MDNet</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#SRDCF"><span class="nav-number">5.11.3.</span> <span class="nav-text">SRDCF</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#DeepSRDCF"><span class="nav-number">5.11.4.</span> <span class="nav-text">DeepSRDCF</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#HCF"><span class="nav-number">5.11.5.</span> <span class="nav-text">HCF</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#FCNT"><span class="nav-number">5.11.6.</span> <span class="nav-text">FCNT</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#LCT"><span class="nav-number">5.11.7.</span> <span class="nav-text">LCT</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#CCT"><span class="nav-number">5.11.8.</span> <span class="nav-text">CCT</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#CFLB"><span class="nav-number">5.11.9.</span> <span class="nav-text">CFLB</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#2016"><span class="nav-number">5.12.</span> <span class="nav-text">2016</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#DLSSVM"><span class="nav-number">5.12.1.</span> <span class="nav-text">DLSSVM</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#C-COT"><span class="nav-number">5.12.2.</span> <span class="nav-text">C-COT</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#SRDCFdecon"><span class="nav-number">5.12.3.</span> <span class="nav-text">SRDCFdecon</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Staple"><span class="nav-number">5.12.4.</span> <span class="nav-text">Staple</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#SINT"><span class="nav-number">5.12.5.</span> <span class="nav-text">SINT</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#TCNN"><span class="nav-number">5.12.6.</span> <span class="nav-text">TCNN</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#2017"><span class="nav-number">5.13.</span> <span class="nav-text">2017</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#ECO"><span class="nav-number">5.13.1.</span> <span class="nav-text">ECO</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#CREST"><span class="nav-number">5.13.2.</span> <span class="nav-text">CREST</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#LMCF"><span class="nav-number">5.13.3.</span> <span class="nav-text">LMCF</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#DeepLMCF"><span class="nav-number">5.13.4.</span> <span class="nav-text">DeepLMCF</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#MCPF"><span class="nav-number">5.13.5.</span> <span class="nav-text">MCPF</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#CFNet"><span class="nav-number">5.13.6.</span> <span class="nav-text">CFNet</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#2018"><span class="nav-number">5.14.</span> <span class="nav-text">2018</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#STRCF"><span class="nav-number">5.14.1.</span> <span class="nav-text">STRCF</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#UPDT"><span class="nav-number">5.14.2.</span> <span class="nav-text">UPDT</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#ACT"><span class="nav-number">5.14.3.</span> <span class="nav-text">ACT</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#DRT"><span class="nav-number">5.14.4.</span> <span class="nav-text">DRT</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#MCCT"><span class="nav-number">5.14.5.</span> <span class="nav-text">MCCT</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#LSART"><span class="nav-number">5.14.6.</span> <span class="nav-text">LSART</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#DaSiamRPN"><span class="nav-number">5.14.7.</span> <span class="nav-text">DaSiamRPN</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Meta-Tracker"><span class="nav-number">5.14.8.</span> <span class="nav-text">Meta-Tracker</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#DorT"><span class="nav-number">5.14.9.</span> <span class="nav-text">DorT</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#LADCF"><span class="nav-number">5.14.10.</span> <span class="nav-text">LADCF</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#2019"><span class="nav-number">5.15.</span> <span class="nav-text">2019</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#GFS-DCF"><span class="nav-number">5.15.1.</span> <span class="nav-text">GFS-DCF</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#研究趋势"><span class="nav-number">6.</span> <span class="nav-text">研究趋势</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#信息提取"><span class="nav-number">6.1.</span> <span class="nav-text">信息提取</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#深度特征（2013-）"><span class="nav-number">6.1.1.</span> <span class="nav-text">深度特征（2013-）</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#时域和空间域结合（2013-）"><span class="nav-number">6.1.2.</span> <span class="nav-text">时域和空间域结合（2013-）</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#学习方式"><span class="nav-number">6.2.</span> <span class="nav-text">学习方式</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#孪生网络（2016-）"><span class="nav-number">6.2.1.</span> <span class="nav-text">孪生网络（2016-）</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#结合语义分割的多任务学习（2017-）"><span class="nav-number">6.2.2.</span> <span class="nav-text">结合语义分割的多任务学习（2017-）</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#强化学习（2017-）"><span class="nav-number">6.2.3.</span> <span class="nav-text">强化学习（2017-）</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#元学习（2018-）"><span class="nav-number">6.2.4.</span> <span class="nav-text">元学习（2018-）</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#对抗网络（2018-）"><span class="nav-number">6.2.5.</span> <span class="nav-text">对抗网络（2018-）</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#新关注点"><span class="nav-number">6.3.</span> <span class="nav-text">新关注点</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#样本采集（2015-）"><span class="nav-number">6.3.1.</span> <span class="nav-text">样本采集（2015-）</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#防止过拟合（2017-）"><span class="nav-number">6.3.2.</span> <span class="nav-text">防止过拟合（2017-）</span></a></li></ol></li></ol></li></ol></div></div></section></div></aside></div></main><footer id="footer" class="footer"><div class="footer-inner"><div class="copyright">&copy; <span itemprop="copyrightYear">2020</span> <span class="with-love"><i class="fa fa-user"></i> </span><span class="author" itemprop="copyrightHolder">高深远</span> <span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-area-chart"></i> </span><span class="post-meta-item-text">Site words total count&#58;</span> <span title="Site words total count">163.4k</span></div><script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><span id="busuanzi_container_site_pv">访客已留下<span id="busuanzi_value_site_pv"></span>个脚印 </span><span id="busuanzi_container_site_uv">你是第<span id="busuanzi_value_site_uv"></span>位小伙伴</span></div></footer><div class="back-to-top"><i class="fa fa-arrow-up"></i> <span id="scrollpercent"><span>0</span>%</span></div></div><script type="text/javascript">"[object Function]"!==Object.prototype.toString.call(window.Promise)&&(window.Promise=null)</script><script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script><script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script><script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script><script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script><script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script><script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script><script type="text/javascript" src="/lib/three/three.min.js"></script><script type="text/javascript" src="/lib/three/three-waves.min.js"></script><script type="text/javascript" src="/js/src/utils.js?v=5.1.4"></script><script type="text/javascript" src="/js/src/motion.js?v=5.1.4"></script><script type="text/javascript" src="/js/src/affix.js?v=5.1.4"></script><script type="text/javascript" src="/js/src/schemes/pisces.js?v=5.1.4"></script><script type="text/javascript" src="/js/src/scrollspy.js?v=5.1.4"></script><script type="text/javascript" src="/js/src/post-details.js?v=5.1.4"></script><script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.4"></script><script src="//cdn1.lncld.net/static/js/3.0.4/av-min.js"></script><script src="//unpkg.com/valine/dist/Valine.min.js"></script><script type="text/javascript">var GUEST = ['nick','mail','link'];
    var guest = 'nick,mail,link';
    guest = guest.split(',').filter(item=>{
      return GUEST.indexOf(item)>-1;
    });
    new Valine({
        el: '#comments' ,
        verify: false,
        notify: false,
        appId: 'SVKPF4oiwJLMtpySQxvJLnLm-gzGzoHsz',
        appKey: '46OfvvEe65XMeUi79STU895I',
        placeholder: '动动手指，写下您的意见、疑惑或者鼓励吧！留下您的邮箱，这样就可以收到别人的回复啦！',
        avatar:'wavatar',
        guest_info:guest,
        pageSize:'10' || 10,
    });

    var infoEle = document.querySelector('#comments .info');
    if (infoEle && infoEle.childNodes && infoEle.childNodes.length > 0){
    infoEle.childNodes.forEach(function(item) {
    item.parentNode.removeChild(item);
    });
  }</script><script type="text/javascript">function proceedsearch(){$("body").append('<div class="search-popup-overlay local-search-pop-overlay"></div>').css("overflow","hidden"),$(".search-popup-overlay").click(onPopupClose),$(".popup").toggle();var t=$("#local-search-input");t.attr("autocapitalize","none"),t.attr("autocorrect","off"),t.focus()}var isfetched=!1,isXml=!0,search_path="search.xml";0===search_path.length?search_path="search.xml":/json$/i.test(search_path)&&(isXml=!1);var path="/"+search_path,onPopupClose=function(t){$(".popup").hide(),$("#local-search-input").val(""),$(".search-result-list").remove(),$("#no-result").remove(),$(".local-search-pop-overlay").remove(),$("body").css("overflow","")},searchFunc=function(t,e,o){"use strict";$("body").append('<div class="search-popup-overlay local-search-pop-overlay"><div id="search-loading-icon"><i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i></div></div>').css("overflow","hidden"),$("#search-loading-icon").css("margin","20% auto 0 auto").css("text-align","center"),$.ajax({url:t,dataType:isXml?"xml":"json",async:!0,success:function(t){isfetched=!0,$(".popup").detach().appendTo(".header-inner");var n=isXml?$("entry",t).map(function(){return{title:$("title",this).text(),content:$("content",this).text(),url:$("url",this).text()}}).get():t,r=document.getElementById(e),s=document.getElementById(o),a=function(){var t=r.value.trim().toLowerCase(),e=t.split(/[\s\-]+/);e.length>1&&e.push(t);var o=[];if(t.length>0&&n.forEach(function(n){function r(e,o,n,r){for(var s=r[r.length-1],a=s.position,i=s.word,l=[],h=0;a+i.length<=n&&0!=r.length;){i===t&&h++,l.push({position:a,length:i.length});var p=a+i.length;for(r.pop();0!=r.length&&(s=r[r.length-1],a=s.position,i=s.word,p>a);)r.pop()}return c+=h,{hits:l,start:o,end:n,searchTextCount:h}}function s(t,e){var o="",n=e.start;return e.hits.forEach(function(e){o+=t.substring(n,e.position);var r=e.position+e.length;o+='<b class="search-keyword">'+t.substring(e.position,r)+"</b>",n=r}),o+=t.substring(n,e.end)}var a=!1,i=0,c=0,l=n.title.trim(),h=l.toLowerCase(),p=n.content.trim().replace(/<[^>]+>/g,""),u=p.toLowerCase(),f=decodeURIComponent(n.url),d=[],g=[];if(""!=l&&(e.forEach(function(t){function e(t,e,o){var n=t.length;if(0===n)return[];var r=0,s=[],a=[];for(o||(e=e.toLowerCase(),t=t.toLowerCase());(s=e.indexOf(t,r))>-1;)a.push({position:s,word:t}),r=s+n;return a}d=d.concat(e(t,h,!1)),g=g.concat(e(t,u,!1))}),(d.length>0||g.length>0)&&(a=!0,i=d.length+g.length)),a){[d,g].forEach(function(t){t.sort(function(t,e){return e.position!==t.position?e.position-t.position:t.word.length-e.word.length})});var v=[];0!=d.length&&v.push(r(l,0,l.length,d));for(var $=[];0!=g.length;){var C=g[g.length-1],m=C.position,x=C.word,w=m-20,y=m+80;0>w&&(w=0),y<m+x.length&&(y=m+x.length),y>p.length&&(y=p.length),$.push(r(p,w,y,g))}$.sort(function(t,e){return t.searchTextCount!==e.searchTextCount?e.searchTextCount-t.searchTextCount:t.hits.length!==e.hits.length?e.hits.length-t.hits.length:t.start-e.start});var T=parseInt("1");T>=0&&($=$.slice(0,T));var b="";b+=0!=v.length?"<li><a href='"+f+"' class='search-result-title'>"+s(l,v[0])+"</a>":"<li><a href='"+f+"' class='search-result-title'>"+l+"</a>",$.forEach(function(t){b+="<a href='"+f+'\'><p class="search-result">'+s(p,t)+"...</p></a>"}),b+="</li>",o.push({item:b,searchTextCount:c,hitCount:i,id:o.length})}}),1===e.length&&""===e[0])s.innerHTML='<div id="no-result"><i class="fa fa-search fa-5x" /></div>';else if(0===o.length)s.innerHTML='<div id="no-result"><i class="fa fa-frown-o fa-5x" /></div>';else{o.sort(function(t,e){return t.searchTextCount!==e.searchTextCount?e.searchTextCount-t.searchTextCount:t.hitCount!==e.hitCount?e.hitCount-t.hitCount:e.id-t.id});var a='<ul class="search-result-list">';o.forEach(function(t){a+=t.item}),a+="</ul>",s.innerHTML=a}};r.addEventListener("input",a),$(".local-search-pop-overlay").remove(),$("body").css("overflow",""),proceedsearch()}})};$(".popup-trigger").click(function(t){t.stopPropagation(),isfetched===!1?searchFunc(path,"local-search-input","local-search-result"):proceedsearch()}),$(".popup-btn-close").click(onPopupClose),$(".popup").click(function(t){t.stopPropagation()}),$(document).on("keyup",function(t){var e=27===t.which&&$(".search-popup").is(":visible");e&&onPopupClose()})</script><script src="https://cdn1.lncld.net/static/js/av-core-mini-0.6.4.js"></script><script>AV.initialize("SVKPF4oiwJLMtpySQxvJLnLm-gzGzoHsz","46OfvvEe65XMeUi79STU895I")</script><script>function showTime(e){var t=new AV.Query(e),n=[],o=$(".leancloud_visitors");o.each(function(){n.push($(this).attr("id").trim())}),t.containedIn("url",n),t.find().done(function(e){var t=".leancloud-visitors-count";if(0===e.length)return void o.find(t).text(0);for(var i=0;i<e.length;i++){var r=e[i],s=r.get("url"),l=r.get("time"),c=document.getElementById(s);$(c).find(t).text(l)}for(var i=0;i<n.length;i++){var s=n[i],c=document.getElementById(s),u=$(c).find(t);""==u.text()&&u.text(0)}}).fail(function(e,t){console.log("Error: "+t.code+" "+t.message)})}function addCount(e){var t=$(".leancloud_visitors"),n=t.attr("id").trim(),o=t.attr("data-flag-title").trim(),i=new AV.Query(e);i.equalTo("url",n),i.find({success:function(t){if(t.length>0){var i=t[0];i.fetchWhenSave(!0),i.increment("time"),i.save(null,{success:function(e){var t=$(document.getElementById(n));t.find(".leancloud-visitors-count").text(e.get("time"))},error:function(e,t){console.log("Failed to save Visitor num, with error message: "+t.message)}})}else{var r=new e,s=new AV.ACL;s.setPublicReadAccess(!0),s.setPublicWriteAccess(!0),r.setACL(s),r.set("title",o),r.set("url",n),r.set("time",1),r.save(null,{success:function(e){var t=$(document.getElementById(n));t.find(".leancloud-visitors-count").text(e.get("time"))},error:function(e,t){console.log("Failed to create")}})}},error:function(e){console.log("Error:"+e.code+" "+e.message)}})}$(function(){var e=AV.Object.extend("Counter");1==$(".leancloud_visitors").length?addCount(e):$(".post-title-link").length>1&&showTime(e)})</script><script>!function(){var t=document.createElement("script"),s=window.location.protocol.split(":")[0];"https"===s?t.src="https://zz.bdstatic.com/linksubmit/push.js":t.src="http://push.zhanzhang.baidu.com/push.js";var e=document.getElementsByTagName("script")[0];e.parentNode.insertBefore(t,e)}()</script><script type="text/javascript">wpac_init=window.wpac_init||[],wpac_init.push({widget:"Rating",id:21228,el:"wpac-rating",color:"fc6423"}),function(){if(!("WIDGETPACK_LOADED"in window)){WIDGETPACK_LOADED=!0;var e=document.createElement("script");e.type="text/javascript",e.async=!0,e.src="//embed.widgetpack.com/widget.js";var t=document.getElementsByTagName("script")[0];t.parentNode.insertBefore(e,t.nextSibling)}}()</script><script type="text/x-mathjax-config">MathJax.Hub.Config({
        tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
          processEscapes: true,
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
        }
      });</script><script type="text/x-mathjax-config">MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for (i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
        }
      });</script><script type="text/javascript" src="//cdn.bootcss.com/mathjax/2.7.1/latest.js?config=TeX-AMS-MML_HTMLorMML"></script><canvas class="fireworks" style="position:fixed;left:0;top:0;z-index:1;pointer-events:none"></canvas><script type="text/javascript" src="//cdn.bootcss.com/animejs/2.2.0/anime.min.js"></script><script type="text/javascript" src="/js/src/fireworks.js"></script><script src="/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({pluginRootPath:"live2dw/",pluginJsPath:"lib/",pluginModelPath:"assets/",tagMode:!0,log:!1,model:{jsonPath:"/live2dw/assets/hijiki.model.json"},display:{position:"left",width:250,height:500},mobile:{show:!1},react:{opacity:.7}})</script></body></html><script type="text/javascript" src="/js/src/crash_cheat.js"></script><!-- rebuild by neat -->