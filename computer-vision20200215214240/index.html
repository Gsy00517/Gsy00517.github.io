<!-- build time:Fri Apr 10 2020 20:18:25 GMT+0800 (GMT+08:00) --><!DOCTYPE html><html class="theme-next gemini use-motion" lang="zh-Hans"><head><meta name="generator" content="Hexo 3.9.0"><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width,initial-scale=1,maximum-scale=1"><meta name="theme-color" content="#222"><meta name="google-site-verification" content="YV24rdmIIf8GuLLOBH5IYEWm0Z3TGAqiLS-LLlspD7w"><script src="/lib/pace/pace.min.js?v=1.0.2"></script><link href="/lib/pace/pace-theme-big-counter.min.css?v=1.0.2" rel="stylesheet"><meta http-equiv="Cache-Control" content="no-transform"><meta http-equiv="Cache-Control" content="no-siteapp"><meta name="baidu-site-verification" content="true"><link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css"><link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css"><link href="/css/main.css?v=5.1.4" rel="stylesheet" type="text/css"><link rel="apple-touch-icon" sizes="180x180" href="/images/haizei.ico?v=5.1.4"><link rel="icon" type="image/png" sizes="32x32" href="/images/haizei.ico?v=5.1.4"><link rel="icon" type="image/png" sizes="16x16" href="/images/haizei.ico?v=5.1.4"><link rel="mask-icon" href="/images/haizei.ico?v=5.1.4" color="#222"><meta name="keywords" content="论文分享,计算机视觉,目标跟踪,"><link rel="alternate" href="/atom.xml" title="高深远的博客" type="application/atom+xml"><meta name="description" content="看了一寒假的目标跟踪，一直想将自己学习到的内容整理归纳一下，迟迟没动笔（其实是打字，但说迟迟没打字比较难听哈哈）。如今快要开学了，决定还是积累一下。本文主要是把目标跟踪中的一些相关要点做一个总结，并且按具体问题对一些主流的或者我阅读过觉得有价值的算法做一个简单概述。近年来各类方法层出不穷，且码字不易，无法涵盖所有的方法。此外，由于包含自己的转述和理解可能会存在错误。在今后的学习过程中会一直保持本文"><meta name="keywords" content="论文分享,计算机视觉,目标跟踪"><meta property="og:type" content="article"><meta property="og:title" content="computer vision笔记：目标跟踪的小总结"><meta property="og:url" content="https://gsy00517.github.io/computer-vision20200215214240/index.html"><meta property="og:site_name" content="高深远的博客"><meta property="og:description" content="看了一寒假的目标跟踪，一直想将自己学习到的内容整理归纳一下，迟迟没动笔（其实是打字，但说迟迟没打字比较难听哈哈）。如今快要开学了，决定还是积累一下。本文主要是把目标跟踪中的一些相关要点做一个总结，并且按具体问题对一些主流的或者我阅读过觉得有价值的算法做一个简单概述。近年来各类方法层出不穷，且码字不易，无法涵盖所有的方法。此外，由于包含自己的转述和理解可能会存在错误。在今后的学习过程中会一直保持本文"><meta property="og:locale" content="zh-Hans"><meta property="og:image" content="https://gsy00517.github.io/computer-vision20200215214240/目标跟踪.png"><meta property="og:image" content="https://gsy00517.github.io/computer-vision20200215214240/目标跟踪伪代码.png"><meta property="og:image" content="https://gsy00517.github.io/computer-vision20200215214240/问题.png"><meta property="og:image" content="https://gsy00517.github.io/computer-vision20200215214240/OTB对问题的划分.png"><meta property="og:image" content="https://gsy00517.github.io/computer-vision20200215214240/生成式模型.png"><meta property="og:image" content="https://gsy00517.github.io/computer-vision20200215214240/判别式模型.png"><meta property="og:image" content="https://gsy00517.github.io/computer-vision20200215214240/论文中的导图.png"><meta property="og:image" content="https://gsy00517.github.io/computer-vision20200215214240/王强的导图.png"><meta property="og:image" content="https://gsy00517.github.io/computer-vision20200215214240/王蒙蒙的导图.png"><meta property="og:image" content="https://gsy00517.github.io/computer-vision20200215214240/光流.gif"><meta property="og:image" content="https://gsy00517.github.io/computer-vision20200215214240/画出光流.png"><meta property="og:image" content="https://gsy00517.github.io/computer-vision20200215214240/Haar特征池.jpg"><meta property="og:image" content="https://gsy00517.github.io/computer-vision20200215214240/cell.png"><meta property="og:image" content="https://gsy00517.github.io/computer-vision20200215214240/FoT.png"><meta property="og:image" content="https://gsy00517.github.io/computer-vision20200215214240/VOT2013.png"><meta property="og:image" content="https://gsy00517.github.io/computer-vision20200215214240/LGT.png"><meta property="og:image" content="https://gsy00517.github.io/computer-vision20200215214240/STC.png"><meta property="og:image" content="https://gsy00517.github.io/computer-vision20200215214240/VOT2014.png"><meta property="og:image" content="https://gsy00517.github.io/computer-vision20200215214240/VOT2015.png"><meta property="og:image" content="https://gsy00517.github.io/computer-vision20200215214240/难例挖掘.png"><meta property="og:image" content="https://gsy00517.github.io/computer-vision20200215214240/热力图.jpg"><meta property="og:image" content="https://gsy00517.github.io/computer-vision20200215214240/HCFT结构.png"><meta property="og:image" content="https://gsy00517.github.io/computer-vision20200215214240/MUSTer结构.png"><meta property="og:image" content="https://gsy00517.github.io/computer-vision20200215214240/VOT2016.png"><meta property="og:image" content="https://gsy00517.github.io/computer-vision20200215214240/TCNN.jpg"><meta property="og:image" content="https://gsy00517.github.io/computer-vision20200215214240/VOT2017.png"><meta property="og:image" content="https://gsy00517.github.io/computer-vision20200215214240/卷积因式分解大大降低参数量.png"><meta property="og:image" content="https://gsy00517.github.io/computer-vision20200215214240/样本分组.png"><meta property="og:image" content="https://gsy00517.github.io/computer-vision20200215214240/VOT2018.png"><meta property="og:image" content="https://gsy00517.github.io/computer-vision20200215214240/STRCF和SRDCF对比.png"><meta property="og:image" content="https://gsy00517.github.io/computer-vision20200215214240/MCCT特征.png"><meta property="og:image" content="https://gsy00517.github.io/computer-vision20200215214240/one-shot检测.png"><meta property="og:image" content="https://gsy00517.github.io/computer-vision20200215214240/三种样本选取方法.png"><meta property="og:image" content="https://gsy00517.github.io/computer-vision20200215214240/VOT2019.png"><meta property="og:image" content="https://gsy00517.github.io/computer-vision20200215214240/考虑三项正则.png"><meta property="og:image" content="https://gsy00517.github.io/computer-vision20200215214240/D3S.png"><meta property="og:image" content="https://gsy00517.github.io/computer-vision20200215214240/GlobalTrack.png"><meta property="og:image" content="https://gsy00517.github.io/computer-vision20200215214240/SPSTracker.png"><meta property="og:image" content="https://gsy00517.github.io/computer-vision20200215214240/发廊旋转柱.png"><meta property="og:updated_time" content="2020-04-10T12:11:47.140Z"><meta name="twitter:card" content="summary"><meta name="twitter:title" content="computer vision笔记：目标跟踪的小总结"><meta name="twitter:description" content="看了一寒假的目标跟踪，一直想将自己学习到的内容整理归纳一下，迟迟没动笔（其实是打字，但说迟迟没打字比较难听哈哈）。如今快要开学了，决定还是积累一下。本文主要是把目标跟踪中的一些相关要点做一个总结，并且按具体问题对一些主流的或者我阅读过觉得有价值的算法做一个简单概述。近年来各类方法层出不穷，且码字不易，无法涵盖所有的方法。此外，由于包含自己的转述和理解可能会存在错误。在今后的学习过程中会一直保持本文"><meta name="twitter:image" content="https://gsy00517.github.io/computer-vision20200215214240/目标跟踪.png"><script type="text/javascript" id="hexo.configurations">var NexT=window.NexT||{},CONFIG={root:"/",scheme:"Gemini",version:"5.1.4",sidebar:{position:"left",display:"post",offset:12,b2t:!1,scrollpercent:!0,onmobile:!1},fancybox:!0,tabs:!0,motion:{enable:!0,async:!1,transition:{post_block:"fadeIn",post_header:"slideDownIn",post_body:"slideDownIn",coll_header:"slideLeftIn",sidebar:"slideUpIn"}},duoshuo:{userId:"0",author:"博主"},algolia:{applicationID:"",apiKey:"",indexName:"",hits:{per_page:10},labels:{input_placeholder:"Search for Posts",hits_empty:"We didn't find any results for the search: ${query}",hits_stats:"${hits} results found in ${time} ms"}}}</script><link rel="canonical" href="https://gsy00517.github.io/computer-vision20200215214240/"><meta name="baidu-site-verification" content="o5QfpvLBz5"><title>computer vision笔记：目标跟踪的小总结 | 高深远的博客</title></head><body itemscope itemtype="http://schema.org/WebPage" lang="zh-Hans"><div class="container sidebar-position-left page-post-detail"><div class="headband"></div><header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="header-inner"><div class="site-brand-wrapper"><div class="site-meta"><div class="custom-logo-site-title"><a href="/" class="brand" rel="start"><span class="logo-line-before"><i></i></span> <span class="site-title">高深远的博客</span> <span class="logo-line-after"><i></i></span></a></div><h1 class="site-subtitle" itemprop="description"></h1></div><div class="site-nav-toggle"><button><span class="btn-bar"></span> <span class="btn-bar"></span> <span class="btn-bar"></span></button></div></div><nav class="site-nav"><ul id="menu" class="menu"><li class="menu-item menu-item-home"><a href="/" rel="section"><i class="menu-item-icon fa fa-fw fa-home"></i><br>首页</a></li><li class="menu-item menu-item-about"><a href="/about/" rel="section"><i class="menu-item-icon fa fa-fw fa-user"></i><br>关于</a></li><li class="menu-item menu-item-tags"><a href="/tags/" rel="section"><i class="menu-item-icon fa fa-fw fa-tags"></i><br>标签</a></li><li class="menu-item menu-item-categories"><a href="/categories/" rel="section"><i class="menu-item-icon fa fa-fw fa-th"></i><br>分类</a></li><li class="menu-item menu-item-archives"><a href="/archives/" rel="section"><i class="menu-item-icon fa fa-fw fa-archive"></i><br>最新发布</a></li><li class="menu-item menu-item-new"><a href="/new/" rel="section"><i class="menu-item-icon fa fa-fw fa-history"></i><br>最近阅读</a></li><li class="menu-item menu-item-rank"><a href="/rank/" rel="section"><i class="menu-item-icon fa fa-fw fa-signal"></i><br>热度排名</a></li><li class="menu-item menu-item-search"><a href="javascript:;" class="popup-trigger"><i class="menu-item-icon fa fa-search fa-fw"></i><br>站内搜索</a></li></ul><div class="site-search"><div class="popup search-popup local-search-popup"><div class="local-search-header clearfix"><span class="search-icon"><i class="fa fa-search"></i> </span><span class="popup-btn-close"><i class="fa fa-times-circle"></i></span><div class="local-search-input-wrapper"><input autocomplete="off" placeholder="搜索..." spellcheck="false" type="text" id="local-search-input"></div></div><div id="local-search-result"></div></div></div><div style="Text-align:center;width:100%"><div style="margin:0 auto"><canvas id="canvas" style="width:60%">当前浏览器不支持canvas，请更换浏览器后再试</canvas></div><script>!function(){function t(t,e){for(var a=0;a<l[e].length;a++)for(var n=0;n<l[e][a].length;n++)1==l[e][a][n]&&(h.beginPath(),h.arc(14*(g+2)*t+2*n*(g+1)+(g+1),2*a*(g+1)+(g+1),g,0,2*Math.PI),h.closePath(),h.fill())}function e(){var t=[],e=/(\d)(\d):(\d)(\d):(\d)(\d)/.exec(new Date),a=[];a.push(e[1],e[2],10,e[3],e[4],10,e[5],e[6]);for(var r=c.length-1;r>=0;r--)a[r]!==c[r]&&t.push(r+"_"+(Number(c[r])+1)%10);for(var r=0;r<t.length;r++)n.apply(this,t[r].split("_"));c=a.concat()}function a(){for(var t=0;t<d.length;t++)d[t].stepY+=d[t].disY,d[t].x+=d[t].stepX,d[t].y+=d[t].stepY,(d[t].x>i+g||d[t].y>f+g)&&(d.splice(t,1),t--)}function n(t,e){for(var a=[1,2,3],n=["#3BE","#09C","#A6C","#93C","#9C0","#690","#FB3","#F80","#F44","#C00"],r=0;r<l[e].length;r++)for(var o=0;o<l[e][r].length;o++)if(1==l[e][r][o]){var h={x:14*(g+2)*t+2*o*(g+1)+(g+1),y:2*r*(g+1)+(g+1),stepX:Math.floor(4*Math.random()-2),stepY:-2*a[Math.floor(Math.random()*a.length)],color:n[Math.floor(Math.random()*n.length)],disY:1};d.push(h)}}function r(){o.height=100;for(var e=0;e<c.length;e++)t(e,c[e]);for(var e=0;e<d.length;e++)h.beginPath(),h.arc(d[e].x,d[e].y,g,0,2*Math.PI),h.fillStyle=d[e].color,h.closePath(),h.fill()}var l=[[[0,0,1,1,1,0,0],[0,1,1,0,1,1,0],[1,1,0,0,0,1,1],[1,1,0,0,0,1,1],[1,1,0,0,0,1,1],[1,1,0,0,0,1,1],[1,1,0,0,0,1,1],[1,1,0,0,0,1,1],[0,1,1,0,1,1,0],[0,0,1,1,1,0,0]],[[0,0,0,1,1,0,0],[0,1,1,1,1,0,0],[0,0,0,1,1,0,0],[0,0,0,1,1,0,0],[0,0,0,1,1,0,0],[0,0,0,1,1,0,0],[0,0,0,1,1,0,0],[0,0,0,1,1,0,0],[0,0,0,1,1,0,0],[1,1,1,1,1,1,1]],[[0,1,1,1,1,1,0],[1,1,0,0,0,1,1],[0,0,0,0,0,1,1],[0,0,0,0,1,1,0],[0,0,0,1,1,0,0],[0,0,1,1,0,0,0],[0,1,1,0,0,0,0],[1,1,0,0,0,0,0],[1,1,0,0,0,1,1],[1,1,1,1,1,1,1]],[[1,1,1,1,1,1,1],[0,0,0,0,0,1,1],[0,0,0,0,1,1,0],[0,0,0,1,1,0,0],[0,0,1,1,1,0,0],[0,0,0,0,1,1,0],[0,0,0,0,0,1,1],[0,0,0,0,0,1,1],[1,1,0,0,0,1,1],[0,1,1,1,1,1,0]],[[0,0,0,0,1,1,0],[0,0,0,1,1,1,0],[0,0,1,1,1,1,0],[0,1,1,0,1,1,0],[1,1,0,0,1,1,0],[1,1,1,1,1,1,1],[0,0,0,0,1,1,0],[0,0,0,0,1,1,0],[0,0,0,0,1,1,0],[0,0,0,1,1,1,1]],[[1,1,1,1,1,1,1],[1,1,0,0,0,0,0],[1,1,0,0,0,0,0],[1,1,1,1,1,1,0],[0,0,0,0,0,1,1],[0,0,0,0,0,1,1],[0,0,0,0,0,1,1],[0,0,0,0,0,1,1],[1,1,0,0,0,1,1],[0,1,1,1,1,1,0]],[[0,0,0,0,1,1,0],[0,0,1,1,0,0,0],[0,1,1,0,0,0,0],[1,1,0,0,0,0,0],[1,1,0,1,1,1,0],[1,1,0,0,0,1,1],[1,1,0,0,0,1,1],[1,1,0,0,0,1,1],[1,1,0,0,0,1,1],[0,1,1,1,1,1,0]],[[1,1,1,1,1,1,1],[1,1,0,0,0,1,1],[0,0,0,0,1,1,0],[0,0,0,0,1,1,0],[0,0,0,1,1,0,0],[0,0,0,1,1,0,0],[0,0,1,1,0,0,0],[0,0,1,1,0,0,0],[0,0,1,1,0,0,0],[0,0,1,1,0,0,0]],[[0,1,1,1,1,1,0],[1,1,0,0,0,1,1],[1,1,0,0,0,1,1],[1,1,0,0,0,1,1],[0,1,1,1,1,1,0],[1,1,0,0,0,1,1],[1,1,0,0,0,1,1],[1,1,0,0,0,1,1],[1,1,0,0,0,1,1],[0,1,1,1,1,1,0]],[[0,1,1,1,1,1,0],[1,1,0,0,0,1,1],[1,1,0,0,0,1,1],[1,1,0,0,0,1,1],[0,1,1,1,0,1,1],[0,0,0,0,0,1,1],[0,0,0,0,0,1,1],[0,0,0,0,1,1,0],[0,0,0,1,1,0,0],[0,1,1,0,0,0,0]],[[0,0,0,0,0,0,0],[0,0,1,1,1,0,0],[0,0,1,1,1,0,0],[0,0,1,1,1,0,0],[0,0,0,0,0,0,0],[0,0,0,0,0,0,0],[0,0,1,1,1,0,0],[0,0,1,1,1,0,0],[0,0,1,1,1,0,0],[0,0,0,0,0,0,0]]],o=document.getElementById("canvas");if(o.getContext){var h=o.getContext("2d"),f=100,i=700;o.height=f,o.width=i,h.fillStyle="#f00",h.fillRect(10,10,50,50);var c=[],d=[],g=o.height/20-1;!function(){var t=/(\d)(\d):(\d)(\d):(\d)(\d)/.exec(new Date);c.push(t[1],t[2],10,t[3],t[4],10,t[5],t[6])}(),clearInterval(v);var v=setInterval(function(){e(),a(),r()},50)}}()</script></div></nav></div></header><main id="main" class="main"><div class="main-inner"><div class="content-wrap"><div id="content" class="content"><div id="posts" class="posts-expand"><article class="post post-type-normal" itemscope itemtype="http://schema.org/Article"><div class="post-block"><link itemprop="mainEntityOfPage" href="https://gsy00517.github.io/computer-vision20200215214240/"><span hidden itemprop="author" itemscope itemtype="http://schema.org/Person"><meta itemprop="name" content="高深远"><meta itemprop="description" content><meta itemprop="image" content="/images/lufei.jpg"></span><span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization"><meta itemprop="name" content="高深远的博客"></span><header class="post-header"><h2 class="post-title" itemprop="name headline">computer vision笔记：目标跟踪的小总结</h2><div class="post-meta"><span class="post-time"><span class="post-meta-item-icon"><i class="fa fa-calendar-o"></i> </span><span class="post-meta-item-text">发表于</span> <time title="创建于" itemprop="dateCreated datePublished" datetime="2020-02-15T21:42:40+08:00">2020-02-15 </time><span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-calendar-check-o"></i> </span><span class="post-meta-item-text">更新于&#58;</span> <time title="更新于" itemprop="dateModified" datetime="2020-04-10T20:11:47+08:00">2020-04-10 </time></span><span class="post-category"><span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-folder-o"></i> </span><span class="post-meta-item-text">分类于</span> <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/人工智能/" itemprop="url" rel="index"><span itemprop="name">人工智能</span> </a></span></span><span class="post-comments-count"><span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-comment-o"></i> </span><a href="/computer-vision20200215214240/#comments" itemprop="discussionUrl"><span class="post-comments-count valine-comment-count" data-xid="/computer-vision20200215214240/" itemprop="commentCount"></span> </a></span><span id="/computer-vision20200215214240/" class="leancloud_visitors" data-flag-title="computer vision笔记：目标跟踪的小总结"><span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-eye"></i> </span><span class="post-meta-item-text">阅读次数&#58;</span> <span class="leancloud-visitors-count"></span></span><div class="post-wordcount"><span class="post-meta-item-icon"><i class="fa fa-file-word-o"></i> </span><span class="post-meta-item-text">字数统计&#58;</span> <span title="字数统计">28.2k字 </span><span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-clock-o"></i> </span><span class="post-meta-item-text">阅读时长 &asymp;</span> <span title="阅读时长">98分钟</span></div></div></header><div class="post-body" itemprop="articleBody"><p>看了一寒假的目标跟踪，一直想将自己学习到的内容整理归纳一下，迟迟没动笔（其实是打字，但说迟迟没打字比较难听哈哈）。如今快要开学了，决定还是积累一下。<br>本文主要是把目标跟踪中的一些相关要点做一个总结，并且按具体问题对一些主流的或者我阅读过觉得有价值的算法做一个简单概述。近年来各类方法层出不穷，且码字不易，无法涵盖所有的方法。此外，由于包含自己的转述和理解可能会存在错误。在今后的学习过程中会一直保持本文的更新，因此这将是一篇LTS的文章哈哈。</p><blockquote><p>注：本文重点关注单目标跟踪。</p></blockquote><p><strong>References</strong>：</p><p>参考文献：<br>[1]统计学习方法（第2版）<br>[2]Understanding and Diagnosing Visual Tracking Systems<br>[3]Survey of Visual Object Tracking Algorithms Based on Deep Learning<br>[4]Handcrafted and Deep Trackers: Recent Visual Object Tracking Approaches and Trends<br>[5]Review of visual object tracking technology<br>[6]A Review of Visual Trackers and Analysis of its Application to Mobile Robot<br>[7]Deep Learning for Visual Tracking: A Comprehensive Survey<br>[8]Video Object Segmentation and Tracking: A Survey<br>[9]Object Tracking Benchmark<br>[10]The Visual Object Tracking VOT2013 challenge results<br>[11]The Visual Object Tracking VOT2014 challenge results<br>[12]The Visual Object Tracking VOT2015 challenge results<br>[13]The Visual Object Tracking VOT2016 challenge results<br>[14]The Visual Object Tracking VOT2017 challenge results<br>[15]The sixth Visual Object Tracking VOT2018 challenge results<br>[16]The Seventh Visual Object Tracking VOT2019 Challenge Results</p><blockquote><p>注：参考文献重新整理中，待补全…</p></blockquote><hr><h1 id="简介与要求"><a href="#简介与要求" class="headerlink" title="简介与要求"></a>简介与要求</h1><p>目标跟踪是利用一个视频或图像序列的上下文信息，对目标的外观和运动信息进行建模，从而对目标运动状态进行预测并标定目标位置的一种技术。一般是在第一帧给出一个框，框中的物体就是我们需要在后续帧中用算法进行跟踪的对象。就目前的单目标跟踪而言，一般有如下要求：<br><strong>monocular</strong>：我们的视频或者图片序列是仅从一个摄像头中获得的，也就是不考虑比如在城市道路场景中跨摄像头对目标跟踪的复杂应用。<br><strong>model-free</strong>：没有任何先验，也就是在获取第一帧的框之前我们并不知道会框出什么物体，也不需要在之前对初始框中的物体进行建模。<br><strong>single-target</strong>：只追踪第一帧框出的那一个物体，也就是除了那个物体之外所有的物体都是back ground。<br><strong>casual/real-time</strong>：目标跟踪是一个在线过程，也就是不能提前获取未来的框对目标进行跟踪。<br><strong>short-term</strong>：没有重检测，也就是目标跟丢了就丢了。<br><strong>long-term</strong>：可以在跟丢之后重检测，这类算法一般除了跟踪之外还需要有检测的功能。<br><img src="/computer-vision20200215214240/目标跟踪.png" title="目标跟踪"><br>下面是目标跟踪流程的伪代码表示（不一定普适，比如有些算法不在线更新，但符合基本的过程）。<br><img src="/computer-vision20200215214240/目标跟踪伪代码.png" title="目标跟踪伪代码"></p><hr><h1 id="问题及挑战"><a href="#问题及挑战" class="headerlink" title="问题及挑战"></a>问题及挑战</h1><p>通俗来讲，目标跟踪的最终目标就是要又快又准。“快”主要表现在计算量小和所需的存储空间小，“准”就是预测出的bounding box要尽可能地接近ground truth。除了上面两个基本需求（也可以说是为了更好地达到这两个基本需求），近年来的算法主要针对目标跟踪中的一些挑战进行突破，从而更好地解决某些问题之后达到更好的整体效果。<br>总的来说，目标跟踪的主要问题有如下这些：遮挡（occlusion）、背景干扰（background clutter）、光照变化（illumination changes）、尺度变化（scale variation）、低分辨率（low resolution）、快速移动（fast motion）、超出画面（out of view）、运动模糊（motion blur）、形变（deformation）、旋转（rotation）等。<br><img src="/computer-vision20200215214240/问题.png" title="问题"><br>OTB数据集依据各种问题对其中的序列进行了一个划分，这对之后针对性的研究提供了重要的参考。<br><img src="/computer-vision20200215214240/OTB对问题的划分.png" title="OTB对问题的划分"></p><hr><h1 id="生成式与判别式"><a href="#生成式与判别式" class="headerlink" title="生成式与判别式"></a>生成式与判别式</h1><p>利用特征判断候选样本是否为跟踪目标，可将目标跟踪的模型分为生成式模型和判别式模型，本小节就介绍一下什么是生成式模型和判别式模型。</p><h2 id="机器学习"><a href="#机器学习" class="headerlink" title="机器学习"></a>机器学习</h2><p>我们首先看看在机器学习中生成式模型和判别式模型定义的一般区分。<br>一般而言，机器学习的任务就是学习一个模型，应用这一个模型，对给定的输入预测相应的输出。输出的一般形式可以是决策函数，也可以是条件概率分布。<br>对于生成式模型，我们需要通过数据学习输入X与输出Y之间的生成关系（比如联合概率分布），也就是认为X和Y都是随机变量。典型的生成式模型有朴素贝叶斯模型、隐马尔可夫模型（HMM）、高斯混合模型（GMM）等。<br>对于判别式模型，我们只需要直接学习决策函数或者条件概率分布，只关心对给定的输入X我们需要输出怎么样的Y，也就是不考虑X是否是随机变量。典型的判别式模型包括k近邻、感知机、决策树、逻辑斯蒂回归模型、最大熵模型、支持向量机（SVM）、提升方法和条件随机场等。此外神经网络也属于判别式模型。<br>相较而言，生成式模型体现了更多的信息，不过这还是因条件而异的，不同情况不同任务两种方法各有优缺点。</p><h2 id="目标跟踪"><a href="#目标跟踪" class="headerlink" title="目标跟踪"></a>目标跟踪</h2><p>在目标跟踪领域，生成式模型通过提取目标特征来构建表观模型，然后在图像中搜索与模型最匹配的区域作为跟踪结果。不论采用全局特征还是局部特征，生成式模型的本质是在目标表示的高维空间中，找到与目标模型最相邻的候选目标作为当前估计。此类方法的缺陷在于只关注目标信息，而忽略了背景信息。<br><img src="/computer-vision20200215214240/生成式模型.png" title="生成式模型"><br>与生成式模型不同的是，判别式模型同时考虑了目标和背景信息。它将跟踪问题看做二分类或者回归问题，其目的是寻找一个判别函数，将目标从背景中分离出来，从而实现对目标的跟踪。<br><img src="/computer-vision20200215214240/判别式模型.png" title="判别式模型"><br>一般来说，在目标跟踪领域，判别式充分利用了目标前景和背景信息，能更加有效地区分出目标，比单单运用目标区域特征进行模板匹配的生成式模型在复杂环境中的鲁棒性更强。</p><hr><h1 id="算法导图"><a href="#算法导图" class="headerlink" title="算法导图"></a>算法导图</h1><p>首先是参考文献[6]中的一个树状导图。<br><img src="/computer-vision20200215214240/论文中的导图.png" title="论文中的导图"><br>下图是中科院博士王强（github名为foolwood…呃不得不说这名字取得真谦虚）在github上上总结的<a href="https://github.com/foolwood/benchmark_results" target="_blank">Benchmark Results</a>中的一个思维导图，同一个链接下还包括了各项成果的paper及code，值得收藏一下。<br><img src="/computer-vision20200215214240/王强的导图.png" title="王强的导图"></p><blockquote><p>补充：这里再推荐一个在github上维护的<a href="https://github.com/HEscop/TBCF" target="_blank">Tracking Benchmark for Correlation Filters</a>，按每篇论文针对或者解决的问题来分类，比较清楚，可以收藏一下。但这个仓库似乎在2017年后就没有更新了，可能是深度学习的进入或者说相关滤波系列和深度学习融合使得独立的相关滤波算法不那么突出了。</p></blockquote><p>下图是浙大硕士王蒙蒙极市平台做分享的时候所用的一张思维导图，归纳得也比较清晰。<br><img src="/computer-vision20200215214240/王蒙蒙的导图.png" title="王蒙蒙的导图"></p><blockquote><p>注：后两张导图中都把历年benchmark的冠军工作作了标注。</p></blockquote><p>对比几张思维导图可以发现，他们都把主流算法分成了相关滤波、深度学习两个分支（或者说是基于handcrafted特征的算法和基于CNN提取特征的算法，其实近年已有所融合），此外还有一些基于强化学习、结构化SVM的模型。其实，目标跟踪算得上是计算机视觉领域中深度学习涉足较晚的一个方向，其主要原因是目标跟踪相关数据集的标注花费较大。此外，相关滤波的速度优势，也就是实时性是十分引人注目的，但在应付当前目标跟踪中的各种挑战、问题时，相关滤波的鲁棒性还是落后于深度学习方法的。<br>在下一节，我将结合上面几张导图，对历年尤其是近几年的算法做一个简单的整理，以方便日后的学习与研究。</p><hr><h1 id="各类算法的梳理与简述"><a href="#各类算法的梳理与简述" class="headerlink" title="各类算法的梳理与简述"></a>各类算法的梳理与简述</h1><p>本节按年份顺序对各个算法进行一个简单地梳理，其中各个算法的年份以论文发表的年份或者参加benchmark的年份为依据，可能会存在1年的区别，但影响不大。其中各年的各个算法根据算法的效果和影响大致上呈递减排序。对2013以后的算法，我拷贝了VOT challenge的结果排名，以供参照。</p><blockquote><p>注意：如果你对计算机视觉或者说目标跟踪方面的一些基础方法、概念和经典算法已经有些了解，可以跳过本条建议。<br>考虑到在后文频繁地插入链接不太好，我就在此先推荐一下我博客的几个标签<a href="https://gsy00517.github.io/tags/%E7%9B%AE%E6%A0%87%E8%B7%9F%E8%B8%AA/" target="_blank">目标跟踪</a>、<a href="https://gsy00517.github.io/tags/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/" target="_blank">计算机视觉</a>、<a href="https://gsy00517.github.io/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/" target="_blank">深度学习</a>、<a href="https://gsy00517.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" target="_blank">机器学习</a>以及<a href="https://gsy00517.github.io/tags/%E7%BA%BF%E6%80%A7%E4%BB%A3%E6%95%B0/" target="_blank">线性代数</a>，其中的文章包含了一部分接下来要提到的概念和算法，可以事先浏览一下。当你在阅读时对相关概念、方法感到迷惑或者想进一步了解，博客内置的搜索功能或许能够为你提供帮助。</p></blockquote><h2 id="1981"><a href="#1981" class="headerlink" title="1981"></a>1981</h2><h3 id="LK-Tracker"><a href="#LK-Tracker" class="headerlink" title="LK Tracker"></a>LK Tracker</h3><p>LK Tracker应该是最早的目标跟踪工作，它使用了光流的概念，如下图所示，不同颜色表示光流不同的方向，颜色的深浅表示运动的速度。<br><img src="/computer-vision20200215214240/光流.gif" title="光流"><br>LK Tracker假定目标灰度在短时间内保持不变，同时目标邻域内的速度向量场变化缓慢。由于光流方程包含坐标x，y和时间t共三个未知数，其中时间变化dt已知而坐标变化dx和dy未知，一个方程两个未知数无法求解，因此作者假定相邻的点它们的光流具有空间一致性，即实际场景中邻近的点投影到图像上也是邻近点，且邻近点速度一致，这样就可以求解方程组了。下图是求解之后的光流向量，其中绿色箭头的方向表示运动方向，线段长度表示运动速度的大小。<br><img src="/computer-vision20200215214240/画出光流.png" title="画出光流"><br>光流的计算非常简单也非常快，而且由于提出得很早，各种库都有实现好的轮子可以轻松调用，但是它的鲁棒性不好，基本上只能对平移且外观不变的物体进行跟踪。</p><h2 id="1994"><a href="#1994" class="headerlink" title="1994"></a>1994</h2><h3 id="KLT"><a href="#KLT" class="headerlink" title="KLT"></a>KLT</h3><p>KLT是一种生成式方法，也是使用了光流特征。在此基础上，作者使用了匹配角点的方法，也就是寻找边角处、纹理处等易辨识的地方计算光流来进行追踪。</p><h2 id="1998"><a href="#1998" class="headerlink" title="1998"></a>1998</h2><h3 id="Condensation"><a href="#Condensation" class="headerlink" title="Condensation"></a>Condensation</h3><p>Condensation（Conditional density propagation）条件密度传播使用了原始的外观作为主要特征来描述目标，采用了粒子滤波，这是一种非参数化滤波方法，属于生成式模型。它定义了一个粒子样本集，该样本集描述了每个粒子的坐标、运动速度、高和宽、尺度变化等状态；此外，通过一个状态转移矩阵和噪声定义系统状态方程。基于蒙特卡洛方法，粒子滤波将贝叶斯滤波方法中的积分运算转化为粒子采样求样本均值问题，通过对状态空间的粒子的随机采样来近似求解后验概率。</p><h2 id="2002"><a href="#2002" class="headerlink" title="2002"></a>2002</h2><h3 id="Mean-Shift"><a href="#Mean-Shift" class="headerlink" title="Mean Shift"></a>Mean Shift</h3><p>Mean Shift采用均值漂移作为搜索策略，这是一种无参概率估计方法，该方法利用图像特征直方图构造空间平滑的概率密度函数，通过沿着概率密度函数的梯度方向迭代，搜索函数局部最大值。在当时成为了常用的视觉跟踪系统的目标搜索方法，简单易实现，但鲁棒性较低。</p><h2 id="2003"><a href="#2003" class="headerlink" title="2003"></a>2003</h2><h3 id="Feature-Selection"><a href="#Feature-Selection" class="headerlink" title="Feature Selection"></a>Feature Selection</h3><p>Feature Selection利用线性判别分析自适应地选择对当前背景和目标最具鉴别性的颜色特征，从而分离出目标。</p><h2 id="2006"><a href="#2006" class="headerlink" title="2006"></a>2006</h2><h3 id="Boosting"><a href="#Boosting" class="headerlink" title="Boosting"></a>Boosting</h3><p>Boosting结合Haar特征和在线Boosting算法对目标进行跟踪。Boosting算法的基本思路就是首先均匀地初始化训练集中各个样本的权重，然后初始化N个弱分类器，通过训练集进行训练。第一次训练时，对第一个弱分类器，通过它在训练集上的错误率确定它的权重，同时更新训练集的样本权重（增加分类错误的样本的权重），然后，用新的训练集训练第二个弱分类器，计算它的权重并更新训练集的权重。如此迭代，将得到的分类器与它们的权重相乘，累加起来便得到一个强分类器。<br>上面所述是针对离线训练的，当在线训练（比如跟踪）时，为了满足实时性，就必须减少样本数量。Boosting的做法是对每一帧采集的样本仅使用一次便丢弃，然后进入下一帧采用新的样本。<br>以上就是在线Boosting算法的简单理解，具体而言，Boosting这里选择的弱分类器其实是Haar特征。由于Haar特征其实是一组特征，于是就需要Boosting算法根据每种Haar特征的响应来从Haar特征池中选出一个子集用于构造强分类器。<br><img src="/computer-vision20200215214240/Haar特征池.jpg" title="Haar特征池"></p><h2 id="2008"><a href="#2008" class="headerlink" title="2008"></a>2008</h2><h3 id="IVT"><a href="#IVT" class="headerlink" title="IVT"></a>IVT</h3><p>IVT渐进地学习一个低维的子空间表示来自适应目标物体的变化，它将以前检测到的目标乘以遗忘因子作为样本在线更新特征空间的基而无需大量的标注样本。</p><h2 id="2010"><a href="#2010" class="headerlink" title="2010"></a>2010</h2><h3 id="MOSSE"><a href="#MOSSE" class="headerlink" title="MOSSE"></a>MOSSE</h3><p>MOSSE（Minimum Output Sum of Squared Error）使用相关滤波来做目标跟踪（不是第一个，但可以看作前期的一个代表），其速度能够达到600多帧每秒，但是效果一般，这主要是因为它只使用了简单的raw pixel特征。<br>相比之前的算法，MOSSE能够形成更加明确的峰值，减少了漂移；此外，MOSSE可以在线更新，同时还采用了PSR来检测遮挡或者跟丢的情况，从而决定是否需要停止更新。<br>值得一提的是，MOSSE在做相关操作之前，对每张图都进行了减去平均值的处理，这有利于淡化背景对相关操作的影响。另外假如发生光照变化的话，减去均值也有利于减小这种变化的影响。此外要注意，输出的特征应乘以汉宁窗（一种余弦窗），用于确定搜索区域（也就是不为0的区域），且有利于突出中心的特征。</p><h3 id="TLD"><a href="#TLD" class="headerlink" title="TLD"></a>TLD</h3><p>TLD（Tracking Learning Detection）主要针对long-term tracking，在跟踪的同时全局检测。它由三部分组成：跟踪模块、检测模块、学习模块。<br>跟踪模块观察帧与帧之间的目标的动向。作者采用了光流来跟踪，此外还提出了一种判断跟踪失效的算法，由于光流跟踪时选取的若干特征点，当其中某一个特征点的位移与所有特征点位移的中值之差过大时，也就是某个特征点离跟踪模块认为的目标中心位置很远时，就认为跟踪失效。作者还通过相似度和错误匹配度来对特征点进行筛选。<br>检测模块把每张图看成独立的，然后对单张图片进行目标检测定位。作者使用了方差检测器、随机森林和最近邻分类器来对目标做检测。<br>学习模块对根据跟踪模块的结果对检测模块的错误进行评估，当置信度较低时，重新组织正负样本对随机深林的后验概率和最近邻分类器的在线模板进行更新，从而避免以后出现类似错误。<br>TLD与传统跟踪算法的显著区别在于将传统的跟踪算法和传统的检测算法相结合来解决被跟踪目标在被跟踪过程中发生的形变、部分遮挡等问题。同时，通过一种改进的在线学习机制不断更新跟踪模块的“显著特征点”和检测模块的目标模型及相关参数，从而使得跟踪能够自适应，效果较之前更加稳定、可靠。</p><h2 id="2011"><a href="#2011" class="headerlink" title="2011"></a>2011</h2><h3 id="FoT"><a href="#FoT" class="headerlink" title="FoT"></a>FoT</h3><p>FoT（Flock of Trackers）首先在目标上抓取多个interesting point并分别放入多个cell中，之后的跟踪就是检测并补偿每个cell中interesting point的偏动量，使其回到中间。如果interesting point超出cell，则让它重新恢复到cell的中点。<br><img src="/computer-vision20200215214240/cell.png" title="cell"><br>此外，FoT还提出了两种简单有效的failure预测方法：neighbourhood consistency predictor（Nh）和Markov predictor（Mp）。Nh的基本思想是认为正确的跟踪情况下每个local tracker给出的位移应当与它相邻的tracker相一致，而Mp主要是针对时域一致性，认为前几帧表现较好的local tracker在当前帧也会有较好的表现。FoT基于这些failure预测方法来控制模型的更新。<br><img src="/computer-vision20200215214240/FoT.png" title="FoT"></p><h3 id="Struck"><a href="#Struck" class="headerlink" title="Struck"></a>Struck</h3><p>Struck的主要贡献是引入了结构化SVM。考虑到传统的跟踪算法将跟踪问题转化为一个分类问题，并通过在线学习技术更新目标模型。然而，为了达到更新的目的，通常需要将一些预估计的目标位置作为已知类别的训练样本，这些分类样本并不一定与实际目标一致，因此难以实现最佳的分类效果。<br>结合上述考虑，Struck利用了结构化SVM直接输出跟踪结果，避免了中间分类环节，这使得在当时效果有明显的提升。同时，为了保证实时性，Struck还引入了阈值机制，防止跟踪过程中支持向量的过增长。</p><h3 id="L1-Tracker"><a href="#L1-Tracker" class="headerlink" title="L1 Tracker"></a>L1 Tracker</h3><p>L1 Tracker是第一个将稀疏编码引入目标跟踪问题中的算法。它把跟踪看做一个稀疏近似问题，主要是用第一帧和最近几帧得到的图像（特征）作为字典，通过求解L1范数最小化问题，实现对目标的跟踪。</p><h3 id="MIL"><a href="#MIL" class="headerlink" title="MIL"></a>MIL</h3><p>MIL采用了多示例学习的方法而不是传统的监督学习（即由原本单独标记的示例变成一组示例，当且仅当所有的示例都判定为负才认为是负，只要有一个示例判定为正则整组都判定为正），对于不精准的tracker和错误标注的训练样本有鲁棒性的提升。</p><h2 id="2012"><a href="#2012" class="headerlink" title="2012"></a>2012</h2><h3 id="CSK"><a href="#CSK" class="headerlink" title="CSK"></a>CSK</h3><p>CSK也称为核相关滤波算法，作者针对MOSSE做出了一些改进，作者认为循环移位能模拟当前正样本的所有的转换版本（除边界以外），因此采用循环移位进行密集采样，并通过核函数将低维线性空间映射到高维空间，提高了相关滤波器的鲁棒性。这里循环移位后的样本匹配可以理解为如果某个候选区域与某一个移位样本的相关操作响应较高，那么就可以理解为物体的移动和该样本移位的方式一致，从而对下一帧目标位置进行定位。<br>随后的工作主要从特征选择、尺度估计、正则化等方面对该算法进行改进和提高。关于循环移位和线性、非线性的核函数计算，我在之前的文章中做了一些分析，感兴趣的话可以看看。</p><h3 id="DF"><a href="#DF" class="headerlink" title="DF"></a>DF</h3><p>DF发现之前在图像中寻找目标的梯度下降方法首先会模糊图像来平滑目标方程，这就会严重损害目标的位置信息。因此作者提出了对每一个像素点设置多个通道，在每个通道进行卷积，这种方法同样也能平滑目标方程但不会严重损害目标的位置信息。其实这就是之后的CNN能做到的，在当时应该也算是一种创新。</p><h3 id="CT"><a href="#CT" class="headerlink" title="CT"></a>CT</h3><p>CT（Compressive Tracking）是一种基于压缩感知的高效跟踪算法。和一般的判别式模型架构一样，CT首先利用符合压缩感知RIP条件的随机感知矩阵对图像特征进行降维，使得到的低维信号可以完全保持高维信号的特性并可以完全重建，然后在降维后的特征上，在感知空间下采用朴素贝叶斯分类器进行分类。另外，CT在每一帧通过在线学习更新分类器，在线学习的样本来自通过相同的稀疏感知矩阵提取的前景目标和背景的特征。</p><h3 id="ORIA"><a href="#ORIA" class="headerlink" title="ORIA"></a>ORIA</h3><p>ORIA假设前一帧是完美的，于是把跟踪问题视作将下一帧图像与上一帧进行对齐，也就是一串连续的凸优化问题。</p><h2 id="2013"><a href="#2013" class="headerlink" title="2013"></a>2013</h2><p>下面是VOT2013的排名结果，其中Experiment 1是在所有序列上使用ground truth初始化的实验结果，Experiment 2使用含噪声（10%的尺寸扰动）的ground truth，Experiment 3使用灰度图像。<br><img src="/computer-vision20200215214240/VOT2013.png" title="VOT2013"></p><h3 id="PLT"><a href="#PLT" class="headerlink" title="PLT"></a>PLT</h3><p>PLT通过一个固定大小的、基于二值特征向量的线性分类器对每一张图像做分类，得分最高即为目标。作者利用一个稀疏的在线结构化SVM来选出一个小的判别特征集合。在训练SVM时，考虑到在bounding box内的像素不一定都属于物体，作者使用了一种基于概率的掩模来分配权重，然后计算初始的结构化SVM，去除分值最小的特征。由于特征向量的二值性，该线性分类器可以作为查找表用于快速检测。</p><h3 id="EDF"><a href="#EDF" class="headerlink" title="EDF"></a>EDF</h3><p>EDF是DF的加强版，在DF的基础上探索了每一个通道之间的联系。</p><h3 id="LGT"><a href="#LGT" class="headerlink" title="LGT"></a>LGT</h3><p>LGT针对模型何时更新与更新哪些部分的问题，考虑到目标模型的整体更新会损失部分有用信息和固定的分块不利于应对目标的变化，借鉴了之前将目标有结构地分块且动态删减的思想，提出了一种由patch组成的集合构成的、用于精确定位的local layer和颜色、移位、形状三个特性组成的、用于指导增加patch的global layer。<br><img src="/computer-vision20200215214240/LGT.png" title="LGT"><br>对于新输入的一帧图像，LGT的处理流程如下：</p><ol><li>首先使用卡尔曼滤波器结合近似匀速模型来确定目标的位置。<script type="math/tex;mode=display">\widehat{x}_{t}=x_{t-1}^{(i)}+\widehat{v}_{t}^{(i)}</script>接下来几步是对位置进行微调。</li><li>对于每一个patch，作者用一个统一的仿射变换和一个独立的微小扰动在5维空间（位置2维+尺度2维+旋转1维）定义其对于初始patch的变换。<script type="math/tex;mode=display">x_{t}^{(i)}=A_{t}^{G}\widehat{x}_{t}^{(i)}+\Delta _{t}^{(i)}</script>这里的$\widehat{x}$指的是初始patch。作者把$A_{t}^{G}$和$\Delta _{t}$中的参数看作正态分布的。先使用交叉熵方法反复迭代寻找最优解，当协方差矩阵的行列式小于0.1（各分量相关性很强）时停止迭代，随后把仿射变换矩阵的参数（也就是学到的正态分布的均值和方差）固定并用于所有的patch。接下来对每一个patch，用同样的交叉熵方法迭代，得到每一个patch的微小扰动的均值和方差。</li><li>结合visual consistency和drift from majority两种估计，更新每一个patch在当前帧的权重，并与前一帧的权重加权求和来确定最终的权重。这里的权重决定了每一个patch在最终所有patch混合决策时的重要性。</li><li>利用上面的patch重新计算之前卡尔曼滤波器的结果，确定目标的位置。</li><li>接着进行local layer中patch的删减与增补。对于权重小于阈值的patch作删去处理；对距离很近的两个patch进行合并，合并后产生的patch的所有参数设为合并前两个patch参数的平均。使用剩下的patch更新global layer，然后用更新后的global layer决定是否以及如何增加新的patch。为了防止突然过度增加patch，作者对增加的样本数施加上限限制，并利用加权的方式平滑调整样本容量。</li><li>进入下一帧。</li></ol><h3 id="LGT-1"><a href="#LGT-1" class="headerlink" title="LGT++"></a>LGT++</h3><p>LGT++是LGT的改进版。在LGT的基础上，LGT++增加了memory、failure detection和用粒子滤波代替卡尔曼滤波的recovery机制。此外对尺度变化和背景干扰也做出了改进。</p><h3 id="DLT"><a href="#DLT" class="headerlink" title="DLT"></a>DLT</h3><p>DLT是最早的基于深度学习的算法（当时AlexNet刚刚被提出），它采用了堆叠去噪自编码器网络，把跟踪视为一个分类问题，直接利用80 Million Tiny Images数据集上的预训练模型提取深度特征，这种强行task转换的训练方法存在缺陷，但在当时是个进步。</p><h3 id="STC"><a href="#STC" class="headerlink" title="STC"></a>STC</h3><p>STC（Spatio-Temporal Context）通过贝叶斯框架目标时间、空间的上下文信息来建模，利用得到的关系结合生物视觉系统中的注意力特性来生成confidence map来预测目标位置。由于上下文信息建模和之后的预测都采用了快速傅里叶变换，因此算法的速度很快。<br><img src="/computer-vision20200215214240/STC.png" title="STC"><br>文章主要举了两个例子来说明空间信息的重要性。当目标物体被部分或者完全遮挡时，周围的信息能帮助定位被遮挡的目标（假设摄像头不移动），也就是说可以利用空间的距离信息；此外，如果目标内部的两个部分比较相似（比如人的一对眼睛），就比较容易发生偏移，而如果这时恰好这两个部分的距离信息相似（距离目标中心长度相同），那就需要引入相对位置也就是方向信息来判断。<br>此外，考虑到生物视觉系统中的注意力特性，作者增加了一项权重函数来构成先验，该函数根据距离目标位置的远近来定义。<br>作者还对confidence map的参数进行了讨论，认为置信度在空间上的分布不能太平滑（增加位置模糊不确定性），也不能太尖锐（导致过拟合）。<br>作者认为目标的形态与近几帧有较强的关联，由此设计了时域信息模型。文中提到的时域滤波器可被证明是低通的，也就是可以滤去一定的噪声。此外，STC还设计了尺度更新方法，最终下一帧的尺度是前n帧估计尺度的均值。</p><h3 id="LT-FLO"><a href="#LT-FLO" class="headerlink" title="LT-FLO"></a>LT-FLO</h3><p>LT-FLO主要针对的是缺少纹理特征的目标，使用边界点来代替在目标上采集点来做跟踪。此外作者还提出了一种基于边界梯度稳定性的failure检测机制。</p><h3 id="GSDT"><a href="#GSDT" class="headerlink" title="GSDT"></a>GSDT</h3><p>GSDT提出了一种采集正负样本进行图嵌入的判别式模型。作者使用了基于图结构的分类器而不是生成一个子空间。此外GSDT还设计了一种新的图结构来区分类内的不同和样本的内在结构。</p><h3 id="SCTT"><a href="#SCTT" class="headerlink" title="SCTT"></a>SCTT</h3><p>SCTT使用了treelets降维方法，由于仅选取较高置信度的样本，相对于PCA只需要更少的样本且对噪声有更好的鲁棒性。</p><h3 id="CCMS"><a href="#CCMS" class="headerlink" title="CCMS"></a>CCMS</h3><p>CCMS（Color Correspondences Mean-Shift）用之前提到的Mean Shift方法对目标候选与目标模型、目标候选与背景模型在每种颜色（也就是直方图中每个bin）中计算相似性，反复迭代，直到收敛或者达到最大迭代次数为止，如此来进行运动估计。</p><h3 id="Matrioska"><a href="#Matrioska" class="headerlink" title="Matrioska"></a>Matrioska</h3><p>Matrioska基于特征点提取的方法（ORB、FREAK、BRISK、SURF等），考虑到目标物体的外观变化和有利于增强模型表现力的负样本提取，使用了增枝和剪枝的方式来对目标模型进行更新。</p><h3 id="AIF"><a href="#AIF" class="headerlink" title="AIF"></a>AIF</h3><p>AIF（adaptive integrated feature）提出了一种评估特征稳定性的方法，并根据不同特征的稳定性动态地分配权重。</p><h3 id="HT"><a href="#HT" class="headerlink" title="HT"></a>HT</h3><p>HT借鉴霍夫森林，也就是霍夫变换结合随机森林，相比一般的随机森林增加了位移信息。HT将目标分割成多个图像块，这些图像块含有它们各自偏离目标中心的向量$d$，对每个图像块提取特征描述子，这样就构成了正样本，即$y=1$；在图像的其他区域也提取同样尺寸的图像块，也提取特征描述子构成负样本，即$y=0$，注意负样本的偏移向量$d=0$。由此训练生成树，再由树构成森林。<br>在跟踪时，将每个图像块输入训练好的森林里，最终会落到森林的每棵树的一个叶节点上，这就得到了该图像块相对目标中心的偏移向量$d$以及概率$p$，随后将每棵树上的结果加权平均，得到了该图像块的结果。最后将所有的目标分割出的图像块的结果组合起来，得到目标预测结果。<br>HT的一个好处就是可以调整bounding box的长宽比，此外作者认为这对非刚性或者铰接的目标也有很大好处。</p><h3 id="STMT"><a href="#STMT" class="headerlink" title="STMT"></a>STMT</h3><p>STMT把目标跟踪分成镜头运动估计和目标运动估计两个阶段，先估计摄像头的运动并进行对齐，然后再定位下一帧的目标位置。</p><h3 id="ASAM"><a href="#ASAM" class="headerlink" title="ASAM"></a>ASAM</h3><p>ASAM（Adaptive Sparse Appearance Model）用一个样本集来表示目标的各种变化，并且基于判别式和生成式的稀疏表示，使用了第一帧、后续帧两阶段的在线跟踪算法。</p><h2 id="2014"><a href="#2014" class="headerlink" title="2014"></a>2014</h2><p>下面是VOT2014的排名结果，这里的A表示accuracy，R表示robustness。<br><img src="/computer-vision20200215214240/VOT2014.png" title="VOT2014"></p><h3 id="DSST"><a href="#DSST" class="headerlink" title="DSST"></a>DSST</h3><p>DSST主要考虑了尺度缩放的问题。它将目标跟踪看成位置变化和尺度变化两个独立问题，提出了一个高、宽、尺度数三维的滤波器，使用先计算平移位置再聚集尺度的“两步”法，即训练了两个滤波器，首先训练位置平移相关滤波器以检测目标中心平移，然后训练尺度相关滤波器来检测目标的尺度变化。</p><h3 id="CN"><a href="#CN" class="headerlink" title="CN"></a>CN</h3><p>CN（Color Naming）考虑到在遇到光照变化、形变、部分遮挡、背景干扰等问题时，颜色特征相比灰度特征能提供更丰富的信息以取得更好的效果，引入了颜色特征来扩展CSK，它将目标RGB（红绿蓝）三维空间的颜色特征映射为黑、蓝、棕、灰、绿、橙、粉、紫、红、白和黄11维空间的颜色特征的多通道颜色特征，后又降维至2维以保证实时性。Color Naming较RGB三原色特征更符合人类的感觉，对目标的表征能力更强，而且具有一定的光学不变性。</p><h3 id="SAMF"><a href="#SAMF" class="headerlink" title="SAMF"></a>SAMF</h3><p>SAMF也考虑了尺度问题，思路比较简单，采用k个尺度去采样，由于核相关操作的点乘需要固定尺度的输入，因此对采集到的样本作双线性插值成为固定尺度，然后再做相关操作。在特征方面，SAMF发现HOG和Color Naming有互补作用，考虑到和相关操作仅包含点乘和向量范数的计算使得多通道很容易被引入，因此使用了HOG和Color Naming多通道特征。</p><h3 id="KCF"><a href="#KCF" class="headerlink" title="KCF"></a>KCF</h3><p>KCF跟CSK是同一个团队提出的，它跟CSK的区别是就是作者对循环性质进行了完整的理论推导，引入HOG特征并提供了一种把多通道特征融合进相关滤波框架的方法，对CSK作了进一步的完善，是一个具有里程碑意义的工作。算法的详解和一些数学理论可以看看我之前的文章。</p><h3 id="DCF"><a href="#DCF" class="headerlink" title="DCF"></a>DCF</h3><p>DCF与KCF出自同一篇paper，不同的是KCF使用的是高斯核，DCF使用的是线性核。</p><blockquote><p>注意：这里的DCF（Dual Correlation Filter）和之后一些文章中提到的DCF（Discriminative Correlation Filter）是两个不同的概念，请注意，别搞错了。</p></blockquote><h3 id="FCT"><a href="#FCT" class="headerlink" title="FCT"></a>FCT</h3><p>FCT（Fast Compressive Tracking）和CT一样，也是使用了压缩感知，主打速度。相比之前，FCT提出了一种由粗到精的搜索策略，而不是穷尽搜索。首先在一个较大的搜索半径内选择一个较大的搜索步长，得到一个粗糙的位置，然后以该位置为中心，在一个较小的搜索范围内，以一个较小的搜索步长进行搜索，最后得到跟踪目标的位置，这样就能在不降低最终精度的前提下加速寻找过程。由于可证明CT特征具有尺度不变特性，FCT在采集候选区域时增加了尺度因子，即在同一位置采集三个尺度的候选区域，从而得到当前帧的尺度。此外，作者采用了每5帧更新一次尺度的策略。</p><h3 id="CMT"><a href="#CMT" class="headerlink" title="CMT"></a>CMT</h3><p>CMT用成对的特征点之间角度的变化来判断目标的旋转情况，此外还用特征点投票的方式来确定目标的位置。为了避免尺度变化引起的投票不准，也就是从特征点出发指向目标位置的投票向量越过了目标中心或者没达到目标中心，作者用欧氏空间和原图像空间之间各对特征点之间距离的比值来进行修正。基于投票出的目标位置的聚类，作者还给出了一种一致性的判别方法，将聚集最多数特征点的投票位置视为一致性聚类，并将投票至其他位置的特征点视为错误从而移除。</p><h2 id="2015"><a href="#2015" class="headerlink" title="2015"></a>2015</h2><p>下面是VOT2015的排名结果。<br><img src="/computer-vision20200215214240/VOT2015.png" title="VOT2015"></p><h3 id="SO-DLT"><a href="#SO-DLT" class="headerlink" title="SO-DLT"></a>SO-DLT</h3><p>SO-DLT针对DLT的缺陷进行改进，使得CNN更加适用于目标跟踪。由于目标跟踪的目的是将物体从背景中分离出来而不是全图识别，DLT的训练方法和标签化输出就不是很合适了。<br>但是，由于当时跟踪方向标注数据的匮乏，作者还是不得不使用ImageNet图像检测数据集来进行预训练，事实证明这是有效的，因为目标检测和目标跟踪两个不同的task中存在一样的共性信息。不同于标签化的单个数值输出，SO-DLT输出的是一个50x50的像素级的概率图。<br>由于上述训练方法训练的是CNN从非物体中提取出物体的能力，因此在实际跟踪接收到第一帧时，还需根据目标对网络进行微调，否则会跟踪出视频或者图像序列中所有的无论是目标与否的物体。<br>类似DSST，SO-DLT采用的是先预测目标中心位置，然后再从小到大确定尺度的策略，如若扩展到预设的最大尺度来检测的概率图依旧达不到要求，则认为已经跟丢目标。<br>此外，为了提升鲁棒性，作者采用了两个CNN网络共同决策而以不同方式更新的策略。两个网络分别针对的是short-term和long-term。针对short-term的网络在负样本的概率图响应和超过一定阈值时进行更新，为的是防止负样本与目标响应近似而导致漂移；针对long-term的网络在当前帧预测结果的置信度达到一定水平以上时才进行更新，因为此时可认为框出的目标较为可信。<br>每次更新时需要采集正负样本，SO-DLT对正负样本的提取方法比较简单，在目标位置及周围形成一个类似九宫格的区域，在中间格用四种尺度提取正样本，对周围的8格采集负样本。</p><h3 id="MDNet"><a href="#MDNet" class="headerlink" title="MDNet"></a>MDNet</h3><p>MDNet设计了一个轻量级的小型网络学习卷积特征表示目标。作者提出了一个多域的网络框架，将一个视频序列视为一个域，其中共享的部分用来学习目标的特征表达，独立的全连接层则用于学习针对特定视频序列的softmax分类。<br>在离线训练时，针对每个视频序列构建一个新的检测分支进行训练，而特征提取网络是共享的。这样特征提取网络可以学习到通用性更强的与域无关的特征。<br>在跟踪时，保留并固定特征提取网络，针对跟踪序列构建一个新的分支检测部分，用第1帧样本在线训练检测部分之后再利用跟踪结果生成正负样本来微调检测分支。<br>此外，MDNet在训练时还采用了难例挖掘技术，随着训练的进行增大样本的分类难度。<br><img src="/computer-vision20200215214240/难例挖掘.png" title="难例挖掘"></p><h3 id="SRDCF"><a href="#SRDCF" class="headerlink" title="SRDCF"></a>SRDCF</h3><p>SRDCF主要考虑到若仅使用单纯的相关滤波，可能会存在边界效应，也就是相关滤波采用循环移位采样导致当目标移位到边缘时会被分割开，此时得到的样本中就没有完整的目标图像从而失去效果。<br>于是，作者采用了大的采样区域，用两倍区域进行循环移位，这就保证了无论如何移位，获得的样本中都能有一个完整的目标存在。<br>然而，由于上述移位方式会导致背景信息被夹在横向以及纵向的两个样本之间而出现在图片的中间区域，这会导致模型判别力的退化。因此作者在移位之前先在滤波器系数上加入权重约束（类似于惩罚项）：越靠近边缘权重越大，越靠近中心权重越小。这就使得滤波器系数主要集中在中心区域，从而让背景信息的影响没有那么明显。尽管作者采用了埃尔米特矩阵的共轭对称性来提升计算量，但是由于SRDCF破坏了原本闭式解的结构使得优化问题只能通过迭代求解，因此速度比较缓慢。</p><h3 id="DeepSRDCF"><a href="#DeepSRDCF" class="headerlink" title="DeepSRDCF"></a>DeepSRDCF</h3><p>DeepSRDCF在SRDCF的基础上，将handcrafted的特征换为CNN的特征，关注点也在解决边界效应。作者使用DCF作为网络的最后一层，也就是对之前卷积网络输出的每一个通道的CNN特征都训练一个滤波器用于分类。作者还对不同的特征进行了实验，说明了CNN特征在解决跟踪的问题采取底层的特征效果会比较好（DeepSRDCF仅用了PCA降维处理的第一层），说明了跟踪问题并不需要太高的语义信息。</p><h3 id="HCF"><a href="#HCF" class="headerlink" title="HCF"></a>HCF</h3><p>HCF的主要贡献是把相关滤波中的HOG特征换成了深度特征，它使用的是VGG的3、4、5三个层来提取特征，针对每层CNN训练一个过滤器，并且按照从深到浅的顺序使用相关滤波，然后利用深层得到的结果来引导浅层从而减少搜索空间。</p><h3 id="FCNT"><a href="#FCNT" class="headerlink" title="FCNT"></a>FCNT</h3><p>FCNT较早地利用CNN网络底层和顶层不同的表达效果来做跟踪。不同于以往的工作把CNN看成一个黑盒而不关注不同层的表现，FCNT关注了不同层的功能，即发现：顶层的CNN layer编码了更多的关于语义特征的信息并且可以作为类别检测器；而底层的CNN layer关注了更多局部特征，这有助于将目标从目标中分离出来。这个发现在之后的许多工作中也得到了应用和体现。如下图所示，这里的a图表示的是ground truth，b图表示的是使用VGG的conv4-3，也就是第10层产生的热力图，c图是通过conv5-3也就是第13层产生的热力图。<br><img src="/computer-vision20200215214240/热力图.jpg" title="热力图"><br>可以看到，较低维的CNN layer（conv4-3）能够更精准地表示目标的细粒度信息，而较高维的CNN layer（conv5-3）热力图显示较模糊，但对同类别的人也做出了响应。这就是说，顶层缺少类内特征区分，对类间识别比较好，更适合作语义分割；底层则反之，能够更好地表达目标的类内特征和位置信息。<br>基于不同层（顶层和底层）之间提取特征的不同，作者提出了一种新的tracking方法，利用两种特征相互补充辅助，来处理剧烈的外观变化（顶层特征发挥的作用）和区分目标本身（底层特征发挥的作用）。由于feature map本身是有内在结构的，有很多的feature map对目标的表达其实并没有起到作用，因此作者设计了一种方法来自动选择高维CNN（GNet）或者低维CNN（SNet）上的feature map，同时忽略另一个feature map和噪声。在线跟踪时，两个网络一起跟踪，采用不同的更新策略，并在不同的情况下选择不同的网络输出来进行预测。<br>顺便提一下，为了简化学习任务，降低模型复杂度，作者采用了稀疏表示的方法。<br>关于FCNT的一些相关概念和具体按步骤的细节实现，可以参考一下我之前写的文章。</p><h3 id="LCT"><a href="#LCT" class="headerlink" title="LCT"></a>LCT</h3><p>LCT主要针对的是long-term tracking的问题。作者配置了一个detector，用于跟丢之后快速重检测。LCT用了两个滤波器，一个是用于平移估算的$R_{c}$，使用padding并施加汉宁窗（一种余弦窗），结合了FHOG和一些其他的特征；另一个是用于尺度估计的$R_{t}$，不使用padding和汉宁窗，使用HOG特征，此外$R_{t}$还用于检测置信度，用来决定是否更新模型和是否重检测。</p><h3 id="CCT"><a href="#CCT" class="headerlink" title="CCT"></a>CCT</h3><p>CCT借鉴KCF中kernel trick的特性和DSST中将定位和尺度估计两步分离的思想，对DSST只利用本来的特征空间表征目标的不足进行改进，在核特征空间对目标进行表征并用尺度因子扩展KCF的核相关滤波器，也就是为了保证计算效率和连贯性，利用尺度因子将每一帧目标尺度都统一为初始帧的目标尺度，然后使用核相关滤波器进行位置估计。然后，通过和DSST一样的方式再进行尺度估计。<br>为了应对漂移的问题，CCT使用了一个在线CUR滤波器。CUR矩阵分解可以近似的表示原矩阵A，其中C是A的列而R是A的行，两者通过一种固定的方式从A中随机采样形成，这既保证了A的内在结构，可以反映A的低秩属性，也可以看作一种映射，即将过去的目标表征矩阵投影到一个可被证明具有误差上界的子空间。此外，作者还引入了一个基于失败检测的自适应学习率调整方法。</p><h3 id="CFwLB"><a href="#CFwLB" class="headerlink" title="CFwLB"></a>CFwLB</h3><p>CFwLB讨论了循环移位带来的边界效应的问题，提出了在目标外围扩大尺寸来进行循环移位的方法，使得有效样本的比例大大提高。此外，由于扩大尺寸后部分参数要在空间域而不是频域计算会导致效率降低，作者利用了增广拉格朗日方法（即在拉格朗日方法的基础上添加了二次惩罚项，从而使得转换后的问题能够更容易求解，不至于因为条件数变大不好求）来解决这个问题。</p><h3 id="KCFDP"><a href="#KCFDP" class="headerlink" title="KCFDP"></a>KCFDP</h3><p>KCFDP借鉴了目标检测中detection proposal的思想（主要用于减少计算和提高质量），来解决之前DCF系列算法中的尺度和长宽比变化的问题。对每一帧，KCFDP首先用KCF对上一帧输入（准确的说是之前每一帧的加权累积）作操作，得到当前帧的位置和响应分数$v$；随后利用EdgeBoxes（一种detection proposal方法）在KCF预测的位置周围搜寻proposal，选取其中的前200个，并排除其中与之前KCF得出的预测目标IoU大于0.9（认为结果一样，无需考虑）或者小于0.6（认为误判，不是目标）；最后，在剩余的proposal中，选择得分最高的proposal，与之前的响应分数$v$作比较：若小于$v$，则把KCF的结果作为预测结果且不更新尺度和长宽比（KCF算法本身具有该功能）；若大于$v$，则把该proposal作为预测结果，并利用该proposal的尺度和长宽比来更新目标的参数。</p><h3 id="HCFT"><a href="#HCFT" class="headerlink" title="HCFT"></a>HCFT</h3><p>HCFT构造了一种阶梯式的深层至浅层由粗到细的定位方法，结合深层网络的语义信息和浅层网络的高分辨率位置信息，其网络结构如下（和FPN很像）。<br><img src="/computer-vision20200215214240/HCFT结构.png" title="HCFT结构"><br>为了保持分辨率相同，作者对池化后的深层输出再进行双线性插值以复原原来的分辨率。假设$l$层最大响应处的坐标为$(\widehat{m},\widehat{n})$，HCFT通过以下式子来确定$l-1$层目标的位置。</p><script type="math/tex;mode=display">\underset{m,n}{argmax}f_{l-1}(m,n)+\gamma f_{l}(m,n)</script><script type="math/tex;mode=display">s.t.\left | m-\widehat{m} \right |+\left | n-\widehat{n} \right |\leq r</script><p>第二行的约束是为了浅层细粒度的位置需保持在深层粗粒度的位置附近，这样便完成了由粗到细的定位方法。<br>此外，在训练过程中需要采集正负样本，由于正负样本边界难以区分的模糊性和二值（也就是0，1标注）的正负样本的绝对性导致一点微小的正负样本区别就会导致drift。为此，作者将训练样本的标注回归到高斯方程的平滑标签。</p><h3 id="MUSTer"><a href="#MUSTer" class="headerlink" title="MUSTer"></a>MUSTer</h3><p>MUSTer模拟了人脑的记忆过程，类似于LSTM那样分成short-term和long-term两种memory，使用了相关滤波（short-term）和特征点检测（short-term+long-term），最后根据两种记忆形式的提供的输出来决策和进行滤波器的更新。<br>人脑的记忆分为感官、短时记忆、长时记忆三个阶段，MUSTer的设计基本采用了这样的三个step，如下图所示。<br><img src="/computer-vision20200215214240/MUSTer结构.png" title="MUSTer结构"><br>MUSTer的结构比较“纵横交错”，下面我对一些比较重要的部分做一个概述。<br>短时记忆和长时记忆都由特征点的集合构成。特征点数据集包括目标和背景两种样本，其中背景主要是用于遮挡的判断，即当位于bounding box中的背景特征点与目标特征点的比例超过一定值时，认为此时发生了遮挡。<br>对于特征点的匹配，作者采用了最近邻方法，在欧几里得空间根据余弦相似度（也就是两个向量余弦夹角的大小，角度越小，余弦值越大，相似度越高）来计算匹配置信度。为了判别离群值（outlier），还需计算第二近邻的相似度，如果第一近邻的相似度比上第二近邻的比值小于某个阈值，就说明该处特征点比较集中应该不是离群值。<br>长时记忆模块通过RANSAC估计的一个版本——MLESAC（引入似然度）来决定目标的状态，从而与相关滤波的输出结合。<br>短时记忆在每一帧都进行更新，若根据前面所说的方法判定为遮挡，则清空短时记忆；若此时并没有判定为遮挡，则用RANSAC估计输出的内围值（inlier）来替换之前的短时记忆。此外，为了避免多余的特征点出现，作者用网格划分目标template并根据相对位置分配ID，若出现重复的ID，则认为两者中之前的特征点是多余的。<br>长时记忆只在判断跟踪成功和无遮挡时进行更新。作者认为匹配失败的特征点能够表示目标发生变化的重要信息，因此长时记忆更新是针对匹配失败的点进行的，将匹配失败且位于bounding box外面的点移入背景数据集，而将匹配失败且位于bounding box内部的点移入目标数据集。模拟人脑，长时记忆采用的是一种对数形式下降的遗忘曲线。</p><h3 id="RPAC"><a href="#RPAC" class="headerlink" title="RPAC"></a>RPAC</h3><p>RPAC将相关滤波器应用于分块跟踪，并且借鉴了粒子滤波中的贝叶斯估计的思想，在提升鲁棒性的同时保证了速度。<br>作者对每一区块（part）都使用一个独立的相关滤波器，使用了PSR（体现置信度）和时域顺滑程度（用于判断遮挡等情况）两者结合来分配每一区块的权重，同时仅当这个权重大于阈值时才更新对应的滤波器以达到自适应更新的效果。这里每一个区块的输出都是一个confidence map，最后需要根据权重和相对位置组成一个大的confidence map用于接下来的预测。<br>为了防止部分区块漂移的问题出现，作者采用了贝叶斯估计框架，即选择使得状态（一组仿射运动参数）先验值最大的候选区域作为结果，考虑到多个confidence map之间重叠的部分直接求和会出现叠加的较大值而影响概率估计，作者根据每一个区块的最大响应值和尺寸来施加余弦窗，从而抑制多张图中较小值的叠加改变某些区块位置上的响应分布。<br>此外，对于偏离较远的区块，RPAC采用自动丢弃并利用其他区块重新生成的方法。得益于分块的方式，使得tracker对尺度变化也有适应性。</p><h3 id="RPT"><a href="#RPT" class="headerlink" title="RPT"></a>RPT</h3><p>RPT把目标物体看作一系列具有相似运动轨迹的patch的集合。作者基于粒子滤波的框架，用patch的两个属性来定义每个patch的可靠性：可追踪性和目标附着性。可追踪性直接用KCF输出的响应图取PSR来定义；目标附着性根据每个patch在近k帧的运动轨迹来定义，具体来说，就是认为正样本与其他正样本的运动轨迹是一致的且远离负样本，同时负样本与正样本的运动轨迹有很大的差别。其公式定义如下，其中$y_{t}$是正样本或者负样本的$\pm 1$标签（正负样本用bounding box来分割）。</p><script type="math/tex;mode=display">l(X)=y_{t}(\frac{1}{N^{-}}\sum_{j\in \Omega ^{-}}\left \| V-V^{(j)} \right \|_{2}-\frac{1}{N^{+}}\sum_{i\in \Omega ^{+}}\left \| V-V^{(i)} \right \|_{2})</script><p>由于粒子滤波是用函数（这里是可靠性函数）对后验概率分布做近似，由于无法达到理想状态（即完全一致），因此为了避免噪声的累积，粒子滤波类算法需要重采样来不断补入正确的信息。RPT采用的是不断补入新样本的方法而不是全部重采样替换的方法，作者在两种情形下采样新的样本：当正样本或者负样本其中一者的比例过高时，采集新样本来平衡；当跟踪置信度（这里定义为PSR）较低时，采集新样本，这种情况往往出现在遇到缺乏纹理的目标物体时。此外，作者对离目标过远的patch进行舍弃，具体就是划定一个比bounding box更大的矩形框，对超出这个矩形框的patch做舍弃。<br>可以说，作者把patch分成三类。第一类是positive patch，也就是在目标上的；第二类是贴着目标周围一圈的negative patch，这对下一帧区分物体和背景很有帮助；第三类就是离目标很远的negative patch，这些patch的作用就比较弱，因此舍弃。</p><h2 id="2016"><a href="#2016" class="headerlink" title="2016"></a>2016</h2><p>下面是VOT2016的排名结果。<br><img src="/computer-vision20200215214240/VOT2016.png" title="VOT2016"></p><h3 id="DLSSVM"><a href="#DLSSVM" class="headerlink" title="DLSSVM"></a>DLSSVM</h3><p>DLSSVM延续之前的Struck，利用结构化SVM，在优化的阶段做了一些改进进行提速。其实结构化SVM分类器非常强大，但是因为它求解优化的过程比较复杂以及使用稠密采样（粒子滤波或者滑窗采样）比较耗时，使得结构化SVM的速度成为一个瓶颈，因此不如一些使用相关滤波的SOTA的算法。</p><h3 id="C-COT"><a href="#C-COT" class="headerlink" title="C-COT"></a>C-COT</h3><p>C-COT（连续空间域卷积操作）发现单一分辨率的输出结果存在扰动，因此作者想到利用CNN中的浅层表观信息和深层语义信息相结合。然而之前的DCF系列算法仅能使用单一分辨率的特征图，这就无法使用预训练CNN中不同分辨率的不同层，这是限制其效果的重要因素。因此，作者提出一种连续周期的插值运算符以利用不同空间分辨率的响应，在频域进行插值得到连续空间分辨率的响应图，最后通过迭代求解最佳位置和尺度（用0.96，0.98，1.00，1.02，1.04五种缩放倍率去搜索）。<br>和名称一样，C-COT最重要的贡献是它把图像$N_{d}$个像素点的离散分布变成了周期为$T$的连续空间响应图。由于本文的理论功底很深，我之前并没有看懂，后来经学长的指点才有所感悟。事实上，这里的插值运算符做的并不是插值的事，而是用一种近似的方式将离散信号重构为连续信号。虽然会导致计算量剧增，但这是一个很重大的突破。<br>文中还提出了一种使得各个分辨率通道的特征自然融合至相同分辨率的方法，这里相同分辨率可理解为最后的各个响应图在空间上拥有相同的样本点数。作者首先对各个不同分辨率的通道进行插值，然后使用对应的滤波器在连续的空间域内卷积，最后将响应求和得到最终的置信度方程。<br>C-COT使用的是类似SRDCF的框架，也引入了空间正则项，当远离目标中心是施加较大的惩罚，这使得能够通过控制滤波器的大小来学习任意大小的图像区域。C-COT在每一帧也会采集一个训练样本，根据过去帧数的远近来设置每个采集样本的重要性权重（每次都做归一化），并且设置了最大的样本容量，当超出容量时删去重要性权值最小的样本。不同于SRDCF使用Gauss-Seidel迭代法，C-COT使用Conjugate Gradient方法来提高效率。<br>得益于浅层特征的高分辨率，C-COT能够达到sub-pixel的精度，也就是仅次于像素级别的精确度。位置细化的过程就是上面所说的用共轭梯度法迭代的过程，在C-COT的代码中有一个迭代次数设置，被设置为1，即就使用一步迭代优化后的位置。换句话说，在当前长时跟踪算法本身误差之下，更精细的位置意义不大。</p><h3 id="SRDCFdecon"><a href="#SRDCFdecon" class="headerlink" title="SRDCFdecon"></a>SRDCFdecon</h3><p>SRDCFdecon针对在线跟踪时采集的样本中有一部分质量不佳的问题，不同于之前把采样和样本的选择作为一个独立的模块，作者提出了一种将样本权重统一到模型参数中的损失函数。<br>不同于之前“加入训练集or舍弃”这样二选一的样本选取方式，SRDCFdecon使得样本的重要性权重连续，同时在跟踪的过程中能够完成权重的重新分配和先验的动态变化。这里的先验其实可以看作对权重的一种约束，使权重在近几帧逐渐增大。作者用一种比较巧妙的方式来控制先验的影响力，具体来说，对权重的正则项为$\frac{1}{\mu }\sum_{k=1}^{t}\frac{\alpha _{k}^{2}}{\rho _{k}}$，当$\mu$趋向于无穷时，损失函数求解得到仅有当前帧的权重$\alpha$趋向于1，相当于丢弃了之前的样本，仅接收并保存当前帧的新样本；当$\mu$趋向于零时，$\alpha$将趋向于先验$\rho$。</p><h3 id="Staple"><a href="#Staple" class="headerlink" title="Staple"></a>Staple</h3><p>Staple提出了一种互补的方式。考虑到HOG特征对形变和运动模糊比较敏感，但是对颜色变化能够达到很好的跟踪效果，color特征对颜色比较敏感，但是对形变和运动模糊能够有很好的跟踪效果，因此作者认为若能将两者互补就能够解决跟踪过程当中遇到的一些主要问题。于是，Staple使用HOG-KCF与color-KCF结合算法对目标进行跟踪，速度很快，效果也很好。</p><h3 id="SINT"><a href="#SINT" class="headerlink" title="SINT"></a>SINT</h3><p>SINT运用匹配学习的思想，最早地把孪生网络（Siamese Network）应用于目标跟踪。它通过孪生网络直接学习目标模板和候选目标的匹配函数，并且在online tracking的过程中只用初始帧的目标作为模板来实现跟踪。</p><h3 id="TCNN"><a href="#TCNN" class="headerlink" title="TCNN"></a>TCNN</h3><p>TCNN使用一个树形的结构来处理CNN特征。作者利用可靠性来分配预测目标的权重，采用的更新策略是每10帧删除最前的节点，同时创建一个新的CNN节点，选择能够使新节点的可靠性最高的节点作为其父节点。这样一直保持一个active set，里面是10个最新更新的CNN模型，用这个active set来做跟踪。TCNN效果较之前有一定提升，但是速度比较慢，而且比较消耗存储空间。<br><img src="/computer-vision20200215214240/TCNN.jpg" title="TCNN"></p><h3 id="SKCF"><a href="#SKCF" class="headerlink" title="SKCF"></a>SKCF</h3><p>SKCF把之前KCF中用于确定搜索区域的余弦窗换成了高斯窗，这么做有两点主要的好处。<br>首先，当搜索区域固定时，余弦窗的带宽就是固定的了，而高斯窗则可以通过调整方差来改变中间响应比较高的区域的宽度。可以这么理解，我们从二维的余弦函数和高斯函数来看，假设搜索区域的宽度是$\pi $，那么距离边缘$\frac{\pi }{6}$的位置一定是中间最大值下降一半的位置，而高斯函数的形状则还受方差控制。这一特性使得高斯窗在搜索区域确定时能够更好地适应目标尺寸，此外论文中还提到了这种方法能方便减轻计算量。<br>此外，高斯分布的傅里叶变换依旧是高斯分布。这种特性可以抑制频谱泄露的问题。简单来说，频谱泄露就是信号从时域转化到频域后，除了原本应该出现的谱线，其旁边还会漏出一些小的频谱，从而造成干扰。因此，抑制频谱泄露能够保持前景与背景在频域内的区分度。<br>SKCF还改进了之前一些基于特征点的算法的不足。由于之前的一些算法在最终决策时用矩形框来划定有效的特征点，并分配相同的权重做出决策。作者认为这样相当于间接的认为目标是矩形的，而大多数目标的几何结构往往不是矩形。于是SKCF对各个特征点采用从中心到周围逐渐减少权重分配的方式，能更好地适应目标的几何结构。<br>SKCF还是用了英特尔CCS（复数共轭对称）文件格式，无论是用在SKCF还是KCF上，计算速度相比原来的KCF都提升了将近一倍，或许是共轭对称减少了一半计算量。</p><h3 id="MRF"><a href="#MRF" class="headerlink" title="MRF"></a>MRF</h3><p>MRF提出了一种基于马尔科夫随机场的模型，挖掘各个patch和目标之间的联系（弹性能量）并判断每个patch的遮挡情况。作者发现当响应值较低时并不能认为该patch被遮挡了，也有可能是目标外观变化等情况。此外作者还发现当发生遮挡时，会出现较大的响应值分散分布的现象。于是当patch中响应值高于$\eta s_{max}^{k}$的像素个数占比高于阈值时（这里的$\eta$是一个小于1的因子），就认为该patch被遮挡。<br>对于尺度变化，作者发现当尺度变小时patch之间的重叠会变多，因此通过计算初始帧的patch距离和当前帧的patch距离之比来确定尺度缩放的比例。<br>不同于之前的马尔科夫随机场模型，作者在文中还给出了一种高效的置信度传播方法。</p><h3 id="GOTURN"><a href="#GOTURN" class="headerlink" title="GOTURN"></a>GOTURN</h3><p>GOTURN提出了一种类似孪生网络的框架，将裁剪过的前一帧和当前帧分别通过在ImageNet上预训练的backbone提取特征，然后用三层全连接层进行特征比较并进行回归。其中前一帧的图像在裁剪时将其置于裁剪后图像的中心，并在周围做一定的扩充以吸收更多上下文信息。当前帧裁剪区域也就是搜索区域由上一帧的位置来确定。<br>GOTURN最突出的贡献就是把基于深度神经网络的速度第一次达到了100FPS。作者通过在视频和静态图像上进行离线训练，在跟踪时不更新来达到这种效果。由于训练集的缺乏，作者冻结之前在ImageNet上预训练的CNN权重，在离线训练时仅更新FC层。考虑到实际跟踪时的特点，作者还设计一种平滑的运动模型。作者发现目标相邻两帧中心点的坐标关于尺度的增量呈均值为0的拉普拉斯分布（类似高斯分布，只不过中间是尖的），也就是说一般物体逐帧的运动是较小的。因此，作者对训练数据做增广处理，使得随机裁剪得到的样本能服从拉普拉斯分布。由于训练方式和网络设计，使得GOTURN仅对目标敏感而不是对类敏感（比如行人检测能检测各种行人而不能检测车）。</p><h3 id="SiamFC"><a href="#SiamFC" class="headerlink" title="SiamFC"></a>SiamFC</h3><p>SiamFC使用孪生网络来解决数据稀少和实时性要求对深度学习在目标跟踪中的限制。作者以第一帧的BBox的中心为中心裁剪出一块图像，并将其缩放至127x127作为template，并保持不变。在后续帧中，search image也用类似的方法得到。分别将template和search image通过5层不带padding且不在线更新的AlexNet，然后用互相关层做相关操作得到输出的score map。响应最大值和中心的偏差表示位移。此外作者用一个小批量的不同尺度的图像去前向传播来实现多尺度判断。<br>由于对于search image来说，表示通过CNN的卷积方程是全卷积的，因此可以使用比template大的search image，也就是可以在它上面的各个子窗口进行计算。<br>基于卷积的平移等变性，我们可以通过score map得到目标的位置，即当目标平移了$n$时，相应的就会在score map上平移$\frac{n}{stride}$。之所以仅使用5层而不是更深的网络，是因为SiamFC没有使用padding来使得网络能够更深。之所以不使用padding，是因为一旦加入了padding，会使得图像边缘像素的响应值在平移的同时会发生改变，这就不利于最后的定位了。具体来讲，就是当目标处于画面中央时，padding进来的是context；而当目标处于画面边缘时，padding进来的就是0了，这种信息的不同会影响定位和目标的判断。</p><h2 id="2017"><a href="#2017" class="headerlink" title="2017"></a>2017</h2><p>下面是VOT2017在隐藏数据集上的排名结果。<br><img src="/computer-vision20200215214240/VOT2017.png" title="VOT2017"></p><h3 id="ECO"><a href="#ECO" class="headerlink" title="ECO"></a>ECO</h3><p>ECO（高效卷积算子）主要是为了解决C-COT速度慢的问题。从参数降维、样本分组和更新策略三个角度对其改进，在不影响算法精确度的同时，将算法速度提高了一个数量级。<br>为了减少模型参数，考虑到在C-COT中许多滤波器的能量（可理解为贡献）小得几乎可以忽略，因此ECO使用了一个较小的滤波器子集，且原来的滤波器都可以用这个子集中的滤波器线性组合表示（即乘上一个行数为高维数、列数为低维数的矩阵）。子集的选取方法就是简单的选取能量高于某一阈值的滤波器，其效果类似于PCA。作者提出了一个因子化的卷积算子来学习这个子集，用PCA初始化，然后仅在第一帧有监督地优化这个降维矩阵，在之后的帧中直接使用，相比C-COT模型参数量大大降低，同时也减轻了计算和存储的负担。<br><img src="/computer-vision20200215214240/卷积因式分解大大降低参数量.png" title="卷积因式分解大大降低参数量"><br>为了减少样本数量，作者提出了一个紧凑的样本空间生成模型，采用高斯混合模型（GMM，可理解为当有多个聚类时用多个不同的高斯模型来表示更好）来合并相似样本。当GMM的聚类（component）数量超过阈值时，如果有权重低于阈值的component，则丢弃之；否则，就合并最近的两个component。如此就可以建立更具代表性和多样性的样本集，既保持样本之间的差异性，也减少了存储的样本数量。<br><img src="/computer-vision20200215214240/样本分组.png" title="样本分组"><br>此外，作者还提出了一种稀疏的更新策略，即每隔N帧（实验发现5帧左右最好）才更新一次参数。由于样本集是每帧更新的，这种稀疏更新策略并不会错过间隔期的样本变化信息。此外，这种方法的另一个好处就是把原本的用逐帧单独样本进行更新，变成了用连续几帧所采集的样本所构成的batch来进行批处理的更新，这样就减小了在某帧遮挡、突变时过拟合的可能性。由此，稀疏更新策略不但提高了算法速度，而且提高了算法的稳定性。</p><h3 id="CREST"><a href="#CREST" class="headerlink" title="CREST"></a>CREST</h3><p>CREST提出了将DCF构建成一层卷积神经网络，并且引入了残差学习来应对目标外观变化带来的模型退化。<br>考虑到之前的DCF系列没有发挥端到端训练的优势和空间卷积与相关滤波中循环输入点乘的相似性，作者用一层卷积神经网络来代替DCF的作用，这不仅使得模型能够通过反向传播训练，同时还避免了边界效应。<br>由于上述一层网络难以达成在多种情况下网络输出和ground truth的一致（模型复杂度较低易受干扰），而若使用多层网络很可能会导致模型退化（我理解为过拟合导致的），作者引入空间残差和时间残差。设我们希望最佳的输出为$H(x)$，而上述单层网络的输出是$F_{B}(x)$，为了补足某些时候（尤其是复杂情况下）单层网络的输出与希望最佳的输出之间的差距，引入残差项$F_{R}(x)=H(x)-F_{B}(x)$。在训练时，$F_{B}(x)$和$F_{R}(x)$中的参数一起训练，使得遇到特殊情况（遮挡、运动模糊等）时，$F_{R}(x)$能够补足纠正$F_{B}(x)$不稳定的响应结果。<br>空间残差和单层网络都是利用当前帧作为输入，考虑到空间残差有时候也会失效，作者又引入了把初始帧作为输入的时间残差，最终表达式如下：</p><script type="math/tex;mode=display">F(X_{t})=F_{B}(X_{t})+F_{SR}(X_{t})+F_{TR}(X_{1})</script><blockquote><p>注意：论文中第一项为$F_{R}(X_{t})$，可能有误，为此我作了修改。</p></blockquote><p>另外，CREST采用当前帧最大响应尺度和上一帧尺度加权求和的方法来决定当前帧的最终预测尺度，从而使尺度能够平滑地变化。</p><h3 id="LMCF"><a href="#LMCF" class="headerlink" title="LMCF"></a>LMCF</h3><p>LMCF借鉴了KCF的循环特征图、Struck的结构化SVM，使用相关滤波在频域加速计算，从而解决了之前结构化SVM系列算法（Struck、DLSSVM）的速度问题。<br>在前向追踪时，LMCF考虑到画面中相似物体的干扰，提出了一种多峰值的目标跟踪算法（Multimodal Target Tracking），即对高于某一阈值的响应峰值做二次检测，把response map和一个用于筛选的二值矩阵作点乘，相当于把不是峰值的位置滤为0。对于通过筛选的峰值，以每个峰值为中心提取patch并用之前的方法再计算一遍峰值，取此时最大的峰值作为结果。<br>在模型更新时，LMCF提出了一种高置信度的更新策略（High-confidence Update），由于LMCF主要关注的是实时性，所以希望在算法简单的情况下能够减少失误。在传统的方法中，一般是当最大响应的峰值高于某一个阈值时（认为没跟丢目标），就对模型进行更新；否则若没有响应值超过峰值，就不对模型进行更新。而该工作的实验发现，当目标被遮挡时，响应图会震荡得非常厉害（存在多个较大的峰值），但同时最大响应的峰值仍旧会很高，这就会指导模型进行错误的更新并导致最后跟丢目标。于是作者提出了一个APCE值，定义如下。</p><script type="math/tex;mode=display">APCE=\frac{\left | F_{max}-F_{min} \right |^{2}}{mean(\underset{w,h}{\sum }(F_{w,h}-F_{min})^{2})}</script><p>只有当最大响应的峰值比较明确，即远超response map中的其他的响应时，APCE值才会比较大。因此LMCF仅当最大峰值和APCE超过阈值时才允许对模型进行参数和尺度模型的更新。</p><h3 id="DeepLMCF"><a href="#DeepLMCF" class="headerlink" title="DeepLMCF"></a>DeepLMCF</h3><p>同LMCF，不同之处是使用了CNN特征。</p><h3 id="MCPF"><a href="#MCPF" class="headerlink" title="MCPF"></a>MCPF</h3><p>MCPF结合多任务相关滤波器（MCF）和粒子滤波器，这里的多任务相关滤波器指的是利用了多种特征滤波器之间的相关性。作者对K种特征，定义了参数$z_{k}$去选择具有判别力的训练样本。作者发现，各个特征中的$z_{k}$往往会选择具有相同循环移位的样本，因此不同的$z_{k}$应该具有相似性和一致性。为此，作者在损失函数中增加了矩阵Z的混和范数。<br>考虑到粒子滤波通过密集采样来覆盖状态空间中的所有状态，这会大大增加计算量，而且并不能保证很好地包括目标物体在一些情况下的状态。因此作者利用MCF对每个采样的粒子进行引导，使其更接近目标的状态分布。这样就可以在提升效果的同时每次采集较少的粒子，从而提高计算效率。另一方面，粒子滤波的密集采样能够在目标尺度发生变化时覆盖状态空间，这就解决了单一相关滤波器的尺度问题。<br>算法的流程分为四步：<br>首先，使用转移模型生成粒子并且重采样。<br>然后，使用MCF对粒子进行微调，使其转移到比较合适的位置。<br>接着，利用响应更新MCF的参数。<br>最后，通过求样本均值问题来决定目标的状态，也就是位置等参数。<br>可见，这里相关滤波仅起到指导的作用，最后的决策由粒子滤波器做出。<br>顺便一提，MCPF使用了Accelerated Proximal Gradient来解决这里不可微分的凸优化问题（含有范数）。</p><h3 id="CFNet"><a href="#CFNet" class="headerlink" title="CFNet"></a>CFNet</h3><p>CFNet结合相关滤波的高效性和CNN的判别力，考虑到端到端训练的优势，从理论对相关滤波在CNN中的应用进行了推导，并将相关滤波改写成可微分的神经网络层，将特征提取网络整合到一起以实现端到端优化，从而训练与相关滤波器相匹配的卷积特征。<br>CFNet采用孪生网络的架构，训练样本（这里指用来匹配的模板）和测试样本（搜索的图像区域）通过一个相同的网络，然后只将训练样本做相关滤波操作，形成一个对变化有鲁棒性的模板。为了抑制边界效应，作者施加了余弦窗并在之后又对训练样本进行了裁剪。<br>在对比实验中作者发现仅使用一层卷积层时CFNet相比Baseline+CF效果提升最显著，对此作者的解释是可以把相关滤波层理解为测试时的先验知识编码，当获得足够的数据和容量时（增加CNN层数时），这个先验知识就会变得冗余甚至是过度限制。</p><h2 id="2018"><a href="#2018" class="headerlink" title="2018"></a>2018</h2><p>下面是VOT2018 short-term的排名结果。<br><img src="/computer-vision20200215214240/VOT2018.png" title="VOT2018"></p><h3 id="STRCF"><a href="#STRCF" class="headerlink" title="STRCF"></a>STRCF</h3><p>STRCF（时空正则相关滤波器）主要针对SRDCF的速度做出改进，同时在精度上也有很好的提高。作者发现SRDCF速度很慢的两个原因是：每次对多张图片进行训练打破了循环矩阵的结构，从而无法发挥循环矩阵的计算优势；巨大的线性方程组和Gauss-Seidel迭代法没有闭式解，效率较低。对此，STRCF提出了引入时间正则和ADMM算法。<br>受online Passive-Aggressive learning的启发，STRCF在SRDCF空间正则的基础上引入了时间正则。我们可以对比两者的回归求解公式具体来看一下。<br>SRDCF：</p><script type="math/tex;mode=display">arg\underset{f}{min}\sum_{k=1}^{T}a_{k}\left \| \sum_{d=1}^{D}x_{k}^{d}\ast f^{d}-y_{k} \right \|^{2}+\sum_{d=1}^{D}\left \| w\cdot f^{d} \right \|^{2}</script><p>STRCF：</p><script type="math/tex;mode=display">arg\underset{f}{min}\frac{1}{2}\left \| \sum_{d=1}^{D}x_{t}^{d}\ast f^{d}-y \right \|^{2}+\frac{1}{2}\sum_{d=1}^{D}\left \| w\cdot f^{d} \right \|^{2}+\frac{\mu }{2}\left \| f-f_{t-1} \right \|^{2}</script><p>这里的$w$表示空间正则化矩阵，越靠近边缘值越大；$f$表示相关滤波器，$f_{t-1}$表示的是$t-1$帧时的滤波器。<br>忽略每项之前的常数系数，我们可以看到两式的第二项是一样的，也就是STRCF保留了SRDCF的空间正则来抑制边界效应；在第一项中，STRCF没有对过去的每一帧进行求和来训练，这就减小了计算量；同时STRCF加入了第三项时间正则，使得新得到的滤波器与之前的滤波器之间的变化尽可能小，相当于保留了之前的信息。<br>这么做有两点好处：首先，STRCF可以看作SRDCF的一个合理近似，能很好地发挥后者同样的作用；此外，由于时间正则的引入，使得STRCF不易于在当前帧上过拟合，在遇到遮挡或者超出画面等问题时，STRCF能很好地保持与之前滤波器的相似度从而降低了跟踪器完全跟丢到另一个物体上去的可能，这一定程度上提高了STRCF的精度。<br><img src="/computer-vision20200215214240/STRCF和SRDCF对比.png" title="STRCF和SRDCF对比"><br>此外，ADMM算法的引入使得最优化求解问题有了闭式解，这比Gauss-Seidel迭代法用稀疏矩阵求解要快得多。得益于SRDCF的凸性，ADMM也能收敛到全局最优点。</p><h3 id="UPDT"><a href="#UPDT" class="headerlink" title="UPDT"></a>UPDT</h3><p>UPDT区别对待深度特征和浅层特征，主要考虑的是缺少数据和深层卷积在增加语义的同时降低分辨率这两个问题。作者分析了数据增强（flip，rotation，shift，blur，dropout）和鲁棒性训练（也就是tracker应对各种复杂场景和恢复的能力，可以通过扩大正样本的采样范围来训练）对deep feature和shallow feature分别的影响，发现deep feature能通过数据增强来提升效果，同时deep feature主打的是鲁棒性而不是精度；相反，shallow feature经数据增强后反而降低了效果，但同时它能够很好地保证精度。因此，作者得出了深度模型和浅层模型应该独立训练，最后再融合的方案。<br>作者在文中还定义了Prediction Quality Measure，考虑了精度和鲁棒性，精度用响应分数的锋利程度（sharpness）来体现，鲁棒性则用响应值的幅度来表示，幅度越高表明tracker越确信跟踪的目标，也就是鲁棒性越高。关于具体公式的推导和分析，以及Prediction Quality Measure在预测过程中的具体使用可以看一看原文。</p><h3 id="ACT"><a href="#ACT" class="headerlink" title="ACT"></a>ACT</h3><p>ACT使用了强化学习，构建了由Actor和Critic组成的学习框架。离线训练时，通过Critic指导Actor进行强化学习；在线跟踪时，使用Actor来定位，Critic进行验证使得tracker更加鲁棒。不同于之前的搜索方案（随机采样或者通过一系列分离的action来定位），ACT希望的是搜索一步到位。这步最优的action也就是离线强化学习所关注的行动，而强化学习的状态由输入到网络中bounding box中框出的图片定义，奖励值根据IoU来定义。<br>在训练的过程中，由于action space比较大，因此要获得一个正奖励比较困难（随机采取action的话IoU恰好高于阈值的可能性较小）。因此作者利用了第一帧的信息来初始化Actor以适应新的环境。同样的，由于巨大的action space，原本DDPG方法中的噪声引入就不适合跟踪任务了，因此在训练前期，Actor采取的行动以某种概率被一种专家决策所替代。随着训练的进行，Actor越来越强大，这时就逐渐减弱专家决策的指导作用。<br>在跟踪的初始帧，作者首先在第一帧提供的ground truth周围采集多个样本。然后使用Actor对这些样本作action，根据得分对Actor进行一次微调；对Critic，根据打分和ground truth也进行一次初始化训练。<br>在之后的跟踪过程中，若Critic的给分大于0，则采用Actor的输出一步到位地预测下一帧的目标；否则，再使用Critic在上一帧周围采集的样本中选出最优作为目标，完成重定向。此外，可以认为Actor在离线训练时已经比较稳定了，因此在跟踪过程中只对Critic进行更新，且仅在Critic给分小于0（认为Critic没能很好地适应目标的变化）时，取前十帧的样本来更新Critic。</p><h3 id="DRT"><a href="#DRT" class="headerlink" title="DRT"></a>DRT</h3><p>DRT引入了可靠性的概念，考虑到空间正则、掩模等抑制边界效应的方法都不能抑制bounding box内部的背景信息，同时这些方法会导致滤波器的权重倾向于集中在某些较小的区域（主要是中央的关键区域，我理解为边缘区域被抑制掉了，因此学习时自然不会去分配权重），作者认为这是不利于目标跟踪的（容易个别不可靠的区域被误导）。为此，作者提出了DRT，它主要是将滤波器分成了一个base filter和一个reliability term的element-wise product：</p><script type="math/tex;mode=display">w_{d}=h_{d}\odot v_{d}</script><p>这里的base filter用于区分目标和背景；reliability term用于决定每片区域的reliability，由目标区域每一个patch的reliability值加权求和决定：</p><script type="math/tex;mode=display">v_{d}=\sum_{m=1}^{M}\beta _{m}p_{d}^{m}</script><p>这里的$p$是对每一个patch的掩模，用于确定每个patch做相关操作的区域；$\beta$有上下界的限定，目的就是为了降低feature map中响应不平衡的影响，防止由于响应的集中而导致仅有一小块区域被关注。</p><script type="math/tex;mode=display">f(h,\beta ;X)=f_{1}(h,\beta ;X)+\eta f_{2}(h;X)+\gamma \left \| h \right \|_{2}^{2}</script><p>需要最小化的目标方程包括三项：分类误差、局部一致性约束和滤波器参数$h$的二范数。分类误差就是与ground truth之间的损失函数，计算时需考虑可靠性；局部一致性约束用于减小循环样本中的每一个片段的响应差距，该项不受$\beta$即可靠性的影响，也就是说base filter在训练时依旧要保持对每个局部区域同样的关注度，使得base filter能独立于可靠性进行训练，这就避免了前面提到的滤波器在训练时边缘区域被抑制所造成权重集中的后果；滤波器参数$h$的二范数用于保证模型的简单程度，防止模型退化（可以理解为过拟合）。<br>由于只有当base filter的参数$h$和reliability的权重$\beta$有一项已知时，目标方程的最小化问题才是凸优化问题，因此作者采用了$h$、$\beta$交替训练的方法。<br>作者还使用权重逐帧退化的方式设计了一种简单的利用多帧信息的目标方程。借鉴ECO，DRT也采用了间隔几帧更新一次的稀疏更新方法和基于高斯混合模型的样本分组策略。类似DSST，DRT采用了先确定位置再计算多个尺度的响应的“两步”尺度估计方法。</p><h3 id="MCCT"><a href="#MCCT" class="headerlink" title="MCCT"></a>MCCT</h3><p>MCCT使用了多特征集成学习，在跟踪时对每一帧分别选用最合适的特征来做出决策。为了应对不同的场景，MCCT选择了low，middle，high三个层级的特征，并通过排列组合得出7种expert。尽管有些特征的鲁棒性明显差于三类特征的组合，但是它们提供的多样性对集成学习是至关重要的。<br><img src="/computer-vision20200215214240/MCCT特征.png" title="MCCT特征"><br>为了评估每个expert在每一帧的好坏以决定具体选用哪一个，作者提出了Expert Pair-Evaluation和Expert Self-Evaluation。<br>Expert Pair-Evaluation分为两项：在第一项中，作者认为一个expert的好坏可以通过它与其他expert的整体一致性来体现，于是首先计算了每个expert相对于其他6个expert在当前帧预测结果的一致性（通过重叠率来衡量）之和；此外，作者认为一个好的expert还必须是temporal stable的，因此他又计算了每个expert相对于其他6个expert在前几帧内预测趋势的一致性，这就可以防止因为在当前帧碰巧预测一致而导致之前一项的分值很好的情况，也保证了expert的可信度。最后两项结合得到Expert Pair-Evaluation。<br>在Expert Self-Evaluation中，作者认为路径的顺滑程度一定程度上能够体现每个expert的可靠程度。<br>最后将Expert Pair-Evaluation和Expert Self-Evaluation加权求和选出每帧最好的expert做出决策。<br>MCCT提出了一种peak-to-sidelobe ratio和鲁棒性的置信度分数来进行模型更新：</p><script type="math/tex;mode=display">S^{t}=P_{mean}^{t}\cdot R_{mean}^{t}</script><p>其中，$P_{mean}^{t}$是每个expert响应图peak-to-sidelobe ratio的平均，$R_{mean}^{t}$亦然。当$R_{mean}^{t}$比较低时，认为采集到了不可靠的样本（比如遮挡问题等）。为此，作者的模型更新策略是，当置信度分数$S^{t}$大于之前置信度均值时，采用正常学习率，否则，根据置信度算出一个较小的学习率以在一定程度上维持模型。<br>为了提升速度，每个expert之间共享了样本和RoI，最后MCCT的速度为7.8FPS，MCCT-H（没采用深度特征）的速度为44.8FPS。（作为参考，ECO的速度为15FPS）</p><h3 id="LSART"><a href="#LSART" class="headerlink" title="LSART"></a>LSART</h3><p>LSART分析了深度特征中的空间信息，提出了两种互补的回归方式来使得跟踪更加鲁棒。<br>作者首先对比了CNN-based和KRR-based（核岭回归）两类tracker，认为它们各有利弊且是互补的。由于KRR的循环采样，目标的结构特征会被打破，对形变和遮挡问题效果不好，而CNN则能够很好地提取位置信息；相反，CNN庞大的参数量使得它容易过拟合，而KRR-based tracker就不会出现这样的问题。因此，若将两者结合（将热力图加权求和），就可以让KRR关注全局而让CNN关注较小、较精确的目标，进而达到更好的效果。<br>对于KRR，作者引入cross-patch similarity，将参数看作训练样本的加权求和，将响应项拆分成三项，这就方便把原本的迭代求解的方式分成三步在神经网络中来求解了。<br>对于CNN，考虑到形变和遮挡等问题会使得目标的一部分比其他区域更加重要，不同于以往在feature map上做文章，作者对卷积层的滤波器施加掩模，使得各个滤波器关注于不同的区域，在跟踪的过程中，这些掩模不做变化。此外，作者还提出了距离变换池化层用于评判输入feature map的可靠性。另外，作者设计了一种two-stream的训练网络，将空间正则的卷积层和距离变换池化层分开训练以防止过拟合，能够比较好的处理旋转问题。</p><h3 id="SiamRPN"><a href="#SiamRPN" class="headerlink" title="SiamRPN"></a>SiamRPN</h3><p>SiamRPN利用了Faster RCNN中的RPN，解决了之前深度学习跟踪算法没有domain specific（可理解为类间不区分）以及还需额外的尺度检测与在线微调的问题。<br>SiamRPN还将目标跟踪看作one-shot检测的问题，也就是用第一帧目标样本的信息来预测RPN网络中的参数，从而实现domain specific。作者把template分支和detection分支卷积视作类别信息在RPN网络上的embedding。<br><img src="/computer-vision20200215214240/one-shot检测.png" title="one-shot检测"></p><h3 id="DaSiamRPN"><a href="#DaSiamRPN" class="headerlink" title="DaSiamRPN"></a>DaSiamRPN</h3><p>DaSiamRPN在之前的孪生网络系列的基础上增加了distractor-aware，这里的distractor指的是在判别式方法中，不同于无语义信息易判别的背景，而存在一定的语义并对前景分割存在干扰的背景。这其中的一大原因是之前的训练集仅从同一个视频序列的不同帧中采样，造成了non-semantic的背景样本具有较大的比重而semantic的背景样本较少，这就弱化了模型准确判别前景的能力。此外，之前的孪生网络系列还存在不能在线更新和不进行全局搜索这两个问题。<br>首先，作者提出了三类样本选取方法来弥补传统采样的不足。考虑到视频数据集中类别缺乏和标注的难度，作者引入了ImageNet和COCO图像检测两个数据集，并把样本分成三类对tracker进行训练。<br><img src="/computer-vision20200215214240/三种样本选取方法.png" title="三种样本选取方法"><br>对于正样本对，其作用是提升tracker的泛化能力和回归精度；对于来自同一类别的样本对，其作用是让tracker更注重细粒度的表达方式，提升判别能力；对于来自不同类别的样本对，其作用是让tracker在遮挡、超出视野等情况下拥有更好的鲁棒性。<br>值得一提，作者发现motion pattern能很好地被浅层网络建模，因此在数据增强时还引入了运动模糊。<br>DaSiamRPN通过上述方法对数据做了增强，可是在跟踪特定目标时，还是很难将一般模型转化为特定视频域所用。考虑到上下文信息和时域信息可以提供特定目标的信息以增加tracker的判别能力，作者提出了一个distractor-aware module。具体来说，在上一帧中选择出的proposal中，通过非极大抑制处理，剩下的proposal中最大的就是目标，剩下的就是会产生误导的distractor；在当前帧，为了抑制这些distractor的干扰，可以减去这些distractor之前响应的加权和，减去之后还是最大的proposal就是我们要找的目标，其基本思想如下公式所示：</p><script type="math/tex;mode=display">q=\underset{p_{k}\in P}{argmax}f\left ( z,p_{k} \right )-\frac{\widehat{\alpha }\sum_{i=1}^{n}\alpha _{i}f\left ( d_{i},p_{k} \right )}{\sum_{i=1}^{n}a_{i}}</script><p>这里的$\alpha$是控制distractor影响大小的权重系数，作者又对上式进行调整，通过引入学习率使得该分类器在线可学习，这就无需利用反向传播更新网络参数，而通过微调一个分类器弥补了传统基于孪生网络的tracker不能在线更新的缺点。<br>此外，当认为目标跟丢时，DaSiamRPN会匀速扩大搜索范围，并且通过高效的bounding box回归来代替图像金字塔，这就通过一个简单的方法在应对长时跟踪目标消失问题时较之前基于孪生网络的tracker取得了一个进步。</p><h3 id="Meta-Tracker"><a href="#Meta-Tracker" class="headerlink" title="Meta-Tracker"></a>Meta-Tracker</h3><p>Meta-Tracker将元学习运用在了目标模型的初始化上。作者认为结合深度特征和在线学习的模型有两大困难，一是训练的样本不容易获得，二是大多数SOTA的tracker在训练阶段都需要花费大量的时间在初始化上面。<br>针对上面的难题，作者提出了一种在未来的帧上训练目标模型的思路，采用了基于预测梯度的策略学习方法获得普适性的初始化模型，使得跟踪模型自适应于后续帧特征的最佳梯度方向，从而在接收到第一帧时仅需一步迭代就能使参数快速收敛到合适的位置。这样做有三点好处，一是能使模型更加关注对后续的帧更有价值的特征，二是避免了在当前帧上过拟合，三是能够使初始化更快速。总而言之，就是能保证精度和鲁棒性。<br>考虑到上述方法在长序列或者目标在帧与帧之间变化不大时表现不佳（会偏离目标），这是因为Meta-Tracker一步到位的思想使得学习率会偏大。因此作者仅在模型初始化时采用学习到的学习率，在之后的跟踪过程中仍旧沿用原来版本的方式进行更新。这里原来的版本指的是CREST和MDNet，作者在这两个tracker的基础上改进出了MetaCREST和MetaSDNet。具体的改进和处理可以看一看我之前写过关于Meta-Tracker的文章。</p><h3 id="DorT"><a href="#DorT" class="headerlink" title="DorT"></a>DorT</h3><p>DorT（Detect or Track）把跟踪看作一个连续决策的过程，它结合目标检测和目标跟踪两个领域内SOTA的结果，在孪生网络输出的结果上再添加一个小型的CNN网络作为scheduler来判断在下一帧是作检测还是作跟踪。</p><h3 id="LADCF"><a href="#LADCF" class="headerlink" title="LADCF"></a>LADCF</h3><p>LADCF针对DCF系列的边界效应和模型退化（后者主要是单帧独立学习和模型更新速率固定导致）的问题，提出了一种空间域特征选择和时间域约束结合的方法，并且使其能在低维流形中有效表示。</p><blockquote><p>补充：流形学习的观点认为，我们所能观察到的数据实际上是由一个低维流形映射到高维空间上的。由于数据内部特征的限制，一些高维中的数据会产生维度上的冗余，实际上只需要比较低的维度就能唯一地表示。</p></blockquote><p>掩模策略应用于目标跟踪时，仅将目标区域的参数激活。LADCF也运用了这个思想，对滤波器中的参数$\theta$作降维处理$\theta _{\phi }=diag(\phi )\theta$，这里$\phi$中的元素要么是0、要么是1，即不激活或者激活。不同于PCA和LLE，这种方法在降维的同时也保持了空间特性，不仅能加速求解，也能除去大部分干扰，使滤波器关注于目标部分从而可以使用更大的搜索域。<br>最后的目标函数如下：</p><script type="math/tex;mode=display">arg\underset{\theta }{min}\left \| \theta \circledast x-y \right \|_{2}^{2}+\lambda _{1}\left \| \theta \right \|_{1}+\lambda _{2}\left \| \theta -\theta _{model} \right \|_{2}^{2}</script><p>可以看到，这里还包括与历史模型的正则项，减轻了滤波器退化。作者让$\lambda _{1}&lt; &lt; \lambda _{2}$，也就是让时间域上的一致性更加重要于特征的稀疏选取。</p><h3 id="FlowTrack"><a href="#FlowTrack" class="headerlink" title="FlowTrack"></a>FlowTrack</h3><p>不同于之前先把光流算好，FlowTrack是第一个把光流信息进行端到端训练的，这无疑提高了光流使用的精度。作者注意到之前的算法大都采用RGB特征也就是外观特征，且缺少对运动特征和帧与帧之间联系的利用，这就导致在部分遮挡和形变等情况下效果会变差。为此，作者将光流引入孪生网络框架，使得仅外观特征的一些不足得到弥补。<br>作者的思想很巧妙，我以遮挡问题为例简述一下。作者的想法是当当前帧的目标有部分被遮挡时，我们可以根据光流的运动信息，把前几帧的特征映射过来，通过插值补全当前帧的特征，简单来说可以理解为把当前帧缺的那块给补全。为了有效地选择补过来的特征，作者使用了空间注意力和时间注意力两种机制结合。空间注意力分别计算前几帧补过来的特征与当前帧的特征的相似度，根据相似度大小来分配权重。但是由于最近的一帧总是与当前帧的特征最为相似，它的权重肯定是最大的，考虑到假如最近一帧由于遮挡等原因等导致特征质量下降而不适合分配最大的权重，因此还需要时间注意力来重新校准权重。具体来说，时间注意力作用在空间注意力的输出上，对于一般情况下的目标基本不会改变空间注意力的输出，而对遮挡等情况下的帧就会减小其在空间注意力中分配到的权重。</p><h2 id="2019"><a href="#2019" class="headerlink" title="2019"></a>2019</h2><p>下面是VOT2019 short-term的排名结果。<br><img src="/computer-vision20200215214240/VOT2019.png" title="VOT2019"></p><h3 id="GFS-DCF"><a href="#GFS-DCF" class="headerlink" title="GFS-DCF"></a>GFS-DCF</h3><p>GFS-DCF考虑到深度网络的高维通道存在许多冗余的信息，因此作者在时域和空间域之外，还考虑了通道维度的影响，在目标函数中使用了三个正则项。<br><img src="/computer-vision20200215214240/考虑三项正则.png" title="考虑三项正则"></p><blockquote><p>注：建议结合上图来看接下来的分析。</p></blockquote><p>对于空间域，作者将每个通道（也就是每个feature map）的对应点相连接，用范数约束，可以理解为提取那些在绝大多数特征图中都是最重要的特征的位置。</p><script type="math/tex;mode=display">R_{S}(W)=\sum_{i=1}^{N}\sum_{j=1}^{N}\left \| w_{ij:} \right \|_{2}</script><p>从通道角度，作者又把每一个通道作为一项来做约束，可以理解为提取那些特征比较重要的通道。</p><script type="math/tex;mode=display">R_{C}(W)=\sum_{k=1}^{C}\left \| W^{k} \right \|_{F}</script><p>对于时域，作者使用了low-rank约束，这里的rank指的是矩阵的秩而不是排名，low-rank主要用于图像对齐（alignment），在文中的目标是最小化$rank(W_{t})-rank(W_{t-1})$，这里的$W_{t}$表示从1到t每个滤波器向量化后形成的矩阵。下面是作者最后修改后的正则项：</p><script type="math/tex;mode=display">R_{T}(W)=\sum_{k=1}^{C}\left \| W_{t}^{k}-W_{t-1}^{k} \right \|_{F}^{2}</script><blockquote><p>注：上述三项在实际目标函数中还要加上权重。</p></blockquote><p>作者发现，空间正则对使用handcrafted特征的模型效果显著，而对使用CNN的模型（文中是ResNet）效果提升不大；相反，通道正则对使用handcrafted特征的模型效果不明显，而对使用CNN的模型效果显著。作者在文中解释认为由于深层CNN特征表示的语义信息丰富而缺少细粒度的信息，因此相比保留更多空间结构handcrafted特征，对深层CNN特征使用空间正则比较难以判别哪些位置的特征反应了目标位置的信息。此外，由于在训练过程中一些通道的权重下降到很小，也就是说模型本身就不怎么关注这些通道，因此使用通道正则在这里取得了比较明显的效果。</p><h3 id="D3S"><a href="#D3S" class="headerlink" title="D3S"></a>D3S</h3><p>D3S考虑到BBox对目标的粗糙表示会影响性能以及视频分割任务中对背景干扰和长时视频不鲁棒的问题，提出了一种视频跟踪、视频分割互补的框架。<br><img src="/computer-vision20200215214240/D3S.png" title="D3S"><br>如上图所示，作者构造了用于分割的GIM和用于定位的GEM两个模块。<br>GIM使用初始帧的目标像素点构造目标的特征向量，使用初始帧目标周围的像素点构造背景的特征向量。在之后的图像中，每个像素点的目标相似度，定义为该点和每个目标的特征向量做相似度计算之后，最大的K个目标相似度的均值；同理，每个像素点的背景相似度，定义为该点和每个背景的特征向量做相似度计算之后，最大的K个背景相似度的均值。最后将目标相似度图和背景相似度图作softmax得到分割结果。该模块没考虑位置信息，用分割使得在应对剧烈形变的目标时能够取得较好的效果。<br>然而，GIM的分割结果对相似的物体或者背景的干扰并不鲁棒，因此我们希望能有鲁棒的位置信息来提升判别力。GEM就是直接使用一个DCF来找到最大响应值的位置，也就是目标的中心位置，然后以该中心为圆心，向周围每个像素点根据半径由大到小分配置信度。最后仅把GIM中得到的且由GEM分配的置信度高于阈值的目标像素点作为分割结果。<br>由于通过backbone的encode导致此时的输出分辨率较低，因此还需要作上采样。具体来讲，就是每次把输入扩大到两倍分辨率，通过两次卷积，然后加上backbone中对应分辨率的层。<br>由于预训练的backbone特征缺少精细的划分，因此作者在初始帧先进行降维的训练。具体来讲，就是首先将预训练网络通过1x1的卷积来降维，再通过一层3x3的卷积，从而达到调整网络参数使得分支划分得到最佳的目的。<br>由于输出是二值掩模，因此作者还对使用BBox的目标跟踪问题作了变换处理。首先，在初始帧，除了进行降维和DCF的训练，作者还先把BBox内部的像素点视为目标点，把BBox外4倍区域的像素点视为背景点，用D3S在第一帧上迭代（文中说只迭代1次就可以了）以产生比BBox更细致的分割，将最后分割出的目标点和其周围的背景点用于构造特征向量，在之后的跟踪过程中保持不变并用于GIM模块。此外，在输出时，作者先用椭圆去近似掩模，然后根据长轴和短轴来确定BBox。由于椭圆的确定遗漏的背景信息（仅考虑怎么把mask包进来）从而导致BBox偏大，因此作者提出了一种考虑背景信息的方法，对长轴进行微调来确定最终的BBox。</p><h3 id="GlobalTrack"><a href="#GlobalTrack" class="headerlink" title="GlobalTrack"></a>GlobalTrack</h3><p>GlobalTrack关注的是long-term tracking的问题。我们知道，在目标跟踪问题中，为了更好的利用前一帧甚至前几帧的信息，往往会对模型做很多假设，包括目标的运动、位置变化、尺度变化（假设平滑变化等等），而这些假设并不能很好地处理所有的情况（比如位置或尺度突变、目标消失、短时跟踪失败等），由此产生了模型的累计误差。而在长时跟踪问题中，这样的累计误差往往会使得后期的目标跟踪结果差很多。<br>基于上述考虑，GlobalTrack根据长时跟踪的特点，把跟踪看作在每一帧作全局检测的问题，设计了一种没有运动模型、没有在线学习、没有位置估计、没有尺度平滑的无累积误差的baseline，其基本框架如下图所示。<br><img src="/computer-vision20200215214240/GlobalTrack.png" title="GlobalTrack"><br>受Faster RCNN启发，GlobalTrack也是基于two-stage的框架。其下面的一条和Faster RCNN基本一致，由于Faster RCNN是目标检测类算法，其目的是在图像中框出所有物体并分类。而目标跟踪仅需要目标，因此作者用添加了上面一条query-specific的引导。为了简单起见，作者把query的RoI看成kxk的方型特征，在第一个feature modulation中，作者把RoI特征卷积成1x1的卷积核，然后再和通过backbone的搜索图像特征做卷积相关操作，得到query-specific的候选框；在第二个feature modulation，作者把RoI特征与每个候选框作哈达玛积（也就是简单地将两个尺寸相同的矩阵的对应位置作乘积），由此来改进标签置信度和BBox的预测。<br>在训练时，作者取多对图像对，其中每一对图像都共同含有M个相同的实例，作者用每对图像来相互预测每对各自的M个实例，使总的损失达到最小以进行训练。<br>在测试时，作者简单地把第一帧作为query，之后的每一帧都视为独立的全局检测，直接取得分最高的BBox作为结果，如此就不存在依赖相邻帧带来的累计误差了，因此作者认为视频长度越长，GlobalTrack的表现就越突出。</p><h3 id="SPSTracker"><a href="#SPSTracker" class="headerlink" title="SPSTracker"></a>SPSTracker</h3><p>SPSTracker针对目标周围噪声引起的响应（sub-peak）导致模型漂移，以及由多尺度样本加权得到的特征图响应最大值和目标真正的几何中心不一致的问题，提出了BRT和PRP两个模块。作者使用的是ATOM的框架，两个模块在框架中的使用方式如下图所示。<br><img src="/computer-vision20200215214240/SPSTracker.png" title="SPSTracker"><br>这里的BRT其实就是简单的将远离目标中心的点置为0，其目的是减小响应图的方差，使得响应图尽可能地呈现单峰响应的形状，此外也起到了抑制边界效应的效果。PRP其实就是作者新设计的一个池化层，该池化层把每一像素点的值设为该像素点所在列所有像素点中的最大值和所在行所有像素点的最大值之和，从而使响应值能更加靠近目标的几何中心，从而能够更好地使用多尺度样本。作者对搜索区域使用BRT，然后将通过分类器的置信度图通过一个PRP构造的残差模块，从而达到抑制sub-peak的目的。</p><hr><h1 id="研究趋势"><a href="#研究趋势" class="headerlink" title="研究趋势"></a>研究趋势</h1><p>以下是我对近几年来目标跟踪领域各种算法主流的研究趋势和发展方向的一个浅析，个人思考，多多指教。</p><blockquote><p>注：其实近几年还出现了一些其他的关注方向，由于不是主流、目前关注较少、本人学识不够等原因，在此不做列举。</p></blockquote><h2 id="信息提取"><a href="#信息提取" class="headerlink" title="信息提取"></a>信息提取</h2><h3 id="深度特征"><a href="#深度特征" class="headerlink" title="深度特征"></a>深度特征</h3><p>早期的目标跟踪算法主要在handcrafted特征方面进行探索和改进，以2012年AlexNet问世为节点，深度特征开始被引入目标跟踪领域。<br>我们知道，在现实场景中，物体是在三维的运动场中移动的。而视频或图像序列都是二维的信息，这其实是一些难题的根本原因之一。一个比较极端的例子就是理发店门前经常会出现的旋转柱，如果单纯地从二维角度来看，柱子是向上运动的，可在实际的运动场中柱子是横向运动的，观测和实际的运动方向是完全垂直的。<br><img src="/computer-vision20200215214240/发廊旋转柱.png" title="发廊旋转柱"><br>因此，为了能够更好地跟踪目标，我们需要提取尽可能好的特征，此外最好能从视频或图像序列中学到更多丰富的信息（尤其是含语义的）。值得注意的一点是，在港科大王乃岩博士2015年所做的ablation experiments中（详见参考文献[2]），发现特征提取是影响tracker效果最重要的因素。<br>考虑到精度是保证目标跟踪鲁棒性的重要因素，不同于一些其他的计算机视觉任务，目标跟踪领域的深度算法比较强调结合与充分利用浅层网络的高分辨率信息。</p><h3 id="时域和空间域结合"><a href="#时域和空间域结合" class="headerlink" title="时域和空间域结合"></a>时域和空间域结合</h3><p>可以说，在目标跟踪的相关算法中，空间正则指的就是抑制边界效应。由于CNN能够在学习的过程中能够产生对样本中各个区域有区分的关注度，因此可以不考虑边界效应。对边界效应的处理主要是在相关滤波类等需要循环移位的算法中出现。<br>事实上，目标跟踪这一个任务本身就在利用时域信息，因为预测下一帧肯定需要上一帧的信息，然而仅仅利用上一帧的信息往往是不够的，充分的利用时域信息在正则或者辅助记忆方面都可以取得一定的效果。</p><h2 id="学习方式"><a href="#学习方式" class="headerlink" title="学习方式"></a>学习方式</h2><h3 id="结合语义分割的多任务学习"><a href="#结合语义分割的多任务学习" class="headerlink" title="结合语义分割的多任务学习"></a>结合语义分割的多任务学习</h3><p>由于bounding box粗糙的对目标的标注表示使得有不少冗余的背景信息进入template，此外bounding box对平面内旋转等场景不鲁棒，这些都会导致模型的退化。早期也有许多算法考虑到了分割，但主要是对目标进行分块（part-based）而不是语义分割。由于跟踪的目标是有具体形状的且是一起运动的，同时判别式方法正是要分离除目标以外的物体，因此近年来有不少算法结合语义分割，通过多任务学习来进一步提高tracker的效果。具体来说，就是引入掩模（mask）来识别预测物体，最后在转化成bounding box作为结果。</p><h3 id="元学习"><a href="#元学习" class="headerlink" title="元学习"></a>元学习</h3><p>实际上，目标跟踪这一个任务本身的特性就决定了它与元学习的思想有共通之处。元学习主要针对的是两个问题：在少样本学习的情况下对样本的利用效率比较低；当进行一个新的任务时对之前学到的经验的可移植性差，我觉得这里的新任务可以指从分类到跟踪这样类别之间的转换，也可以指在不同的视频序列上训练和测试这样“域”之间的转换。<br>当深度特征兴起之后，目标跟踪中的许多算法都选择迁移目标分类任务中的一些预训练模型来提取特征，这种迁移学习其实就包含了元学习的思想。MDNet将每个视频看做一个域，在测试时新建一个域但同时保留了之前训练时在其他域上学到的经验，既能够更快更好地在新的视频序列上学习也避免了过拟合。孪生网络实际上也是元学习领域一种比较常用的结构，它学习了如何去学习输入之间的相似度。</p><h2 id="其他关注点"><a href="#其他关注点" class="headerlink" title="其他关注点"></a>其他关注点</h2><h3 id="样本采集"><a href="#样本采集" class="headerlink" title="样本采集"></a>样本采集</h3><p>样本采集主要包括样本的数量、样本的有效性、正负样本和难易样本的平衡性。</p><h3 id="防止过拟合"><a href="#防止过拟合" class="headerlink" title="防止过拟合"></a>防止过拟合</h3><p>每帧获取的少量信息和目标的意外变化导致信息的丢失，使得过拟合问题成为目标跟踪任务中一个比较重要的关注点，下面是一些比较常见的方法：</p><ul><li>冻结一些层的参数（设置学习率为0），仅更新一部分层的权重。</li><li>采用coarse-to-fine的网络框架，用语义信息来避免过拟合。</li><li>通过在目标方程中添加正则项来限制模型的稀疏度，也即参数量。</li><li>采用two-stream的模型结构，短时更新和长时更新相结合。</li><li>采用稀疏更新的方式（隔几帧更新一次），相当于将利用单帧信息的更新变成了批处理的形式。</li><li>每次更新采用最近几帧的信息而不是只用目前帧的信息，其原理类似上一条。</li><li>利用初始帧或者质量比较好的几帧存储的样本来进行时域正则。</li><li>对不同的情况采用不同的更新或者初始化的策略。</li><li>使用dropout来防止过拟合。</li><li>使用基于patch的跟踪算法。</li><li>使用掩模去除不可靠的信息。</li></ul><script type="text/javascript" src="/js/src/bai.js"></script></div><div><div><hr style="FILTER:progid:DXImageTransform.Microsoft.Shadow(color:#987cb9,direction:145,strength:15)" width="100%" color="#987cb9" size="1"><div style="text-align:center;color:#555;font-size:16px"><i class="fa fa-hand-o-up" aria-hidden="true"></i> 碰到底线咯 <i class="fa fa-handshake-o" aria-hidden="true"></i> 后面没有啦 <i class="fa fa-hand-o-down" aria-hidden="true"></i></div></div></div><div><div class="my_post_copyright"><script src="//cdn.bootcss.com/clipboard.js/1.5.10/clipboard.min.js"></script><script src="https://cdn.bootcss.com/jquery/2.0.0/jquery.min.js"></script><script src="https://unpkg.com/sweetalert/dist/sweetalert.min.js"></script><p><span>本文标题:</span><a href="/computer-vision20200215214240/">computer vision笔记：目标跟踪的小总结</a></p><p><span>文章作者:</span><a href="/" title="访问 高深远 的个人博客">高深远</a></p><p><span>发布时间:</span>2020年02月15日 - 21:42</p><p><span>最后更新:</span>2020年04月10日 - 20:11</p><p><span>原始链接:</span><a href="/computer-vision20200215214240/" title="computer vision笔记：目标跟踪的小总结">https://gsy00517.github.io/computer-vision20200215214240/</a> <span class="copy-path" title="点击复制文章链接"><i class="fa fa-clipboard" data-clipboard-text="https://gsy00517.github.io/computer-vision20200215214240/" aria-label="复制成功！"></i></span></p><p><span>许可协议:</span><i class="fa fa-creative-commons"></i> <a rel="license" href="https://creativecommons.org/licenses/by-nc-nd/4.0/" target="_blank" title="Attribution-NonCommercial-NoDerivatives 4.0 International (CC BY-NC-ND 4.0)">署名-非商业性使用-禁止演绎 4.0 国际</a> 转载请保留原文链接及作者。</p></div><script>var clipboard=new Clipboard(".fa-clipboard");$(".fa-clipboard").click(function(){clipboard.on("success",function(){swal({title:"复制成功",text:"感谢您的阅读与参考！欢迎留下任何建议噢！",icon:"success",showConfirmButton:!0})})})</script></div><footer class="post-footer"><div class="post-tags"><a href="/tags/论文分享/" rel="tag"><i class="fa fa-tag"></i> 论文分享</a> <a href="/tags/计算机视觉/" rel="tag"><i class="fa fa-tag"></i> 计算机视觉</a> <a href="/tags/目标跟踪/" rel="tag"><i class="fa fa-tag"></i> 目标跟踪</a></div><div class="post-widgets"><div class="wp_rating"><div id="wpac-rating"></div></div></div><div class="post-nav"><div class="post-nav-next post-nav-item"><a href="/vot-toolkit20200215185238/" rel="next" title="vot toolkit笔记：解决无法连接TraX支持的问题"><i class="fa fa-chevron-left"></i> vot toolkit笔记：解决无法连接TraX支持的问题</a></div><span class="post-nav-divider"></span><div class="post-nav-prev post-nav-item"><a href="/anaconda20200222000018/" rel="prev" title="anaconda笔记：解决conda无法下载pytorch的问题">anaconda笔记：解决conda无法下载pytorch的问题 <i class="fa fa-chevron-right"></i></a></div></div></footer></div></article><div class="post-spread"></div></div></div><div class="comments" id="comments"></div></div><div class="sidebar-toggle"><div class="sidebar-toggle-line-wrap"><span class="sidebar-toggle-line sidebar-toggle-line-first"></span> <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span> <span class="sidebar-toggle-line sidebar-toggle-line-last"></span></div></div><aside id="sidebar" class="sidebar"><div class="sidebar-inner"><ul class="sidebar-nav motion-element"><li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap">文章目录</li><li class="sidebar-nav-overview" data-target="site-overview-wrap">站点概览</li></ul><section class="site-overview-wrap sidebar-panel"><div class="site-overview"><div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person"><img class="site-author-image" itemprop="image" src="/images/lufei.jpg" alt="高深远"><p class="site-author-name" itemprop="name">高深远</p><p class="site-description motion-element" itemprop="description"></p></div><nav class="site-state motion-element"><div class="site-state-item site-state-posts"><a href="/archives/"><span class="site-state-item-count">86</span> <span class="site-state-item-name">日志</span></a></div><div class="site-state-item site-state-categories"><a href="/categories/index.html"><span class="site-state-item-count">7</span> <span class="site-state-item-name">分类</span></a></div><div class="site-state-item site-state-tags"><a href="/tags/index.html"><span class="site-state-item-count">33</span> <span class="site-state-item-name">标签</span></a></div></nav><div class="feed-link motion-element"><a href="/atom.xml" rel="alternate"><i class="fa fa-rss"></i> RSS</a></div><div class="links-of-blogroll motion-element links-of-blogroll-inline"><div class="links-of-blogroll-title"><i class="fa fa-fw fa-link"></i> 网站收藏</div><ul class="links-of-blogroll-list"><li class="links-of-blogroll-item"><a href="https://www.runoob.com/" title="菜鸟教程" target="_blank">菜鸟教程</a></li><li class="links-of-blogroll-item"><a href="https://paperswithcode.com/" title="PaperWithCode" target="_blank">PaperWithCode</a></li><li class="links-of-blogroll-item"><a href="https://www.jiqizhixin.com/sota" title="机器之心" target="_blank">机器之心</a></li><li class="links-of-blogroll-item"><a href="http://pytorch123.com/" title="Pytorch中文文档" target="_blank">Pytorch中文文档</a></li></ul></div></div></section><section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active"><div class="post-toc"><div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#简介与要求"><span class="nav-number">1.</span> <span class="nav-text">简介与要求</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#问题及挑战"><span class="nav-number">2.</span> <span class="nav-text">问题及挑战</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#生成式与判别式"><span class="nav-number">3.</span> <span class="nav-text">生成式与判别式</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#机器学习"><span class="nav-number">3.1.</span> <span class="nav-text">机器学习</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#目标跟踪"><span class="nav-number">3.2.</span> <span class="nav-text">目标跟踪</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#算法导图"><span class="nav-number">4.</span> <span class="nav-text">算法导图</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#各类算法的梳理与简述"><span class="nav-number">5.</span> <span class="nav-text">各类算法的梳理与简述</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#1981"><span class="nav-number">5.1.</span> <span class="nav-text">1981</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#LK-Tracker"><span class="nav-number">5.1.1.</span> <span class="nav-text">LK Tracker</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#1994"><span class="nav-number">5.2.</span> <span class="nav-text">1994</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#KLT"><span class="nav-number">5.2.1.</span> <span class="nav-text">KLT</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#1998"><span class="nav-number">5.3.</span> <span class="nav-text">1998</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Condensation"><span class="nav-number">5.3.1.</span> <span class="nav-text">Condensation</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#2002"><span class="nav-number">5.4.</span> <span class="nav-text">2002</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Mean-Shift"><span class="nav-number">5.4.1.</span> <span class="nav-text">Mean Shift</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#2003"><span class="nav-number">5.5.</span> <span class="nav-text">2003</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Feature-Selection"><span class="nav-number">5.5.1.</span> <span class="nav-text">Feature Selection</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#2006"><span class="nav-number">5.6.</span> <span class="nav-text">2006</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Boosting"><span class="nav-number">5.6.1.</span> <span class="nav-text">Boosting</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#2008"><span class="nav-number">5.7.</span> <span class="nav-text">2008</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#IVT"><span class="nav-number">5.7.1.</span> <span class="nav-text">IVT</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#2010"><span class="nav-number">5.8.</span> <span class="nav-text">2010</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#MOSSE"><span class="nav-number">5.8.1.</span> <span class="nav-text">MOSSE</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#TLD"><span class="nav-number">5.8.2.</span> <span class="nav-text">TLD</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#2011"><span class="nav-number">5.9.</span> <span class="nav-text">2011</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#FoT"><span class="nav-number">5.9.1.</span> <span class="nav-text">FoT</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Struck"><span class="nav-number">5.9.2.</span> <span class="nav-text">Struck</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#L1-Tracker"><span class="nav-number">5.9.3.</span> <span class="nav-text">L1 Tracker</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#MIL"><span class="nav-number">5.9.4.</span> <span class="nav-text">MIL</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#2012"><span class="nav-number">5.10.</span> <span class="nav-text">2012</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#CSK"><span class="nav-number">5.10.1.</span> <span class="nav-text">CSK</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#DF"><span class="nav-number">5.10.2.</span> <span class="nav-text">DF</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#CT"><span class="nav-number">5.10.3.</span> <span class="nav-text">CT</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#ORIA"><span class="nav-number">5.10.4.</span> <span class="nav-text">ORIA</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#2013"><span class="nav-number">5.11.</span> <span class="nav-text">2013</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#PLT"><span class="nav-number">5.11.1.</span> <span class="nav-text">PLT</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#EDF"><span class="nav-number">5.11.2.</span> <span class="nav-text">EDF</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#LGT"><span class="nav-number">5.11.3.</span> <span class="nav-text">LGT</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#LGT-1"><span class="nav-number">5.11.4.</span> <span class="nav-text">LGT++</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#DLT"><span class="nav-number">5.11.5.</span> <span class="nav-text">DLT</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#STC"><span class="nav-number">5.11.6.</span> <span class="nav-text">STC</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#LT-FLO"><span class="nav-number">5.11.7.</span> <span class="nav-text">LT-FLO</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#GSDT"><span class="nav-number">5.11.8.</span> <span class="nav-text">GSDT</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#SCTT"><span class="nav-number">5.11.9.</span> <span class="nav-text">SCTT</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#CCMS"><span class="nav-number">5.11.10.</span> <span class="nav-text">CCMS</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Matrioska"><span class="nav-number">5.11.11.</span> <span class="nav-text">Matrioska</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#AIF"><span class="nav-number">5.11.12.</span> <span class="nav-text">AIF</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#HT"><span class="nav-number">5.11.13.</span> <span class="nav-text">HT</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#STMT"><span class="nav-number">5.11.14.</span> <span class="nav-text">STMT</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#ASAM"><span class="nav-number">5.11.15.</span> <span class="nav-text">ASAM</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#2014"><span class="nav-number">5.12.</span> <span class="nav-text">2014</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#DSST"><span class="nav-number">5.12.1.</span> <span class="nav-text">DSST</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#CN"><span class="nav-number">5.12.2.</span> <span class="nav-text">CN</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#SAMF"><span class="nav-number">5.12.3.</span> <span class="nav-text">SAMF</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#KCF"><span class="nav-number">5.12.4.</span> <span class="nav-text">KCF</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#DCF"><span class="nav-number">5.12.5.</span> <span class="nav-text">DCF</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#FCT"><span class="nav-number">5.12.6.</span> <span class="nav-text">FCT</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#CMT"><span class="nav-number">5.12.7.</span> <span class="nav-text">CMT</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#2015"><span class="nav-number">5.13.</span> <span class="nav-text">2015</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#SO-DLT"><span class="nav-number">5.13.1.</span> <span class="nav-text">SO-DLT</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#MDNet"><span class="nav-number">5.13.2.</span> <span class="nav-text">MDNet</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#SRDCF"><span class="nav-number">5.13.3.</span> <span class="nav-text">SRDCF</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#DeepSRDCF"><span class="nav-number">5.13.4.</span> <span class="nav-text">DeepSRDCF</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#HCF"><span class="nav-number">5.13.5.</span> <span class="nav-text">HCF</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#FCNT"><span class="nav-number">5.13.6.</span> <span class="nav-text">FCNT</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#LCT"><span class="nav-number">5.13.7.</span> <span class="nav-text">LCT</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#CCT"><span class="nav-number">5.13.8.</span> <span class="nav-text">CCT</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#CFwLB"><span class="nav-number">5.13.9.</span> <span class="nav-text">CFwLB</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#KCFDP"><span class="nav-number">5.13.10.</span> <span class="nav-text">KCFDP</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#HCFT"><span class="nav-number">5.13.11.</span> <span class="nav-text">HCFT</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#MUSTer"><span class="nav-number">5.13.12.</span> <span class="nav-text">MUSTer</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#RPAC"><span class="nav-number">5.13.13.</span> <span class="nav-text">RPAC</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#RPT"><span class="nav-number">5.13.14.</span> <span class="nav-text">RPT</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#2016"><span class="nav-number">5.14.</span> <span class="nav-text">2016</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#DLSSVM"><span class="nav-number">5.14.1.</span> <span class="nav-text">DLSSVM</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#C-COT"><span class="nav-number">5.14.2.</span> <span class="nav-text">C-COT</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#SRDCFdecon"><span class="nav-number">5.14.3.</span> <span class="nav-text">SRDCFdecon</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Staple"><span class="nav-number">5.14.4.</span> <span class="nav-text">Staple</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#SINT"><span class="nav-number">5.14.5.</span> <span class="nav-text">SINT</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#TCNN"><span class="nav-number">5.14.6.</span> <span class="nav-text">TCNN</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#SKCF"><span class="nav-number">5.14.7.</span> <span class="nav-text">SKCF</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#MRF"><span class="nav-number">5.14.8.</span> <span class="nav-text">MRF</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#GOTURN"><span class="nav-number">5.14.9.</span> <span class="nav-text">GOTURN</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#SiamFC"><span class="nav-number">5.14.10.</span> <span class="nav-text">SiamFC</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#2017"><span class="nav-number">5.15.</span> <span class="nav-text">2017</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#ECO"><span class="nav-number">5.15.1.</span> <span class="nav-text">ECO</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#CREST"><span class="nav-number">5.15.2.</span> <span class="nav-text">CREST</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#LMCF"><span class="nav-number">5.15.3.</span> <span class="nav-text">LMCF</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#DeepLMCF"><span class="nav-number">5.15.4.</span> <span class="nav-text">DeepLMCF</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#MCPF"><span class="nav-number">5.15.5.</span> <span class="nav-text">MCPF</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#CFNet"><span class="nav-number">5.15.6.</span> <span class="nav-text">CFNet</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#2018"><span class="nav-number">5.16.</span> <span class="nav-text">2018</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#STRCF"><span class="nav-number">5.16.1.</span> <span class="nav-text">STRCF</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#UPDT"><span class="nav-number">5.16.2.</span> <span class="nav-text">UPDT</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#ACT"><span class="nav-number">5.16.3.</span> <span class="nav-text">ACT</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#DRT"><span class="nav-number">5.16.4.</span> <span class="nav-text">DRT</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#MCCT"><span class="nav-number">5.16.5.</span> <span class="nav-text">MCCT</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#LSART"><span class="nav-number">5.16.6.</span> <span class="nav-text">LSART</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#SiamRPN"><span class="nav-number">5.16.7.</span> <span class="nav-text">SiamRPN</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#DaSiamRPN"><span class="nav-number">5.16.8.</span> <span class="nav-text">DaSiamRPN</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Meta-Tracker"><span class="nav-number">5.16.9.</span> <span class="nav-text">Meta-Tracker</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#DorT"><span class="nav-number">5.16.10.</span> <span class="nav-text">DorT</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#LADCF"><span class="nav-number">5.16.11.</span> <span class="nav-text">LADCF</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#FlowTrack"><span class="nav-number">5.16.12.</span> <span class="nav-text">FlowTrack</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#2019"><span class="nav-number">5.17.</span> <span class="nav-text">2019</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#GFS-DCF"><span class="nav-number">5.17.1.</span> <span class="nav-text">GFS-DCF</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#D3S"><span class="nav-number">5.17.2.</span> <span class="nav-text">D3S</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#GlobalTrack"><span class="nav-number">5.17.3.</span> <span class="nav-text">GlobalTrack</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#SPSTracker"><span class="nav-number">5.17.4.</span> <span class="nav-text">SPSTracker</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#研究趋势"><span class="nav-number">6.</span> <span class="nav-text">研究趋势</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#信息提取"><span class="nav-number">6.1.</span> <span class="nav-text">信息提取</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#深度特征"><span class="nav-number">6.1.1.</span> <span class="nav-text">深度特征</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#时域和空间域结合"><span class="nav-number">6.1.2.</span> <span class="nav-text">时域和空间域结合</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#学习方式"><span class="nav-number">6.2.</span> <span class="nav-text">学习方式</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#结合语义分割的多任务学习"><span class="nav-number">6.2.1.</span> <span class="nav-text">结合语义分割的多任务学习</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#元学习"><span class="nav-number">6.2.2.</span> <span class="nav-text">元学习</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#其他关注点"><span class="nav-number">6.3.</span> <span class="nav-text">其他关注点</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#样本采集"><span class="nav-number">6.3.1.</span> <span class="nav-text">样本采集</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#防止过拟合"><span class="nav-number">6.3.2.</span> <span class="nav-text">防止过拟合</span></a></li></ol></li></ol></li></ol></div></div></section></div></aside></div></main><footer id="footer" class="footer"><div class="footer-inner"><div class="copyright">&copy; <span itemprop="copyrightYear">2020</span> <span class="with-love"><i class="fa fa-user"></i> </span><span class="author" itemprop="copyrightHolder">高深远</span> <span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-area-chart"></i> </span><span class="post-meta-item-text">Site words total count&#58;</span> <span title="Site words total count">175.4k</span></div><script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><span id="busuanzi_container_site_pv">访客已留下<span id="busuanzi_value_site_pv"></span>个脚印 </span><span id="busuanzi_container_site_uv">你是第<span id="busuanzi_value_site_uv"></span>位小伙伴</span></div></footer><div class="back-to-top"><i class="fa fa-arrow-up"></i> <span id="scrollpercent"><span>0</span>%</span></div></div><script type="text/javascript">"[object Function]"!==Object.prototype.toString.call(window.Promise)&&(window.Promise=null)</script><script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script><script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script><script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script><script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script><script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script><script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script><script type="text/javascript" src="/lib/three/three.min.js"></script><script type="text/javascript" src="/lib/three/three-waves.min.js"></script><script type="text/javascript" src="/js/src/utils.js?v=5.1.4"></script><script type="text/javascript" src="/js/src/motion.js?v=5.1.4"></script><script type="text/javascript" src="/js/src/affix.js?v=5.1.4"></script><script type="text/javascript" src="/js/src/schemes/pisces.js?v=5.1.4"></script><script type="text/javascript" src="/js/src/scrollspy.js?v=5.1.4"></script><script type="text/javascript" src="/js/src/post-details.js?v=5.1.4"></script><script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.4"></script><script src="//cdn1.lncld.net/static/js/3.0.4/av-min.js"></script><script src="//unpkg.com/valine/dist/Valine.min.js"></script><script type="text/javascript">var GUEST = ['nick','mail','link'];
    var guest = 'nick,mail,link';
    guest = guest.split(',').filter(item=>{
      return GUEST.indexOf(item)>-1;
    });
    new Valine({
        el: '#comments' ,
        verify: false,
        notify: false,
        appId: 'SVKPF4oiwJLMtpySQxvJLnLm-gzGzoHsz',
        appKey: '46OfvvEe65XMeUi79STU895I',
        placeholder: '动动手指，写下您的意见、疑惑或者鼓励吧！留下您的邮箱，这样就可以收到别人的回复啦！',
        avatar:'wavatar',
        guest_info:guest,
        pageSize:'10' || 10,
    });

    var infoEle = document.querySelector('#comments .info');
    if (infoEle && infoEle.childNodes && infoEle.childNodes.length > 0){
    infoEle.childNodes.forEach(function(item) {
    item.parentNode.removeChild(item);
    });
  }</script><script type="text/javascript">function proceedsearch(){$("body").append('<div class="search-popup-overlay local-search-pop-overlay"></div>').css("overflow","hidden"),$(".search-popup-overlay").click(onPopupClose),$(".popup").toggle();var t=$("#local-search-input");t.attr("autocapitalize","none"),t.attr("autocorrect","off"),t.focus()}var isfetched=!1,isXml=!0,search_path="search.xml";0===search_path.length?search_path="search.xml":/json$/i.test(search_path)&&(isXml=!1);var path="/"+search_path,onPopupClose=function(t){$(".popup").hide(),$("#local-search-input").val(""),$(".search-result-list").remove(),$("#no-result").remove(),$(".local-search-pop-overlay").remove(),$("body").css("overflow","")},searchFunc=function(t,e,o){"use strict";$("body").append('<div class="search-popup-overlay local-search-pop-overlay"><div id="search-loading-icon"><i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i></div></div>').css("overflow","hidden"),$("#search-loading-icon").css("margin","20% auto 0 auto").css("text-align","center"),$.ajax({url:t,dataType:isXml?"xml":"json",async:!0,success:function(t){isfetched=!0,$(".popup").detach().appendTo(".header-inner");var n=isXml?$("entry",t).map(function(){return{title:$("title",this).text(),content:$("content",this).text(),url:$("url",this).text()}}).get():t,r=document.getElementById(e),s=document.getElementById(o),a=function(){var t=r.value.trim().toLowerCase(),e=t.split(/[\s\-]+/);e.length>1&&e.push(t);var o=[];if(t.length>0&&n.forEach(function(n){function r(e,o,n,r){for(var s=r[r.length-1],a=s.position,i=s.word,l=[],h=0;a+i.length<=n&&0!=r.length;){i===t&&h++,l.push({position:a,length:i.length});var p=a+i.length;for(r.pop();0!=r.length&&(s=r[r.length-1],a=s.position,i=s.word,p>a);)r.pop()}return c+=h,{hits:l,start:o,end:n,searchTextCount:h}}function s(t,e){var o="",n=e.start;return e.hits.forEach(function(e){o+=t.substring(n,e.position);var r=e.position+e.length;o+='<b class="search-keyword">'+t.substring(e.position,r)+"</b>",n=r}),o+=t.substring(n,e.end)}var a=!1,i=0,c=0,l=n.title.trim(),h=l.toLowerCase(),p=n.content.trim().replace(/<[^>]+>/g,""),u=p.toLowerCase(),f=decodeURIComponent(n.url),d=[],g=[];if(""!=l&&(e.forEach(function(t){function e(t,e,o){var n=t.length;if(0===n)return[];var r=0,s=[],a=[];for(o||(e=e.toLowerCase(),t=t.toLowerCase());(s=e.indexOf(t,r))>-1;)a.push({position:s,word:t}),r=s+n;return a}d=d.concat(e(t,h,!1)),g=g.concat(e(t,u,!1))}),(d.length>0||g.length>0)&&(a=!0,i=d.length+g.length)),a){[d,g].forEach(function(t){t.sort(function(t,e){return e.position!==t.position?e.position-t.position:t.word.length-e.word.length})});var v=[];0!=d.length&&v.push(r(l,0,l.length,d));for(var $=[];0!=g.length;){var C=g[g.length-1],m=C.position,x=C.word,w=m-20,y=m+80;0>w&&(w=0),y<m+x.length&&(y=m+x.length),y>p.length&&(y=p.length),$.push(r(p,w,y,g))}$.sort(function(t,e){return t.searchTextCount!==e.searchTextCount?e.searchTextCount-t.searchTextCount:t.hits.length!==e.hits.length?e.hits.length-t.hits.length:t.start-e.start});var T=parseInt("1");T>=0&&($=$.slice(0,T));var b="";b+=0!=v.length?"<li><a href='"+f+"' class='search-result-title'>"+s(l,v[0])+"</a>":"<li><a href='"+f+"' class='search-result-title'>"+l+"</a>",$.forEach(function(t){b+="<a href='"+f+'\'><p class="search-result">'+s(p,t)+"...</p></a>"}),b+="</li>",o.push({item:b,searchTextCount:c,hitCount:i,id:o.length})}}),1===e.length&&""===e[0])s.innerHTML='<div id="no-result"><i class="fa fa-search fa-5x" /></div>';else if(0===o.length)s.innerHTML='<div id="no-result"><i class="fa fa-frown-o fa-5x" /></div>';else{o.sort(function(t,e){return t.searchTextCount!==e.searchTextCount?e.searchTextCount-t.searchTextCount:t.hitCount!==e.hitCount?e.hitCount-t.hitCount:e.id-t.id});var a='<ul class="search-result-list">';o.forEach(function(t){a+=t.item}),a+="</ul>",s.innerHTML=a}};r.addEventListener("input",a),$(".local-search-pop-overlay").remove(),$("body").css("overflow",""),proceedsearch()}})};$(".popup-trigger").click(function(t){t.stopPropagation(),isfetched===!1?searchFunc(path,"local-search-input","local-search-result"):proceedsearch()}),$(".popup-btn-close").click(onPopupClose),$(".popup").click(function(t){t.stopPropagation()}),$(document).on("keyup",function(t){var e=27===t.which&&$(".search-popup").is(":visible");e&&onPopupClose()})</script><script src="https://cdn1.lncld.net/static/js/av-core-mini-0.6.4.js"></script><script>AV.initialize("SVKPF4oiwJLMtpySQxvJLnLm-gzGzoHsz","46OfvvEe65XMeUi79STU895I")</script><script>function showTime(e){var t=new AV.Query(e),n=[],o=$(".leancloud_visitors");o.each(function(){n.push($(this).attr("id").trim())}),t.containedIn("url",n),t.find().done(function(e){var t=".leancloud-visitors-count";if(0===e.length)return void o.find(t).text(0);for(var i=0;i<e.length;i++){var r=e[i],s=r.get("url"),l=r.get("time"),c=document.getElementById(s);$(c).find(t).text(l)}for(var i=0;i<n.length;i++){var s=n[i],c=document.getElementById(s),u=$(c).find(t);""==u.text()&&u.text(0)}}).fail(function(e,t){console.log("Error: "+t.code+" "+t.message)})}function addCount(e){var t=$(".leancloud_visitors"),n=t.attr("id").trim(),o=t.attr("data-flag-title").trim(),i=new AV.Query(e);i.equalTo("url",n),i.find({success:function(t){if(t.length>0){var i=t[0];i.fetchWhenSave(!0),i.increment("time"),i.save(null,{success:function(e){var t=$(document.getElementById(n));t.find(".leancloud-visitors-count").text(e.get("time"))},error:function(e,t){console.log("Failed to save Visitor num, with error message: "+t.message)}})}else{var r=new e,s=new AV.ACL;s.setPublicReadAccess(!0),s.setPublicWriteAccess(!0),r.setACL(s),r.set("title",o),r.set("url",n),r.set("time",1),r.save(null,{success:function(e){var t=$(document.getElementById(n));t.find(".leancloud-visitors-count").text(e.get("time"))},error:function(e,t){console.log("Failed to create")}})}},error:function(e){console.log("Error:"+e.code+" "+e.message)}})}$(function(){var e=AV.Object.extend("Counter");1==$(".leancloud_visitors").length?addCount(e):$(".post-title-link").length>1&&showTime(e)})</script><script>!function(){var t=document.createElement("script"),s=window.location.protocol.split(":")[0];"https"===s?t.src="https://zz.bdstatic.com/linksubmit/push.js":t.src="http://push.zhanzhang.baidu.com/push.js";var e=document.getElementsByTagName("script")[0];e.parentNode.insertBefore(t,e)}()</script><script type="text/javascript">wpac_init=window.wpac_init||[],wpac_init.push({widget:"Rating",id:21228,el:"wpac-rating",color:"fc6423"}),function(){if(!("WIDGETPACK_LOADED"in window)){WIDGETPACK_LOADED=!0;var e=document.createElement("script");e.type="text/javascript",e.async=!0,e.src="//embed.widgetpack.com/widget.js";var t=document.getElementsByTagName("script")[0];t.parentNode.insertBefore(e,t.nextSibling)}}()</script><script type="text/x-mathjax-config">MathJax.Hub.Config({
        tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
          processEscapes: true,
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
        }
      });</script><script type="text/x-mathjax-config">MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for (i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
        }
      });</script><script type="text/javascript" src="//cdn.bootcss.com/mathjax/2.7.1/latest.js?config=TeX-AMS-MML_HTMLorMML"></script><canvas class="fireworks" style="position:fixed;left:0;top:0;z-index:1;pointer-events:none"></canvas><script type="text/javascript" src="//cdn.bootcss.com/animejs/2.2.0/anime.min.js"></script><script type="text/javascript" src="/js/src/fireworks.js"></script><script src="/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({pluginRootPath:"live2dw/",pluginJsPath:"lib/",pluginModelPath:"assets/",tagMode:!0,log:!1,model:{jsonPath:"/live2dw/assets/hijiki.model.json"},display:{position:"left",width:250,height:500},mobile:{show:!1},react:{opacity:.7}})</script></body></html><script type="text/javascript" src="/js/src/crash_cheat.js"></script><!-- rebuild by neat -->