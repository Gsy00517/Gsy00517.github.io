<!-- build time:Mon Feb 17 2020 22:43:24 GMT+0800 (GMT+08:00) --><!DOCTYPE html><html class="theme-next gemini use-motion" lang="zh-Hans"><head><meta name="generator" content="Hexo 3.9.0"><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width,initial-scale=1,maximum-scale=1"><meta name="theme-color" content="#222"><meta name="google-site-verification" content="YV24rdmIIf8GuLLOBH5IYEWm0Z3TGAqiLS-LLlspD7w"><script src="/lib/pace/pace.min.js?v=1.0.2"></script><link href="/lib/pace/pace-theme-big-counter.min.css?v=1.0.2" rel="stylesheet"><meta http-equiv="Cache-Control" content="no-transform"><meta http-equiv="Cache-Control" content="no-siteapp"><meta name="baidu-site-verification" content="true"><link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css"><link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css"><link href="/css/main.css?v=5.1.4" rel="stylesheet" type="text/css"><link rel="apple-touch-icon" sizes="180x180" href="/images/haizei.ico?v=5.1.4"><link rel="icon" type="image/png" sizes="32x32" href="/images/haizei.ico?v=5.1.4"><link rel="icon" type="image/png" sizes="16x16" href="/images/haizei.ico?v=5.1.4"><link rel="mask-icon" href="/images/haizei.ico?v=5.1.4" color="#222"><meta name="keywords" content="计算机视觉,目标跟踪,"><link rel="alternate" href="/atom.xml" title="高深远的博客" type="application/atom+xml"><meta name="description" content="看了一寒假的目标跟踪，一直想将自己学习到的内容整理归纳一下，迟迟没动笔（其实是打字，但说迟迟没打字比较难听哈哈）。如今快要开学了，决定还是积累一下。本文主要是把目标跟踪中的一些相关要点做一个总结，并且按具体问题对一些主流的或者我阅读过觉得有价值的算法做一个简单概述，方法层出不穷，码字不易，无法涵盖所有的方法，也可能会存在错误，在今后的学习过程中会一直保持更新，这将是一篇LTS的文章哈哈。注：本文重"><meta name="keywords" content="计算机视觉,目标跟踪"><meta property="og:type" content="article"><meta property="og:title" content="computer vision笔记：目标跟踪的小总结"><meta property="og:url" content="https://gsy00517.github.io/computer-vision20200215214240/index.html"><meta property="og:site_name" content="高深远的博客"><meta property="og:description" content="看了一寒假的目标跟踪，一直想将自己学习到的内容整理归纳一下，迟迟没动笔（其实是打字，但说迟迟没打字比较难听哈哈）。如今快要开学了，决定还是积累一下。本文主要是把目标跟踪中的一些相关要点做一个总结，并且按具体问题对一些主流的或者我阅读过觉得有价值的算法做一个简单概述，方法层出不穷，码字不易，无法涵盖所有的方法，也可能会存在错误，在今后的学习过程中会一直保持更新，这将是一篇LTS的文章哈哈。注：本文重"><meta property="og:locale" content="zh-Hans"><meta property="og:image" content="https://gsy00517.github.io/computer-vision20200215214240/目标跟踪.png"><meta property="og:image" content="https://gsy00517.github.io/computer-vision20200215214240/目标跟踪伪代码.png"><meta property="og:image" content="https://gsy00517.github.io/computer-vision20200215214240/王强的导图.png"><meta property="og:image" content="https://gsy00517.github.io/computer-vision20200215214240/王蒙蒙的导图.png"><meta property="og:image" content="https://gsy00517.github.io/computer-vision20200215214240/问题.png"><meta property="og:image" content="https://gsy00517.github.io/computer-vision20200215214240/光流.gif"><meta property="og:image" content="https://gsy00517.github.io/computer-vision20200215214240/画出光流.png"><meta property="og:image" content="https://gsy00517.github.io/computer-vision20200215214240/卷积因式分解大大降低参数量.png"><meta property="og:image" content="https://gsy00517.github.io/computer-vision20200215214240/样本分组.png"><meta property="og:image" content="https://gsy00517.github.io/computer-vision20200215214240/发廊旋转柱.png"><meta property="og:image" content="https://gsy00517.github.io/computer-vision20200215214240/不利于匹配.jpg"><meta property="og:image" content="https://gsy00517.github.io/computer-vision20200215214240/TCNN.jpg"><meta property="og:updated_time" content="2020-02-17T14:41:54.987Z"><meta name="twitter:card" content="summary"><meta name="twitter:title" content="computer vision笔记：目标跟踪的小总结"><meta name="twitter:description" content="看了一寒假的目标跟踪，一直想将自己学习到的内容整理归纳一下，迟迟没动笔（其实是打字，但说迟迟没打字比较难听哈哈）。如今快要开学了，决定还是积累一下。本文主要是把目标跟踪中的一些相关要点做一个总结，并且按具体问题对一些主流的或者我阅读过觉得有价值的算法做一个简单概述，方法层出不穷，码字不易，无法涵盖所有的方法，也可能会存在错误，在今后的学习过程中会一直保持更新，这将是一篇LTS的文章哈哈。注：本文重"><meta name="twitter:image" content="https://gsy00517.github.io/computer-vision20200215214240/目标跟踪.png"><script type="text/javascript" id="hexo.configurations">var NexT=window.NexT||{},CONFIG={root:"/",scheme:"Gemini",version:"5.1.4",sidebar:{position:"left",display:"post",offset:12,b2t:!1,scrollpercent:!0,onmobile:!1},fancybox:!0,tabs:!0,motion:{enable:!0,async:!1,transition:{post_block:"fadeIn",post_header:"slideDownIn",post_body:"slideDownIn",coll_header:"slideLeftIn",sidebar:"slideUpIn"}},duoshuo:{userId:"0",author:"博主"},algolia:{applicationID:"",apiKey:"",indexName:"",hits:{per_page:10},labels:{input_placeholder:"Search for Posts",hits_empty:"We didn't find any results for the search: ${query}",hits_stats:"${hits} results found in ${time} ms"}}}</script><link rel="canonical" href="https://gsy00517.github.io/computer-vision20200215214240/"><meta name="baidu-site-verification" content="o5QfpvLBz5"><title>computer vision笔记：目标跟踪的小总结 | 高深远的博客</title></head><body itemscope itemtype="http://schema.org/WebPage" lang="zh-Hans"><div class="container sidebar-position-left page-post-detail"><div class="headband"></div><header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="header-inner"><div class="site-brand-wrapper"><div class="site-meta"><div class="custom-logo-site-title"><a href="/" class="brand" rel="start"><span class="logo-line-before"><i></i></span> <span class="site-title">高深远的博客</span> <span class="logo-line-after"><i></i></span></a></div><h1 class="site-subtitle" itemprop="description"></h1></div><div class="site-nav-toggle"><button><span class="btn-bar"></span> <span class="btn-bar"></span> <span class="btn-bar"></span></button></div></div><nav class="site-nav"><ul id="menu" class="menu"><li class="menu-item menu-item-home"><a href="/" rel="section"><i class="menu-item-icon fa fa-fw fa-home"></i><br>首页</a></li><li class="menu-item menu-item-about"><a href="/about/" rel="section"><i class="menu-item-icon fa fa-fw fa-user"></i><br>关于</a></li><li class="menu-item menu-item-tags"><a href="/tags/" rel="section"><i class="menu-item-icon fa fa-fw fa-tags"></i><br>标签</a></li><li class="menu-item menu-item-categories"><a href="/categories/" rel="section"><i class="menu-item-icon fa fa-fw fa-th"></i><br>分类</a></li><li class="menu-item menu-item-archives"><a href="/archives/" rel="section"><i class="menu-item-icon fa fa-fw fa-archive"></i><br>最新发布</a></li><li class="menu-item menu-item-new"><a href="/new/" rel="section"><i class="menu-item-icon fa fa-fw fa-history"></i><br>最近阅读</a></li><li class="menu-item menu-item-rank"><a href="/rank/" rel="section"><i class="menu-item-icon fa fa-fw fa-signal"></i><br>热度排名</a></li><li class="menu-item menu-item-search"><a href="javascript:;" class="popup-trigger"><i class="menu-item-icon fa fa-search fa-fw"></i><br>站内搜索</a></li></ul><div class="site-search"><div class="popup search-popup local-search-popup"><div class="local-search-header clearfix"><span class="search-icon"><i class="fa fa-search"></i> </span><span class="popup-btn-close"><i class="fa fa-times-circle"></i></span><div class="local-search-input-wrapper"><input autocomplete="off" placeholder="搜索..." spellcheck="false" type="text" id="local-search-input"></div></div><div id="local-search-result"></div></div></div><div style="Text-align:center;width:100%"><div style="margin:0 auto"><canvas id="canvas" style="width:60%">当前浏览器不支持canvas，请更换浏览器后再试</canvas></div><script>!function(){function t(t,e){for(var a=0;a<l[e].length;a++)for(var n=0;n<l[e][a].length;n++)1==l[e][a][n]&&(h.beginPath(),h.arc(14*(g+2)*t+2*n*(g+1)+(g+1),2*a*(g+1)+(g+1),g,0,2*Math.PI),h.closePath(),h.fill())}function e(){var t=[],e=/(\d)(\d):(\d)(\d):(\d)(\d)/.exec(new Date),a=[];a.push(e[1],e[2],10,e[3],e[4],10,e[5],e[6]);for(var r=c.length-1;r>=0;r--)a[r]!==c[r]&&t.push(r+"_"+(Number(c[r])+1)%10);for(var r=0;r<t.length;r++)n.apply(this,t[r].split("_"));c=a.concat()}function a(){for(var t=0;t<d.length;t++)d[t].stepY+=d[t].disY,d[t].x+=d[t].stepX,d[t].y+=d[t].stepY,(d[t].x>i+g||d[t].y>f+g)&&(d.splice(t,1),t--)}function n(t,e){for(var a=[1,2,3],n=["#3BE","#09C","#A6C","#93C","#9C0","#690","#FB3","#F80","#F44","#C00"],r=0;r<l[e].length;r++)for(var o=0;o<l[e][r].length;o++)if(1==l[e][r][o]){var h={x:14*(g+2)*t+2*o*(g+1)+(g+1),y:2*r*(g+1)+(g+1),stepX:Math.floor(4*Math.random()-2),stepY:-2*a[Math.floor(Math.random()*a.length)],color:n[Math.floor(Math.random()*n.length)],disY:1};d.push(h)}}function r(){o.height=100;for(var e=0;e<c.length;e++)t(e,c[e]);for(var e=0;e<d.length;e++)h.beginPath(),h.arc(d[e].x,d[e].y,g,0,2*Math.PI),h.fillStyle=d[e].color,h.closePath(),h.fill()}var l=[[[0,0,1,1,1,0,0],[0,1,1,0,1,1,0],[1,1,0,0,0,1,1],[1,1,0,0,0,1,1],[1,1,0,0,0,1,1],[1,1,0,0,0,1,1],[1,1,0,0,0,1,1],[1,1,0,0,0,1,1],[0,1,1,0,1,1,0],[0,0,1,1,1,0,0]],[[0,0,0,1,1,0,0],[0,1,1,1,1,0,0],[0,0,0,1,1,0,0],[0,0,0,1,1,0,0],[0,0,0,1,1,0,0],[0,0,0,1,1,0,0],[0,0,0,1,1,0,0],[0,0,0,1,1,0,0],[0,0,0,1,1,0,0],[1,1,1,1,1,1,1]],[[0,1,1,1,1,1,0],[1,1,0,0,0,1,1],[0,0,0,0,0,1,1],[0,0,0,0,1,1,0],[0,0,0,1,1,0,0],[0,0,1,1,0,0,0],[0,1,1,0,0,0,0],[1,1,0,0,0,0,0],[1,1,0,0,0,1,1],[1,1,1,1,1,1,1]],[[1,1,1,1,1,1,1],[0,0,0,0,0,1,1],[0,0,0,0,1,1,0],[0,0,0,1,1,0,0],[0,0,1,1,1,0,0],[0,0,0,0,1,1,0],[0,0,0,0,0,1,1],[0,0,0,0,0,1,1],[1,1,0,0,0,1,1],[0,1,1,1,1,1,0]],[[0,0,0,0,1,1,0],[0,0,0,1,1,1,0],[0,0,1,1,1,1,0],[0,1,1,0,1,1,0],[1,1,0,0,1,1,0],[1,1,1,1,1,1,1],[0,0,0,0,1,1,0],[0,0,0,0,1,1,0],[0,0,0,0,1,1,0],[0,0,0,1,1,1,1]],[[1,1,1,1,1,1,1],[1,1,0,0,0,0,0],[1,1,0,0,0,0,0],[1,1,1,1,1,1,0],[0,0,0,0,0,1,1],[0,0,0,0,0,1,1],[0,0,0,0,0,1,1],[0,0,0,0,0,1,1],[1,1,0,0,0,1,1],[0,1,1,1,1,1,0]],[[0,0,0,0,1,1,0],[0,0,1,1,0,0,0],[0,1,1,0,0,0,0],[1,1,0,0,0,0,0],[1,1,0,1,1,1,0],[1,1,0,0,0,1,1],[1,1,0,0,0,1,1],[1,1,0,0,0,1,1],[1,1,0,0,0,1,1],[0,1,1,1,1,1,0]],[[1,1,1,1,1,1,1],[1,1,0,0,0,1,1],[0,0,0,0,1,1,0],[0,0,0,0,1,1,0],[0,0,0,1,1,0,0],[0,0,0,1,1,0,0],[0,0,1,1,0,0,0],[0,0,1,1,0,0,0],[0,0,1,1,0,0,0],[0,0,1,1,0,0,0]],[[0,1,1,1,1,1,0],[1,1,0,0,0,1,1],[1,1,0,0,0,1,1],[1,1,0,0,0,1,1],[0,1,1,1,1,1,0],[1,1,0,0,0,1,1],[1,1,0,0,0,1,1],[1,1,0,0,0,1,1],[1,1,0,0,0,1,1],[0,1,1,1,1,1,0]],[[0,1,1,1,1,1,0],[1,1,0,0,0,1,1],[1,1,0,0,0,1,1],[1,1,0,0,0,1,1],[0,1,1,1,0,1,1],[0,0,0,0,0,1,1],[0,0,0,0,0,1,1],[0,0,0,0,1,1,0],[0,0,0,1,1,0,0],[0,1,1,0,0,0,0]],[[0,0,0,0,0,0,0],[0,0,1,1,1,0,0],[0,0,1,1,1,0,0],[0,0,1,1,1,0,0],[0,0,0,0,0,0,0],[0,0,0,0,0,0,0],[0,0,1,1,1,0,0],[0,0,1,1,1,0,0],[0,0,1,1,1,0,0],[0,0,0,0,0,0,0]]],o=document.getElementById("canvas");if(o.getContext){var h=o.getContext("2d"),f=100,i=700;o.height=f,o.width=i,h.fillStyle="#f00",h.fillRect(10,10,50,50);var c=[],d=[],g=o.height/20-1;!function(){var t=/(\d)(\d):(\d)(\d):(\d)(\d)/.exec(new Date);c.push(t[1],t[2],10,t[3],t[4],10,t[5],t[6])}(),clearInterval(v);var v=setInterval(function(){e(),a(),r()},50)}}()</script></div></nav></div></header><main id="main" class="main"><div class="main-inner"><div class="content-wrap"><div id="content" class="content"><div id="posts" class="posts-expand"><article class="post post-type-normal" itemscope itemtype="http://schema.org/Article"><div class="post-block"><link itemprop="mainEntityOfPage" href="https://gsy00517.github.io/computer-vision20200215214240/"><span hidden itemprop="author" itemscope itemtype="http://schema.org/Person"><meta itemprop="name" content="高深远"><meta itemprop="description" content><meta itemprop="image" content="/images/lufei.jpg"></span><span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization"><meta itemprop="name" content="高深远的博客"></span><header class="post-header"><h2 class="post-title" itemprop="name headline">computer vision笔记：目标跟踪的小总结</h2><div class="post-meta"><span class="post-time"><span class="post-meta-item-icon"><i class="fa fa-calendar-o"></i> </span><span class="post-meta-item-text">发表于</span> <time title="创建于" itemprop="dateCreated datePublished" datetime="2020-02-15T21:42:40+08:00">2020-02-15 </time><span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-calendar-check-o"></i> </span><span class="post-meta-item-text">更新于&#58;</span> <time title="更新于" itemprop="dateModified" datetime="2020-02-17T22:41:54+08:00">2020-02-17 </time></span><span class="post-category"><span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-folder-o"></i> </span><span class="post-meta-item-text">分类于</span> <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/人工智能/" itemprop="url" rel="index"><span itemprop="name">人工智能</span> </a></span></span><span class="post-comments-count"><span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-comment-o"></i> </span><a href="/computer-vision20200215214240/#comments" itemprop="discussionUrl"><span class="post-comments-count valine-comment-count" data-xid="/computer-vision20200215214240/" itemprop="commentCount"></span> </a></span><span id="/computer-vision20200215214240/" class="leancloud_visitors" data-flag-title="computer vision笔记：目标跟踪的小总结"><span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-eye"></i> </span><span class="post-meta-item-text">阅读次数&#58;</span> <span class="leancloud-visitors-count"></span></span><div class="post-wordcount"><span class="post-meta-item-icon"><i class="fa fa-file-word-o"></i> </span><span class="post-meta-item-text">字数统计&#58;</span> <span title="字数统计">8.2k字 </span><span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-clock-o"></i> </span><span class="post-meta-item-text">阅读时长 &asymp;</span> <span title="阅读时长">28分钟</span></div></div></header><div class="post-body" itemprop="articleBody"><p>看了一寒假的目标跟踪，一直想将自己学习到的内容整理归纳一下，迟迟没动笔（其实是打字，但说迟迟没打字比较难听哈哈）。如今快要开学了，决定还是积累一下。<br>本文主要是把目标跟踪中的一些相关要点做一个总结，并且按具体问题对一些主流的或者我阅读过觉得有价值的算法做一个简单概述，方法层出不穷，码字不易，无法涵盖所有的方法，也可能会存在错误，在今后的学习过程中会一直保持更新，这将是一篇LTS的文章哈哈。</p><blockquote><p>注：本文重点关注单目标跟踪。其中各个算法的年份以参加benchmark的年份或者论文发表年份为依据，可能会存在1年的区别，但影响不大。</p></blockquote><p><strong>References</strong>：</p><p>电子文献：<br><a href="http://geyao1995.com/extract_optical_flow/" target="_blank" rel="noopener">http://geyao1995.com/extract_optical_flow/</a><br><a href="https://blog.csdn.net/u012905422/article/details/52396372" target="_blank" rel="noopener">https://blog.csdn.net/u012905422/article/details/52396372</a><br><a href="https://cloud.tencent.com/developer/article/1327410" target="_blank" rel="noopener">https://cloud.tencent.com/developer/article/1327410</a></p><p>参考文献：<br>[1]统计学习方法（第2版）<br>[2]Understanding and Diagnosing Visual Tracking Systems<br>[3]Survey of Visual Object Tracking Algorithms Based on Deep Learning<br>[4]Handcrafted and Deep Trackers: Recent Visual Object Tracking Approaches and Trends<br>[5]Visual Object Tracking using Adaptive Correlation Filters<br>[6]Discriminative Scale Space Tracking<br>[7]High-Speed Tracking with Kernelized Correlation Filters<br>[8]ECO: Efﬁcient Convolution Operators for Tracking<br>[9]Learning Multi-Domain Convolutional Neural Networks for Visual Tracking<br>[10]Visual Tracking with Fully Convolutional Networks<br>[11]End-to-end representation learning for Correlation Filter based tracking<br>[12]Meta-Tracker: Fast and Robust Online Adaptation for Visual Object Trackers</p><hr><h1 id="简介与要求"><a href="#简介与要求" class="headerlink" title="简介与要求"></a>简介与要求</h1><p>目标跟踪是利用一个视频或图像序列的上下文信息，对目标的外观和运动信息进行建模，从而对目标运动状态进行预测并标定目标位置的一种技术。一般是在第一帧给出一个框，框中的物体就是我们需要在后续帧中用算法进行跟踪的对象。就目前的单目标跟踪而言，一般有如下要求：<br><strong>monocular</strong>：我们的视频或者图片序列是仅从一个摄像头中获得的，也就是不考虑比如在城市道路场景中跨摄像头对目标跟踪的复杂应用。<br><strong>model-free</strong>：没有任何先验，也就是在获取第一帧的框之前我们并不知道会框出什么物体，也不需要在之前对初始框中的物体进行建模。<br><strong>single-target</strong>：只追踪第一帧框出的那一个物体，也就是除了那个物体之外所有的物体都是back ground。<br><strong>casual/real-time</strong>：目标跟踪是一个在线过程，也就是不能提前获取未来的框对目标进行跟踪。<br><strong>short-term</strong>：没有重检测，也就是目标跟丢了就丢了。<br><strong>long-term</strong>：可以在跟丢之后重检测，这类算法一般除了跟踪之外还需要有检测的功能。<br><img src="/computer-vision20200215214240/目标跟踪.png" title="目标跟踪"><br>下面是目标跟踪流程的伪代码表示（不一定普适，比如有些算法不在线更新，但符合基本的过程）。<br><img src="/computer-vision20200215214240/目标跟踪伪代码.png" title="目标跟踪伪代码"></p><hr><h1 id="生成式与判别式"><a href="#生成式与判别式" class="headerlink" title="生成式与判别式"></a>生成式与判别式</h1><p>利用特征判断候选样本是否为跟踪目标，可将目标跟踪的模型分为生成式模型和判别式模型，本小节就介绍一下什么是生成式模型和判别式模型。</p><h2 id="机器学习"><a href="#机器学习" class="headerlink" title="机器学习"></a>机器学习</h2><p>我们首先看看在机器学习中生成式模型和判别式模型定义的一般区分。<br>一般而言，机器学习的任务就是学习一个模型，应用这一个模型，对给定的输入预测相应的输出。输出的一般形式可以是决策函数，也可以是条件概率分布。<br>对于生成式模型，我们需要通过数据学习输入X与输出Y之间的生成关系（比如联合概率分布），也就是认为X和Y都是随机变量。典型的生成式模型有朴素贝叶斯模型、隐马尔可夫模型（HMM）、高斯混合模型（GMM）等。<br>对于判别式模型，我们只需要直接学习决策函数或者条件概率分布，只关心对给定的输入X我们需要输出怎么样的Y，也就是不考虑X是否是随机变量。典型的判别式模型包括k近邻、感知机、决策树、逻辑斯蒂回归模型、最大熵模型、支持向量机（SVM）、提升方法和条件随机场等。此外神经网络也属于判别式模型。<br>相较而言，生成式模型体现了更多的信息，不过这还是因条件而异的，不同情况不同任务两种方法各有优缺点。</p><h2 id="目标跟踪"><a href="#目标跟踪" class="headerlink" title="目标跟踪"></a>目标跟踪</h2><p>在目标跟踪领域，生成式模型通过提取目标特征来构建表观模型，然后在图像中搜索与模型最匹配的区域作为跟踪结果。不论采用全局特征还是局部特征，生成式模型的本质是在目标表示的高维空间中，找到与目标模型最相邻的候选目标作为当前估计。此类方法的缺陷在于只关注目标信息，而忽略了背景信息。<br>与生成式模型不同的是，判别式模型同时考虑了目标和背景信息。它将跟踪问题看做分类或者回归问题，其目的是寻找一个判别函数，将目标从背景中分离出来，从而实现对目标的跟踪。<br>一般来说，在目标跟踪领域，判别式充分利用了目标前景和背景信息，能更加有效地区分出目标，比单单运用目标区域特征进行模板匹配的生成式模型在复杂环境中的鲁棒性更强。</p><hr><h1 id="算法导图"><a href="#算法导图" class="headerlink" title="算法导图"></a>算法导图</h1><p>下图是中科院博士王强（github名为foolwood…呃不得不说这名字取得真谦虚）在<a href="https://github.com/foolwood/benchmark_results" target="_blank">github</a>上总结的历年各大benchmark的优秀成果的一个思维导图，同一个链接下还包括了各项成果的paper及code，值得收藏一下。<br><img src="/computer-vision20200215214240/王强的导图.png" title="王强的导图"><br>下图是浙大硕士王蒙蒙极市平台做分享的时候所用的一张思维导图，归纳得也比较清晰。<br><img src="/computer-vision20200215214240/王蒙蒙的导图.png" title="王蒙蒙的导图"></p><blockquote><p>注：两张导图中都把历年benchmark的冠军工作作了标注。</p></blockquote><p>对比两张思维导图可以发现，两位学者都把主流算法分成了相关滤波、深度学习两个分支（或者说是基于Handcrafted特征的算法和基于CNN提取特征的算法），此外还有一些基于强化学习、结构化SVM的模型。其实，目标跟踪算得上是计算机视觉领域中深度学习涉足较晚的一个方向，其主要原因是目标跟踪相关数据集的标注花费较大。此外，相关滤波的速度优势，也就是实时性是十分引人注目的，但在应付当前目标跟踪中的各种挑战、问题时，相关滤波的鲁棒性还是落后于深度学习方法的。</p><hr><h1 id="问题及策略"><a href="#问题及策略" class="headerlink" title="问题及策略"></a>问题及策略</h1><p>通俗来讲，目标跟踪的最终目标就是要又快又准。“快”主要表现在计算量小和所需的存储空间小，“准”就是预测出的bounding box要尽可能地接近ground truth。除了上面两个基本需求（也可以说是为了更好地达到这两个基本需求），近年来的算法主要针对目标跟踪中的一些挑战进行突破，从而更好地解决某些问题之后达到更好的整体效果。<br>总的来说，目标跟踪的主要问题有如下这些：遮挡（occlusion）、背景干扰（background clutter）、光照变化（illumination changes）、尺度变化（scale variation）、低分辨率（low resolution）、快速移动（fast motion）、超出画面（out of view）、运动模糊（motion blur）、形变（deformation）、旋转（rotation）等。<br><img src="/computer-vision20200215214240/问题.png" title="问题"><br>下面我就对各个方面提出的策略和所做的工作做一个小结。其中部分问题的解决方法有相通之处，为方便整理，我会将其归到一起。</p><h2 id="实时性"><a href="#实时性" class="headerlink" title="实时性"></a>实时性</h2><p>尽管许多评价指标中都把速度放在次要地位，但实时性对于实际应用来说还是非常重要的。</p><h3 id="LK-Tracker（1981）"><a href="#LK-Tracker（1981）" class="headerlink" title="LK Tracker（1981）"></a>LK Tracker（1981）</h3><p>这应该是最早的目标跟踪工作，它使用了光流的概念，如下图所示，不同颜色表示光流不同的方向，颜色的深浅表示运动的速度。<br><img src="/computer-vision20200215214240/光流.gif" title="光流"><br>LK Tracker假定目标灰度在短时间内保持不变，同时目标邻域内的速度向量场变化缓慢。由于光流方程包含坐标x，y和时间t共三个未知数，其中时间变化dt已知而坐标变化dx和dy未知，一个方程两个未知数无法求解，因此作者假定相邻的点它们的光流是一致的，这样就可以求解方程组了。下图是求解之后的光流向量，其中绿色箭头的方向表示运动方向，线段长度表示运动速度的大小。<br><img src="/computer-vision20200215214240/画出光流.png" title="画出光流"><br>光流的计算非常简单也非常快，而且由于提出得很早，各种库都有实现好的轮子可以轻松调用，但是它的鲁棒性不好，基本上只能对平移且外观不变的物体进行跟踪。</p><h3 id="MOSSE（2010）"><a href="#MOSSE（2010）" class="headerlink" title="MOSSE（2010）"></a>MOSSE（2010）</h3><p>MOSSE是第一个把相关滤波引入到目标跟踪领域的算法，其速度能够达到600多帧每秒，但是效果一般，主要是因为它只使用了简单的raw pixel特征。</p><h3 id="CT（2012）"><a href="#CT（2012）" class="headerlink" title="CT（2012）"></a>CT（2012）</h3><p>CT（Compressive Tracking）也就是使用压缩感知来做跟踪，它用压缩过的特征来表示目标，使得跟踪速度能够很快。</p><h3 id="FCT（2014）"><a href="#FCT（2014）" class="headerlink" title="FCT（2014）"></a>FCT（2014）</h3><p>和CT一样，也是使用了压缩感知，主打速度。</p><h3 id="Staple（2016）"><a href="#Staple（2016）" class="headerlink" title="Staple（2016）"></a>Staple（2016）</h3><p>Staple的算法比较简单，速度很快，效果也比较好。</p><h3 id="SINT（2016）"><a href="#SINT（2016）" class="headerlink" title="SINT（2016）"></a>SINT（2016）</h3><p>SINT是最早把孪生网络（Siamese Network）应用于目标跟踪的工作，它通过孪生网络直接学习目标模板和候选目标的匹配函数，在online tracking的过程中只用初始帧的目标作为模板来实现跟踪。</p><h3 id="SiameseFC（2016）"><a href="#SiameseFC（2016）" class="headerlink" title="SiameseFC（2016）"></a>SiameseFC（2016）</h3><p>SiameseFC主打的是速度问题，利用孪生网络，在视频序列ILSVRC2015离线训练一个相似性度量函数，在跟踪过程中利用该模型，选择与模板最相似的候选作为跟踪结果。尽管使用了CNN特征，但其跟踪速度仍能够远超实时，这是进步之处，但是效果比较一般。</p><h3 id="Learnet（2016）"><a href="#Learnet（2016）" class="headerlink" title="Learnet（2016）"></a>Learnet（2016）</h3><p>也是来自上面提出SiameseFC的牛津大学团队的工作，主要在one-shot learning的理论方面进行了讨论。</p><h3 id="GOTURN（2016）"><a href="#GOTURN（2016）" class="headerlink" title="GOTURN（2016）"></a>GOTURN（2016）</h3><p>GOTURN网络的输出是一个回归网络，也就是bounding box的4个坐标位置。它主要关注的是速度问题，速度非常得快，但是效果一般。</p><h3 id="ECO（2017）"><a href="#ECO（2017）" class="headerlink" title="ECO（2017）"></a>ECO（2017）</h3><p>高效卷积算子ECO（efficient convolution operators）主要是为了解决C-COT速度慢的问题。通过卷积因式分解操作、样本分组和更新策略对其改进，在不影响算法精确度的同时，将算法速度提高了一个数量级。<br><img src="/computer-vision20200215214240/卷积因式分解大大降低参数量.png" title="卷积因式分解大大降低参数量"><br><img src="/computer-vision20200215214240/样本分组.png" title="样本分组"></p><h3 id="Meta-tracker（2018）"><a href="#Meta-tracker（2018）" class="headerlink" title="Meta-tracker（2018）"></a>Meta-tracker（2018）</h3><p>Meta-tracker基于元学习的思想，采用基于预测梯度的策略学习方法获得普适性的初始化模型，可以使得跟踪模型自适应于后续帧特征的最佳梯度方向，是模型在接收初始帧之后能够快速收敛。</p><h2 id="精确度"><a href="#精确度" class="headerlink" title="精确度"></a>精确度</h2><p>目标跟踪算法的评价标准通常包含两个基本参数：中心位置误差和区域重叠面积比率。因此精确度是衡量一个算法好坏的重要指标。</p><h3 id="FCNT（2015）"><a href="#FCNT（2015）" class="headerlink" title="FCNT（2015）"></a>FCNT（2015）</h3><p>FCNT较早地利用CNN网络底层和顶层不同的表达效果来做跟踪。</p><h3 id="C-COT（2016）"><a href="#C-COT（2016）" class="headerlink" title="C-COT（2016）"></a>C-COT（2016）</h3><p>C-COT（continuous convolution operators for visual tracking）将浅层表观信息和深层语义信息结合起来，根据不同空间分辨率的响应，在频域进行插值得到连续空间分辨率的响应图，通过迭代求解最佳位置和尺度。实验效果很好，但是Martin Danelljan前期的工作普遍速度是硬伤，不够在前面提到的2017年的ECO中有进一步改进。</p><h3 id="DNT（2017）"><a href="#DNT（2017）" class="headerlink" title="DNT（2017）"></a>DNT（2017）</h3><p>DNT是一个充分利用卷积神经网络的不同层进行特征提取而实现目标跟踪的双重网络，为了突出目标的几何轮廓，首先把卷积神经网络提取的级联特征和拉普拉斯高斯滤波得到的边缘特征整合为粗糙的先验图，再把双重网络的输出和边缘特征整合为混合成分，最后用参考独立成分分析算法得到精确的特征图。</p><h3 id="CREST（2017）"><a href="#CREST（2017）" class="headerlink" title="CREST（2017）"></a>CREST（2017）</h3><p>CREST采用残差学习来逼近真实值和网络输出相应值之间的残差，从而可以更好地获得目标的表观变化。</p><h3 id="DRT（2018）"><a href="#DRT（2018）" class="headerlink" title="DRT（2018）"></a>DRT（2018）</h3><p>DRT在ECO的基础上引入了稳定性概念，对滤波器的每一部分引入一个权值，由此决定是否使用它进行跟踪。具体来说，就是通过构造一个与过滤器大小相同的矩阵，在使用滤波器前与之相乘，最终使得滤波器不可靠部分数值较小，从而提升跟踪精度。</p><h3 id="UPDT（2018）"><a href="#UPDT（2018）" class="headerlink" title="UPDT（2018）"></a>UPDT（2018）</h3><p>UPDT（unveiling the power of deep tracking）区别对待深度特征和浅层特征，利用数据增强和差异响应函数提高鲁棒性和准确性，同时利用提出的质量评估方法自适应融合响应图，从而得到最优的目标跟踪结果。</p><h3 id="SiamMask（2019）"><a href="#SiamMask（2019）" class="headerlink" title="SiamMask（2019）"></a>SiamMask（2019）</h3><p>SiamMask是SiamRPN的后续之作，结合语义分割多任务学习。相比SiamRPN，SiamMask的网络结构增加了预测目标分割掩码的分支，从而给出了视觉目标跟踪（VOT）和视频目标分割（VOS）统一框架。SiamMask很好地实现了目标跟踪和目标分割任务之间的互相补充，不仅可以得到目标更精准的包围框，还可以得到目标的像素级标注。</p><h2 id="快速移动和运动模糊"><a href="#快速移动和运动模糊" class="headerlink" title="快速移动和运动模糊"></a>快速移动和运动模糊</h2><p>运动模糊一般是由物体或者相机的快速移动造成的，解决该问题的主要思路有：扩大搜索范围；挖掘更有用的特征信息来辅助目标定位。</p><h2 id="形变和旋转"><a href="#形变和旋转" class="headerlink" title="形变和旋转"></a>形变和旋转</h2><p>形变和旋转是导致drift的主要原因之一，因此可以考虑如何降低背景的影响，此外也可以考虑利用顶层CNN能识别出类别的高维特征的特性来提高对形变和旋转的鲁棒性。</p><h3 id="CNT（2016）"><a href="#CNT（2016）" class="headerlink" title="CNT（2016）"></a>CNT（2016）</h3><p>CNT对其形成的稀疏表示特征（后面还会提到）采用简单的在线更新策略来抑制drift，对目标形变更为鲁棒。</p><h3 id="ECO（2017）-1"><a href="#ECO（2017）-1" class="headerlink" title="ECO（2017）"></a>ECO（2017）</h3><p>ECO通过对目标物体的外观计算高斯混合模型来抑制drift的问题，通过高斯混合模型，可以保持样本之间的差异性，同时也减少了存储的样本数量。</p><h3 id="MenTrack（2018）"><a href="#MenTrack（2018）" class="headerlink" title="MenTrack（2018）"></a>MenTrack（2018）</h3><p>MenTrack引入了具有外部存储功能的动态存储网络，通过更新外部存储单元来适应目标形状的变化，而不需要高代价的在线网络微调。</p><h2 id="光照变化"><a href="#光照变化" class="headerlink" title="光照变化"></a>光照变化</h2><p>对付光照变化的主要思路是在保留目标模型的同时也要考虑背景信息。</p><h3 id="MOSSE（2010）-1"><a href="#MOSSE（2010）-1" class="headerlink" title="MOSSE（2010）"></a>MOSSE（2010）</h3><p>MOSSE在做相关操作之前，对每张图都进行了减去平均值的处理，这有利于淡化背景对相关操作的影响。另外假如发生光照变化的话，减去均值也有利于减小这种变化的影响。</p><h2 id="低分辨率"><a href="#低分辨率" class="headerlink" title="低分辨率"></a>低分辨率</h2><p>对于视频或者图像序列分辨率低的问题，可以考虑使用更强大的特征来表征目标，深度的特征就是一个很好的解决方案。</p><h2 id="遮挡和超出画面"><a href="#遮挡和超出画面" class="headerlink" title="遮挡和超出画面"></a>遮挡和超出画面</h2><p>对于这类问题，主要用重检测和改善更新策略（见后文）两种思路，同时一般需要保留一定量目标样本。</p><h3 id="TLD（2010）"><a href="#TLD（2010）" class="headerlink" title="TLD（2010）"></a>TLD（2010）</h3><p>TLD（tracking learning detection）主要针对long-term tracking，在跟踪的同时全局检测。它利用在线的Ferns检测目标，同时利用在线随机森林算法跟踪目标。</p><h3 id="LCT（2015）"><a href="#LCT（2015）" class="headerlink" title="LCT（2015）"></a>LCT（2015）</h3><p>主要针对的是long-term tracking的问题，配置了一个detector，用于跟丢之后快速重检测。用了两个滤波器，一个是用于平移估算的Rc，是同padding并施加海明窗（一种余弦窗），结合了FHOG和一些其他的特征；另一个是用于尺度估计的Rt，不使用padding和海明窗，使用了HOG特征，此外Rt还用于检测置信度，用来决定是否更新模型和是否重检测。</p><h3 id="DaSiamRPN（2018）"><a href="#DaSiamRPN（2018）" class="headerlink" title="DaSiamRPN（2018）"></a>DaSiamRPN（2018）</h3><p>DaSiamRPN提出从局部到全局的搜索策略来检测得分判断目标是否丢失，再进一步实现目标重检测。</p><h2 id="尺度变化"><a href="#尺度变化" class="headerlink" title="尺度变化"></a>尺度变化</h2><h3 id="DSST（2014）"><a href="#DSST（2014）" class="headerlink" title="DSST（2014）"></a>DSST（2014）</h3><p>DSST主要考虑了尺度缩放的问题。它将目标跟踪看成位置变化和尺度变化两个独立问题，训练了两个滤波器，首先训练位置平移相关滤波器以检测目标中心平移，然后训练尺度相关滤波器来检测目标的尺度变化。</p><h2 id="背景干扰"><a href="#背景干扰" class="headerlink" title="背景干扰"></a>背景干扰</h2><h3 id="SANet（2016）"><a href="#SANet（2016）" class="headerlink" title="SANet（2016）"></a>SANet（2016）</h3><p>SANet主要是对MDNet进行改进，通过在中间添加两层RNN来从空间进行建模，主要解决了MDNet对一些相似的物体跟踪效果不太好的问题。不同于一维的时序任务，二维图像数据中物体的结构信息以无向循环图编码，循环结构使得无法直接应用RNN来提取结构信息。因此，作者将无向循环图拓扑结构近似为4个有向不循环图的组合从而适用于RNN。</p><h3 id="LMCF（2017）"><a href="#LMCF（2017）" class="headerlink" title="LMCF（2017）"></a>LMCF（2017）</h3><p>LMCF借鉴了KCF的循环特征图、Struck的结构化SVM，使用相关滤波。在前向追踪时，LMCF考虑到画面中相似物体的干扰，提出了一种多峰值的目标跟踪算法（Multimodal Target Tracking），即对高于某一阈值的响应峰值做二次检测，把response map和一个用于筛选的二值矩阵作点乘，相当于把不是峰值的位置滤为0。</p><h3 id="DiMP（2019）"><a href="#DiMP（2019）" class="headerlink" title="DiMP（2019）"></a>DiMP（2019）</h3><p>DiMP针对Siamese跟踪系列对于背景和目标区分性不足的问题，设计了一种鲁棒的判别能力较强的Loss，并通过端到端的训练学习Loss重点的关键参数。同时结合提出的权重预测模块对网络进行良好的初始化，最终DiMP在速度和准确性上都有所提高。</p><hr><h1 id="各类方法总结"><a href="#各类方法总结" class="headerlink" title="各类方法总结"></a>各类方法总结</h1><h2 id="样本采集方法"><a href="#样本采集方法" class="headerlink" title="样本采集方法"></a>样本采集方法</h2><h3 id="Mean-Shift（2002）"><a href="#Mean-Shift（2002）" class="headerlink" title="Mean Shift（2002）"></a>Mean Shift（2002）</h3><p>采用均值漂移作为搜索策略，成为当时常用的视觉跟踪系统的目标搜索方法。</p><h3 id="IVT（2008）"><a href="#IVT（2008）" class="headerlink" title="IVT（2008）"></a>IVT（2008）</h3><p>在线更新特征空间的基，直接将以前检测到的目标作为样本在线学习而无需大量的标注样本。</p><h3 id="L1-Tracker（2011）"><a href="#L1-Tracker（2011）" class="headerlink" title="L1 Tracker（2011）"></a>L1 Tracker（2011）</h3><p>L1 Tracker是第一个将稀疏编码引入目标跟踪问题中的算法。它把跟踪看做一个稀疏近似问题，主要是用第一帧和最近几帧得到的图像（特征）作为字典，通过求解L1范数最小化问题，实现对目标的跟踪。</p><h3 id="CSK（2012）"><a href="#CSK（2012）" class="headerlink" title="CSK（2012）"></a>CSK（2012）</h3><p>CSK也称为核相关滤波算法，它针对MOSSE做出了一些改进，采用循环移位进行密集采样，并通过核函数将低维线性空间映射到高维空间，提高了相关滤波器的鲁棒性。随后的工作主要从特征选择、尺度估计、正则化等方面对该算法进行改进和提高。</p><h3 id="RPT（2015）"><a href="#RPT（2015）" class="headerlink" title="RPT（2015）"></a>RPT（2015）</h3><p>RPT主要关注了第一帧初始化容易受到干扰的问题。</p><h3 id="BACF（2017）"><a href="#BACF（2017）" class="headerlink" title="BACF（2017）"></a>BACF（2017）</h3><p>BACF（background-aware correlation filters）通过补零操作获取更大搜索域的样本，进行循环采样时保证了真实的负样本。</p><h3 id="VITAL（2018）"><a href="#VITAL（2018）" class="headerlink" title="VITAL（2018）"></a>VITAL（2018）</h3><p>VITAL通过对抗学习的方法来解决正样本采样时高度重合而不丰富和正负样本比例不平衡的问题。为了增强正样本对形变的鲁棒性，VITAL在最后1个卷积层和第1个全连接层之间引入对抗网络随机生成特征的权重掩码，每一个掩码表示一类具体的形变，而引入对抗学习就能够识别那些长期保留目标形变的掩码。为了解决各类之间的不平衡问题，VITAL引入高阶敏感损失函数降低易分负样本对分类网络的影响。</p><h3 id="DaSiamRPN（2018）-1"><a href="#DaSiamRPN（2018）-1" class="headerlink" title="DaSiamRPN（2018）"></a>DaSiamRPN（2018）</h3><p>DaSiamRPN在训练阶段采取样本增强策略，利用现有的目标检测数据集（如ImageNet检测集和COCO检测集）扩充正样本数据，以此提升目标跟踪器的泛化能力，并显式地增加不同视频段同类样本以及不同类样本作为负样本，以此提升目标跟踪器的判别能力。在相似性度量阶段。DaSiamRPN采取非极大抑制选择难以区分的错误样本，从而实现更加有效的增量式学习。</p><h2 id="特征提取方法"><a href="#特征提取方法" class="headerlink" title="特征提取方法"></a>特征提取方法</h2><p>我们知道，在现实场景中，物体是在三维的运动场中移动的。而视频或图像序列都是二维的信息，这其实是一些难题的根本原因之一。一个比较极端的例子就是理发店门前经常会出现的旋转柱，如果单纯地从二维角度来看，柱子是向上运动的，可在实际的运动场中柱子是横向运动的，观测和实际的运动方向是完全垂直的。<br><img src="/computer-vision20200215214240/发廊旋转柱.png" title="发廊旋转柱"><br>因此，为了能够更好地跟踪目标，我们需要提取尽可能好的特征，此外最好能从视频或图像序列中学到更多丰富的信息。值得注意的一点是，在港科大王乃岩博士2015年所做的ablation experiments中，发现特征提取是影响tracker效果最重要的因素。</p><h3 id="KLT（1994）"><a href="#KLT（1994）" class="headerlink" title="KLT（1994）"></a>KLT（1994）</h3><p>特征和LT一样也是光流特征，在此基础上使用匹配角点的方法，也就是寻找边角处、纹理处等易辨识的地方计算光流来进行追踪。</p><h3 id="Condensation（1998）"><a href="#Condensation（1998）" class="headerlink" title="Condensation（1998）"></a>Condensation（1998）</h3><p>采用原始的外观作为主要特征来描述目标。</p><h3 id="Feature-Selection（2003）"><a href="#Feature-Selection（2003）" class="headerlink" title="Feature Selection（2003）"></a>Feature Selection（2003）</h3><p>利用线性判别分析自适应地选择对当前背景和目标最具鉴别性的颜色特征，从而分离出目标。</p><h3 id="Boosting（2008）"><a href="#Boosting（2008）" class="headerlink" title="Boosting（2008）"></a>Boosting（2008）</h3><p>结合Haar特征和在线Boosting算法对目标进行跟踪。</p><h3 id="DLT（2013）"><a href="#DLT（2013）" class="headerlink" title="DLT（2013）"></a>DLT（2013）</h3><p>DLT是（应该来说）最早的基于CNN的算法，它直接利用ImageNet数据上的预训练模型提取深度特征，当时AlexNet刚刚被提出（2012年），实验证明效果不错。</p><h3 id="CN（2014）"><a href="#CN（2014）" class="headerlink" title="CN（2014）"></a>CN（2014）</h3><p>引入了颜色特征color name（CN）来扩展CSK，也使用了相关滤波。</p><h3 id="CNN-SVM（2015）"><a href="#CNN-SVM（2015）" class="headerlink" title="CNN-SVM（2015）"></a>CNN-SVM（2015）</h3><p>根据名称易知，该模型就是结合CNN特征和SVM分类器来做的。</p><h3 id="KCF（2015）"><a href="#KCF（2015）" class="headerlink" title="KCF（2015）"></a>KCF（2015）</h3><p>KCF跟CSK是同一个团队提出的，它跟CSK的区别是就是KCF首先进行了完整的理论推导，然后引入HOG特征，对CSK作了进一步的完善，是一个具有里程碑意义的工作。</p><h3 id="SRDCF（2015）"><a href="#SRDCF（2015）" class="headerlink" title="SRDCF（2015）"></a>SRDCF（2015）</h3><p>SRDCF主要考虑到若仅使用单纯的相关滤波，可能会存在边界效应，也就是相关滤波采用循环移位采样导致除了中心样本以外的其他样本中都会存在边界。于是作者采用了大的检测区域，在滤波器系数上加入权重约束：越靠近边缘权重越大，越靠近中心权重越小。这就使得滤波器系数主要集中在中心区域，从而让边界的影响没有那么明显。最终的效果不错，但是速度比较缓慢。</p><h3 id="DeepSRDCF（2015）"><a href="#DeepSRDCF（2015）" class="headerlink" title="DeepSRDCF（2015）"></a>DeepSRDCF（2015）</h3><p>同上，关注点也在解决相关滤波的边界效应。</p><h3 id="HCF（2015）"><a href="#HCF（2015）" class="headerlink" title="HCF（2015）"></a>HCF（2015）</h3><p>HCF的主要贡献是把相关滤波中的HOG特征换成了深度特征，它使用的是VGG的3、4、5三个层来提取特征，针对每层CNN训练一个过滤器，并且按照从深到浅的顺序使用相关滤波，然后利用深层得到的结果来引导浅层从而减少搜索空间，在当时效果很好。</p><h3 id="SCT（2016）"><a href="#SCT（2016）" class="headerlink" title="SCT（2016）"></a>SCT（2016）</h3><p>在KCF的基础上，加入attention。</p><h3 id="DMT（2016）"><a href="#DMT（2016）" class="headerlink" title="DMT（2016）"></a>DMT（2016）</h3><p>DMT用到了MotionNet，使用了光流信息，同时还结合了普通的CNN的特征、HOG特征以及color name，这样fusion之后效果不错，但是速度就可想而知会非常慢。</p><h3 id="CAT（2017）"><a href="#CAT（2017）" class="headerlink" title="CAT（2017）"></a>CAT（2017）</h3><p>相对于之前的KCF，CAT考虑了更多背景的信息，它增加了上下左右4个大块的背景信息并在它推导的框架上进行了一些改进，在速度方面表现也不错。</p><h3 id="DeepLMCF（2017）"><a href="#DeepLMCF（2017）" class="headerlink" title="DeepLMCF（2017）"></a>DeepLMCF（2017）</h3><p>与LMCF类似，不同的是使用了CNN特征。</p><h3 id="ACFN（2017）"><a href="#ACFN（2017）" class="headerlink" title="ACFN（2017）"></a>ACFN（2017）</h3><p>ACFN主要是对SCT做改进，都是加入了attention，但由于ACFN使用的是CNN特征，因此效果比SCT更好。</p><h3 id="CFCF（2017）"><a href="#CFCF（2017）" class="headerlink" title="CFCF（2017）"></a>CFCF（2017）</h3><p>CFCF通过精调网络模型，学习适用于相关滤波的深度特征，然后将学到的深度特征引入C-COT的跟踪框架，获得了当年VOT冠军。</p><h3 id="SiamDW（2019）"><a href="#SiamDW（2019）" class="headerlink" title="SiamDW（2019）"></a>SiamDW（2019）</h3><p>这是CVPR2019的一篇Oral，主要是基于Siamese网络进行改进，使网络更深更宽。作者注意到了之前基于Siamese网络的跟踪算法都使用了较浅的backbone且没有使用padding，因为对于靠近画面边缘的图像，使用padding采样会导致它与template的信息存在更多差异（超出画面的部分），这就不利于预测下一帧的位置。<br><img src="/computer-vision20200215214240/不利于匹配.jpg" title="不利于匹配"><br>于是作者对Siamese网络的感受野、步长、padding进行系统的实验，最终得出了更深且效果更好的Siamese网络模型。</p><h2 id="模型更新方法"><a href="#模型更新方法" class="headerlink" title="模型更新方法"></a>模型更新方法</h2><h3 id="MUSTer（2015）"><a href="#MUSTer（2015）" class="headerlink" title="MUSTer（2015）"></a>MUSTer（2015）</h3><p>模拟了人脑的记忆过程来做跟踪。</p><h3 id="TCNN（2016）"><a href="#TCNN（2016）" class="headerlink" title="TCNN（2016）"></a>TCNN（2016）</h3><p>TCNN使用了一个树形的结构来处理CNN特征。它利用可靠性来分配预测目标的权重，采用的更新策略是每10帧删除最前的节点，同时创建一个新的CNN节点，选择能够使新节点的可靠性最高的节点作为其父节点。这样一直保持一个active set，里面是10个最新更新的CNN模型，用这个active set来做跟踪。这个算法的速度比较慢。<br><img src="/computer-vision20200215214240/TCNN.jpg" title="TCNN"></p><h3 id="LMCF（2017）-1"><a href="#LMCF（2017）-1" class="headerlink" title="LMCF（2017）"></a>LMCF（2017）</h3><p>前文已提到LMCF，这里不做过多介绍。<br>除了多峰值的跟踪算法，LMCF在模型更新方面，提出了一种高置信度的更新策略（High-confidence Update），由于LMCF主要关注的是实时性，所以希望在算法简单的情况下能够减少失误。在传统的方法中，一般是当最大响应的峰值高于某一个阈值时（认为没跟丢目标），就对模型进行更新；否则若没有响应值超过峰值，就不对模型进行更新。而该工作的实验发现，当目标被遮挡时，响应图会震荡得非常厉害（存在多个较大的峰值），但同时最大响应的峰值仍旧会很高，这就会指导模型进行错误的更新并导致最后跟丢目标。于是作者提出了一个APCE值，定义如下。</p><script type="math/tex;mode=display">APCE=\frac{\left | F_{max}-F_{min} \right |^{2}}{mean(\underset{w,h}{\sum }(F_{w,h}-F_{min})^{2})}</script><p>只有当最大响应的峰值比较明确，即远超response map中的其他的响应时，APCE值才会比较大，此时允许对模型进行更新。</p><h3 id="UpdateNet（2019）"><a href="#UpdateNet（2019）" class="headerlink" title="UpdateNet（2019）"></a>UpdateNet（2019）</h3><p>UpdateNet旨在解决Siamese跟踪系列一直存在的模板更新难题，提出了用学习更新模板的方式来替代手工更新模板的方式。UpdateNet使用一个卷积神经网络根据初始帧模板、当前帧模板以及上次计算出的模板直接生成下一帧可用的最优模板，通过学习这样一个网络函数来实现模板更新功能。</p><h2 id="模型训练方法"><a href="#模型训练方法" class="headerlink" title="模型训练方法"></a>模型训练方法</h2><h3 id="MDNet（2016）"><a href="#MDNet（2016）" class="headerlink" title="MDNet（2016）"></a>MDNet（2016）</h3><p>MDNet设计一个轻量级的小型网络学习卷积特征表示目标，提出了一个多域的网络框架，将一个视频序列视为一个域，其中共享的部分用来学习目标的特征表达，独立的全连接层则用于学习针对特定视频序列的softmax分类。在离线训练时，针对每个视频序列构建一个新的检测分支进行训练，而特征提取网络是共享的。这样特征提取网络可以学习到通用性更强的与域无关的特征。在跟踪时，保留并固定特征提取网络，针对跟踪序列构建一个新的分支检测部分，用第1帧样本在线训练检测部分之后再利用跟踪结果生成正负样本来微调检测分支。</p><h3 id="STCT（2016）"><a href="#STCT（2016）" class="headerlink" title="STCT（2016）"></a>STCT（2016）</h3><p>跟前面的FCNT一样来自同一个团队，它使用了sequential training method来提升效果，即把CNN看做一个ensemble，其中每一个channel的卷积层map被看做一个base learner，通过新的loss相对独立地更新。这样在线微调就作为一个连续的集成学习，而在线跟踪转化成这个ensemble中的前景背景的分割问题。</p><h3 id="CNT（2016）-1"><a href="#CNT（2016）-1" class="headerlink" title="CNT（2016）"></a>CNT（2016）</h3><p>CNT采用了一个轻型的两层卷积神经网络，该网络无需大量辅助数据离线训练就能学到较为鲁棒的特征。具体实现为：首先在第1帧中使用k近邻算法从目标区域提取很多归一化的图像块作为固定滤波器，然后结合后续帧目标周围的一系列自适应上下文滤波器来构成特征图集合，最后用自适应阈值的软收缩算法对卷积后的全局表征进行去噪处理，从而得到鲁棒的稀疏表示特征作为目标模板。</p><h3 id="CFNet（2017）"><a href="#CFNet（2017）" class="headerlink" title="CFNet（2017）"></a>CFNet（2017）</h3><p>CFNet的主要贡献是从理论对相关滤波在CNN中的应用进行了推导，将相关滤波改写成可微分的神经网络层，将特征提取网络整合到一起以实现端到端优化，训练与相关滤波器相匹配的卷积特征。</p><h3 id="ADNet（2017）"><a href="#ADNet（2017）" class="headerlink" title="ADNet（2017）"></a>ADNet（2017）</h3><p>使用强化学习来做目标跟踪，其增强学习策略网络是通过CNN构建的，是一个创新。</p><h3 id="RLT（2017）"><a href="#RLT（2017）" class="headerlink" title="RLT（2017）"></a>RLT（2017）</h3><p>顾名思义，也使用了RL。</p><h3 id="SiamRPN（2018）"><a href="#SiamRPN（2018）" class="headerlink" title="SiamRPN（2018）"></a>SiamRPN（2018）</h3><p>SiamRPN将目标跟踪构造成单样本检测任务，其网络结构分为特征提取Siamese子网络和候选目标区域生成RPN子网络。RPN子网络又包含分类和回归两条分支。SiamRPN可以利用ILSVRC和YouTube-BB大量的标注数据进行离线端到端训练，从而取得了较好的性能和跟踪速度。</p><h2 id="目标分类方法"><a href="#目标分类方法" class="headerlink" title="目标分类方法"></a>目标分类方法</h2><h3 id="Struck（2011）"><a href="#Struck（2011）" class="headerlink" title="Struck（2011）"></a>Struck（2011）</h3><p>Struck的主要贡献是引入了结构化SVM。它利用结构化SVM直接输出跟踪结果，避免了中间分类环节，在当时效果较之前明显提升。</p><h3 id="DLSSVM（2016）"><a href="#DLSSVM（2016）" class="headerlink" title="DLSSVM（2016）"></a>DLSSVM（2016）</h3><p>延续之前的Struck，利用结构化SVM，在优化的阶段做了一些改进进行提速。其实结构化SVM分类器非常强大，但是因为它求解优化的过程比较复杂以及使用稠密采样（粒子滤波或者滑窗采样）比较耗时，使得结构化SVM的速度成为一个瓶颈，因此不如之后使用相关滤波的算法。</p><h3 id="Obli-RaF（2017）"><a href="#Obli-RaF（2017）" class="headerlink" title="Obli-RaF（2017）"></a>Obli-RaF（2017）</h3><p>使用了斜随机森林（Oblique Random Forest），思路新奇，效果不错。它将样本块视为粒子输入到一个斜随机森林分类器中，根据每个树节点的投票结果判断该粒子是目标还是背景，最高得分即为预测的目标位置。如果投票最高得分没有超过预设的阈值，则重新采样。如果达到了maximum limit（我认为这个limit应该不同于前面的阈值），那么模型更新，否则则保留之前的模型。</p><script type="text/javascript" src="/js/src/bai.js"></script></div><div><div><hr style="FILTER:progid:DXImageTransform.Microsoft.Shadow(color:#987cb9,direction:145,strength:15)" width="100%" color="#987cb9" size="1"><div style="text-align:center;color:#555;font-size:16px"><i class="fa fa-hand-o-up" aria-hidden="true"></i> 碰到底线咯 <i class="fa fa-handshake-o" aria-hidden="true"></i> 后面没有啦 <i class="fa fa-hand-o-down" aria-hidden="true"></i></div></div></div><div><div class="my_post_copyright"><script src="//cdn.bootcss.com/clipboard.js/1.5.10/clipboard.min.js"></script><script src="https://cdn.bootcss.com/jquery/2.0.0/jquery.min.js"></script><script src="https://unpkg.com/sweetalert/dist/sweetalert.min.js"></script><p><span>本文标题:</span><a href="/computer-vision20200215214240/">computer vision笔记：目标跟踪的小总结</a></p><p><span>文章作者:</span><a href="/" title="访问 高深远 的个人博客">高深远</a></p><p><span>发布时间:</span>2020年02月15日 - 21:42</p><p><span>最后更新:</span>2020年02月17日 - 22:41</p><p><span>原始链接:</span><a href="/computer-vision20200215214240/" title="computer vision笔记：目标跟踪的小总结">https://gsy00517.github.io/computer-vision20200215214240/</a> <span class="copy-path" title="点击复制文章链接"><i class="fa fa-clipboard" data-clipboard-text="https://gsy00517.github.io/computer-vision20200215214240/" aria-label="复制成功！"></i></span></p><p><span>许可协议:</span><i class="fa fa-creative-commons"></i> <a rel="license" href="https://creativecommons.org/licenses/by-nc-nd/4.0/" target="_blank" title="Attribution-NonCommercial-NoDerivatives 4.0 International (CC BY-NC-ND 4.0)">署名-非商业性使用-禁止演绎 4.0 国际</a> 转载请保留原文链接及作者。</p></div><script>var clipboard=new Clipboard(".fa-clipboard");$(".fa-clipboard").click(function(){clipboard.on("success",function(){swal({title:"复制成功",text:"感谢您的阅读与参考！欢迎留下任何建议噢！",icon:"success",showConfirmButton:!0})})})</script></div><footer class="post-footer"><div class="post-tags"><a href="/tags/计算机视觉/" rel="tag"><i class="fa fa-tag"></i> 计算机视觉</a> <a href="/tags/目标跟踪/" rel="tag"><i class="fa fa-tag"></i> 目标跟踪</a></div><div class="post-widgets"><div class="wp_rating"><div id="wpac-rating"></div></div></div><div class="post-nav"><div class="post-nav-next post-nav-item"><a href="/vot-toolkit20200215185238/" rel="next" title="vot toolkit笔记：解决无法连接TraX支持的问题"><i class="fa fa-chevron-left"></i> vot toolkit笔记：解决无法连接TraX支持的问题</a></div><span class="post-nav-divider"></span><div class="post-nav-prev post-nav-item"></div></div></footer></div></article><div class="post-spread"></div></div></div><div class="comments" id="comments"></div></div><div class="sidebar-toggle"><div class="sidebar-toggle-line-wrap"><span class="sidebar-toggle-line sidebar-toggle-line-first"></span> <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span> <span class="sidebar-toggle-line sidebar-toggle-line-last"></span></div></div><aside id="sidebar" class="sidebar"><div class="sidebar-inner"><ul class="sidebar-nav motion-element"><li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap">文章目录</li><li class="sidebar-nav-overview" data-target="site-overview-wrap">站点概览</li></ul><section class="site-overview-wrap sidebar-panel"><div class="site-overview"><div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person"><img class="site-author-image" itemprop="image" src="/images/lufei.jpg" alt="高深远"><p class="site-author-name" itemprop="name">高深远</p><p class="site-description motion-element" itemprop="description"></p></div><nav class="site-state motion-element"><div class="site-state-item site-state-posts"><a href="/archives/"><span class="site-state-item-count">81</span> <span class="site-state-item-name">日志</span></a></div><div class="site-state-item site-state-categories"><a href="/categories/index.html"><span class="site-state-item-count">7</span> <span class="site-state-item-name">分类</span></a></div><div class="site-state-item site-state-tags"><a href="/tags/index.html"><span class="site-state-item-count">33</span> <span class="site-state-item-name">标签</span></a></div></nav><div class="feed-link motion-element"><a href="/atom.xml" rel="alternate"><i class="fa fa-rss"></i> RSS</a></div><div class="links-of-blogroll motion-element links-of-blogroll-inline"><div class="links-of-blogroll-title"><i class="fa fa-fw fa-link"></i> 网站收藏</div><ul class="links-of-blogroll-list"><li class="links-of-blogroll-item"><a href="https://www.runoob.com/" title="菜鸟教程" target="_blank">菜鸟教程</a></li><li class="links-of-blogroll-item"><a href="https://paperswithcode.com/" title="PaperWithCode" target="_blank">PaperWithCode</a></li><li class="links-of-blogroll-item"><a href="https://www.jiqizhixin.com/sota" title="机器之心" target="_blank">机器之心</a></li><li class="links-of-blogroll-item"><a href="http://pytorch123.com/" title="Pytorch中文文档" target="_blank">Pytorch中文文档</a></li></ul></div></div></section><section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active"><div class="post-toc"><div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#简介与要求"><span class="nav-number">1.</span> <span class="nav-text">简介与要求</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#生成式与判别式"><span class="nav-number">2.</span> <span class="nav-text">生成式与判别式</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#机器学习"><span class="nav-number">2.1.</span> <span class="nav-text">机器学习</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#目标跟踪"><span class="nav-number">2.2.</span> <span class="nav-text">目标跟踪</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#算法导图"><span class="nav-number">3.</span> <span class="nav-text">算法导图</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#问题及策略"><span class="nav-number">4.</span> <span class="nav-text">问题及策略</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#实时性"><span class="nav-number">4.1.</span> <span class="nav-text">实时性</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#LK-Tracker（1981）"><span class="nav-number">4.1.1.</span> <span class="nav-text">LK Tracker（1981）</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#MOSSE（2010）"><span class="nav-number">4.1.2.</span> <span class="nav-text">MOSSE（2010）</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#CT（2012）"><span class="nav-number">4.1.3.</span> <span class="nav-text">CT（2012）</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#FCT（2014）"><span class="nav-number">4.1.4.</span> <span class="nav-text">FCT（2014）</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Staple（2016）"><span class="nav-number">4.1.5.</span> <span class="nav-text">Staple（2016）</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#SINT（2016）"><span class="nav-number">4.1.6.</span> <span class="nav-text">SINT（2016）</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#SiameseFC（2016）"><span class="nav-number">4.1.7.</span> <span class="nav-text">SiameseFC（2016）</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Learnet（2016）"><span class="nav-number">4.1.8.</span> <span class="nav-text">Learnet（2016）</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#GOTURN（2016）"><span class="nav-number">4.1.9.</span> <span class="nav-text">GOTURN（2016）</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#ECO（2017）"><span class="nav-number">4.1.10.</span> <span class="nav-text">ECO（2017）</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Meta-tracker（2018）"><span class="nav-number">4.1.11.</span> <span class="nav-text">Meta-tracker（2018）</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#精确度"><span class="nav-number">4.2.</span> <span class="nav-text">精确度</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#FCNT（2015）"><span class="nav-number">4.2.1.</span> <span class="nav-text">FCNT（2015）</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#C-COT（2016）"><span class="nav-number">4.2.2.</span> <span class="nav-text">C-COT（2016）</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#DNT（2017）"><span class="nav-number">4.2.3.</span> <span class="nav-text">DNT（2017）</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#CREST（2017）"><span class="nav-number">4.2.4.</span> <span class="nav-text">CREST（2017）</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#DRT（2018）"><span class="nav-number">4.2.5.</span> <span class="nav-text">DRT（2018）</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#UPDT（2018）"><span class="nav-number">4.2.6.</span> <span class="nav-text">UPDT（2018）</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#SiamMask（2019）"><span class="nav-number">4.2.7.</span> <span class="nav-text">SiamMask（2019）</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#快速移动和运动模糊"><span class="nav-number">4.3.</span> <span class="nav-text">快速移动和运动模糊</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#形变和旋转"><span class="nav-number">4.4.</span> <span class="nav-text">形变和旋转</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#CNT（2016）"><span class="nav-number">4.4.1.</span> <span class="nav-text">CNT（2016）</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#ECO（2017）-1"><span class="nav-number">4.4.2.</span> <span class="nav-text">ECO（2017）</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#MenTrack（2018）"><span class="nav-number">4.4.3.</span> <span class="nav-text">MenTrack（2018）</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#光照变化"><span class="nav-number">4.5.</span> <span class="nav-text">光照变化</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#MOSSE（2010）-1"><span class="nav-number">4.5.1.</span> <span class="nav-text">MOSSE（2010）</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#低分辨率"><span class="nav-number">4.6.</span> <span class="nav-text">低分辨率</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#遮挡和超出画面"><span class="nav-number">4.7.</span> <span class="nav-text">遮挡和超出画面</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#TLD（2010）"><span class="nav-number">4.7.1.</span> <span class="nav-text">TLD（2010）</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#LCT（2015）"><span class="nav-number">4.7.2.</span> <span class="nav-text">LCT（2015）</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#DaSiamRPN（2018）"><span class="nav-number">4.7.3.</span> <span class="nav-text">DaSiamRPN（2018）</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#尺度变化"><span class="nav-number">4.8.</span> <span class="nav-text">尺度变化</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#DSST（2014）"><span class="nav-number">4.8.1.</span> <span class="nav-text">DSST（2014）</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#背景干扰"><span class="nav-number">4.9.</span> <span class="nav-text">背景干扰</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#SANet（2016）"><span class="nav-number">4.9.1.</span> <span class="nav-text">SANet（2016）</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#LMCF（2017）"><span class="nav-number">4.9.2.</span> <span class="nav-text">LMCF（2017）</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#DiMP（2019）"><span class="nav-number">4.9.3.</span> <span class="nav-text">DiMP（2019）</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#各类方法总结"><span class="nav-number">5.</span> <span class="nav-text">各类方法总结</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#样本采集方法"><span class="nav-number">5.1.</span> <span class="nav-text">样本采集方法</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Mean-Shift（2002）"><span class="nav-number">5.1.1.</span> <span class="nav-text">Mean Shift（2002）</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#IVT（2008）"><span class="nav-number">5.1.2.</span> <span class="nav-text">IVT（2008）</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#L1-Tracker（2011）"><span class="nav-number">5.1.3.</span> <span class="nav-text">L1 Tracker（2011）</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#CSK（2012）"><span class="nav-number">5.1.4.</span> <span class="nav-text">CSK（2012）</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#RPT（2015）"><span class="nav-number">5.1.5.</span> <span class="nav-text">RPT（2015）</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#BACF（2017）"><span class="nav-number">5.1.6.</span> <span class="nav-text">BACF（2017）</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#VITAL（2018）"><span class="nav-number">5.1.7.</span> <span class="nav-text">VITAL（2018）</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#DaSiamRPN（2018）-1"><span class="nav-number">5.1.8.</span> <span class="nav-text">DaSiamRPN（2018）</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#特征提取方法"><span class="nav-number">5.2.</span> <span class="nav-text">特征提取方法</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#KLT（1994）"><span class="nav-number">5.2.1.</span> <span class="nav-text">KLT（1994）</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Condensation（1998）"><span class="nav-number">5.2.2.</span> <span class="nav-text">Condensation（1998）</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Feature-Selection（2003）"><span class="nav-number">5.2.3.</span> <span class="nav-text">Feature Selection（2003）</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Boosting（2008）"><span class="nav-number">5.2.4.</span> <span class="nav-text">Boosting（2008）</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#DLT（2013）"><span class="nav-number">5.2.5.</span> <span class="nav-text">DLT（2013）</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#CN（2014）"><span class="nav-number">5.2.6.</span> <span class="nav-text">CN（2014）</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#CNN-SVM（2015）"><span class="nav-number">5.2.7.</span> <span class="nav-text">CNN-SVM（2015）</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#KCF（2015）"><span class="nav-number">5.2.8.</span> <span class="nav-text">KCF（2015）</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#SRDCF（2015）"><span class="nav-number">5.2.9.</span> <span class="nav-text">SRDCF（2015）</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#DeepSRDCF（2015）"><span class="nav-number">5.2.10.</span> <span class="nav-text">DeepSRDCF（2015）</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#HCF（2015）"><span class="nav-number">5.2.11.</span> <span class="nav-text">HCF（2015）</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#SCT（2016）"><span class="nav-number">5.2.12.</span> <span class="nav-text">SCT（2016）</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#DMT（2016）"><span class="nav-number">5.2.13.</span> <span class="nav-text">DMT（2016）</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#CAT（2017）"><span class="nav-number">5.2.14.</span> <span class="nav-text">CAT（2017）</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#DeepLMCF（2017）"><span class="nav-number">5.2.15.</span> <span class="nav-text">DeepLMCF（2017）</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#ACFN（2017）"><span class="nav-number">5.2.16.</span> <span class="nav-text">ACFN（2017）</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#CFCF（2017）"><span class="nav-number">5.2.17.</span> <span class="nav-text">CFCF（2017）</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#SiamDW（2019）"><span class="nav-number">5.2.18.</span> <span class="nav-text">SiamDW（2019）</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#模型更新方法"><span class="nav-number">5.3.</span> <span class="nav-text">模型更新方法</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#MUSTer（2015）"><span class="nav-number">5.3.1.</span> <span class="nav-text">MUSTer（2015）</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#TCNN（2016）"><span class="nav-number">5.3.2.</span> <span class="nav-text">TCNN（2016）</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#LMCF（2017）-1"><span class="nav-number">5.3.3.</span> <span class="nav-text">LMCF（2017）</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#UpdateNet（2019）"><span class="nav-number">5.3.4.</span> <span class="nav-text">UpdateNet（2019）</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#模型训练方法"><span class="nav-number">5.4.</span> <span class="nav-text">模型训练方法</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#MDNet（2016）"><span class="nav-number">5.4.1.</span> <span class="nav-text">MDNet（2016）</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#STCT（2016）"><span class="nav-number">5.4.2.</span> <span class="nav-text">STCT（2016）</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#CNT（2016）-1"><span class="nav-number">5.4.3.</span> <span class="nav-text">CNT（2016）</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#CFNet（2017）"><span class="nav-number">5.4.4.</span> <span class="nav-text">CFNet（2017）</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#ADNet（2017）"><span class="nav-number">5.4.5.</span> <span class="nav-text">ADNet（2017）</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#RLT（2017）"><span class="nav-number">5.4.6.</span> <span class="nav-text">RLT（2017）</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#SiamRPN（2018）"><span class="nav-number">5.4.7.</span> <span class="nav-text">SiamRPN（2018）</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#目标分类方法"><span class="nav-number">5.5.</span> <span class="nav-text">目标分类方法</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Struck（2011）"><span class="nav-number">5.5.1.</span> <span class="nav-text">Struck（2011）</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#DLSSVM（2016）"><span class="nav-number">5.5.2.</span> <span class="nav-text">DLSSVM（2016）</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Obli-RaF（2017）"><span class="nav-number">5.5.3.</span> <span class="nav-text">Obli-RaF（2017）</span></a></li></ol></li></ol></li></ol></div></div></section></div></aside></div></main><footer id="footer" class="footer"><div class="footer-inner"><div class="copyright">&copy; <span itemprop="copyrightYear">2020</span> <span class="with-love"><i class="fa fa-user"></i> </span><span class="author" itemprop="copyrightHolder">高深远</span> <span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-area-chart"></i> </span><span class="post-meta-item-text">Site words total count&#58;</span> <span title="Site words total count">151.2k</span></div><script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><span id="busuanzi_container_site_pv">访客已留下<span id="busuanzi_value_site_pv"></span>个脚印 </span><span id="busuanzi_container_site_uv">你是第<span id="busuanzi_value_site_uv"></span>位小伙伴</span></div></footer><div class="back-to-top"><i class="fa fa-arrow-up"></i> <span id="scrollpercent"><span>0</span>%</span></div></div><script type="text/javascript">"[object Function]"!==Object.prototype.toString.call(window.Promise)&&(window.Promise=null)</script><script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script><script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script><script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script><script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script><script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script><script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script><script type="text/javascript" src="/lib/three/three.min.js"></script><script type="text/javascript" src="/lib/three/three-waves.min.js"></script><script type="text/javascript" src="/js/src/utils.js?v=5.1.4"></script><script type="text/javascript" src="/js/src/motion.js?v=5.1.4"></script><script type="text/javascript" src="/js/src/affix.js?v=5.1.4"></script><script type="text/javascript" src="/js/src/schemes/pisces.js?v=5.1.4"></script><script type="text/javascript" src="/js/src/scrollspy.js?v=5.1.4"></script><script type="text/javascript" src="/js/src/post-details.js?v=5.1.4"></script><script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.4"></script><script src="//cdn1.lncld.net/static/js/3.0.4/av-min.js"></script><script src="//unpkg.com/valine/dist/Valine.min.js"></script><script type="text/javascript">var GUEST = ['nick','mail','link'];
    var guest = 'nick,mail,link';
    guest = guest.split(',').filter(item=>{
      return GUEST.indexOf(item)>-1;
    });
    new Valine({
        el: '#comments' ,
        verify: false,
        notify: false,
        appId: 'SVKPF4oiwJLMtpySQxvJLnLm-gzGzoHsz',
        appKey: '46OfvvEe65XMeUi79STU895I',
        placeholder: '动动手指，写下您的意见、疑惑或者鼓励吧！留下您的邮箱，这样就可以收到别人的回复啦！',
        avatar:'wavatar',
        guest_info:guest,
        pageSize:'10' || 10,
    });

    var infoEle = document.querySelector('#comments .info');
    if (infoEle && infoEle.childNodes && infoEle.childNodes.length > 0){
    infoEle.childNodes.forEach(function(item) {
    item.parentNode.removeChild(item);
    });
  }</script><script type="text/javascript">function proceedsearch(){$("body").append('<div class="search-popup-overlay local-search-pop-overlay"></div>').css("overflow","hidden"),$(".search-popup-overlay").click(onPopupClose),$(".popup").toggle();var t=$("#local-search-input");t.attr("autocapitalize","none"),t.attr("autocorrect","off"),t.focus()}var isfetched=!1,isXml=!0,search_path="search.xml";0===search_path.length?search_path="search.xml":/json$/i.test(search_path)&&(isXml=!1);var path="/"+search_path,onPopupClose=function(t){$(".popup").hide(),$("#local-search-input").val(""),$(".search-result-list").remove(),$("#no-result").remove(),$(".local-search-pop-overlay").remove(),$("body").css("overflow","")},searchFunc=function(t,e,o){"use strict";$("body").append('<div class="search-popup-overlay local-search-pop-overlay"><div id="search-loading-icon"><i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i></div></div>').css("overflow","hidden"),$("#search-loading-icon").css("margin","20% auto 0 auto").css("text-align","center"),$.ajax({url:t,dataType:isXml?"xml":"json",async:!0,success:function(t){isfetched=!0,$(".popup").detach().appendTo(".header-inner");var n=isXml?$("entry",t).map(function(){return{title:$("title",this).text(),content:$("content",this).text(),url:$("url",this).text()}}).get():t,r=document.getElementById(e),s=document.getElementById(o),a=function(){var t=r.value.trim().toLowerCase(),e=t.split(/[\s\-]+/);e.length>1&&e.push(t);var o=[];if(t.length>0&&n.forEach(function(n){function r(e,o,n,r){for(var s=r[r.length-1],a=s.position,i=s.word,l=[],h=0;a+i.length<=n&&0!=r.length;){i===t&&h++,l.push({position:a,length:i.length});var p=a+i.length;for(r.pop();0!=r.length&&(s=r[r.length-1],a=s.position,i=s.word,p>a);)r.pop()}return c+=h,{hits:l,start:o,end:n,searchTextCount:h}}function s(t,e){var o="",n=e.start;return e.hits.forEach(function(e){o+=t.substring(n,e.position);var r=e.position+e.length;o+='<b class="search-keyword">'+t.substring(e.position,r)+"</b>",n=r}),o+=t.substring(n,e.end)}var a=!1,i=0,c=0,l=n.title.trim(),h=l.toLowerCase(),p=n.content.trim().replace(/<[^>]+>/g,""),u=p.toLowerCase(),f=decodeURIComponent(n.url),d=[],g=[];if(""!=l&&(e.forEach(function(t){function e(t,e,o){var n=t.length;if(0===n)return[];var r=0,s=[],a=[];for(o||(e=e.toLowerCase(),t=t.toLowerCase());(s=e.indexOf(t,r))>-1;)a.push({position:s,word:t}),r=s+n;return a}d=d.concat(e(t,h,!1)),g=g.concat(e(t,u,!1))}),(d.length>0||g.length>0)&&(a=!0,i=d.length+g.length)),a){[d,g].forEach(function(t){t.sort(function(t,e){return e.position!==t.position?e.position-t.position:t.word.length-e.word.length})});var v=[];0!=d.length&&v.push(r(l,0,l.length,d));for(var $=[];0!=g.length;){var C=g[g.length-1],m=C.position,x=C.word,w=m-20,y=m+80;0>w&&(w=0),y<m+x.length&&(y=m+x.length),y>p.length&&(y=p.length),$.push(r(p,w,y,g))}$.sort(function(t,e){return t.searchTextCount!==e.searchTextCount?e.searchTextCount-t.searchTextCount:t.hits.length!==e.hits.length?e.hits.length-t.hits.length:t.start-e.start});var T=parseInt("1");T>=0&&($=$.slice(0,T));var b="";b+=0!=v.length?"<li><a href='"+f+"' class='search-result-title'>"+s(l,v[0])+"</a>":"<li><a href='"+f+"' class='search-result-title'>"+l+"</a>",$.forEach(function(t){b+="<a href='"+f+'\'><p class="search-result">'+s(p,t)+"...</p></a>"}),b+="</li>",o.push({item:b,searchTextCount:c,hitCount:i,id:o.length})}}),1===e.length&&""===e[0])s.innerHTML='<div id="no-result"><i class="fa fa-search fa-5x" /></div>';else if(0===o.length)s.innerHTML='<div id="no-result"><i class="fa fa-frown-o fa-5x" /></div>';else{o.sort(function(t,e){return t.searchTextCount!==e.searchTextCount?e.searchTextCount-t.searchTextCount:t.hitCount!==e.hitCount?e.hitCount-t.hitCount:e.id-t.id});var a='<ul class="search-result-list">';o.forEach(function(t){a+=t.item}),a+="</ul>",s.innerHTML=a}};r.addEventListener("input",a),$(".local-search-pop-overlay").remove(),$("body").css("overflow",""),proceedsearch()}})};$(".popup-trigger").click(function(t){t.stopPropagation(),isfetched===!1?searchFunc(path,"local-search-input","local-search-result"):proceedsearch()}),$(".popup-btn-close").click(onPopupClose),$(".popup").click(function(t){t.stopPropagation()}),$(document).on("keyup",function(t){var e=27===t.which&&$(".search-popup").is(":visible");e&&onPopupClose()})</script><script src="https://cdn1.lncld.net/static/js/av-core-mini-0.6.4.js"></script><script>AV.initialize("SVKPF4oiwJLMtpySQxvJLnLm-gzGzoHsz","46OfvvEe65XMeUi79STU895I")</script><script>function showTime(e){var t=new AV.Query(e),n=[],o=$(".leancloud_visitors");o.each(function(){n.push($(this).attr("id").trim())}),t.containedIn("url",n),t.find().done(function(e){var t=".leancloud-visitors-count";if(0===e.length)return void o.find(t).text(0);for(var i=0;i<e.length;i++){var r=e[i],s=r.get("url"),l=r.get("time"),c=document.getElementById(s);$(c).find(t).text(l)}for(var i=0;i<n.length;i++){var s=n[i],c=document.getElementById(s),u=$(c).find(t);""==u.text()&&u.text(0)}}).fail(function(e,t){console.log("Error: "+t.code+" "+t.message)})}function addCount(e){var t=$(".leancloud_visitors"),n=t.attr("id").trim(),o=t.attr("data-flag-title").trim(),i=new AV.Query(e);i.equalTo("url",n),i.find({success:function(t){if(t.length>0){var i=t[0];i.fetchWhenSave(!0),i.increment("time"),i.save(null,{success:function(e){var t=$(document.getElementById(n));t.find(".leancloud-visitors-count").text(e.get("time"))},error:function(e,t){console.log("Failed to save Visitor num, with error message: "+t.message)}})}else{var r=new e,s=new AV.ACL;s.setPublicReadAccess(!0),s.setPublicWriteAccess(!0),r.setACL(s),r.set("title",o),r.set("url",n),r.set("time",1),r.save(null,{success:function(e){var t=$(document.getElementById(n));t.find(".leancloud-visitors-count").text(e.get("time"))},error:function(e,t){console.log("Failed to create")}})}},error:function(e){console.log("Error:"+e.code+" "+e.message)}})}$(function(){var e=AV.Object.extend("Counter");1==$(".leancloud_visitors").length?addCount(e):$(".post-title-link").length>1&&showTime(e)})</script><script>!function(){var t=document.createElement("script"),s=window.location.protocol.split(":")[0];"https"===s?t.src="https://zz.bdstatic.com/linksubmit/push.js":t.src="http://push.zhanzhang.baidu.com/push.js";var e=document.getElementsByTagName("script")[0];e.parentNode.insertBefore(t,e)}()</script><script type="text/javascript">wpac_init=window.wpac_init||[],wpac_init.push({widget:"Rating",id:21228,el:"wpac-rating",color:"fc6423"}),function(){if(!("WIDGETPACK_LOADED"in window)){WIDGETPACK_LOADED=!0;var e=document.createElement("script");e.type="text/javascript",e.async=!0,e.src="//embed.widgetpack.com/widget.js";var t=document.getElementsByTagName("script")[0];t.parentNode.insertBefore(e,t.nextSibling)}}()</script><script type="text/x-mathjax-config">MathJax.Hub.Config({
        tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
          processEscapes: true,
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
        }
      });</script><script type="text/x-mathjax-config">MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for (i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
        }
      });</script><script type="text/javascript" src="//cdn.bootcss.com/mathjax/2.7.1/latest.js?config=TeX-AMS-MML_HTMLorMML"></script><canvas class="fireworks" style="position:fixed;left:0;top:0;z-index:1;pointer-events:none"></canvas><script type="text/javascript" src="//cdn.bootcss.com/animejs/2.2.0/anime.min.js"></script><script type="text/javascript" src="/js/src/fireworks.js"></script><script src="/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({pluginRootPath:"live2dw/",pluginJsPath:"lib/",pluginModelPath:"assets/",tagMode:!0,log:!1,model:{jsonPath:"/live2dw/assets/hijiki.model.json"},display:{position:"left",width:250,height:500},mobile:{show:!1},react:{opacity:.7}})</script></body></html><script type="text/javascript" src="/js/src/crash_cheat.js"></script><!-- rebuild by neat -->