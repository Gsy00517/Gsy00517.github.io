<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[hexo笔记：ssh与https以及双线部署的一些注意点]]></title>
    <url>%2F2019%2F10%2F06%2Fhexo%E7%AC%94%E8%AE%B0%EF%BC%9Assh%E4%B8%8Ehttp%E4%BB%A5%E5%8F%8A%E5%8F%8C%E7%BA%BF%E9%83%A8%E7%BD%B2%E7%9A%84%E4%B8%80%E4%BA%9B%E6%B3%A8%E6%84%8F%E7%82%B9%2F</url>
    <content type="text"><![CDATA[考虑到每次打开博客的加载速度问题，我前几天尝试了把博客部署到coding上，实现了coding+github双线部署。coding现已经被腾讯云收购，可以直接用微信登录。事实上，我双线部署之后对两者进行比较，并没有发现coding更快，搜索之后发现似乎是coding的服务器也不在内地而在香港的原因。这里附上我的两个链接，可以看看效果，择优访问：github page：https://gsy00517.github.io/coding page：https://gsy00517.coding.me/关于双线部署如何具体操作，网上有许多较为详尽的教程可以参考，如果有问题的话可以参考多篇不同的教程找出原因解决，这篇文章主要讲讲我这期间遇到的一些小事项。ssh与https在网上的一个教程中，作者提到使用ssh比https更加稳定，尝试后暂时没有发现明显的区别，但是另一个直观的改变就是在push代码时，使用ssh url就不需要输入账号和密码。下面是我在hexo配置文件中的设置，也就是位于站点根目录下的_config.yml文件，其中后面注释中的https://github.com/Gsy00517/Gsy00517.github.io.git是原本的https url。上面对应的ssh url一般可以从平台上直接复制获取，也可以参照我的格式进行设置。这里简要说一说ssh与https的区别。一般默认情况下使用的是https，除了需要在fetch和push时使用密码之外，使用https的配置比较方便。然而，使用ssh url却需要先配置和添加好ssh key，并且你必须是这个项目的拥有或者管理者，而https就没有这些要求。其实，配置ssh key也并没有那么繁琐，而且这是一劳永逸的，所以推荐还是使用ssh。要注意的是，ssh key保存的默认位置或许会不同于网上的教程，不过可以自行更改。我的默认地址是在用户文件夹下的AppData\Roaming\SPB_16.6的ssh文件夹中。AppData文件夹默认是隐藏的，可以通过查看隐藏的项目打开。此外，如果需要经常清理temp文件的话，不妨取消这个文件夹的隐藏，这在释放windows空间中还是挺有效的，可以参见windows笔记：释放空间。key所在的文件是上图所示的第二个publisher文件，然而似乎无法直接用office打开，选择打开方式为记事本即可。当然，如果实在找不到key所在的文件，也可以直接使用文件资源管理器的搜索功能查找名为.ssh的文件夹即可。注：http与https的区别在于，http是明文传输的，而https是使用ssl加密的，更加安全。若要将连接提交百度站点验证，就需要使用https协议，这个在github和coding都有强制https访问的选项。双线部署注意事项LeanCloud这里主要针对hexo博客双线部署后可能会出现的几个问题说明一下注意点。首先，如果之前使用的是LeanCloud来接受记录评论和统计阅读量的，那么为了共享数据，必须在LeanCloud控制台设置的安全中心中，添加新增的web安全域名，保存后即可解决问题。Widget如果使用的是基于Widget的评分系统，那么必须更改Widget设置中的domain。我是免费使用Widget，只能同时添加一个domain。我继续使用github page的域名，因此只能在我的github page中看到评分系统。文内链接因为双线部署用的依旧还是同一份本地源码文件，因此在博文中提供的链接依旧是一致的。这里我也将继续使用github page的链接，也就是文内推荐的我本人的博文链接依旧还是指向github page的。事实上，这并无任何影响。]]></content>
      <categories>
        <category>操作与使用</category>
      </categories>
      <tags>
        <tag>hexo</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[deep learning笔记：使网络能够更深——ResNet简介与pytorch实现]]></title>
    <url>%2F2019%2F10%2F01%2Fdeep-learning%E7%AC%94%E8%AE%B0%EF%BC%9A%E4%BD%BF%E7%BD%91%E7%BB%9C%E8%83%BD%E5%A4%9F%E6%9B%B4%E6%B7%B1%E2%80%94%E2%80%94ResNet%E7%AE%80%E4%BB%8B%E4%B8%8Epytorch%E5%AE%9E%E7%8E%B0%2F</url>
    <content type="text"><![CDATA[之前我用pytorch把ResNet18实现了一下，但由于上周准备国家奖学金答辩没有时间来写我实现的过程与总结。今天是祖国70周年华诞，借着这股喜庆劲，把这篇文章补上。本文参考自：https://blog.csdn.net/weixin_43624538/article/details/85049699https://blog.csdn.net/u013289254/article/details/98785869简介ResNet残差网络是由何凯明等四名微软研究院的华人提出的，当初看到论文标题下面的中国名字还是挺高兴的。文章引入部分，作者就探讨了深度神经网络的优化是否就只是叠加层数、增加深度那么简单。显然这是不可能的，增加深度带来的首要问题就是梯度爆炸、消散的问题，这是由于随着层数的增多，在网络中反向传播的梯度会随着连乘变得不稳定，从而变得特别大或者特别小。其中以梯度消散更为常见。值得注意的是，论文中还提到深度更深的网络反而出现准确率下降并不是由过拟合所引起的。为了解决这个问题，研究者们做出了很多思考与尝试，其中的代表有relu激活函数的使用，Batch Normalization的使用等。关于这两种方法，可以参考网上的资料以及我的博文deep-learning笔记：开启深度学习热潮——AlexNet和deep-learning笔记：学习率衰减与批归一化。对于上面这个问题，ResNet作出的贡献是引入skip/identity connection。如下所示就是两个基本的残差模块。上面这个block可表示为：$ F(X)=H(X)-x $。在这里，X为浅层输出，H(x)为深层的输出。当浅层的X代表的特征已经足够成熟，即当任何对于特征X的改变都会让loss变大时，F(X)会自动趋向于学习成为0，X则从恒等映射的路径继续传递。这样，我们就可以在不增加计算成本的情况下使得在前向传递过程中，如果浅层的输出已经足够成熟（optimal），那么就让深层网络后面的层仅实现恒等映射的作用。当X与F（X）通道数目不同时，作者尝试了两种identity mapping的方式。一种即对X缺失的通道直接补零从而使其能够对齐，这种方式比较简单直接，无需额外的参数；另一种则是通过使用1x1的conv来映射从而使通道也能达成一致。论文老规矩，这里还是先呈上我用黄色荧光高亮出我认为比较重要的要点的论文原文，这里我只有英文版。如果需要没有被我标注过的原文，可以直接搜索，这里我仅提供一次，可以点击这里下载。不过，虽然没有pdf中文版，但其实深度学习CV方向一些比较经典的论文的英文、中文、中英对照都可以到Deep Learning Papers Translation上看到，非常方便。自己实现在论文中，作者提到了如下几个ResNet的版本的结构。这里我实现的是ResNet18。由于这不是我第一次使用pytorch进行实现，一些基本的使用操作我就不加注释了，想看注释来理解的话可以参考我之前VGG的实现。由于残差的引入，导致ResNet的结构比较复杂，而论文中并没有非常详细的阐述，在研究官方源码之后，我对它的结构才有了完整的了解，这里我画出来以便参考。ResNet18的每一layer包括了两个这样的basic block，其中1x1的卷积核仅在X与F（X）通道数目不一致时进行操作，在我的代码中，我定义shortcut函数来对应一切通道一致、无需处理的情况。123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103import torchimport torch.nn as nnimport torch.nn.functional as Fclass ResNet(nn.Module): def __init__(self): super(ResNet, self).__init__() self.conv1 = nn.Conv2d(in_channels = 3, out_channels = 64, kernel_size = 7, stride = 2, padding = 3, bias=False) self.max = nn.MaxPool2d(kernel_size = 3, stride = 2, padding = 1) self.bn1 = nn.BatchNorm2d(64) self.bn2 = nn.BatchNorm2d(64) self.bn3 = nn.BatchNorm2d(128) self.bn4 = nn.BatchNorm2d(256) self.bn5 = nn.BatchNorm2d(512) self.shortcut = nn.Sequential() self.shortcut3 = nn.Sequential(nn.Conv2d(64, 128, kernel_size = 1, stride = 2, bias = False), nn.BatchNorm2d(128)) self.shortcut4 = nn.Sequential(nn.Conv2d(128, 256, kernel_size = 1, stride = 2, bias = False), nn.BatchNorm2d(256)) self.shortcut5 = nn.Sequential(nn.Conv2d(256, 512, kernel_size = 1, stride = 2, bias = False), nn.BatchNorm2d(512)) self.conv2 = nn.Conv2d(in_channels = 64, out_channels = 64, kernel_size = 3, stride = 1, padding = 1, bias = False) self.conv3_1 = nn.Conv2d(in_channels = 64, out_channels = 128, kernel_size = 3, stride = 2, padding = 1, bias = False) self.conv3_2 = nn.Conv2d(in_channels = 128, out_channels = 128, kernel_size = 3, stride = 1, padding = 1, bias = False) self.conv4_1 = nn.Conv2d(in_channels = 128, out_channels = 256, kernel_size = 3, stride = 2, padding = 1, bias = False) self.conv4_2 = nn.Conv2d(in_channels = 256, out_channels = 256, kernel_size = 3, stride = 1, padding = 1, bias = False) self.conv5_1 = nn.Conv2d(in_channels = 256, out_channels = 512, kernel_size = 3, stride = 2, padding = 1, bias = False) self.conv5_2 = nn.Conv2d(in_channels = 512, out_channels = 512, kernel_size = 3, stride = 1, padding = 1, bias = False) self.avg = nn.AdaptiveAvgPool2d((1, 1)) self.fc = nn.Linear(512, 1000) def forward(self, x): x = F.relu(self.bn1(self.conv1(x))) x1 = self.max(x) #layer1 x = F.relu(self.bn2(self.conv2(x1))) x = self.bn2(self.conv2(x)) x += self.shortcut(x1) x2 = F.relu(x) x = F.relu(self.bn2(self.conv2(x2))) x = self.bn2(self.conv2(x)) x += self.shortcut(x2) x3 = F.relu(x) #layer2 x = F.relu(self.bn3(self.conv3_1(x3))) x = self.bn3(self.conv3_2(x)) x += self.shortcut3(x3) x4 = F.relu(x) x = F.relu(self.bn3(self.conv3_2(x4))) x = self.bn3(self.conv3_2(x)) x += self.shortcut(x4) x5 = F.relu(x) #layer3 x = F.relu(self.bn4(self.conv4_1(x5))) x = self.bn4(self.conv4_2(x)) x += self.shortcut4(x5) x6 = F.relu(x) x = F.relu(self.bn4(self.conv4_2(x6))) x = self.bn4(self.conv4_2(x)) x += self.shortcut(x6) x7 = F.relu(x) #layer4 x = F.relu(self.bn5(self.conv5_1(x7))) x = self.bn5(self.conv5_2(x)) x += self.shortcut5(x7) x8 = F.relu(x) x = F.relu(self.bn5(self.conv5_2(x8))) x = self.bn5(self.conv5_2(x)) x += self.shortcut(x8) x = F.relu(x) #ending x = self.avg(x) x = x.view(-1, self.num_flat_features(x)) x = self.fc(x) x = F.softmax(x, dim = 1) return x def num_flat_features(self, x): size = x.size()[1:] num_features = 1 for s in size: num_features *= s return num_features net = ResNet()同样的，我们可以随机生成一个张量来进行验证：123input = torch.randn(1, 3, 48, 48)out = net(input)print(out)如果可以顺利地输出，那么模型基本上是没有问题的。出现的问题在这里我还是想把自己踩的一些简单的坑记下来，引以为戒。softmax输出全为1当我使用F.softmax之后，出现了这样的一个问题：查找资料后发现，我错误的把对每一行softmax当作了对每一列softmax。因为这个softmax语句是我从之前的自己做的一道kaggle题目写的代码中ctrl+C+V过来的，复制过来的是x = F.softmax(x, dim = 0)，在这里，dim = 0意味着我对张量的每一列进行softmax，这是因为我之前的场景中需要处理的张量是一维的，也就是tensor（）里面只有一对“[]”，此时它默认只有一列，我对列进行softmax自然就没有问题。而放到这里，我再对列进行softmax时，每列上就只有一个元素。那么结果就都是1即100%了。解决的方法就是把dim设为1。下面我在用一组代码直观地展示一下softmax的用法与区别。1234567import torchimport torch.nn.functional as Fx1= torch.Tensor( [ [1, 2, 3, 4], [1, 3, 4, 5], [3, 4, 5, 6]])y11= F.softmax(x1, dim = 0) #对每一列进行softmaxy12 = F.softmax(x1, dim = 1) #对每一行进行softmaxx2 = torch.Tensor([1, 2, 3, 4])y2 = F.softmax(x2, dim = 0)我们输出每个结果，可以看到：bias或许你可以发现，在我的代码中，每个卷积层中都设置了bias = False，这是我在参考官方源码之后补上的。那么，这个bias是什么，又有什么用呢？我们在学深度学习的时候，最早接触到的神经网络应该是感知器，它的结构如下图所示。 要想激活这个感知器，就必须使x1 * w1 + x2 * w2 + ... + xn * wn &gt; T（T为一个阈值），而T越大，想激活这个感知器的难度越大。考虑样本较多的情况，我不可能手动选择一个阈值，使得模型整体表现最佳，因此我们不如使得T变成可学习的，这样一来，T会自动学习到一个数，使得模型的整体表现最佳。当把T移动到左边，它就成了bias偏置，x1 * w1 + x2 * w2 + ... + xn * wn - T &gt; 0。显然，偏置的大小控制着激活这个感知器的难易程度。在比感知器高级的神经网络中，也是如此。但倘若我们要在卷积后面加上归一化操作，那么bias的作用就无法体现了。我们以ResNet卷积层后的BN层为例。可参考我的上一篇博文，BN处理过程中有这样一步： 对于分子而言，无论有没有bias，对结果都没有影响；而对于下面分母而言，因为是方差操作，所以也没有影响。因此，在ResNet中，因为每次卷积之后都要进行BN操作，那就不需要启用bias，否则非但不起作用，还会消耗一定的显卡内存。官方源码如果你此时对ResNet的结构已经有了比较清晰的理解，那么可以尝试着来理解一下官方源码的思路。其实我觉得先看像我这样直观的代码实现再看官方源码更有助理解且更高效。123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256257258259260261262263264265266267268269270271272273274275276277278279280281282283284285286287288289290291292293294295296297298299300301302303304305306307308309310311312313314315316317318319320321322323324325326327328329330331332333334335336337338339340341342343344import torchimport torch.nn as nnfrom .utils import load_state_dict_from_url__all__ = ['ResNet', 'resnet18', 'resnet34', 'resnet50', 'resnet101', 'resnet152', 'resnext50_32x4d', 'resnext101_32x8d', 'wide_resnet50_2', 'wide_resnet101_2']model_urls = &#123; 'resnet18': 'https://download.pytorch.org/models/resnet18-5c106cde.pth', 'resnet34': 'https://download.pytorch.org/models/resnet34-333f7ec4.pth', 'resnet50': 'https://download.pytorch.org/models/resnet50-19c8e357.pth', 'resnet101': 'https://download.pytorch.org/models/resnet101-5d3b4d8f.pth', 'resnet152': 'https://download.pytorch.org/models/resnet152-b121ed2d.pth', 'resnext50_32x4d': 'https://download.pytorch.org/models/resnext50_32x4d-7cdf4587.pth', 'resnext101_32x8d': 'https://download.pytorch.org/models/resnext101_32x8d-8ba56ff5.pth', 'wide_resnet50_2': 'https://download.pytorch.org/models/wide_resnet50_2-95faca4d.pth', 'wide_resnet101_2': 'https://download.pytorch.org/models/wide_resnet101_2-32ee1156.pth',&#125;def conv3x3(in_planes, out_planes, stride=1, groups=1, dilation=1): """3x3 convolution with padding""" return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride, padding=dilation, groups=groups, bias=False, dilation=dilation)def conv1x1(in_planes, out_planes, stride=1): """1x1 convolution""" return nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=stride, bias=False)class BasicBlock(nn.Module): expansion = 1 __constants__ = ['downsample'] def __init__(self, inplanes, planes, stride=1, downsample=None, groups=1, base_width=64, dilation=1, norm_layer=None): super(BasicBlock, self).__init__() if norm_layer is None: norm_layer = nn.BatchNorm2d if groups != 1 or base_width != 64: raise ValueError('BasicBlock only supports groups=1 and base_width=64') if dilation &gt; 1: raise NotImplementedError("Dilation &gt; 1 not supported in BasicBlock") # Both self.conv1 and self.downsample layers downsample the input when stride != 1 self.conv1 = conv3x3(inplanes, planes, stride) self.bn1 = norm_layer(planes) self.relu = nn.ReLU(inplace=True) self.conv2 = conv3x3(planes, planes) self.bn2 = norm_layer(planes) self.downsample = downsample self.stride = stride def forward(self, x): identity = x out = self.conv1(x) out = self.bn1(out) out = self.relu(out) out = self.conv2(out) out = self.bn2(out) if self.downsample is not None: identity = self.downsample(x) out += identity out = self.relu(out) return outclass Bottleneck(nn.Module): expansion = 4 def __init__(self, inplanes, planes, stride=1, downsample=None, groups=1, base_width=64, dilation=1, norm_layer=None): super(Bottleneck, self).__init__() if norm_layer is None: norm_layer = nn.BatchNorm2d width = int(planes * (base_width / 64.)) * groups # Both self.conv2 and self.downsample layers downsample the input when stride != 1 self.conv1 = conv1x1(inplanes, width) self.bn1 = norm_layer(width) self.conv2 = conv3x3(width, width, stride, groups, dilation) self.bn2 = norm_layer(width) self.conv3 = conv1x1(width, planes * self.expansion) self.bn3 = norm_layer(planes * self.expansion) self.relu = nn.ReLU(inplace=True) self.downsample = downsample self.stride = stride def forward(self, x): identity = x out = self.conv1(x) out = self.bn1(out) out = self.relu(out) out = self.conv2(out) out = self.bn2(out) out = self.relu(out) out = self.conv3(out) out = self.bn3(out) if self.downsample is not None: identity = self.downsample(x) out += identity out = self.relu(out) return outclass ResNet(nn.Module): def __init__(self, block, layers, num_classes=1000, zero_init_residual=False, groups=1, width_per_group=64, replace_stride_with_dilation=None, norm_layer=None): super(ResNet, self).__init__() if norm_layer is None: norm_layer = nn.BatchNorm2d self._norm_layer = norm_layer self.inplanes = 64 self.dilation = 1 if replace_stride_with_dilation is None: # each element in the tuple indicates if we should replace # the 2x2 stride with a dilated convolution instead replace_stride_with_dilation = [False, False, False] if len(replace_stride_with_dilation) != 3: raise ValueError("replace_stride_with_dilation should be None " "or a 3-element tuple, got &#123;&#125;".format(replace_stride_with_dilation)) self.groups = groups self.base_width = width_per_group self.conv1 = nn.Conv2d(3, self.inplanes, kernel_size=7, stride=2, padding=3, bias=False) self.bn1 = norm_layer(self.inplanes) self.relu = nn.ReLU(inplace=True) self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1) self.layer1 = self._make_layer(block, 64, layers[0]) self.layer2 = self._make_layer(block, 128, layers[1], stride=2, dilate=replace_stride_with_dilation[0]) self.layer3 = self._make_layer(block, 256, layers[2], stride=2, dilate=replace_stride_with_dilation[1]) self.layer4 = self._make_layer(block, 512, layers[3], stride=2, dilate=replace_stride_with_dilation[2]) self.avgpool = nn.AdaptiveAvgPool2d((1, 1)) self.fc = nn.Linear(512 * block.expansion, num_classes) for m in self.modules(): if isinstance(m, nn.Conv2d): nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu') elif isinstance(m, (nn.BatchNorm2d, nn.GroupNorm)): nn.init.constant_(m.weight, 1) nn.init.constant_(m.bias, 0) # Zero-initialize the last BN in each residual branch, # so that the residual branch starts with zeros, and each residual block behaves like an identity. # This improves the model by 0.2~0.3% according to https://arxiv.org/abs/1706.02677 if zero_init_residual: for m in self.modules(): if isinstance(m, Bottleneck): nn.init.constant_(m.bn3.weight, 0) elif isinstance(m, BasicBlock): nn.init.constant_(m.bn2.weight, 0) def _make_layer(self, block, planes, blocks, stride=1, dilate=False): norm_layer = self._norm_layer downsample = None previous_dilation = self.dilation if dilate: self.dilation *= stride stride = 1 if stride != 1 or self.inplanes != planes * block.expansion: downsample = nn.Sequential( conv1x1(self.inplanes, planes * block.expansion, stride), norm_layer(planes * block.expansion), ) layers = [] layers.append(block(self.inplanes, planes, stride, downsample, self.groups, self.base_width, previous_dilation, norm_layer)) self.inplanes = planes * block.expansion for _ in range(1, blocks): layers.append(block(self.inplanes, planes, groups=self.groups, base_width=self.base_width, dilation=self.dilation, norm_layer=norm_layer)) return nn.Sequential(*layers) def forward(self, x): x = self.conv1(x) x = self.bn1(x) x = self.relu(x) x = self.maxpool(x) x = self.layer1(x) x = self.layer2(x) x = self.layer3(x) x = self.layer4(x) x = self.avgpool(x) x = torch.flatten(x, 1) x = self.fc(x) return xdef _resnet(arch, block, layers, pretrained, progress, **kwargs): model = ResNet(block, layers, **kwargs) if pretrained: state_dict = load_state_dict_from_url(model_urls[arch], progress=progress) model.load_state_dict(state_dict) return modeldef resnet18(pretrained=False, progress=True, **kwargs): r"""ResNet-18 model from `"Deep Residual Learning for Image Recognition" &lt;https://arxiv.org/pdf/1512.03385.pdf&gt;`_ Args: pretrained (bool): If True, returns a model pre-trained on ImageNet progress (bool): If True, displays a progress bar of the download to stderr """ return _resnet('resnet18', BasicBlock, [2, 2, 2, 2], pretrained, progress, **kwargs)def resnet34(pretrained=False, progress=True, **kwargs): r"""ResNet-34 model from `"Deep Residual Learning for Image Recognition" &lt;https://arxiv.org/pdf/1512.03385.pdf&gt;`_ Args: pretrained (bool): If True, returns a model pre-trained on ImageNet progress (bool): If True, displays a progress bar of the download to stderr """ return _resnet('resnet34', BasicBlock, [3, 4, 6, 3], pretrained, progress, **kwargs)def resnet50(pretrained=False, progress=True, **kwargs): r"""ResNet-50 model from `"Deep Residual Learning for Image Recognition" &lt;https://arxiv.org/pdf/1512.03385.pdf&gt;`_ Args: pretrained (bool): If True, returns a model pre-trained on ImageNet progress (bool): If True, displays a progress bar of the download to stderr """ return _resnet('resnet50', Bottleneck, [3, 4, 6, 3], pretrained, progress, **kwargs)def resnet101(pretrained=False, progress=True, **kwargs): r"""ResNet-101 model from `"Deep Residual Learning for Image Recognition" &lt;https://arxiv.org/pdf/1512.03385.pdf&gt;`_ Args: pretrained (bool): If True, returns a model pre-trained on ImageNet progress (bool): If True, displays a progress bar of the download to stderr """ return _resnet('resnet101', Bottleneck, [3, 4, 23, 3], pretrained, progress, **kwargs)def resnet152(pretrained=False, progress=True, **kwargs): r"""ResNet-152 model from `"Deep Residual Learning for Image Recognition" &lt;https://arxiv.org/pdf/1512.03385.pdf&gt;`_ Args: pretrained (bool): If True, returns a model pre-trained on ImageNet progress (bool): If True, displays a progress bar of the download to stderr """ return _resnet('resnet152', Bottleneck, [3, 8, 36, 3], pretrained, progress, **kwargs)def resnext50_32x4d(pretrained=False, progress=True, **kwargs): r"""ResNeXt-50 32x4d model from `"Aggregated Residual Transformation for Deep Neural Networks" &lt;https://arxiv.org/pdf/1611.05431.pdf&gt;`_ Args: pretrained (bool): If True, returns a model pre-trained on ImageNet progress (bool): If True, displays a progress bar of the download to stderr """ kwargs['groups'] = 32 kwargs['width_per_group'] = 4 return _resnet('resnext50_32x4d', Bottleneck, [3, 4, 6, 3], pretrained, progress, **kwargs)def resnext101_32x8d(pretrained=False, progress=True, **kwargs): r"""ResNeXt-101 32x8d model from `"Aggregated Residual Transformation for Deep Neural Networks" &lt;https://arxiv.org/pdf/1611.05431.pdf&gt;`_ Args: pretrained (bool): If True, returns a model pre-trained on ImageNet progress (bool): If True, displays a progress bar of the download to stderr """ kwargs['groups'] = 32 kwargs['width_per_group'] = 8 return _resnet('resnext101_32x8d', Bottleneck, [3, 4, 23, 3], pretrained, progress, **kwargs)def wide_resnet50_2(pretrained=False, progress=True, **kwargs): r"""Wide ResNet-50-2 model from `"Wide Residual Networks" &lt;https://arxiv.org/pdf/1605.07146.pdf&gt;`_ The model is the same as ResNet except for the bottleneck number of channels which is twice larger in every block. The number of channels in outer 1x1 convolutions is the same, e.g. last block in ResNet-50 has 2048-512-2048 channels, and in Wide ResNet-50-2 has 2048-1024-2048. Args: pretrained (bool): If True, returns a model pre-trained on ImageNet progress (bool): If True, displays a progress bar of the download to stderr """ kwargs['width_per_group'] = 64 * 2 return _resnet('wide_resnet50_2', Bottleneck, [3, 4, 6, 3], pretrained, progress, **kwargs)def wide_resnet101_2(pretrained=False, progress=True, **kwargs): r"""Wide ResNet-101-2 model from `"Wide Residual Networks" &lt;https://arxiv.org/pdf/1605.07146.pdf&gt;`_ The model is the same as ResNet except for the bottleneck number of channels which is twice larger in every block. The number of channels in outer 1x1 convolutions is the same, e.g. last block in ResNet-50 has 2048-512-2048 channels, and in Wide ResNet-50-2 has 2048-1024-2048. Args: pretrained (bool): If True, returns a model pre-trained on ImageNet progress (bool): If True, displays a progress bar of the download to stderr """ kwargs['width_per_group'] = 64 * 2 return _resnet('wide_resnet101_2', Bottleneck, [3, 4, 23, 3], pretrained, progress, **kwargs)pth文件在阅读官方源码时，我们会注意到官方提供了一系列版本的model_urls，其中，每一个url都是以.pth结尾的。当我下载了对应的文件之后，并不知道如何处理，于是我通过搜索，简单的了解了pth文件的概念与使用方法。简单来说，pth文件就是一个表示Python的模块搜索路径（module search path）的文本文件，在xxx.pth文件里面，会书写一些路径，一行一个。如果我们将xxx.pth文件放在特定位置，则可以让python在加载模块时，读取xxx.pth中指定的路径。下面我使用pytorch对pth文件进行加载操作。首先，我把ResNet18对应的pth文件下载到桌面。1234567891011import torchimport torchvision.models as models# pretrained = True就可以使用预训练的模型net = models.resnet18(pretrained = False)#注意，根据model的不同，这里models.xxx的内容也是不同的，比如models.squeezenet1_1pthfile = r'C:\Users\sheny\Desktop\resnet18-5c106cde.pth'#pth文件所在路径net.load_state_dict(torch.load(pthfile))print(net)输出结果如下：123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384ResNet( (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False) (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False) (layer1): Sequential( (0): BasicBlock( (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) (1): BasicBlock( (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) ) (layer2): Sequential( (0): BasicBlock( (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (downsample): Sequential( (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False) (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) ) (1): BasicBlock( (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) ) (layer3): Sequential( (0): BasicBlock( (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (downsample): Sequential( (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False) (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) ) (1): BasicBlock( (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) ) (layer4): Sequential( (0): BasicBlock( (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (downsample): Sequential( (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False) (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) ) (1): BasicBlock( (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) ) (avgpool): AdaptiveAvgPool2d(output_size=(1, 1)) (fc): Linear(in_features=512, out_features=1000, bias=True))这样你就可以看到很详尽的参数设置了。我们还可以加载所有的参数。1234567import torchpthfile = r'C:\Users\sheny\Desktop\resnet18-5c106cde.pth'net = torch.load(pthfile)print(net)输出如下：12345678OrderedDict([(&apos;conv1.weight&apos;, Parameter containing:tensor([[[[-1.0419e-02, -6.1356e-03, -1.8098e-03, ..., 5.6615e-02, 1.7083e-02, -1.2694e-02], [ 1.1083e-02, 9.5276e-03, -1.0993e-01, ..., -2.7124e-01, -1.2907e-01, 3.7424e-03], [-6.9434e-03, 5.9089e-02, 2.9548e-01, ..., 5.1972e-01, 2.5632e-01, 6.3573e-02], ...,]]></content>
      <categories>
        <category>人工智能</category>
      </categories>
      <tags>
        <tag>深度学习</tag>
        <tag>论文分享</tag>
        <tag>代码实现</tag>
        <tag>pytorch</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[deep learning笔记：学习率衰减与批归一化]]></title>
    <url>%2F2019%2F10%2F01%2Fdeep-learning%E7%AC%94%E8%AE%B0%EF%BC%9A%E5%AD%A6%E4%B9%A0%E7%8E%87%E8%A1%B0%E5%87%8F%E4%B8%8E%E6%89%B9%E5%BD%92%E4%B8%80%E5%8C%96%2F</url>
    <content type="text"><![CDATA[一段时间之前，在一个深度学习交流群里看到一个群友发问：为什么他的训练误差最后疯狂上下抖动而不是一直降低。作为一个入门小白，我当时也很疑惑。但后来我结合所学，仔细思考之后，发现这是一个挺容易犯的错误。本文参考自：https://blog.csdn.net/bestrivern/article/details/86301619https://www.jianshu.com/p/9643cba47655https://www.cnblogs.com/eilearn/p/9780696.htmlhttps://blog.csdn.net/donkey_1993/article/details/81871132问题事实上，这是一个在机器学习中就有可能遇到的问题，当学习速率α设置得过大时，往往在模型训练的后期难以达到最优解，而是在最优解附近来回抖动。还有可能反而使损失函数越来越大，甚至达到无穷，如下图所示。而在深度学习中，假设我们使用mini-batch梯度下降法，由于mini-batch的数量不大，大概64或者128个样本，在迭代过程中会有噪声。这个时候使用固定的学习率导致的结果就是虽然下降朝向最小值，但不会精确地收敛，只会在附近不断地波动（蓝色线）。但如果慢慢减少学习率，在初期，学习还是相对较快地，但随着学习率的变小，步伐也会变慢变小，所以最后当开始收敛时，你的曲线（绿色线）会在最小值附近的一个较小区域之内摆动，而不是在训练过程中，大幅度地在最小值附近摆动。对于这个问题，我目前收集了有下面这些解决办法。直接修改学习率在吴恩达的机器学习课程中，他介绍了一种人为选择学习率的规则：每三倍选择一个学习率。比如：我们首先选择了0.1为学习率，那么当这个学习率过大时，我们修改成0.3。倘若还是偏大，我们继续改为0.01、0.003、0.001…以此类推，当学习率偏小是也是以三倍增加并尝试检验，最终选出比较合适的学习率。但这种方法只适用于模型数量小的情况，且这种方法终究还是固定的学习率，依旧无法很好地权衡从而达到前期快速下降与后期稳定收敛的目的。学习率动态衰减学习率衰减的本质在于，在学习初期，你能承受并且需要较大的步伐，但当开始收敛的时候，小一些的学习率能让你步伐小一些，从而更稳定地达到精确的最优解。为此，我们另外增添衰减率超参数，构建函数使学习率能够在训练的过程中动态衰减。\alpha = \frac{1}{1+decayrate*epochnum}*\alpha _{0}\其中decay rate称为衰减率，epoch num是代数，$ \alpha _{0} $是初始学习率。此外还有下面这些构造方法：指数衰减：$ \alpha =0.95^{epochnum}*\alpha _{0} $其他常用方法：\alpha =\frac{k}{\sqrt{epochnum}}*\alpha _{0}\\alpha =\frac{k}{\sqrt{t}}\alpha _{0}\其中k为mini-batch的数字。几种衰减方法的实现在pytorch中，学习率调整主要有两种方式：1.直接修改optimizer中的lr参数。2.利用lr_scheduler()提供的几种衰减函数。下面提供几种实现方法：准备（对下列通用）：1234567891011import torchfrom torch.optim import *import torch.nn as nn#生成一个简单全连接神经网络class net(nn.Module): def __init__(self): super(net, self).__init__() self.fc = nn.Linear(1, 10) def forward(self, x): return self.fc(x)手动阶梯式衰减1234567model = net()LR = 0.01optimizer = Adam(model.parameters(), lr = LR)for epoch in range(100): if epoch % 5 == 0: for p in optimizer.param_groups: p['lr'] *= 0.9这里是每过5个epoch就进行一次衰减。lambda自定义衰减12345678import numpy as np model = net()LR = 0.01optimizer = Adam(model.parameters(), lr = LR)lambda1 = lambda epoch: np.sin(epoch) / epochscheduler = lr_scheduler.LambdaLR(optimizer, lr_lambda = lambda1)for epoch in range(100): scheduler.step()lr_lambda会接收到一个int参数：epoch，然后根据epoch计算出对应的lr。如果设置多个lambda函数的话，会分别作用于optimizer中的不同的params_group。StepLR阶梯式衰减123456model = net()LR = 0.01optimizer = Adam(model.parameters(), lr = LR)scheduler = lr_scheduler.StepLR(optimizer, step_size = 5, gamma = 0.8)for epoch in range(100): scheduler.step()每个epoch，lr会自动乘以gamma。三段式衰减123456model = net()LR = 0.01optimizer = Adam(model.parameters(), lr = LR)scheduler = lr_scheduler.MultiStepLR(optimizer, milestones = [20,80], gamma = 0.9)for epoch in range(100): scheduler.step()这种方法就是，当epoch进入milestones范围内即乘以gamma，离开milestones范围之后再乘以gamma。这种衰减方式也是在学术论文中最常见的方式，一般手动调整也会采用这种方法。连续衰减123456model = net()LR = 0.01optimizer = Adam(model.parameters(), lr = LR)scheduler = lr_scheduler.ExponentialLR(optimizer, gamma = 0.9)for epoch in range(100): scheduler.step()这种方法就是在每个epoch中lr都乘以gamma，从而达到连续衰减的效果。余弦式调整123456model = net()LR = 0.01optimizer = Adam(model.parameters(), lr = LR)scheduler = lr_scheduler.CosineAnnealingLR(optimizer, T_max = 20)for epoch in range(100): scheduler.step()这里的T_max对应1/2个cos周期所对应的epoch数值。基于loss和accuracy123456model = net()LR = 0.01optimizer = Adam(model.parameters(), lr = LR)scheduler = lr_scheduler.ReduceLROnPlateau(optimizer, mode = 'min', factor = 0.1, patience = 10, verbose = False, threshold = 0.0001, threshold_mode = 'rel', cooldown = 0, min_lr = 0, eps = 1e-08)for epoch in range(100): scheduler.step()当发现loss不再降低或者accuracy不再提高之后，就降低学习率。注：上面代码中各参数意义如下：mode：’min’模式检测metric是否不再减小，’max’模式检测metric是否不再增大；factor：触发条件后lr*=factor；patience：不再减小（或增大）的累计次数；verbose：触发条件后print；threshold：只关注超过阈值的显著变化；threshold_mode：有rel和abs两种阈值计算模式，rel规则：max模式下如果超过best(1+threshold)为显著，min模式下如果低于best(1-threshold)为显著；abs规则：max模式下如果超过best+threshold为显著，min模式下如果低于best-threshold为显著；cooldown：触发一次条件后，等待一定epoch再进行检测，避免lr下降过速；min_lr：最小的允许lr；eps：如果新旧lr之间的差异小与1e-8，则忽略此次更新。这里非常感谢facebook的员工给我们提供了如此多的选择与便利！批归一化（Batch Normalization）除了对学习率进行调整之外，Batch Normalization也可以有效地解决之前的问题。我是在学习ResNet的时候第一次遇到批归一化这个概念的。随着深度神经网络深度的加深，训练越来越困难，收敛越来越慢。为此，很多论文都尝试解决这个问题，比如ReLU激活函数，再比如Residual Network，而BN本质上也是解释并从某个不同的角度来解决这个问题的。通过使用Batch Normalization，我们可以加快网络的收敛速度，这样我们就可以使用较大的学习率来训练网络了。此外，BN还提高了网络的泛化能力。BN的基本思想其实相当直观：首先，因为深层神经网络在做非线性变换前的激活输入值（就是x=WU+B，U是输入）随着网络深度加深或者在训练过程中，其分布逐渐发生偏移或者变动，之所以训练收敛慢，一般是整体分布逐渐往非线性函数的取值区间的上下限两端靠近（对于Sigmoid函数来说，意味着激活输入值WU+B是大的负值或正值），这就导致了反向传播时低层神经网络的梯度消失，这是训练深层神经网络收敛越来越慢的本质原因。事实上，神经网络学习过程本质上是为了学习数据的分布，而BN就是通过一定的规范化手段，把每层神经网络任意神经元这个输入值的分布强行拉回到均值为0、方差为1的标准正态分布，其实就是把越来越偏的分布强制拉回比较标准的分布，这样使得激活输入值落在非线性函数对输入比较敏感的区域，这样输入的小变化就会导致损失函数较大的变化，从而让梯度变大，避免梯度消失问题产生，而且梯度变大意味着学习收敛速度快，因此通过BN能大大加快训练速度。下面来看看BN的具体操作过程：即以下四个步骤：1.计算样本均值。2.计算样本方差。3.对样本数据进行标准化处理。4.进行平移和缩放处理。这里引入了γ和β两个参数。通过训练可学习重构的γ和β这两个参数，让我们的网络可以学习恢复出原始网络所要学习的特征分布。下面是BN层的训练流程：这里的详细过程如下：输入：待进入激活函数的变量。输出：1.这里的K，在卷积网络中可以看作是卷积核个数，如网络中第n层有64个卷积核，就需要计算64次。注意：在正向传播时，会使用γ与β使得BN层输出与输入一样。2.在反向传播时利用γ与β求得梯度从而改变训练权值（变量）。3.通过不断迭代直到训练结束，求得关于不同层的γ与β。4.不断遍历训练集中的图片，取出每个batch_size中的γ与β，最后统计每层BN的γ与β各自的和除以图片数量得到平均值，并对其做无偏估计直作为每一层的E[x]与Var[x]。5.在预测的正向传播时，对测试数据求取γ与β，并使用该层的E[x]与Var[x]，通过图中11：所表示的公式计算BN层输出。注意：在预测时，BN层的输出已经被改变，因此BN层在预测中的作用体现在此处。上面输入的是待进入激活函数的变量，在残差网络ResNet中，的确也是先经过BN层再用relu函数做非线性处理的。那么，为什么BN层一般用在线性层和卷积层的后面，而不是放在非线性单元即激活函数之后呢？因为非线性单元的输出分布形状会在训练过程中变化，归一化无法消除他的方差偏移。相反的，全连接和卷积层的输出一般是一个对称、非稀疏的一个分布，更加类似高斯分布，对他们进行归一化会产生更加稳定的分布。比如，我们对一个高斯分布的数据relu激活，那么小于0的直接就被抑制了，这样得到的结果很难是高斯分布了，这时候再添加一个BN层就很难达到所需的效果。批归一化实现这里还是使用pytorch进行实现。准备（对下列通用）：12import torchimport torch.nn as nn2d或3d输入123456# 添加了可学习的仿射变换参数m = nn.BatchNorm1d(100)# 未添加可学习的仿射变换参数m = nn.BatchNorm1d(100, affine = False)input = torch.autograd.Variable(torch.randn(20, 100))output = m(input)我们查看m，可以看到有如下形式：1BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)这里解释一下涉及到的参数：num_features：来自期望输入的特征数，该期望输入的大小为：batch_size num_features（ width）eps：为保证数值稳定性（分母不能趋近或取0），给分母加上的值，默认为1e-5。momentum：计算动态均值和动态方差并进行移动平均所使用的动量，默认为0.1。affine：一个布尔值，当设为true时，就给该层添加可学习的仿射变换参数。仿射变换将在后文做简单介绍。BatchNorm1d可以有两种输入输出：1.输入（N，C），输出（N，C）。2.输入（N，C，L），输出（N，C，L）。3d或4d输入12345m = nn.BatchNorm2d(100)#或者m = nn.BatchNorm2d(100, affine = False)input = torch.autograd.Variable(torch.randn(20, 100, 35, 45))output = m(input)BatchNorm2d也可以有两种输入输出：1.输入（N，C，L），输出（N，C，L）。2.输入（N，C，H，W），输出（N，C，H，W）。4d或5d输入123m = nn.BatchNorm3d(100)#或者m = nn.BatchNorm3d(100, affine=False)BatchNorm3d同样支持两种输入输出：1.输入（N，C，H，W），输出（N，C，H，W）。2.输入（N，C，D，H，W），输出（N，C，D，H，W）。仿射变换这里我简单介绍一下仿射变换的概念，仿射变换（Affine Transformation或Affine Map）是一种二维坐标（x, y）到二维坐标（u, v）的变换，它是另外两种简单变换的叠加，一是线性变换，二是平移变换。同时，仿射变换保持了二维图形的“平直性”、“平行性”和“共线比例不变性”，非共线的三对对应点确定一个唯一的仿射变换。补充：共线性：若几个点变换前在一条线上，则仿射变换后仍然在一条线上。平行性：若两条线变换前平行，则变换后仍然平行。共线比例不变性：变换前一条线上两条线段的比例，在变换后比例不变。在二维图像变换中，它的一般表达如下：可以视为线性变换R和平移变换T的叠加。另外，仿射变换可以通过一系列的原子变换的复合来实现，包括平移，缩放，翻转，旋转和剪切。因此我们可以将几种简单的变换矩阵相乘来实现仿射变换。]]></content>
      <categories>
        <category>人工智能</category>
      </categories>
      <tags>
        <tag>深度学习</tag>
        <tag>代码实现</tag>
        <tag>pytorch</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[windows笔记：一些快捷的操作]]></title>
    <url>%2F2019%2F10%2F01%2Fwindows%E7%AC%94%E8%AE%B0%EF%BC%9A%E4%B8%80%E4%BA%9B%E5%BF%AB%E6%8D%B7%E7%9A%84%E6%93%8D%E4%BD%9C%2F</url>
    <content type="text"><![CDATA[之前在网上看到一个windows系统下的上帝模式，很好奇，尝试之后感觉不错，这里介绍一下创建的方法。除此之外附上一些类似的快捷操作。上帝模式上帝模式，即”God Mode”，或称为“完全控制面板”。它是windows系统中隐藏的一个简单的文件夹窗口，包含了几乎所有windows系统的设置，如控制面板的功能、界面个性化、辅助功能选项等控制设置，用户只需通过这一个窗口就能实现所有的操控，而不必再去为调整一个小小的系统设置细想半天究竟该在什么地方去打开设置。打开上帝模式后你将会看到如下界面：好吧我承认和想象中的上帝模式不太一样，不过下面我还是介绍一下这个略显简陋的上帝模式是怎么设置的。方式一：添加桌面快捷方式首先在桌面新建一个文件夹。将新建的文件夹命名为：GodMode.{ED7BA470-8E54-465E-825C-99712043E01C}。重命名完成后，你将看到一个类似于控制面板但没有名称的图标，双击打开，就可以看到之前所展示的上帝模式的界面了。方式二：添加到快捷菜单win+R运行，输入regedit打开注册表编辑器，允许更改。依次展开路径至HKEY_CLASSES_ROOT\DesktopBackground\Shell。点击shell后在右侧窗口鼠标右击，选择新建项。把新建的项重命名为“上帝模式”。点击上帝模式后，双击右侧窗口中的默认，在数值数据处输入上帝模式，点击确定。右击上帝模式，选择新建项。把新建的项重命名为“command”。点击command后，双击右侧窗口中的默认，在数值数据处输入：explorer shell:::{ED7BA470-8E54-465E-825C-99712043E01C}，确定。这时候在桌面空白处右键打开快捷菜单，就可以看到上帝模式已成功添加。类似的操作在上面我的快捷菜单中，可以看到还有关机、重启、锁屏等选项。其实它们添加的操作和添加上帝模式的步骤是一样的，只需把命名为“上帝模式”的地方修改成“关机”等文字，并且在上文中的第8步中，用对应的数值数据即可。这里提供四种功能对应的数值数据：关机Shutdown -s -f -t 00注销Shutdown -l重启Shutdown -r -f -t 00锁屏Rundll32 User32.dll,LockWorkStation事实上，锁屏功能可以直接使用win+L快捷键达到目的。除此之外，win还可以搭配其他的一些按键完成一些快捷操作，比如win+D可以快速最小化一切窗口回到桌面，想知道win有哪些搭配可以右键左下角的win图标查看。]]></content>
      <categories>
        <category>操作与使用</category>
      </categories>
      <tags>
        <tag>命令操作</tag>
        <tag>windows</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[machine learning笔记：过拟合与欠拟合]]></title>
    <url>%2F2019%2F10%2F01%2Fmachine-learning%E7%AC%94%E8%AE%B0%EF%BC%9A%E8%BF%87%E6%8B%9F%E5%90%88%E4%B8%8E%E6%AC%A0%E6%8B%9F%E5%90%88%2F</url>
    <content type="text"><![CDATA[本文介绍在模型评估可能会出现的过拟合与欠拟合两种现象，并对解决方法做一个总结。解释我们先通过图片来直观地解释这两种现象：在上图中，右边是过拟合的情况，它指的是模型对于训练数据拟合过度，反映到评估指标上，就是模型在训练集上的表现很好，但在测试集和新数据上的表现较差。这是因为在这种条件下，模型过于复杂，导致把噪声数据的特征也学习到了模型中，导致模型的泛化能力下降，从而在后期的应用过程中很容易输出错误的预测结果。左边是欠拟合的情况，它指的是在训练和预测时的表现都不好，这样的模型没有很好地捕捉到数据地特征，从而不能够很好地拟合数据。相比而言，中间是拟合适当的情况，这种模型在应用中就具有很好的鲁棒性。解决方法针对过拟合获取更多数据。更多的样本可以让模型学到更多有效的特征，从而减小噪声的影响。当然，一般情况下直接增加数据是很困难的，因此我们需要通过一定的规则来扩充训练数据。比如，在图像分类问题上，我们可以使用数据增强的方法，通过对图像的平移、旋转、缩放等方式来扩充数据；更进一步地，可以使用生成式对抗网络来合成大量新的训练数据。降低模型复杂度。模型复杂度过高是数据量较小时过拟合的主要原因。适当降低模型的复杂度可以避免模型拟合过多的噪声。比如，在神经网络模型中减少网络层数、神经元个数等；在决策树模型中降低树的深度、进行剪枝等。注意：网络深度增加引起的准确率退化不一定是过拟合引起的，这是因为深度造成的梯度消失、梯度爆炸等问题，这在ResNet的论文中有讨论，详细可以看我的博文deep-learning笔记：使网络能够更深——ResNet简介与pytorch实现。正则化方法。这里的方法主要是权重正则化法，具体说明可以参考machine-learning笔记：机器学习中正则化的理解。集成学习。即把多个模型集成在一起，从而降低单一模型的过拟合风险。主要有Bagging（bootstrap aggregating）和Boosting（adaptive boosting）这两种集成学习方法。针对欠拟合解决欠拟合问题也可以参照解决过拟合问题的思路；添加新特征。当特征不足或者现有特征与样本标签的相关性不强时，模型容易出现欠拟合。因此，通过挖掘“上下文特征”、“组合特征”等新的特征，往往能够取得更好的效果。在深度学习中，也有很多模型可以帮助完成特征工程，比如因此分解机、梯度提升决策树、Deep-crossing等都可以成为丰富特征的方法。增加模型复杂度。当模型过于简单时，增加模型复杂度可以使模型拥有更强的拟合能力。比如，在线性模型中添加高次项，在神经网络模型中增加网络层数、神经元个数等。对于模型的选择，我在文末补充了两种模型选择的准则供参考。减小正则化系数。正则化是用来防止过拟合的，但当模型出现欠拟合现象时，我们就应该有针对性地减小正则化系数。模型选择准则模型选择的信息准则有很多，我这里介绍我知道的两个比较常用的模型选择准则：AIC准则赤池信息准则（Akaike Information Criterion，AIC）公式定义如下：AIC=2k-2ln(L)\其中k表示模型参数个数（复杂度），L表示经验误差（似然函数）。当需要从一组可供选择的模型中选择最佳模型时，通常选择AIC最小的模型。BIC准则贝叶斯信息准则（Bayesian Information Criterion，BIC）是对AIC准则的改进，定义如下：BIC=kln(n)-2ln(L)\与AIC不同，这里k的系数不再是常数。其中n代表的是样本量（数据量），这样，BIC准则就与样本量相关了。当样本量足够时，过拟合的风险变小，我们就可以允许模型复杂一些。这里再次附上这张直观的图片，方便理解与体会。简析可参考machine-learning笔记：机器学习中正则化的理解。]]></content>
      <categories>
        <category>人工智能</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>模型评估</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[artificial intelligence笔记：人工智能前沿发展情况分享]]></title>
    <url>%2F2019%2F10%2F01%2Fartificial-intelligence%E7%AC%94%E8%AE%B0%EF%BC%9A%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E5%89%8D%E6%B2%BF%E5%8F%91%E5%B1%95%E6%83%85%E5%86%B5%E5%88%86%E4%BA%AB%2F</url>
    <content type="text"><![CDATA[这是我在一个相关的群里看到的一个论文，这篇论文比较新，看完之后觉得对目前AI发展状况的了解有一定价值，就放了上来。论文这里直接提供图片形式的原文：其他难得有一篇以AI开篇的文章，由于在我不到一年前真正接触AI相关知识时，一直疑惑人工智能、机器学习与深度学习之间的关系。直到看了台大教授李宏毅的课才知道三者之间的包含关系，这里就把课件中的一张图片放上来，一目了然：最后，再补张和deep-learning笔记：一篇非常经典的论文——NatureDeepReview文末对应的一张我觉得挺真实的图哈哈。不得不说，目前丰富的库和各种深度学习框架的确极大地方便了AI的学习与研究，许多轮子都已造好。学会运用这些工具还是很有帮助的！]]></content>
      <categories>
        <category>人工智能</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>深度学习</tag>
        <tag>论文分享</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[machine learning笔记：一个支持向量机的问题]]></title>
    <url>%2F2019%2F10%2F01%2Fmachine-learning%E7%AC%94%E8%AE%B0%EF%BC%9A%E4%B8%80%E4%B8%AA%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA%E7%9A%84%E9%97%AE%E9%A2%98%2F</url>
    <content type="text"><![CDATA[在学习机器学习理论的过程中，支持向量机（SVM）应该是我们会遇到的第一个对数学要求比较高的概念。理解它的原理要花费了我不少时间，写这篇博文是因为我之前看到的一个有关SVM的问题，其解答需用到SVM的相关数学原理，可以促使我思考。支持向量机的具体原理以及推导网上有大量资源，我也会在文中提供一些供参考。简介支持向量机是一种有监督的学习方法，主要思想是建立一个最优决策超平面，使得该平面两侧距离该平面最近的两类样本之间的距离最大化，从而对分类问题提供良好的泛化能力。这里有个小故事，也是我第一次看SVM课程时老师提到的，可以通过这个小故事大致理解一下SVM在做什么。它的优点主要有如下四点：1.相对于其他训练分类算法，SVM不需要过多的样本。2.SVM引入了核函数，可以处理高维的样本。3.结构风险最小。也就是说，分类器对问题真实模型的逼近与真实解之间的累计误差最小。4.由于SVM的非线性，它擅长应付线性不可分的问题。这主要是用松弛变量（惩罚变量）和核函数来实现的。这里我附上我所知的三个SVM的常用软件工具包：SVMLight、LibSVM、Liblinear。问题下面就是我在文章开头提到的问题，直接搬运：解析中提到的拉格朗日乘子法和KKT条件，也是我在看到这个问题后才尝试去理解的。能力有限，不能自己很好的解释，这里附上瑞典皇家理工学院（KTH）“统计学习基础”课程的KKT课件，个人觉得讲的很直观且详细了。]]></content>
      <categories>
        <category>人工智能</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[database笔记：范式的理解]]></title>
    <url>%2F2019%2F09%2F21%2Fdatabase%E7%AC%94%E8%AE%B0%EF%BC%9A%E8%8C%83%E5%BC%8F%E7%9A%84%E7%90%86%E8%A7%A3%2F</url>
    <content type="text"><![CDATA[今天终于完成了计算机三级数据库的考试，这也是本学期的第一门考试。听说计算机三级中要属计算机网络最简单，然而出于学到更多有用的知识的目的，我报了数据库。然而事实证明也没学到多少，毕竟这个计算机等级考试是给非计算机专业的人设置的，现在只求能过。不过两三天书看下来，还是有些收获，现在考完了有时间就在这里记一下，方便自己和别人今后有需要看。本文参考自：https://blog.csdn.net/he626shidizai/article/details/90707037https://blog.csdn.net/u013011841/article/details/39023859范式注意：本文中的范式指的是数据库范式。在设计数据库时，为了设计一个良好的逻辑关系，必须要使关系受一定条件的约束，这种约束逐渐成为一种规范，就是我们所说的范式。目前关系数据库有六种范式：第一范式（1NF）、第二范式（2NF）、第三范式（3NF）、巴斯-科德范式（BCNF）、第四范式(4NF）和第五范式（5NF）。要求最低的是1NF，往后依次变得严格。其中最后的5NF又称完美范式。数据库一般只需满足3NF，下面我就介绍一下前三种范式。第一范式数据库考试官方教程并没有对每个范式的定义进行讲解，另外因为文字定义比较晦涩难懂，我这里通过多方参考，用图片的形式来展示各个约束条件。首先，1NF是所有关系型数据库最基本的要求，它的定义为：符合1NF的关系中的每个属性都不可再分。下图就是一个违反1NF的例子：修改如下：上面的情况就符合1NF了。我们还可以把第一范式分成两点来理解：每个字段都只能存放单一值还是上反例： 上图中，第一行的课程有两个值，这就不符合第一范式了。因此要修改成这样：每笔记录都要能用一个唯一的主键识别 这里出现了重复组，同样也不满足1NF，因为缺乏唯一的标识码。因此修改如下：第二范式第二范式是建立在第一范式的基础上的，它的改进在于：消除了非主属性对于码的部分函数依赖。第二范式消除了非主属性对于码的部分函数依赖，也就是说，第二范式中所有非主属性完全依赖于主键，即不能依赖于主键的一部分属性。为了解释明白，还是通过实例的说明：上表中，学号和课程号组合在一起是主键，但是姓名只由学号决定，这就违反了第二范式。同样的，课程名只由课程号决定，这也违反了第二范式。此外，只需要知道学号和课程号就能知道成绩。为了满足第二范式，我们就需要对上表做如下拆分：第三范式同样的，第三范式建立在第二范式的基础上。不同之处在于，在第二范式的基础之上，第三范式中非主属性都不传递依赖于主键。这是什么意思？还是看图说话：上表中，主键是学号，且已满足第二范式。然而，学校的地址也可以根据学校名称来确定，第三范式就是在这里再做一个分解：]]></content>
      <categories>
        <category>知识点与小技巧</category>
      </categories>
      <tags>
        <tag>数据库</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[hexo笔记：在linux（ubuntu）下安装使用]]></title>
    <url>%2F2019%2F09%2F17%2Fhexo%E7%AC%94%E8%AE%B0%EF%BC%9A%E5%9C%A8linux%EF%BC%88ubuntu%EF%BC%89%E4%B8%8B%E5%AE%89%E8%A3%85%E4%BD%BF%E7%94%A8%2F</url>
    <content type="text"><![CDATA[之前一直在win10下使用hexo搭建部署博客，方法参见：hexo笔记：开始创建个人博客——方法及原因。那么，如果想在linux环境下使用hexo，该如何操作呢？本文参考自：https://blog.csdn.net/y5492853/article/details/79529410原因由于在更改主题配置文件_config.yml时，长达八百多行的配置文件总是让我找得头晕目眩。由于vscode（好像）没有提供字符的快速查找匹配功能，我之前一直采用一种笨拙的办法，即在文件的空处输入想要查找的字符然后左键选中，这个时候文件中同样的字符也会被选中，这样快速拉动滚动条时就可以比较明显地发现想找的目标字符了。然而对于这种办法，我觉得主要有两大问题：忘记删除在空白处添加的文字我就犯过这样低级的错误，找到并更改之后没有删除自己添加的字符就直接快乐地ctrl+S了，于是就造成了网站能打开但是一片空白的bug。所以大家没事还是不要随意在主题配置文件中添加文字。不是长久之计虽然这个八百多行的文件已经让我够呛了，然后或许今后还会遇到更长的文件，那么这种方法就会变得极其低效（而且伤眼睛）。基于这些因素，我脑子里的第一个反映就是vim编辑器中的对文件字符的查找定位的功能（关于vim的使用，等我多多尝试并熟练之后再做小结）。好了，接下来就开始操作吧。安装首先安装node.js。这里就没windows下直接双击exe安装包那么easy啦，打开终端，老老实实输命令：123sudo apt-get install nodejssudo apt install nodejs-legacysudo apt install npm由于ubuntu源中的node.js是旧版本，下面会出现问题，我在后文解释。由于npm服务器在国外可能会影响下载速度，和windows下的步骤一样，我们换成淘宝镜像：1sudo npm config set registry https://registry.npm.taobao.org这时候如果我们直接安装hexo，会出现如下错误：因此，我们安装node升级工具n：1sudo npm install n -g并且使用sudo n stable升级版本，若看到如下输出，说明升级成功：注意：fetch可能需要花费一点时间，这时候终端不会有任何输出，不要以为出错了，耐心等待即可，不要ctrl+C中止。最后，我们安装hexo：1sudo npm install -g hexo注：-g表示安装到全局环境。接下来的初始化操作跟windows下基本一样，可以参照我之前的博文。我继续对原来的博客进行编辑，所以无需初始化一个新的，直接把windows的对应文件夹整个copy过来就行了。]]></content>
      <categories>
        <category>操作与使用</category>
      </categories>
      <tags>
        <tag>hexo</tag>
        <tag>ubuntu</tag>
        <tag>安装教程</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[machine learning笔记：机器学习中正则化的理解]]></title>
    <url>%2F2019%2F09%2F15%2Fmachine-learning%E7%AC%94%E8%AE%B0%EF%BC%9A%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B8%AD%E6%AD%A3%E5%88%99%E5%8C%96%E7%9A%84%E7%90%86%E8%A7%A3%2F</url>
    <content type="text"><![CDATA[在接触了一些ml的知识后，大家一定会对正则化这个词不陌生，但是我感觉根据这个词的字面意思不能够直接地理解它的概念。因此我打算写一篇文章做个记录，方便以后回忆。线性代数中的正则化如果直接搜索正则化这个名词，首先得到的一般是代数几何中的一个概念。百度词条对它的解释是：给平面不可约代数曲线以某种形式的全纯参数表示。怎么样？是不是觉得一头雾水。这里我推荐使用谷歌或者维基百科来查询这些专业名词。对于不能科学上网的朋友，没关系，我这里提供了谷歌镜像和wikiwand，大家可以在上面得到一样的搜索结果。我们直接到维基百科搜索regularization：里面第一段是这样解释的：In mathematics, statistics, and computer science, particularly in machine learning and inverse problems, regularization is the process of adding information in order to solve an ill-posed problem or to prevent overfitting.这就和我们在机器学习应用中的目的比较相近了。机器学习中的正则化在机器学习中，正则化是一种为了减小测试误差的行为（有时候会增加训练误差）。我们在构造机器学习模型时，最终目的是让模型在面对新数据的时候，可以有很好的表现。当你用比较复杂的模型比如神经网络去拟合数据时，很容易出现过拟合现象（训练集表现很好，测试集表现较差），这会导致模型的泛化能力下降，这时候，我们就需要使用正则化，来降低模型的复杂度。为了加深印象，我下面简单介绍几种常用的机器学习正则化方法：早停法（Early Stopping）早停法，就是当训练集的误差变好，但是验证集的误差变坏（即泛化效果变差）的时候停止训练。这种方法可以一定程度上有效地防止过拟合，同时这也说明了验证集在机器学习中的重要性。权重正则化法因为噪声相比于正常信号而言，通常会在某些点出现较大的峰值。所以，只要我们保证权重系数在绝对值意义上足够小，就能够保证噪声不会被过度相应，这也是奥卡姆剃刀原理的表现，即模型不应过度复杂，尤其是当数据量不大的时候。上面是在一个网课上看到的、我觉得可以较好地呈现模型的复杂度与数据量对模型预测表现的影响的一张图片，其中向左的横轴表示数据量大小，向右的横轴表示模型复杂度，竖轴是预测表现。通过这张图，可以很明显地观察到：模型的复杂度提升需要大量的数据作为依托。权重正则化主要有两种：L1正则：$ J=J_{0}+\lambda \left | w \right |_{1} $，其中J代表损失函数（也称代价函数），$ \left | w \right |_{1} $代表参数向量w的L1范数。L2正则（weight decay）：$ J=J_{0}+\lambda \left | w \right |_{2} $，其中$ \left | w \right |_{2} $代表参数向量w的L2范数。 这里就产生了Lasso回归与岭回归两大机器学习经典算法。其中Lasso回归是一种压缩估计，可以通过构造惩罚函数得到一个较为精炼的模型，使得它可以压缩一些系数，同时设定一些系数为0，从而达到特征选择的目的。基于Lasso回归这种可以选择特征并降维的特性，它主要有这些适用情况：样本量比较小，但指标量非常多的时候（易于过拟合）。进行高维统计时。需要对特征进行选择时。对于这些回归的详细解释，大家可以到网上搜集相关信息。补充：L0范数：向量中非零元素的个数。L1范数：向量中每个元素绝对值的和。L2范数：向量元素绝对值的平方和再平方。下面我再附上一组图，希望能帮助更好地理解权重正则化：首先我们可视化一个损失函数。下面我们看一看正则化项的图示，这里使用L1范数作为正则化项。接着，我们将上面两者线性组合：我们来看看结果：可见，正则化项的引入排除了大量原本属于最优解的点，上图的情况中剩下一个唯一的局部最优解。数据增强数据增强可以丰富图像数据集，有效防止过拟合。这种方法在AlexNet中有很好的应用，大家可以看看我的博文deep-learning笔记：开启深度学习热潮——AlexNet。随机失活（dropout）dropout即随机砍掉一部分神经元之间的连接，每次只更新一部分，这可以有效地增加它的鲁棒性，提高泛化能力。这个方法在AlexNet中也有详细的解释，推荐大家去看一下。 以上就是比较常规且流行的正则化方式，今后或许会有补充，也欢迎大家提供意见~]]></content>
      <categories>
        <category>人工智能</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[deep learning笔记：开启深度学习热潮——AlexNet]]></title>
    <url>%2F2019%2F09%2F15%2Fdeep-learning%E7%AC%94%E8%AE%B0%EF%BC%9A%E5%BC%80%E5%90%AF%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%83%AD%E6%BD%AE%E2%80%94%E2%80%94AlexNet%2F</url>
    <content type="text"><![CDATA[继之前那篇deep-learning笔记：着眼于深度——VGG简介与pytorch实现，我觉得还是有必要提一下VGG的前辈——具有历史意义的AlexNet，于是就写了这篇文章简要介绍一下。简介ALexNet是第一个运用大型深度卷积神经网络的模型，在ILSVRC中一下子比前一年把错误率降低了10%，这是非常惊人的，也很快引起了注意。于是，自2012年开始，深度学习热潮由此引发。根据我之前听网课的笔记以及网上的其他文章，我把AlexNet主要的进步归纳如下：使用大型深度卷积神经网络。分组卷积（groupconvolution）来充分利用GPU。随机失活dropout：一种有效的正则化方法。关于正则化，可以看我的博文machine-learning笔记：机器学习中正则化的理解。数据增强data augumentation：增大数据集以减小过拟合问题。relu激活函数：即max（0，x），至今还被广泛应用。个人思考这段时间也看了不少东西，对于如何提升神经网络的性能这个问题，我觉得主要有如下三个方面：从网络本身入手：增加深度。增加宽度。减少参数量。防止过拟合。解决梯度消失的问题。从数据集入手：尽可能使用多的数据。从硬件入手：提升GPU性能。充分利用现有的GPU性能。当你阅读完AlexNet的论文，你会发现它在这几个方面都有思考且做出了非常优秀的改进。论文在放论文之前，我还是先贴一张流程图，方便在阅读论文的时候进行对照与理解。下面奉上宝贵的论文：论文原版论文中文版从introduction第一句开始，作者就开始了一段长长的吐槽：Current approaches to object recognition make essential use of machine learning methods…吐槽Yann LeCun大佬的论文被顶会拒收了仅仅因为Yann LeCun使用了神经网络。其实，那段时间之前，由于SVM等机器学习方法的兴起，神经网络是一种被许多ml大佬们看不起的算法模型。在introduction的最后，作者留下了这样一句经典的话：All of our experiments suggest that our results can be improved simply by waiting for faster GPUs and bigger datasets to become available.有没有感觉到一种新世界大门被打开的感觉呢？有关论文别的内容，我暂不多说了，大家可以自己看论文学习与体会。附上推荐重点阅读的章节：3.1 ReLUNonlinearity；3.5 OverallArchitecture；4 ReducingOverfitting。说明同我写VGG的那篇文章中一样，我在英文原版中用黄颜色高亮了我觉得重要的内容给自己和大家今后参考。另外，我在这里推荐大家还是先尝试阅读英文原版。一方面由于一些公式、符号以及名词的原因，英文原版叙述更精准，中文翻译有缺漏、偏颇之处；另一方面更重要的，接触这些方面的知识仅参考中文是远远不够的。在这里我推荐一个chrome英文pdf阅读插件，大家可以自己到chrome里面搜索安装：有了这个插件，遇到不认识的单词，只需双击单词，就可以看到中文释义，一定程度上可以保证阅读的流畅性。但是如果想从根本上解决问题，只有好好背单词吧（我也在朝这个方向努力…）。另外，iPad的上也有好多强大的app，在这里不一一推荐了。]]></content>
      <categories>
        <category>人工智能</category>
      </categories>
      <tags>
        <tag>深度学习</tag>
        <tag>论文分享</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[markdown笔记：公式插入和代码高亮]]></title>
    <url>%2F2019%2F09%2F15%2Fmarkdown%E7%AC%94%E8%AE%B0%EF%BC%9A%E5%85%AC%E5%BC%8F%E6%8F%92%E5%85%A5%E5%92%8C%E4%BB%A3%E7%A0%81%E9%AB%98%E4%BA%AE%2F</url>
    <content type="text"><![CDATA[在上一篇文章deep-learning笔记：着眼于深度——VGG简介与pytorch实现中，我用到了markdown其他的一些使用方法，因此我想在此对之前的一篇文章markdown笔记：markdown的基本使用做一些补充。本文参考自：https://www.jianshu.com/p/25f0139637b7https://www.jianshu.com/p/fd97e1f8f699https://www.jianshu.com/p/68e6f82d88b7https://www.jianshu.com/p/7c02c112d532公式插入无论是学习ml还是dl，我们总是离不开数学的，于是利用markdown插入数学公式就成了一个的需求。那么怎么在markdown中插入公式呢？markdown中的公式分为两类，即行内公式与行间公式。它们对应的代码如下：12$ \Gamma(z) = \int_0^\infty t^&#123;z-1&#125;e^&#123;-t&#125;dt\,. $$$\Gamma(z) = \int_0^\infty t^&#123;z-1&#125;e^&#123;-t&#125;dt\,.$$让我们来看一下效果：行内公式：$ \Gamma(z) = \int_0^\infty t^{z-1}e^{-t}dt\,. $行间公式：\Gamma(z) = \int_0^\infty t^{z-1}e^{-t}dt\,.注意：使用单个$时，需要在公式两边与$之间空一格，我之前没空就一直转换不成公式的形式。如果你是使用hexo编写博客，那么默认的设置是无法转义markdown公式的，解决这个问题的配置方法可以参考本文顶部给出的第三个链接。另外要注意，在使用公式时，对应文件需开启mathjax选项。markdown公式的具体语法可以参照本文的第一个链接，你可以在typora中根据它的官方文档进行尝试。注意：在typora中，只需输入$或者$$就可直接进入公式编辑，无需输入一对。有机会我再对上面提到的语法进行搬运。下面介绍一种更简单省力的方法（也是我在用的方法）：打开在线LaTex公式编辑器在上方的框框中输入你想要的公式： 你可以在下方的GIF图中随时观察你的输入时候符合预期。拷贝下方黄颜色方框中的代码到markdown文件 你可以选择去掉前面的“\”和两边的方括号，否则你的公式两侧将会套有方括号，另外你还需要使用上文提到的$来确定公式显示方式。这里我们这样输入：$ x+y=z\ $。得到：$ x+y=z $以上就是使用LaTex给markdown添加公式的方法。你也可以使用黄颜色框中的URL选项来添加代码，格式是![](URL)。例如，输入：![](https://latex.codecogs.com/gif.latex?x&amp;plus;y=z)可以看到：这种方法就不需要文章顶部链接三中的配置了，也是一种推荐的方法。代码高亮markdown中使代码高亮的格式如下：三个反引号+语言名代码…三个反引号例如，输入：可以看到：1print("hello world!")同样的，在typora中，你也不必输入成对的三个反引号。这里我要提醒一个我以前用Rmarkdown时踩过的坑：注意！他俩是不一样的！真正的“`”在这里：]]></content>
      <categories>
        <category>操作与使用</category>
      </categories>
      <tags>
        <tag>markdown</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[deep learning笔记：着眼于深度——VGG简介与pytorch实现]]></title>
    <url>%2F2019%2F09%2F15%2Fdeep-learning%E7%AC%94%E8%AE%B0%EF%BC%9A%E7%9D%80%E7%9C%BC%E4%BA%8E%E6%B7%B1%E5%BA%A6%E2%80%94%E2%80%94VGG%E7%AE%80%E4%BB%8B%E4%B8%8Epytorch%E5%AE%9E%E7%8E%B0%2F</url>
    <content type="text"><![CDATA[VGG是我第一个自己编程实践的卷积神经网络，也是挺高兴的，下面我就对VGG在这篇文章中做一个分享。本文参考自：https://blog.csdn.net/xiaohuihui1994/article/details/89207534https://blog.csdn.net/sinat_33487968/article/details/83584289简介VGG模型在2014年取得了ILSVRC竞赛的第二名，第一名是GoogLeNet。但是VGG在多个迁移学习任务中的表现要优于googLeNet。相比之前的神经网路，VGG主要有两大进步：其一是它增加了深度，其二是它使用了小的3x3的卷积核，这可以使它在增加深度的时候一定程度上防止了参数的增长。缺点是它的参数量比较庞大，但这并不意味着它不值得我们仔细研究。下图展示的是VGG的结构。为了通过对比来对VGG的一些改进进行解释，VGG的作者在论文中提供了多个版本。论文要详细分析VGG，我可能不能像网上写的那样好，更不可能像论文一样明白。那么我在这里就先附上论文。论文原版论文中文版我在英文原版中用黄颜色高亮了我觉得比较重要的内容，大家可以参考一下。大家也可以自己到网上进行搜索，这类经典的网络网上有许多介绍与分析。通过论文或者网上的资源对这个网络有一定理解之后，你可以看看我下面的代码实现。自己实现这里我使用pytorch框架来实现VGG。pytorch是一个相对较新的框架，但热度上升很快。根据网上的介绍，pytorch是一个非常适合于学习与科研的深度学习框架。我尝试了之后，也发现上手很快。在pytorch中，神经网络可以通过torch.nn包来构建。这里我不一一介绍了，大家可以参考pytorch官方中文教程来学习，照着文档自己动手敲一遍之后，基本上就知道了pytorch如何使用了。为了方便直观的理解，我先提供一个VGG16版本的流程图。下面是我实现VGG19版本的代码：首先，我们import所需的包。12import torchimport torch.nn as nn接下来，我们定义神经网络。1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980class VGG(nn.Module): def __init__(self, num_classes = 1000): #imagenet图像库总共1000个类 super(VGG, self).__init__() #先运行父类nn.Module初始化函数 self.conv1_1 = nn.Conv2d(in_channels = 3, out_channels = 64, kernel_size = 3, padding = 1) #定义图像卷积函数：输入为图像（3个频道，即RGB图）,输出为64张特征图,卷积核为3x3正方形，为保留原空间分辨率，卷积层的空间填充为1 self.conv1_2 = nn.Conv2d(in_channels = 64, out_channels = 64, kernel_size = 3, padding = 1) self.conv2_1 = nn.Conv2d(in_channels = 64, out_channels = 128, kernel_size = 3, padding = 1) self.conv2_2 = nn.Conv2d(in_channels = 128, out_channels = 128, kernel_size = 3, padding = 1) self.conv3_1 = nn.Conv2d(in_channels = 128, out_channels = 256, kernel_size = 3, padding = 1) self.conv3_2 = nn.Conv2d(in_channels = 256, out_channels = 256, kernel_size = 3, padding = 1) self.conv3_3 = nn.Conv2d(in_channels = 256, out_channels = 256, kernel_size = 3, padding = 1) self.conv3_4 = nn.Conv2d(in_channels = 256, out_channels = 256, kernel_size = 3, padding = 1) self.conv4_1 = nn.Conv2d(in_channels = 256, out_channels = 512, kernel_size = 3, padding = 1) self.conv4_2 = nn.Conv2d(in_channels = 512, out_channels = 512, kernel_size = 3, padding = 1) self.conv4_3 = nn.Conv2d(in_channels = 512, out_channels = 512, kernel_size = 3, padding = 1) self.conv4_4 = nn.Conv2d(in_channels = 512, out_channels = 512, kernel_size = 3, padding = 1) self.conv5_1 = nn.Conv2d(in_channels = 512, out_channels = 512, kernel_size = 3, padding = 1) self.conv5_2 = nn.Conv2d(in_channels = 512, out_channels = 512, kernel_size = 3, padding = 1) self.conv5_3 = nn.Conv2d(in_channels = 512, out_channels = 512, kernel_size = 3, padding = 1) self.conv5_4 = nn.Conv2d(in_channels = 512, out_channels = 512, kernel_size = 3, padding = 1) self.relu = nn.ReLU(inplace = True) #inplace=TRUE表示原地操作 self.max = nn.MaxPool2d(kernel_size = 2, stride = 2) self.fc1 = nn.Linear(512 * 7 * 7, 4096) #定义全连接函数1为线性函数:y = Wx + b，并将512*7*7个节点连接到4096个节点上。 self.fc2 = nn.Linear(4096, 4096) self.fc3 = nn.Linear(4096, num_classes) #定义全连接函数3为线性函数:y = Wx + b，并将4096个节点连接到num_classes个节点上，然后可用softmax进行处理。 #定义该神经网络的向前传播函数，该函数必须定义，一旦定义成功，向后传播函数也会自动生成（autograd） def forward(self, x): x = self.relu(self.conv1_1(x)) x = self.relu(self.conv1_2(x)) x = self.max(x) #输入x经过卷积之后，经过激活函数ReLU，循环两次，最后使用2x2的窗口进行最大池化Max pooling，然后更新到x。 x = self.relu(self.conv2_1(x)) x = self.relu(self.conv2_2(x)) x = self.max(x) x = self.relu(self.conv3_1(x)) x = self.relu(self.conv3_2(x)) x = self.relu(self.conv3_3(x)) x = self.relu(self.conv3_4(x)) x = self.max(x) x = self.relu(self.conv4_1(x)) x = self.relu(self.conv4_2(x)) x = self.relu(self.conv4_3(x)) x = self.relu(self.conv4_4(x)) x = self.max(x) x = self.relu(self.conv5_1(x)) x = self.relu(self.conv5_2(x)) x = self.relu(self.conv5_3(x)) x = self.relu(self.conv5_4(x)) x = self.max(x) x = x.view(-1, self.num_flat_features(x)) #view函数将张量x变形成一维的向量形式,总特征数并不改变,为接下来的全连接作准备。 x = self.fc1(x) #输入x经过全连接1，然后更新x x = self.fc2(x) x = self.fc3(x) return x def num_flat_features(self, x): size = x.size()[1:] #all dimensions except the batch dimension num_features = 1 for s in size: num_features *= s return num_featuresvgg = VGG()print(vgg)我们print网络，可以看到输出如下：1234567891011121314151617181920212223VGG( (conv1_1): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (conv1_2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (conv2_1): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (conv2_2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (conv3_1): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (conv3_2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (conv3_3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (conv3_4): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (conv4_1): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (conv4_2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (conv4_3): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (conv4_4): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (conv5_1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (conv5_2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (conv5_3): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (conv5_4): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (relu): ReLU(inplace=True) (max): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False) (fc1): Linear(in_features=25088, out_features=4096, bias=True) (fc2): Linear(in_features=4096, out_features=4096, bias=True) (fc3): Linear(in_features=4096, out_features=1000, bias=True))最后我们随机生成一个张量来进行验证。123input = torch.randn(1, 3, 224, 224)out = vgg(input)print(out)其中(1, 3, 224, 224)表示1个3x224x224的矩阵，这是因为VGG输入的是固定尺寸的224x224的RGB（三通道）图像。如果没有报错，那么就说明你的神经网路可以运行通过了。我们也可以使用torch.nn.functional来实现激活函数与池化层，这样的话，你需要还需要多引入一个包：123import torchimport torch.nn as nnimport torch.nn.functional as F #新增同时，你不需要在init中实例化激活函数与最大池化层，相应的，你需要对forward前馈函数进行更改：1234567891011121314151617181920212223242526272829303132333435def forward(self, x): x = F.relu(self.conv1_1(x)) x = F.relu(self.conv1_2(x)) x = F.max_pool2d(x, kernel_size = 2, stride = 2) #输入x经过卷积之后，经过激活函数ReLU，循环两次，最后使用2x2的窗口进行最大池化Max pooling，然后更新到x。 x = F.relu(self.conv2_1(x)) x = F.relu(self.conv2_2(x)) x = F.max_pool2d(x, kernel_size = 2, stride = 2) x = F.relu(self.conv3_1(x)) x = F.relu(self.conv3_2(x)) x = F.relu(self.conv3_3(x)) x = F.relu(self.conv3_4(x)) x = F.max_pool2d(x, kernel_size = 2, stride = 2) x = F.relu(self.conv4_1(x)) x = F.relu(self.conv4_2(x)) x = F.relu(self.conv4_3(x)) x = F.relu(self.conv4_4(x)) x = F.max_pool2d(x, kernel_size = 2, stride = 2) x = F.relu(self.conv5_1(x)) x = F.relu(self.conv5_2(x)) x = F.relu(self.conv5_3(x)) x = F.relu(self.conv5_4(x)) x = F.max_pool2d(x, kernel_size = 2, stride = 2) x = x.view(-1, self.num_flat_features(x)) #view函数将张量x变形成一维的向量形式,总特征数并不改变,为接下来的全连接作准备。 x = self.fc1(x) #输入x经过全连接1，然后更新x x = self.fc2(x) x = self.fc3(x) return x如果你之前运行通过的话，那么这里也是没有问题的。这里我想说明一下torch.nn与torch.nn.functional的区别。这两个包中有许多类似的激活函数与损失函数，但是它们又有如下不同：首先，在定义函数层（继承nn.Module）时，init函数中应该用torch.nn，例如torch.nn.ReLU，torch.nn.Dropout2d，而forward中应该用torch.nn.functionl，例如torch.nn.functional.relu，不过请注意，init里面定义的是标准的网络层。只有torch.nn定义的才会进行训练。torch.nn.functional定义的需要自己手动设置参数。所以通常，激活函数或者卷积之类的都用torch.nn定义。另外，torch.nn是类，必须要先在init中实例化，然后在forward中使用，而torch.nn.functional可以直接在forward中使用。大家还可以通过官方文档torch.nn.functional与torch.nn来进一步了解两者的区别。大家或许发现，我的代码中有大量的重复性工作。是的，你将在文章后面的官方实现中看到优化的代码，但是相对来说，我的代码更加直观些，完全是按照网络的结构顺序从上到下编写的，可以方便初学者（including myself）的理解。出现的问题虽然我的代码比较简单直白，但是过程中并不是一帆风顺的，出现了两次报错：输入输出不匹配当我第一遍运行时，出现了一个RuntimeError： 这是一个超级低级的错误，经学长提醒后我才发现，我两个卷积层之间输出输入的channel数并不匹配： 唉又是ctrl+C+V惹的祸，改正后的网络可以参见上文。在这里，我想提醒我自己和大家注意一下卷积层输入输出的维度公式：假设输入的宽、高记为W、H。超参数中，卷积核的维度是F，stride步长是S，padding是P。那么输出的宽X与高Y可用如下公式表示：X=\frac{W+2P-F}{S}+1\Y=\frac{H+2P-F}{S}+1\然而，当我在计算ResNet的维度的时候，发现套用这个公式是除不尽的。于是我搜索到了如下规则：1.对卷积层操作，除不尽时，向下取整。2.对池化层操作，除不尽时，向上取整。没有把张量转化成一维向量上面的问题解决了，结果还有错误： 根据报错，可以发现3584x7是等于25088的，结合pytorch官方文档，我意识到我在把张量输入全连接层时，没有把它拍扁成一维。因此，我按照官方文档添加了如下代码：12345678x = x.view(-1, self.num_flat_features(x))def num_flat_features(self, x): size = x.size()[1:] #all dimensions except the batch dimension num_features = 1 for s in size: num_features *= s return num_features再运行，问题解决。另外，我原本写的代码中，在卷积层之间的对应位置都加上了relu激活函数与池化层。后来我才意识到，由于它们不具有任何需要学习的参数，我可以直接把它们拿出来单独定义：12self.relu = nn.ReLU(inplace = True)self.max = nn.MaxPool2d(kernel_size = 2, stride = 2)虽然是一些很低级的坑，但我还是想写下来供我自己和大家今后参考。官方源码由于VGG的结构设计非常有规律，因此官方源码给出了更简洁的版本：123456789101112131415161718192021222324252627282930313233343536373839import torch.nn as nnimport mathclass VGG(nn.Module): def __init__(self, features, num_classes=1000, init_weights=True): super(VGG, self).__init__() self.features = features self.classifier = nn.Sequential( nn.Linear(512 * 7 * 7, 4096), nn.ReLU(True), nn.Dropout(), nn.Linear(4096, 4096), nn.ReLU(True), nn.Dropout(), nn.Linear(4096, num_classes), ) if init_weights: self._initialize_weights() def forward(self, x): x = self.features(x) x = x.view(x.size(0), -1) x = self.classifier(x) return x def _initialize_weights(self): for m in self.modules(): if isinstance(m, nn.Conv2d): n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels m.weight.data.normal_(0, math.sqrt(2. / n)) if m.bias is not None: m.bias.data.zero_() elif isinstance(m, nn.BatchNorm2d): m.weight.data.fill_(1) m.bias.data.zero_() elif isinstance(m, nn.Linear): m.weight.data.normal_(0, 0.01) m.bias.data.zero_()因为VGG中卷积层的重复性比较高，所以官方使用一个函数来循环产生卷积层：1234567891011121314def make_layers(cfg, batch_norm=False): layers = [] in_channels = 3 for v in cfg: if v == 'M': layers += [nn.MaxPool2d(kernel_size=2, stride=2)] else: conv2d = nn.Conv2d(in_channels, v, kernel_size=3, padding=1) if batch_norm: layers += [conv2d, nn.BatchNorm2d(v), nn.ReLU(inplace=True)] else: layers += [conv2d, nn.ReLU(inplace=True)] in_channels = v return nn.Sequential(*layers)接下来定义各个版本的卷积层（可参考上文中对论文的截图），这里的“M”表示的是最大池化层。1234567891011121314151617181920212223242526272829303132333435363738394041424344cfg = &#123; 'A': [64, 'M', 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512, 'M'], 'B': [64, 64, 'M', 128, 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512, 'M'], 'D': [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 'M', 512, 512, 512, 'M', 512, 512, 512, 'M'], 'E': [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 256, 'M', 512, 512, 512, 512, 'M', 512, 512, 512, 512, 'M'],&#125;def vgg11(**kwargs): model = VGG(make_layers(cfg['A']), **kwargs) return modeldef vgg11_bn(**kwargs): model = VGG(make_layers(cfg['A'], batch_norm=True), **kwargs) return modeldef vgg13(**kwargs): model = VGG(make_layers(cfg['B']), **kwargs) return modeldef vgg13_bn(**kwargs): model = VGG(make_layers(cfg['B'], batch_norm=True), **kwargs) return modeldef vgg16(**kwargs): model = VGG(make_layers(cfg['D']), **kwargs) return modeldef vgg16_bn(**kwargs): model = VGG(make_layers(cfg['D'], batch_norm=True), **kwargs) return modeldef vgg19(**kwargs): model = VGG(make_layers(cfg['E']), **kwargs) return modeldef vgg19_bn(**kwargs): model = VGG(make_layers(cfg['E'], batch_norm=True), **kwargs) return modelif __name__ == '__main__': # 'VGG', 'vgg11', 'vgg11_bn', 'vgg13', 'vgg13_bn', 'vgg16', 'vgg16_bn', 'vgg19_bn', 'vgg19' # Example net11 = vgg11() print(net11)附上pytorch官方源码链接。可以在vision/torchvision/models/下找到一系列用pytorch实现的经典神经网路模型。好了，以上就是VGG的介绍与实现，如有不足之处欢迎大家补充！]]></content>
      <categories>
        <category>人工智能</category>
      </categories>
      <tags>
        <tag>深度学习</tag>
        <tag>论文分享</tag>
        <tag>代码实现</tag>
        <tag>pytorch</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[deep learning笔记：一篇非常经典的论文——NatureDeepReview]]></title>
    <url>%2F2019%2F09%2F14%2Fdeep-learning%E7%AC%94%E8%AE%B0%EF%BC%9A%E4%B8%80%E7%AF%87%E9%9D%9E%E5%B8%B8%E7%BB%8F%E5%85%B8%E7%9A%84%E8%AE%BA%E6%96%87%E2%80%94%E2%80%94NatureDeepReview%2F</url>
    <content type="text"><![CDATA[这是一篇非常经典的有关深度学习的论文，最近在看一个网课的时候又被提到了，因此特地找了pdf文档放在这里和大家分享。简述这篇文章首先介绍了深度学习的基本前期储备知识、发展背景，并对机器学习范畴内一个重要方向——监督学习进行完整介绍，然后介绍了反向传播算法和微积分链式法则等深度学习基础内容。文章的接下来重点介绍了卷积神经网络CNN的实现过程、几个非常重要的经典卷积神经网络以及深度卷积神经网络对于视觉任务理解的应用。文章最后探讨了分布表示和语言模型，循环神经网络RNN原理以及对未来的展望和现实的实现。总而言之，我觉得这是一篇值得逐字逐句反复阅读咀嚼的文章，读完这篇文章，大概就相当于打开了深度学习的大门了吧。这篇文章的个人理解与感悟或许我以后会补上，在接触还不深的情况下我不说废话啦，先附上原文，其中黄色高亮部分是一些比较重要的内容，大家有时间的话可以认真看一下。下面附上原文链接。最后贴一张我觉得挺搞笑的图。这张图片还有张兄弟图，可以看看我的另一篇论文分享artificial-intelligence笔记：人工智能前沿发展情况分享。]]></content>
      <categories>
        <category>人工智能</category>
      </categories>
      <tags>
        <tag>深度学习</tag>
        <tag>论文分享</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ubuntu笔记：重装ubuntu——记一次辛酸血泪史]]></title>
    <url>%2F2019%2F09%2F14%2Fubuntu%E7%AC%94%E8%AE%B0%EF%BC%9A%E9%87%8D%E8%A3%85ubuntu%E2%80%94%E2%80%94%E8%AE%B0%E4%B8%80%E6%AC%A1%E8%BE%9B%E9%85%B8%E8%A1%80%E6%B3%AA%E5%8F%B2%2F</url>
    <content type="text"><![CDATA[这是不久前我踩过的一个巨坑，在这里我想先强调一下：不要升级linux发行版！！！重要的事情说三遍！！！不要升级linux发行版！！！重要的事情说三遍！！！不要升级linux发行版！！！重要的事情说三遍！！！为什么？不要问我为什么。我按系统提示升级ubuntu到18.04LTS后，就再也进不去系统了。不信你可以尝试一下，你将会看到如下界面：注：图片来自网络，我就不再为了截图而踩一次坑了。不仅是图形界面，命令行界面也进不去了（据说可以在重启时选择recovery mode并且狂按回车强行进入界面，但我失败了）。不过如果你真的尝试了并且掉坑里了的话，没关系，你获得了一个很好的重装系统的锻炼机会。下面我们就按步骤锻炼一下。本文参考自：https://blog.csdn.net/Spacegene/article/details/86659349准备U盘准备一个2G以上的无用的U盘，或者备份好里面的文件。然后将其格式化。镜像下载ubuntu16.04LTS镜像到本地。删除分区你可以通过控制面板中的创建并格式化硬盘分区来看到你的windows与ubuntu分区的情况。（以下操作都是针对重装，不再重新分区，需要的可以自行上网查找教程）在重新安装ubuntu16.04之前我们需要删除原先Ubuntu的EFI分区及启动引导项，这里推荐直接使用windows下的diskpart来删除。使用win+R输入diskpart打开diskpart.exe，允许其对设备进行更改。 接下来使用如下命令查看分区，我的笔记本只有一块SSD，两个系统都装在上面，故进入disk 0。 其中类型未知的便是分给ubuntu的分区，我这里有一块8G的swap分区和60G的/分区。接下来执行如下命令：1234select partition 7delete partition override #删除该分区select partition 8delete partition override #删除该分区注意：以上命令是针对我的情况，具体请按照对应ubuntu分区的序号删除。现在你可以在控制面板中的创建并格式化硬盘分区中看到你删除的分区已经合并成一块未分配的空间，这也意味着你与你原来ubuntu上的数据彻底说再见了。删除ubuntu启动引导项首先下载EasyUEFI，使用免费试用版即可。下载完成后安装，打开EasyUEFI如图： 选择管理EFI启动选项Manage EFI Boot Option，然后选择ubuntu启动引导项，点击中间的删除按钮来删除该引导项。现在重新启动，你会发现已经没有让你选择系统的引导界面，而是直接进入windows系统。制作启动U盘首先我们下载一个免费的U盘制作工具rufus。此时插入已经格式化的U盘，打开rufus，一般情况下它会自动选择U盘，你也可以在device选项下手动选择或确认。点击select，选择之前下载好的镜像文件。其他设置保留默认即可，不放心的话可以比对下图： 然后start开始制作。如果此时rufus提示需要下载一些其它文件，选择Yes继续即可。没有问题的话制作完的U盘会如图所示： 在下面的步骤中，请一直插着U盘不要拔。安装现在重新启动电脑，开机的过程中不停地快按F12进入bios界面（我的是戴尔的电脑，不同电脑按键或有不同，自行百度；如果快按不行的话再次重启试一试长按，因为网上有些教程说的是长按，而我是长按不行而快按可以）。随后选择U盘启动（不同电脑这个界面也可能不一样，具体可以百度，印象中是选择UEFI BOOT中UEFI：U盘名那项）。接下来就进入了紫红色的GNU GRUB界面，选择install ubuntu。随后就是些比较简单的安装过程，基本上可以按默认进行，因为是重装，好像不需要联网安装且有汉化包。接下来是比较重要的部分：进入安装类型installation type界面后，选择其他选项something else，这样我们就可以自己配置ubuntu分区了，点击继续。接下来会进入一个磁盘分区的界面，选中之前清出来的未分配分区（名为“空闲”，也可以通过大小来判断），点击下方+号，新建一个swap分区，大小为8G左右（一般和电脑的内存相当即可，具体还有待研究，不分这个区会有警告）。再次双击空闲分区，挂载点下拉，选择/。在安装启动引导器的设备选项中，选择Windows boot manager。结果可以参考下图：确认无误后点击现在安装，然后就一路默认直到安装完成。后期别忘了把U盘格式化回来，可以继续使用，留着做纪念也行，说不定哪天又要重装。下面我展示一下我目前的一部分美化效果，亲测发现这只会牺牲一点点儿CPU，所以并不用担心，大胆地美化就是，可能这也是使用linux发行版不多的几种乐趣之一吧。以上就是重装ubuntu的全部内容，由于是基于几天前的回忆可能会有疏漏，欢迎补充！我也会在新问题出现时及时更新。]]></content>
      <categories>
        <category>操作与使用</category>
      </categories>
      <tags>
        <tag>ubuntu</tag>
        <tag>安装教程</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ubuntu笔记：释放空间]]></title>
    <url>%2F2019%2F09%2F14%2Fubuntu%E7%AC%94%E8%AE%B0%EF%BC%9A%E9%87%8A%E6%94%BE%E7%A9%BA%E9%97%B4%2F</url>
    <content type="text"><![CDATA[前一篇讲了如何清理windows下的空间，然而虽然ubuntu中垃圾文件没win10那么多，可是我给ubuntu分配的空间比win10少得多了，于是我又找了些清理ubuntu的方法。删除apt-get下载的软件包直接在终端执行sudo apt-get autoclean删除缓存的所有软件包sudo apt-get clean删除其他软件依赖的但现在已不用的软件包sudo apt-get autoremove这条命令执行后，软件的配置文件还是会保留的。清除所有已删除包的残余配置文件dpkg -l |grep ^rc|awk &#39;{print $2}&#39; |sudo xargs dpkg -P这时候如果出现如下错误，那无需担心，因为已经不存在残余的配置文件了。 可以把上面四个命令按顺序执行一遍，就完成了对ubuntu系统的空间释放。]]></content>
      <categories>
        <category>操作与使用</category>
      </categories>
      <tags>
        <tag>命令操作</tag>
        <tag>ubuntu</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[windows笔记：释放空间]]></title>
    <url>%2F2019%2F09%2F14%2Fwindows%E7%AC%94%E8%AE%B0%EF%BC%9A%E9%87%8A%E6%94%BE%E7%A9%BA%E9%97%B4%2F</url>
    <content type="text"><![CDATA[暑假里想跑CVPR中的代码，发现作者提供的环境配置都是基于linux终端的，这样windows的git bash就满足不了我了。二话不说我花了两天时间装了个ubuntu+windows双系统，好不容易装好了，却发现我的硬盘空间已经岌岌可危（理论上要留内存的三倍左右可以保证系统顺畅运行，我的内存是8G，也就是说我C盘应空出20G左右为宜）。于是我就找了些释放空间的办法，分享在这里。利用磁盘属性进行清理这是最稳的方法，但释放的空间也相对较少，不过还是有效的。选择“此电脑”，右键C盘，属性，然后就可以在常规下面看到磁盘清理。一般按默认选择的进行清理，当然全点上勾也无所谓。也可以选择其中的清理系统文件进行进一步清理，这里面有一项是以前的windows安装文件，一般情况下不建议清除。注意：千万不能选择压缩此驱动器以节约磁盘空间！另外在工具下面你可以看到一个优化的选项，一般系统会定期自动执行优化，如果你是强迫症，时时刻刻都容不得一点冗余的话，可以手动优化。据我们数据结构的老师说，由于数据在存储时大多是稀疏矩阵，存在许多的空间浪费，而磁盘碎片整理优化的就是这个。删除临时文件这里有两个临时文件中的全部文件可以删除，一个是C:\Users\用户名\AppData\Local\Temp目录下的文件，这里是临时文件最多的地方，可以上到几个G；另一个是C:\Windows\Temp，这里文件大小相对较小，可以忽略不计。另外我也找到了一些其他的临时文件，但似乎它们的体积都是0，可能是系统自动清理了。注意：千万不要误删上一级目录！如果担心删除出错，可先放到回收站，重启之后看有无异常再做决定。亲测上述两个文件夹中的所有文件均可删除。删除冗余更新众所周知，windows系统会自动更新，这也是许多人弃windows的一大原因，然而windows还是要用的，于是我找到了一种可以清理多余的windows更新文件的方法。首先右键左下角开始菜单，选择“Windows PowerShell（管理员）” 弹出是否允许进行更改选择“是”。输入命令dism.exe /Online /Cleanup-Image /AnalyzeComponentStore这时候会显示“推荐使用组件存储清理：是or否”，因为我前不久清过，所以这里显示为“否”，那么就别清理了。如果显示为“是”，那么进行第四步。输入命令dism.exe /online /Cleanup-Image /StartComponentCleanup这样电脑就会开始清理啦。这个过程会比较长，不要着急和担心，在这期间你可以做些别的事情，比如看看我其他的几篇博客。重装系统俗话说得好“大力出奇迹”，重装系统无疑是最有效清理空间的一种方法。只要备份好数据，重装其实没有想象的那么困难。我本人这一年多来就重装过3次系统(两次是被迫的…)，其实装了几次就熟练了，我曾看到某linux大牛（忘了是谁）总共重装了19次linux。你可以在我的另一篇博文ubuntu笔记：重装ubuntu——记一次辛酸血泪史中看到如何在双系统情况下重装ubuntu的过程。]]></content>
      <categories>
        <category>操作与使用</category>
      </categories>
      <tags>
        <tag>命令操作</tag>
        <tag>windows</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[html笔记：制作web时的一些小技巧与小问题]]></title>
    <url>%2F2019%2F09%2F14%2Fhtml%E7%AC%94%E8%AE%B0%EF%BC%9A%E5%88%B6%E4%BD%9Cweb%E6%97%B6%E7%9A%84%E4%B8%80%E4%BA%9B%E5%B0%8F%E6%8A%80%E5%B7%A7%E4%B8%8E%E5%B0%8F%E9%97%AE%E9%A2%98%2F</url>
    <content type="text"><![CDATA[大一有一段时间，我沉迷于web前端制作网页，比较熟练地掌握了html的语法，还根据需要接触了一些CSS以及js的内容。说白了，html只是一种标记语言（不属于编程语言），但是它简单易学，且很容易获得可视化的效果，对于培养兴趣而言我感觉是很有帮助的。油管up主，现哈佛在读学霸John Fish（请科学上网）当初就是从html进入计算机世界的。下面贴一个我自己做的网页，是综合web三大语言编写的，大一的时候把自己需要的网站都放上面了，也有一种归属感吧。主页上那个是python之禅，也是我很喜欢的一段文字，在python环境下import this就可以看到。由于上面的网页是我学web的时候边看书边编的，各种元素都尝试了一下，最后也没有美化一直到现在，所以大佬们勿喷哈。小技巧我这里强烈推荐使用VScode写前端，它有很多强大的插件，我这里推荐其中一个吧。如介绍所写，使用alt+B快捷键可以直接在默认浏览器下查看你写的网页，而shift+alt+B可以选浏览器查看，因为有些时候microsoft自带的edge浏览器无法实现你编写的效果（巨坑），推荐使用chrome打开浏览。使用这个插件能让你更快捷地预览你编写的效果并进行修改，大大提高了效率。其他的插件网上有很多推荐，也等待着你自己去发现，这里就不一一列出了。还有一个快捷的操作就是快速生成代码块，在VScode中是这样操作的（其他编辑器也应该类似）：输入一个！：按tab键或者回车： 这样就可以节省很多时间，非常方便。小问题在我想使用web来打开我本地的txt文件时，我遇到过这样一个问题：打开的中文文档在浏览器中显示为乱码。在尝试其他浏览器后，我发现这不是浏览器的问题。最后我大致找到了两种解决办法：一种是找到head下面的meta charset，修改代码如下：另一种是另存为文件时修改一下格式，这里我们修改成“UTF-8”。另外我室友在使用python导入文件的时候也因为格式导致报错，修改成ANSI后即可。 希望通过上面两种方法的尝试能让你解决乱码问题，几种编码格式的区别在这里暂不说明，以后有空补上。]]></content>
      <categories>
        <category>编程及环境</category>
      </categories>
      <tags>
        <tag>前端</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[anaconda笔记：conda的各种命令行操作]]></title>
    <url>%2F2019%2F09%2F13%2Fanaconda%E7%AC%94%E8%AE%B0%EF%BC%9Aconda%E7%9A%84%E5%90%84%E7%A7%8D%E5%91%BD%E4%BB%A4%E8%A1%8C%E6%93%8D%E4%BD%9C%2F</url>
    <content type="text"><![CDATA[anaconda是一个开源的包、环境管理器，可以比较有效地配置多个虚拟环境，当python入门到一定程度时，安装anaconda是很必要的。前段时间室友学习python的时候问到过我一些相关的问题，我就在这里简单写一些我知道的以及我搜集到的知识。环境变量安装anaconda过程中一个很重要的步骤就是配置环境变量，网上有很多手动添加环境变量的教程，其实很简单，只需添加三个路径，当然更简单的是直接在安装的时候添加到path（可以无视warning）。我想在这里写的是环境变量的概念问题，其实直到不久前帮同学安装我才明白。环境变量是指在操作系统中用来指定操作系统运行环境的一些参数。当要求系统运行一个程序而没有告诉它程序所在的完整路径时，系统除了在当前目录下面寻找此程序外，还会到path中指定的路径去找。这就是为什么不添加C:\Users\用户名\Anaconda3\Scripts到path就无法执行conda命令，因为此时conda.exe无法被找到。conda与pip利用conda install与pip install命令来安装各种包的过程中，想必你也对两者之间的区别很疑惑，下面我就总结一下我搜集到的相关解答。简而言之，pip是python包的通用管理器，而conda是一个与语言无关的跨平台环境管理器。对我们而言，最显着的区别可能是这样的：pip在任何环境中安装python包，conda安装在conda环境中装任何包。因此往往conda list的数量会大于pip list。要注意的是，如果使用conda install多个环境时，对于同一个包只需要安装一次，有conda集中进行管理。但是如果使用pip，因为每个环境安装使用的pip在不同的路径下，故会重复安装，而包会从缓存中取。总的来说，我推荐尽早安装anaconda并且使用conda来管理python的各种包。升级我们可以在命令行中或者anaconda prompt中执行命令进行操作。123conda update conda #升级condaconda update anaconda #升级anaconda前要先升级condaconda update --all #升级所有包在升级完成之后，我们可以使用命令来清理一些无用的包以释放一些空间：12conda clean -p #删除没有用的包conda clean -t #删除保存下来的压缩文件（.tar）虚拟环境conda list命令用于查看conda下的包，而conda env list命令可以用来查看conda创建的所有虚拟环境。下面就简述一下如何创建这些虚拟环境。使用如下命令，可以创建一个新的环境：conda create -n Python27 python=2.7其中Python27是自定义的一个名称，而python=2.7是一个格式，可以变动等号右边的数字来改变python环境的kernel版本，这里我们安装的是python2.7版本（将于2020年停止维护）。在anaconda prompt中，我们可以看到我们处在的是base环境下，也就是我安装的python3环境下，我们可以使用下面两个命令来切换环境：在创建环境的过程中，难免会不小心取了个难听的环境名，别担心，我们有方法来删除环境。conda remove -n 难听的名字 --all有时候一个环境已经配置好了，但我们想要重命名，这怎么办呢？可以这样办：12conda create -n 新名字 --clone 老名字conda remove -n 老名字 --all把环境添加到jupyter notebook首先通过activate进入想要添加的环境中，然后安装ipykernel，接下来就可以进行添加了。12pip install ipykernelpython -m ipykernel install --name Python27 #Python27可以取与环境名不一样的名字，但方便起见建议统一我们可以使用如下命令来查看已添加到jupyter notebook的kernel：jupyter kernelspec list显示如下：我们也可以在jupyter notebook中的new或者kernel下查看新环境是否成功添加。在这里我想说明一下为什么要分开python的环境。由于python是不向后兼容的，分开环境可以避免语法版本不一引起的错误，同时这也可以避免工具包安装与调用的混乱。]]></content>
      <categories>
        <category>编程及环境</category>
      </categories>
      <tags>
        <tag>命令操作</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[jupyter notebook笔记：一些细小的操作]]></title>
    <url>%2F2019%2F09%2F13%2Fjupyter-notebook%E7%AC%94%E8%AE%B0%EF%BC%9A%E4%B8%80%E4%BA%9B%E7%BB%86%E5%B0%8F%E7%9A%84%E6%93%8D%E4%BD%9C%2F</url>
    <content type="text"><![CDATA[首先强烈安利jupyter notebook，它是一种交互式笔记本，安装anaconda的时候会一并安装，下载VS的时候也可以选择安装。我是在初学机器学习的时候接触jupyter notebook的，立刻就被它便捷的交互与结果呈现方式所吸引，现在python编程基本不使用其他的软件。其实完全可以在初学python的时候使用jupyter notebook，可以立即得到反馈以及分析错误，可以进步很快！打开操作当初安装好之后还不了解，每次打开jupyter notebook都会先弹出一个黑框框，这时候千万别关掉，等一会就能来到网页。另外打开之后也别关掉，使用的时候是一直需要的，因为只有开着才能访问本机web服务器发布的内容。另外你也可以不通过快捷方式，直接在命令行中直接输入jupyter notebook来打开它。有些时候，当你插入硬盘或者需要直接在特定的目录下打开jupyter notebook（它的默认打开是在“usr/用户名/”路径下），那你可以在输入命令的后面加上你想要的路径。123jupyter notebook D:\ #打开D盘，于我是我的移动硬盘jupyter notebook E:\ #我的U盘jupyter notebook C:\Users\用户名\Desktop #在桌面打开快捷键操作在jupyter notebook中可以通过选中cell然后按h的方式查询快捷键。其他在jupyter notebook中可以直接使用markdown，这对学习可以起到很大的辅助作用，markdown的基本操作可以看我的另一篇博文markdown笔记：markdown的基本使用此外，在我的博文anaconda笔记：conda的各种命令行操作中，也介绍了如何将python的虚拟环境添加到jupyter notebook中，欢迎阅读。]]></content>
      <categories>
        <category>编程及环境</category>
      </categories>
      <tags>
        <tag>命令操作</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[markdown笔记：markdown的基本使用]]></title>
    <url>%2F2019%2F09%2F13%2Fmarkdown%E7%AC%94%E8%AE%B0%EF%BC%9Amarkdown%E7%9A%84%E5%9F%BA%E6%9C%AC%E4%BD%BF%E7%94%A8%2F</url>
    <content type="text"><![CDATA[既然要写技术博客，那么markdown肯定是必备的了，这篇文章就来介绍一下markdown的基本使用操作。本文参考自：https://www.jianshu.com/p/191d1e21f7ed介绍Markdown是一种可以使用普通文本编辑器编写的标记语言，其功能比纯文本更强，因此许多程序员用它来写blog。在这里我先推荐一款markdown编辑器——typora，大家可以免费下载使用。注意在我刚开始使用markdown的时候总是跳进这个坑，在这里提上来提醒一下，在使用markdown标记后要添加文字时，需要在相应标记后空一格，否则标记也会被当作文本来处理，例如我输入“#####错误”时：错误正确的做法是输入“##### 正确”：正确一种简单的判别方法就是使用IDE，这样对应的标记就会有语法高亮。使用标题话不多说，直接示范：123456# 这是一级标题## 这是二级标题### 这是三级标题#### 这是四级标题##### 这是五级标题###### 这是六级标题效果如下：这是一级标题这是二级标题这是三级标题这是四级标题这是五级标题这是六级标题字体还是直接示范：1234**这是加粗的文字***这是倾斜的文字*`***这是斜体加粗的文字***~~这是加删除线的文字~~这是加粗的文字这是倾斜的文字这是斜体加粗的文字这是加删除线的文字引用1234&gt;我引用&gt;&gt;我还引用&gt;&gt;&gt;我再引用&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;扶我起来，我还能继续引用！我引用我还引用我再引用扶我起来，我还能继续引用！引用是可以嵌套的，可以加很多层，我一般使用一个&gt;来表示额外的需要注意的内容。另外，如果想让下一段文字不被引用，需要空一行。分割线分割线使用三个及以上的-或*就可以。有时候用---会造成别的文字的格式变化，因此我在使用VScode编辑时，如果看到---被高亮（分割线正常其作用时应该不高亮），就会改用***。12---***效果如下：图片markdown中添加图片的语法是这样的：![显示在图片下方的文字]（图片地址 &quot;图片title&quot;）其中title可加可不加，它就是鼠标移动到图片上时显示的文字。然而我在使用hexo搭建我的个人博客的过程中，遇到了使用上述语法图片却无法显示的情况，因此我改用了下列标签插件：1&#123;% asset_img xxxxx.xxx 图片下方的名字 %&#125;其中xxxxx.xxx只需直接输入图片名称以及格式即可，因为我使用了hexo-asset-image插件，它可以在_posts文件中创建与博文名称相同的对应的文件夹，只需把图片移入即可。其安装命令：npm install hexo-asset-image --save也可用cnpm更快地安装：cnpm install hexo-asset-image --save超链接由于我希望在新的页面打开链接，而似乎markdown本身的语法不支持在新标签页打开链接，因此我推荐直接使用html语言来代替。&lt;a href=&quot;超链接地址&quot; target=&quot;_blank&quot;&gt;超链接名&lt;/a&gt;列表无序列表123- 列表内容+ 列表内容* 列表内容列表内容列表内容列表内容有序列表1231. 列表内容2. 列表内容3. 列表内容列表内容列表内容列表内容可以看到，上面显示的列表是有嵌套的，方法就是敲三个空格缩进。表格1234表头|表头|表头---|:--:|---:内容|内容|内容内容|内容|内容表头|表头|表头—-|:—:|—-:内容|内容|内容内容|内容|内容其中第二行的作用分割表头和内容，-有一个就行，为了对齐可多加几个。此外文字默认居左，有两种改变方法：两边加：表示文字居中。右边加：表示文字居右。代码最后的最后，是我最喜欢ctrl+C+V的代码了。单行或句中代码输入方式：1`来复制我呀`显示：来复制我呀其中`在键盘的左上角，我当初找了好久。多行代码块的写法就是用上下两对```围住。好了于是你现在就可以自由的复制粘贴啦。]]></content>
      <categories>
        <category>操作与使用</category>
      </categories>
      <tags>
        <tag>markdown</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[hexo笔记：开始创建个人博客——方法及原因]]></title>
    <url>%2F2019%2F09%2F13%2Fhexo%E7%AC%94%E8%AE%B0%EF%BC%9A%E5%BC%80%E5%A7%8B%E5%88%9B%E5%BB%BA%E4%B8%AA%E4%BA%BA%E5%8D%9A%E5%AE%A2%E2%80%94%E2%80%94%E6%96%B9%E6%B3%95%E5%8F%8A%E5%8E%9F%E5%9B%A0%2F</url>
    <content type="text"><![CDATA[大家好，这是我的第一篇博文，这也是我的第一个自己搭建的网站，既然搭了，那第一篇就讲讲我搭建的过程吧。安装步骤安装node.js进入官网。选择对应系统（我这里用win10），选择LTS（长期支持版本）安装，安装步骤中一直选择next即可。安装完后就可以把安装包删除了。安装git进入官网。选择对应系统的版本下载，同样也是按默认安装。安装成功后，你会在开始菜单中看到git文件夹。 其中Git Bash与linux和mac中的终端类似，它是git自带的程序，提供了linux风格的shell，我们可以在这里面执行相应的命令。注意：bash中的复制粘贴操作与linux中类似，ctrl+C用于终止进程，可以用鼠标中键进行粘贴操作。不嫌麻烦的话可以使用ctrl+shift+C和ctrl+shift+V进行复制粘贴操作。安装hexohexo是一个快速、简洁且高效的博客框架，在这里我们使用hexo来搭建博客。首先，新建一个名为“blog”的空文件夹，以后我们的操作都在这个文件夹里进行，可以在bash中使用pwd命令查看当前所处位置。创建这个文件夹的目的是万一因为创建的博客出现问题或者不满意想重来等原因可以直接简单地把文件夹删除，也方便了对整个网站本地内容的移动。打开新建的文件夹，右键空白处，选择Git Bash Here。 接下来我们输入两行命令来验证node.js是否安装成功。 如出现如图所示结果，则表明安装成功。为了提高以后的下载速度，我们需要安装cnpm。cnpm是淘宝的国内镜像，因为npm的服务器位于国外有时可能会影响安装。继续在bash中输入如下命令安装cnpm：npm install -g cnpm --registry=https://registry.npm.taobao.org检验安装是否成功，输入cnpm： 接下来我们安装hexo，输入命令：cnpm install -g hexo-cli和上面一样，我们可以用hexo -v来验证是否成功安装hexo，这里就不贴图了。接下来我们输入如下命令来建立整个项目：hexo init你会发现你的文件夹中多了许多文件，你也可以用ls -l命令来看到新增的文件。 完成本地环境的搭建至此，我们已经完成了本地环境的搭建，在这里，我想先介绍hexo中常用的命令。hexo n &quot;文章标题&quot;用于创建新的博文（欲删除文章，直接删除md文件并用下面的命令更新即可）。hexo shexo会监视文件变动并自动更新，通过所给的localhost:4000/就可以直接在本地预览更新后的网站了。部署到远端服务器三步曲：123hexo clean #清除缓存，网页正常情况下可以忽略此条命令，执行该指令后，会删掉站点根目录下的public文件夹。hexo g #generate静态网页（静态网页这里指没有前端后端的网页而不是静止），该命令把md编译为html并存到public文件目录下。hexo d #将本地的更改部署到远端服务器（需要一点时间，请过一会再刷新网页）。此外，上面最后两步也可以使用hexo g -d直接代替。如果出现ERROR Deployer not found: git报错，可以使用npm install --save hexo-deployer-git命令解决。注意：由于部署到远端输入密码时密码不可见，有时候会导致部署失败，只有出现INFO Deploy done: git的结果才表明部署成功，否则再次部署重输密码即可。现在我们在bash中运行hexo s，打开浏览器，输入localhost:4000/，就可以看到hexo默认创建的页面了。部署到远端服务器为了让别人能访问到你搭建的网站，我们需要部署到远端服务器。这里有两种选择，一种是部署到github上，新建一个repository，然后创建一个xxxxx.github.io域名（这里xxxxx必须为你的github用户名）。另一种选择是部署到国内的coding，这是考虑到访问速度的问题，不过我选择的是前者，亲测并没感觉有速度的困扰。个人比较推荐用github pages创建个人博客。部署这块网上有许多教程，这里不详细解释了，以后有机会补上。在部署的时候涉及到对主题配置文件的操作，linux和mac用户可以使用vim进行编辑，不过也可以使用VScode、sublime等代码编辑器进行操作。注：为了国内的访问速度，我最后添加了coding/github双线部署，两者的操作方式大同小异。值得注意的是，如果使用的是leancloud的阅读量与评论统计系统，那么还得在leancloud的安全中心中添加coding的web域名。创建原因首先说明，我只是一个刚起步的入门级小白，懂得不多，别喷我哈~步入大二，虽然我是大学才算真正接触编程，但一年多下来我也接触并且学习了不少技术知识。接触的多了、遇到的问题也复杂了起来，导致每次百度到的答案不一定能够解决我遇到的问题。此外，之前在学习编程语言、操作系统、ml、dl等知识的时候，为方便起见利用文本记了些笔记。然而笔记分散在四处，不方便管理与查看，因此就萌生了写博客的想法。由于个人比较喜欢自由DIY，所以没有使用CSDN、博客园等知名技术博客网站。最后还是非常感谢我们华科的校友程序羊在b站和其他站点上分享的各种经验，我就是通过他的视频来搭建起自己的第一个博客网站的。他的其他视频也给了我很多启迪。最后，最关键的原因，还是因为今天中秋节有空闲的时间哈哈，祝大家节日快乐！]]></content>
      <categories>
        <category>操作与使用</category>
      </categories>
      <tags>
        <tag>hexo</tag>
        <tag>安装教程</tag>
      </tags>
  </entry>
</search>
