<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[matlab笔记：安装MinGW编译器]]></title>
    <url>%2Fmatlab20200115222641%2F</url>
    <content type="text"><![CDATA[因为目标追踪领域最著名的比赛VOT（Visual Object Tracking），同时也拥有一个非常重要的数据集和一套比较权威的评价指标，基于的是matlab，因此我又开始用起了matlab（这么看下来貌似matlab要成我本学期用的最多的语言了）。当我download官方的toolkit之后，按着document一路比较顺利地操作了下来，结果突然遇到一个报错，说我没有C和C++编译器。WTF？我用了这么久居然都没发现这个问题…本以为按照提示就能很快解决，结果这个问题折腾了我一整个晚上。好吧既然被折磨得这么惨那我还是本着逢血泪必写博的原则在这里写一下吧。不过不得不说，最后成功的时候真的还是挺爽的哈哈！本文参考自：https://ww2.mathworks.cn/help/matlab/matlab_external/compiling-c-mex-files-with-mingw.html?requestedDomain=uk.mathworks.comhttps://ww2.mathworks.cn/matlabcentral/fileexchange/52848-matlab-support-for-mingw-w64-c-c-compilerhttps://www.cnblogs.com/Vae1990Silence/p/10102375.htmlhttps://blog.csdn.net/fly910905/article/details/86222946MinGWMinGW，是Minimalist GNU for Windows的缩写。它是一个可自由使用和自由发布的Windows特定头文件和使用GNU工具集导入库的集合，允许你在GNU/Linux和Windows平台生成本地的Windows程序而不需要第三方C运行时（C Runtime）库。当初报错的时候，我也是很诧异，因为之前使用CodeBlocks和Visual Studio的时候明明是有的。而这次在matlab中编译C/C++时怎么就找不到了。因为之前使用CodeBlocks也有找不到的情况，我当时是重装了一遍CodeBlocks解决问题的，因此我上网开了一下有没有类似的方法。按道理来说已经装了VS是可以找到的，但好像也存在即使安装了VS、matlab还是找不到编译器的情况。可以使用mex看一下具体是哪些路径没有匹配上，似乎可以通过修改注册表的方法解决，但我没有尝试。注：Matlab调用C/C++的方式主要有两种：利用MEX技术和调用C/C++动态连接库。MEX是Matlab Executable的缩写，它是一种“可在Matlab中调用的C（或Fortran）语言衍生程序”。后文中还会用到。失败的方法毕竟是花了一个晚上，看了mathworks上网友们的各种solution，试错了许多方法，这里记录两个貌似要成功的方法（其实最后还是失败了），或许会有参考价值。这里有一个被许多网友强调、要注意的是：下载后的mingw文件（没错它只有15kb看上去好假）要在打开的matlab中，找到相应的下载目录，右键点击然后选择下载并安装（download and install），否则似乎会出错。禁用IPv6IPv6，顾名思义，就是IP地址的第6版协议。我们现在用的是IPv4，它的地址是32位，总数有43亿个左右，还要减去内网专用的192、170地址段，这样一来就更少了。然而，IPv6的地址是128位的，大概是43亿的4次方，地址极为丰富。网友Kshitij Mall给出了这样一个解决思路：首先打开控制面板中的编辑系统环境变量。在高级选项中，点击环境变量。在系统变量栏中添加如下两个变量：（1）variable name：“JAVA_TOOL_OPTIONS”；value：“-Djava.net.preferIPv4Stack=true”；（2）variable name：“JAVA_OPTIONS”；value：“-Djava.net.preferIPv4Stack=true”。另外根据该网友所述，安装完成后可以删去这两个环境变量。注意：仅输入引号内部的内容。java文档指示，设置jvm属性java.net.preferIPv4Stack为true时，就可以禁用IPv6。反之，若设为0，则启用。注：禁用设置时不需要重启系统。由于该网友的系统配置和我相同（MATLAB 2019a on Windows 10 system with 64 bits），于是我毫不犹豫地首先尝试了他的方法，不知道时卡进去的原因还是为何，我成功看到了协议界面（原本一直卡在附加功能管理器加载的空白界面），但是最终开始下载支持包失败。关闭防火墙失败之后，我继续看评论，看到一个网友感谢另一个网友提供的solution，我非常激动，感觉自己也要跟着解决了。似乎好像取得了一定的进展，但是在下载第三方包的时候还是卡住了（3rt party download error），我浏览了大多数网友的评论，貌似大家基本上都卡在了这里。成功的方法当我快要绝望的时候，突然看到了感谢三连，都是感谢同一个人的。这已经是2018年的一个回复了，看评论发现他们使用的是Windows 7系统，但我决定还是试一下，结果真的成功了，非常感谢最初的solution提供者pawan singh！具体方法如下：到这个网址下载合适的TDM-GCC。下载之后，create一个新的到设定的安装路径中。注意：根据matlab文档。MinGW的安装文件夹名称不能包含空格。例如，不要使用：C:\Program Files\mingw-64。应改用：C:\mingw-64。我建议直接装在C盘下面，默认似乎也是这样，维持不变即可。与之前修改系统变量方式类似。添加新的系统变量名为MW_MINGW64_LOC，值为MinGW-w64编译器的安装位置，于我是C:\TDM-GCC-64。最后别忘了确定设置。在matlab命令行内执行命令：setenv(&#39;MW_MINGW64_LOC&#39;, &#39;path&#39;)，folder为TDM-GCC的安装位置，要加单引号。例如我是：setenv(&#39;MW_MINGW64_LOC&#39;, &#39;C:\TDM-GCC-64&#39;)。可以继续在命令行中执行命令：mex -setup。若没有报错，则表明成功了。]]></content>
      <categories>
        <category>环境配置</category>
      </categories>
      <tags>
        <tag>matlab</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[python笔记：pandas基本使用]]></title>
    <url>%2Fpython20200113202851%2F</url>
    <content type="text"><![CDATA[在python中，Pandas可以说是最实用的库之一，它提供了非常丰富的数据读写方法。可以看一下Pandas官方文档中对所有I/O函数的总结。Pandas是一个开源的，BSD许可的库，为python提供高性能、易于使用的数据结构和数据分析工具。它的使用基础是Numpy（提供高性能的矩阵运算）；可以用于数据挖掘和数据分析，同时也提供数据清洗的功能。本文就对Pandas的基本使用做一个简单的归纳。本文参考自：https://www.cnblogs.com/chenhuabin/p/11477076.htmlhttps://blog.csdn.net/weixin_39791387/article/details/81487549https://kanoki.org/2019/09/16/dataframe-visualization-with-pandas-plot/csv这里我想介绍一下一种新的数据格式：csv。它和excel很像，但又不同于excel。csv主要有如下特点：纯文本，使用某个字符集，比如ASCII、Unicode、EBCDIC或GB2312（简体中文环境）等；由记录组成（典型的是每行一条记录）；每条记录被分隔符（英语：Delimiter）分隔为字段（英语：Field (computer science)）（典型分隔符有逗号、分号或制表符；有时分隔符可以包括可选的空格）；每条记录都有同样的字段序列。在Pandas的使用以及AI相关竞赛数据集、结果的存储与使用中，csv文件往往承担着主角的位置。准备在具体使用之前，别忘了先导入所需相应的库。12import pandas as pdimport numpy as np读取无论是txt文件还是csv文件，在Pandas中都使用read_csv()读取，当然也使用同一个方法写入到文件，那就是to_csv()方法。1df = pd.read_csv(abs_path) #此为绝对路径为了提供更加多样化、可定制的功能，read_csv()方法定义了数十个参数，还在大部分参数并不常用，以下是几个比较常用的参数：filepath_or_buffer：文件所在路径，可以是一个描述路径的字符串、pathlib.Path对象、http或ftp的连接，也可以是任何可调用read()的对象。这是唯一一个必传的参数，也就是上面的abs_path。encoding：编码，字符型，通常为utf-8，如果中文读取不正常，可以将encoding设为gbk。当然，也可以直接将对应文件改成utf-8编码。header：整数或者由整数组成的列表，用来指定由哪一行或者哪几行作为列名，默认为header = 0，表示用第一列作为列名。若设置header = 0，则指定第二列作为列名。要注意的是，当指定第一行之后的数据作为列名时，前面的所有行都会被略过。也可以传递一个包含多个整数的列表给header，这样每一列就会有多个列名。如果中间某一行没有指定，那么该行会被略过。例如header = [0, 2]，则原本的第二行会被省去。而当文件中没有列名一行数据时，可以传递header = None，表示不从文件数据中指定行作为列名，这时Pandas会自动生成从零开始的序列作为列名。names：接着上面的header，很快就想到是不是可以自己设置列名。names就可以用来生成一个列表，为数据额外指定列名。例如：df = pd.read_csv(&#39;abs_path, names=[&#39;第一列&#39;, &#39;第二列&#39;, &#39;第三列&#39;, &#39;第四列&#39;])。在数据读取完毕之后，我们可以使用如下代码来快速查看数据是否正确地导入了。1234df.head() #看一下导入后df（DataFrame）的前几行，可在括号内输入数字来设定具体显示几行，默认5行df.tail() #类似，查看后几行type(df) #查看类型，DataFrame的输出应该是pandas.core.frame.DataFrameDataFrameDataFrame是Pandas中的一个表格型的数据结构，包含有一组有序的列，每列可以是不同的值类型（数值、字符串、布尔型等），DataFrame即有行索引也有列索引，可以被看做是由Series组成的字典。事实上，Pandas中的DataFrame的操作，有很大一部分跟numpy中的二维数组的操作是近似的。在上面的读取处理之后，我们下面对其进行一些简单的操作：查看12345678910111213141516171819 df.head() #上文已提及 df.tail() #查看列名 print(df.columns) #查看索引 print(df.index) #查看各列的数据格式 print(df.dtypes) #查看整个DataFrame的属性信息 print(df.info()) #访问对应行 df.loc[0] #这里访问了第一行，将显示列名和对应每一列第一行的数据 #具体有关索引请看后文2. ## 筛选在numpy中，我们可以这样判断一个数组中每一个数和对应数值的比较结果：12a = np.array(range(10))a &gt; 3输出将是一串布尔型（True、False）的array。而在DataFrame中，我们可以用类似的方法通过指定列来进行筛选：12#筛选第二列中数值大于80df[df.第二列 &gt; 80]这样就会得到只用符合条件数据的对应行的一个DataFrame。我们也可以使用df[(df.第一列 &gt; 80) &amp; (df.第二列 &gt; 80) &amp; (df.第三列 &gt; 80)]来进行多条件的复杂筛选。此外，我们可以直接根据列名提取出一个新的表格：1new = df[['第一列', '第二列']] #new为仅由第一列和第二列组成的一个新的DataFrame排序可以使用如下代码根据单列或者多列的值对数据进行排序：12df.sort_values(['第二列', '第一列', '第三列'], ascending = [True, True, True])#使用df.sort_values(['第二列', '第一列', '第三列']).head()查看排序完后前几行的结果这里排序的规则是：根据设置的顺序（这里是先按第二列排），从小到大升序对所有数据进行排序。其中ascending是设置升序（默认True）和降序（False）。若仅选择单列，则无需添加[]，这里[]的作用是把选择的行列转换为列表。重命名如果觉得我前面取得列名称不好听，可以使用下面这个代码来改成需要的名字：1df.rename(columns = &#123;'第一列': '好听的第一列', '第二列': '好听的第二列', '第三列': '好听的第三列', '第四列': '好听的第四列',&#125;, inplace = True)这里用到了字典。索引前面提到了使用索引来查看第一行，可当没有数字索引，例如我们通过df = pd.DataFrame(scores, index = [&#39;one&#39;, &#39;two&#39;, &#39;three&#39;])把index设为one、two、three时，df.loc[0]就失效了。因此有下面几种处理方法：123456789#访问index为“one”的行df.loc['one']#访问实实在在所谓的第几行（无论index为何）df.iloc[0] #注意0指的是第一行#ix合并了loc和iloc的功能，当索引为数字索引的时候，ix和loc是等价的df.ix[0] #访问第一行df.ix['one'] #访问“one”行，这里也指的是第一行切片类似的，DataFrame也支持切片操作，但还是需要注意的。这里总结两种切片方式：利用索引即使用df.loc[:2]ordf.ix[:2]等索引方式，这里这样的话输出为前三行。直接切片这种方法只能在访问多行数据时使用，例如df[:2]将输出前两行，注意，这里比上面的方法要少一行。此外，值得强调的是，用这种方法访问单行数据是禁止的，例如不能使用df[0]来访问第一行数据。数组我们可以使用df.第一列.values以array的形式输出指定列的所用值。基于此，我们可以使用df.第一列.value_counts()来做简单的统计，也就是对该列中每一个出现数字作频次的统计。我们还可以直接对DataFrame做计算，例如：df * n（n为具体数值），结果就是对表中的每一个数值都乘上对应的倍数。元素操作map函数map()是python自带的方法, 可以对DataFrame某列内的元素进行操作。下面是一种使用实例：1234567891011def func(grade):if grade &gt;= 80: return "A"elif grade &gt;= 70: return "B"elif grade &gt;= 60: return "C"else: return "D"df['评级'] = df.第一列.map(func)这样DataFrame后面会自动添加一列名为“评级”，并根据第一列来生成数据填入。apply函数当我们需要进行根据多列生成新的一个列的操作时，就需要用到apply。其用法简单示例如下：1df['求和'] = df.apply(lambda x: x.第一列 + x.第二列, axis = 1)applymap函数applymap时对dataframe中所有的数据进行操作的一个函数，非常重要。例如，我要让之前所用的score和grade都变成scroe+或者grade+，那么我就可以这样：1df.applymap(lambda x: str(x) + '+')如果是成绩单的话，那么这样操作之后打出来就会好看些啦，哈哈。plot数据可视化本来是一个非常复杂的过程，但Pandas数据帧plot函数的出现，使得创建可视化图形变得很容易。这个函数的具体使用可以访问文首给出的第三个参考链接，为一个印度小哥利用kaggle上的数据对df.plot()做的一个非常详尽的介绍。有机会的话我会结合matplotlib对其做一个搬运与总结，先留个坑。写入通过to_csv()可以将Pandas数据写入到文本文件中，和读取read_csv()类似，它也有几个常用参数：path_or_buf：表示路径的字符串或者文件句柄，也是必需的。例如：df.to_csv(abs_path)。要注意的是，这里如果abs_path对应的文件不存在，则会新建abs_path的同名文件后再写入，如果本来已存在该文件，则会自动清空该文件后再写入。sep：分隔符，默认为逗号。当写入txt文件时，就需要这个参数来确定数据之间的分隔符了。header：元素为字符串的列表或布尔型数据。当为列表时表示重新指定列名，当为布尔型时，表示是否写入列名。这和读取时的使用基本类似。columns：后接一个列表，用于重新指定写入文件中列的顺序。例如：df.to_csv(abs_path, columns = [&#39;第四列&#39;, &#39;第二列&#39;, &#39;第三列&#39;, &#39;第一列&#39;])。index_label：字符串或布尔型变量，设置索引列列名。原本的索引是空的，使用这个参数就可以给索引添加一个列名。如果觉得不需要添加，同时空着不好看（空的话还是会有分隔符），可以设置为False去掉（同时也将不显示分隔符）。index：布尔型，是否写入索引列，默认为True。encoding：写入时所用的编码，默认是utf-8。这个和上述的许多参数其实保持默认即可。匿名函数在上面DataFrame一节的最后，我用到了两个匿名函数，这里我想举个例子来简单展示一下匿名函数的使用方法，的确很好用！当我们对一个数进行操作时，若使用函数，一般会：12def func(number): return number + 10这样看上去就有点费代码了，因此有下面的等价匿名函数可以替代：1func = lambda number: number + 10当然假如想追求代码行数的话也不拦着你~]]></content>
      <categories>
        <category>程序与设计</category>
      </categories>
      <tags>
        <tag>代码实现</tag>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[deep learning笔记：记首次ResNet实战]]></title>
    <url>%2Fdeep-learning20200113174731%2F</url>
    <content type="text"><![CDATA[实践出真知。在之前的博文deep-learning笔记：使网络能够更深——ResNet简介与pytorch实现中，我对何凯明大神的CVPR最佳论文中提出的残差网络做了简单介绍。而就在第二年（2016年），何凯明的团队就发表了“Identity Mappings in Deep Residual Networks”这篇文章，分析了ResNet成功的关键因素——residual block背后的算法，并对residual block以及after-addition activation进行改进，通过一系列的ablation experiments验证了，在residual block和after-addition activation上都使用identity mapping（恒等映射）时，能对模型训练产生很好的效果。不知道为什么，我今天从arXiv上download这篇paper的时候发现上不去了，莫非现在上arXiv也要科学上网了？本次实战主要是基于之前的ResNet实现和flyAI平台，并结合上面提到的何凯明团队分析ResNet的论文做出一些改进，并检验效果。本文参考自：https://blog.csdn.net/Sandwichsauce/article/details/89162570https://www.jianshu.com/p/184799230f20https://blog.csdn.net/wspba/article/details/60572886https://www.cnblogs.com/4991tcl/p/10395574.htmlhttps://blog.csdn.net/DuinoDu/article/details/80435127ablation experiments在上面我提到了这个名词，中文翻译是“消融实验”。或许在阅读论文的过程中会接触到这个名词，如果仅根据字面翻译的话或许会很纳闷。在查找了一定的资料后，我对这种方法有了大致地了解。ablation的原本释义是通过机械方法切除身体组织，如手术，从身体中去除尤指器官以及异常生长的有害物质。事实上，这种方法类似于物理实验中的控制变量法，即当在一个新提出的模型中同时改变了多个条件或者参数，那么为了分析和检验，在接下去的消融实验中，会一一控制每个条件或者参数不变，来根据结果分析到底是哪个条件或者参数对模型的优化、影响更大。在机器学习、特别是复杂的深度神经网络的背景下，科研工作者们已经采用“消融研究”来描述去除网络的某些部分的过程，以便更好地理解网络的行为。ResNet的分析与改进残差单元在2015年ResNet首次发布的时候，设计的残差单元在最后的输出之前是要经过一个激活函数的。而在2016年新提出的残差单元中，去掉了这个激活函数，并通过实验证明新提出的残差单元训练更简单。 这种新的构造的关键在于不仅仅是在残差单元的内部，而是在整个网络中创建一个“直接”的计算传播路径来分析深度残差网络。通过构造这样一个“干净”的信息通路，可以在前向和反向传播阶段，使信号能够直接的从一个单元传递到其他任意一个单元。实验表明，当框架接近于上面的状态时，训练会变得更加简单。shortcut对于恒等跳跃连接$h(x_{l})=x_{l}$，作者设计了5种新的连接方式来与原本的方式作对比，设计以及实验结果如下所示： 其中fail表示测试误差超过了20%。实验结果表明，原本的连接方式误差衰减最快，同时误差也最低，而其他形式的shortcut都产生了较大的损失和误差。作者认为，shortcut连接中的操作 (缩放、门控、1×1的卷积以及dropout) 会阻碍信息的传递，以致于对优化造成困难。此外，虽然1×1的卷积shortcut连接引入了更多的参数，本应该比恒等shortcut连接具有更加强大的表达能力。但是它的效果并不好，这表明了这些模型退化问题的原因是优化问题，而不是表达能力的问题。激活函数对于激活函数的设置，作者设计了如下几种方式进行比较： 在这里，作者将激活项分为了预激活（pre-activation）和后激活（post-activation）。通过实验可以发现，将ReLU和BN都放在预激活中，即full pre-activation最为有效。ResNet实战根据论文中的实验结果，我使用了新的残差模块进行实践。并结合在deep-learning笔记：学习率衰减与批归一化中的分析总结对BN层的位置选取作了简单调整。在本次实验中，我尝试使用了StepLR阶梯式衰减和连续衰减两种学习率衰减方式，事实证明，使用StepLR阶梯式衰减的效果在这里要略好一些（连续衰减前期学得太快，后面大半部分都学不动了…）。首次训练的结果并不理想，于是我加大了学习率每次衰减的幅度，即让最后阶段的学习率更小，这使我的模型的评分提高了不少。由于训练资源有限，我没能进行更深（仅设置了10层左右）、更久（每次仅进行20个epoch）的训练，但在每个batch中，最高的accuracy也能达到65%左右，平均大约能超过50%。相比之前使用浅层网络仅能达到20%左右的accuracy，这已经提升不少了。然而最终的打分还是没有显著提高，因此我思考是否存在过拟合的问题。为此我尝试着在全连接层和捷径连接中加入dropout正则化来提高在测试集中的泛化能力，结果最终打分仅提高了0.1，而训练时间稍短。由于我除了dropout之外并没有改变网络的层数等影响参数量的因素，因此似乎与何大神在论文中original版和dropout版shortchut的比较有一些矛盾，但的确还是说明了dropout在这里的作用微乎其微，优化模型时可以排除至考虑范围之外了。遇到的问题TabError: inconsistent use of tabs and spaces in indentation当我在flyAI提供的窗口中修改代码并提交GPU训练时，就出现了这个报错。它说我在缩进时错误的使用了制表符和空格。于是我只好把报错处的缩进删除并重敲tab缩进，问题就得到了解决。如果使用PyCharm等IDE的话，这个错误会直接显示出来，即在缩进处会有灰色的颜色警告，将光标移过去就会有具体报错。这就省得提交GPU之后才能收到报错，所以以后写代码、改代码能用IDE还是用起来好啦。RuntimeError: one of the variables needed for gradient computation has been modified by an inplace operation这是在shortcut残差连接时遇到的一个报错，上网后发现原因很简单：版本问题。在新版的pytorch中，由于0.4.0之后把Varible和Tensor融合为一个Tensor，因此inplace操作在之前对Varible时还能用，但现在只有Tensor，就会出错了。解决的办法是将x += self.shortcut(x1)替换成x = x + self.shortcut(x1)。若网络很大，找起来很麻烦，可以在网络的中间变量加一句x.backward()，看会不会报错，如果不会的话，那就说明至少这之前是没毛病的。张量第一维是batch size起初，我根据输入的torch.Size([64, 1, 128, 128])，使用如下函数将输出拍平成1维的：123456def num_flat_features(self, x): size = x.size()[0:] num_features = 1 for s in size: num_features *= s return num_features同时，为了匹配，我将第一个全连接层的输入乘上了64。其实这个时候我已经开始怀疑这个64是哪来的了，为什么这个张量第一维尺度有64。直到后来平台报错，我才意识到这个表示的不是数据的维度，而是我设计的batch size。为此我将上面的代码调整如下：123456def num_flat_features(self, x): size = x.size()[1:] num_features = 1 for s in size: num_features *= s return num_features如此，问题得到解决，最终的输出应该是batch size乘上总类别数的一个张量。arXiv文前提到了上arXiv下论文要科学上网的事情，后来我发现了一个中科院理论物理所的一个备选镜像，但是据说好像不是特别稳定，不过还是先留在这里吧，万一的话可以拿来试试。]]></content>
      <categories>
        <category>人工智能</category>
      </categories>
      <tags>
        <tag>深度学习</tag>
        <tag>论文分享</tag>
        <tag>踩坑血泪</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[python笔记：打印进度]]></title>
    <url>%2Fpython20200108214052%2F</url>
    <content type="text"><![CDATA[在我们训练模型的时候，我们总希望能够直接看到训练的进度，下面我就总结几个我收集的打印进度的方法。本文参考自：https://blog.csdn.net/u013985241/article/details/86653356https://blog.csdn.net/zkp_987/article/details/81748098利用回车符打印百分比应该是最常见的方法，也是我一直使用的。不过如果简单地逐次打印百分比的话，就会占据大量的屏幕空间，甚至装不下而需要手动拖动滚动条，让人眼花缭乱。这时我就想到了利用转义符“\r”，在print完本次的进度之后，下一次直接回车将其清除覆盖，这样就达到了既不占用屏幕又清晰的目的。大致的方法如下：12345import time #这里是为了用来延时，代替训练的时间numOfTimes = 200 #总循环次数，可以是总训练数据量等，这里设为200for i in range(numOfTimes): print("\r", "progress percentage:&#123;0&#125;%".format((round(i + 1) * 100 / numOfTimes)), end = "", flush = True) time.sleep(0.02) #若前面from time import sleep，这里直接sleep(0.02)即可这里用到了python的format格式化函数，format中计算出的数值对应的位置是{0}，将在实际print的过程中被替换。此外，这里还用到了round()函数，其作用是返回浮点数的四舍五入值。关于上面在print()函数中出现的flush，文首的参考链接中已给出解释，这里做个搬运：因为print()函数会把内容放到内存中，内存中的内容并不一定能够及时刷新显示到屏幕中。而当我们使用flush = True之后，会在print结束之后，立即将内存中的东西显示到屏幕上，清空缓存。基于上述原理，flush大致有下面两个使用场景：在循环中，要想每进行一次循环体，在屏幕上更新打印的内容就得使用flush = True的参数。（我这里就是这种情况）打开一个文件，向其写入字符串，在关闭文件f.close()之前 打开文件是看不到写入的字符的。因此，如果要想在关闭之前实时地看到写入的字符串，那么就应该使用flush = True。利用tqdm库有需求就有市场，一搜果然还是有库能满足我的需求的。tqdm就是其中之一，它是一个快速，可扩展的python进度条，可以在python长循环中添加一个进度提示信息。大致用法如下：123456import tqdmimport timenumOfTimes = 200 #总循环次数，可以是总训练数据量等，这里设为200for i in tqdm.tqdm(range(numOfTimes)): time.sleep(0.02) #代替训练等耗时过程 pass也可以直接from tqdm import tqdm，这样后面就不需要tqdm.tqdm了。利用progressbar库如其名，这个库就是用来做进度条的。如果没有的话，它和tqdm都可以使用pip来安装。123456import progressbarfrom time import sleepnumOfTimes = 200 #总循环次数，可以是总训练数据量等，这里设为200progress = progressbar.ProgressBar()for i in progress(range(numOfTimes)): sleep(0.02)]]></content>
      <categories>
        <category>程序与设计</category>
      </categories>
      <tags>
        <tag>代码实现</tag>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[circuit笔记：有线双工对讲机的电子线路设计]]></title>
    <url>%2Fcircuit20200107222439%2F</url>
    <content type="text"><![CDATA[模拟电路实验是我进入大学本科以来第一个付出大量课外时间的实验课，其最后的大项目是让我们自行设计一个电路，而我们小组选择的是有线双工对讲机。今天我就简单对我们小组的设计方案做一个整理。在这里还是非常感谢组员们的共同努力和巨大帮助！本文参考自：http://www.ttic.cc/file/TDA1013B_76329.html.comhttps://wenku.baidu.com/view/c8b016e7ed630b1c58eeb520.htmlhttps://tech.hqew.com/fangan_1909806设计目的有线对讲机是用导线直接连接进行通话，而双工通信则是像电话机一样同时进行双方的“听”和“讲”。此外，我们希望可以具有音量可调节、消侧音等一些对讲机需要的功能。设计思路利用驻极体话筒将声音信号转化为微弱的电信号。通过反相比例放大器将微弱的电信号放大。利用相位抵消法实现消侧音。只使用一根传输线进行信号互传。利用电压跟随器避免远距离导线传输时衰减过大的问题。添加了低通滤波器电路，滤去高频的噪音信号。使用TDA1013B进行功率放大并将信号传输到扬声器。最后由扬声器将电信号转化成声音信号，发出声音。设计有线双工对讲机的思路可以用如下所示的系统图表示。主要由弱声音采集、前置运算放大器、消侧音电路、减小信号衰减电路、低通滤波器电路、功率放大电路、扬声器等模块组成。基本原理驻极体话筒 话筒的基本结构由一片单面涂有金属的驻极体薄膜与一个上面有若干小孔的金属电极（背称为背电极）构成。驻极体面与背电极相对，中间有一个极小的空气隙，形成一个以空气隙和驻极体作绝缘介质，以背电极和驻极体上的金属层作为两个电极构成一个平板电容器。电容的两极之间有输出电极。由于驻极体薄膜上分布有自由电荷，当声波引起驻极体薄膜振动而产生位移时；改变了电容两极板之间的距离，从而引起电容的容量发生变化，由于驻极体上的电荷数始终保持恒定，根据公式：Q =CU 所以当C变化时必然引起电容器两端电压U的变化，从而输出电信号，实现声电的变换。不管是源极输出或漏极输出，驻极体话筒必须提供直流电压才能工作，因为它内部装有场效应管。反向比例放大 相比例放大电路的原理如上图所示，输出信号电压增益为R2与R1之比，相位反相变化180°（后面会再次反相）。在本实验中，我们将两个电阻的比值调整在10~100之间，即放大比例为10~100。消侧音在模拟音频收发信号共用一个信道的对讲系统中，为减小侧音对通话效果的影响，防止侧音的干扰，所有对讲设备均需增加消侧音电路。一方面让音频发送信号按一定比例出现在传输线上，另一方面让本方音频接收电路获得的信号足够小，不至于说话者从己方喇叭听到自己的声音，提高通话的质量。 在上图所示的串联分压电路中，R1、R2为纯电阻，v1、v2为输入电压，vo为输出电压，据叠加定理：v_{o}=\frac{v_{1}R_{2}+v_{2}R_{1}}{R_{1}+R_{2}}令vo=0，则v1R2+v2R1=0，即：\frac{v_{1}}{v_{2}}=-\frac{R_{1}}{R_{2}}特别地，当R1=R2时，v1=-v2。由上式可见，欲使vo=0，v1，v2须满足2个条件：每个频率分量的相位相反。每个频率分量幅度呈一定比例且比例相同。下面是该方法的一种实现方式的电路图： 根据文首所列的参考资料，我们找到了另一种原理相同且成本更低的实现方式，如下图所示。三极管发射极和集电极的信号反相且幅度相同，可以相叠加后将信号消去，一个三极管的作用相当于上述方法中的U1和U2。图中三级管的偏置电路没有画出，C1和C2将直流分量同传输线隔离开。需要特别提出的是，如果可调电阻P1足够大，从而对三极管的偏置影响足够小，可将C2去掉，可调电阻P1直接和三极管的c，e极并联。 需要注意的是，在这里传送到对方的信号的相位再次反相，与原始信号相一致。实验中，三极管采用9013，电位器采用104。为了使三极管正常工作，在三极管B端由R1和R3分压，使三极管的静态工作点VCQ≥4.5V，则令VEQ=VCC-VCQ，需要VR2=VR4，因此选R2=R4=1kΩ，则由IEQ=ICQ得VR2=VR4。由于有VBE=0.7V，则VBQ=VEQ+0.7，VBQ+VR1=VCC=12V。此时若ICQ=4mA，经计算，则R1与R3之比约为1.5。我们最终选取R1=6.7kΩ，R3=4.7kΩ来分压。减小信号衰减由于我们的目标是实现长导线远程传输信号，因此导线上的电阻是不可忽略的，这就导致了信号衰减的问题。在弱电的情况下，解决导线上信号衰减的方法有选取更优质的导线、改用电流信号输出、增大接收端的输入电阻等。利用电压跟随器输入电阻高的特性，我们决定在信号接收的两端分别添加一个电压跟随器（如下图所示）来抑制信号传输的衰减。 低通滤波人耳可以听到20HZ到20kHZ的音频信号，而人正常对话所发出的声音频率约为300HZ—3000HZ，频率较低，因此我们设计一个低通语音滤波器来滤除杂音，提高声音清晰度。我们的目的是设计一个低频增益A0=2，Q≈1（品质因数，越小则通带或阻带越平坦，电路的稳定性越好）, fH=3kHz，图如下所示是一个二阶压控电压源低通滤波器。 根据该电路低频增益A0=K=1+R27/R28=2，可知R27=R28,因此我们选R27=R28=10kΩ。根据fC=ωC/2π，当R25=R26=R，C10=C12=C时，有ωC=1/(RC)。因此fC=1/2πRC。由所需上限截止频率fH为3kHz，我们选择C=0.01μF，算出R=1/(2πfC) ≈5.3kΩ。这里我们选用4.7kΩ的电阻，可实现近似的功能。使用TDA1013B功率放大在最后的输出之前，我们需要对信号进行功率的放大。在查阅相关资料之后，我们发现TDA1013B比较符合我们的功能需求。TDA1013B是一个音频功率放大器集成电路，内部具有按对数曲线变化的直流音量控制电路，控制范围可达80dB，它具有很宽的电源电压范围（10V~40V），输出功率位4W~10W，是理想的音频功率放大器。根据文首列出的数据手册，TDA1013B的伴音电路连接方式如下。 其中各个引脚的功能分别是：1脚：电源地。2脚：放大器输出，这里作伴音输出。3脚：电源。4脚：电源。5脚：功放输入。6脚：控制单元输出。7脚：控制电压，这里可用于音量控制。8脚：控制单元输入，这里输入音频。9脚：信号地。通过分析和查阅资料（见本文参考），我们确定了芯片的连接方式如下图所示。 设计实现如下是我们的仿真总电路图（还缺少最后的两级功率放大电路）。我们使用的是multisim14.0，由于软件的库中没有TDA1013B芯片，且声音信号难以在仿真软件中模拟，因此我们在仿真模拟阶段选择分模块检验功能的实现效果。我们在第一级放大电路前后使用示波器检测放大效果，我们输入100mV峰峰值、1000Hz频率的交流信号，得到输出如下，其中通道A为放大之后的的信号，通道B显示的是放大之前的信号。我们还检测了声音低通滤波器的功能实现情况，我们将对应的模块分离出来，利用波特测试仪画出该电路的波特图，结果如下所示。分别使用了对数和线性的横坐标轴（频率），且分别设扫描上限为100kHz和20kHz，由图易知，在3kHz左右处，增益开始下降，基本符合我们的设计要求。]]></content>
      <categories>
        <category>程序与设计</category>
      </categories>
      <tags>
        <tag>模拟电路</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[verilog笔记：频率计实现与verilog中timescale的解释]]></title>
    <url>%2Fverilog20200107215404%2F</url>
    <content type="text"><![CDATA[之前每次创建完一个新的文件，文件最上方总会显示一行timescale。当初觉得没什么作用，删了之后依旧没有任何问题便没把它当回事，直到后来做频率计数器的时候我才决定一探究竟。那么这篇文章也就一并谈一下这个问题。本文参考自：https://blog.csdn.net/m0_37652453/article/details/90301902https://blog.csdn.net/ciscomonkey/article/details/83661395https://blog.csdn.net/qq_16923717/article/details/81099833基本原理一个简易的频率计数器主要由分频器和计数器构成，其基本原理就是记录由分频器得到的一段时间内被测信号上升沿的个数，从而求得被测信号的频率。控制信号转换模块123456789101112131415161718`timescale 1ns / 1ps //unit 1ns, precision 1psmodule control( output reg Cnt_EN, //enable the counter count, so that the counting period can be controled output wire Cnt_CR, //clear the counter every time when the measure begins output wire Latch_Sig, //at its posedge, the value of the counter will be stored/latched input nRST, //system reset signal input CP //1Hz standard clock signal ); always @ (posedge CP or negedge nRST) begin if(~nRST) //generate enable counting signal Cnt_EN = 1'b0; //don't count else Cnt_EN = ~Cnt_EN; //two frequency divider for the clock signal end assign Latch_Sig = ~Cnt_EN; //generate latch signal assign Cnt_CR = nRST &amp; (~CP &amp; Latch_Sig); //generate the clear signal for the counterendmodule计数模块1234567891011121314151617`timescale 1ns / 1ps //unit 1ns, precision 1psmodule counter( output reg [3:0] Q, input CR, EN, CP ); always @ (posedge CP or posedge CR) begin if(CR) Q &lt;= 4'b0000; //reset to zero else if(~EN) Q &lt;= Q; //stop counting else if(Q == 4'b1001) Q &lt;= 4'b0000; else Q &lt;= Q + 1'b1; //counting, plus one endendmodule寄存模块123456789101112`timescale 1ns / 1ps //unit 1ns, precision 1psmodule Latch( output reg [15:0] Qout, input [15:0] Din, input Load, CR ); always @ (posedge Load or posedge CR) if(CR) Qout &lt;= 16'h0000; //reset to zero first else Qout &lt;= Din; endmodule顶层文件12345678910111213141516171819202122`timescale 1ns / 1ps //unit 1ns, precision 1psmodule Fre_Measure( output wire [15:0] BCD, //transfer to display part input _1HzIN, SigIN, nRST_Key, output wire Cnt_EN, Cnt_CR, //control signals of the counter output wire Latch_Sig, //latch singal output wire [15:0] Cnt //8421BCDcode output ); //call control block control U0(Cnt_EN, Cnt_CR, Latch_Sig, nRST_Key, _1HzIN); //measure counter counter U1(Cnt[3:0], Cnt_CR, Cnt_EN, SigIN); counter U2(Cnt[7:4], Cnt_CR, Cnt_EN, ~(Cnt[3:0] == 4'h9)); counter U3(Cnt[11:8], Cnt_CR, Cnt_EN, ~(Cnt[7:0] == 8'h99)); counter U4(Cnt[15:12], Cnt_CR, Cnt_EN, ~(Cnt[11:0] == 12'h999)); //call latch block Latch U5(BCD, Cnt, Latch_Sig, ~nRST_Key); endmodule仿真文件1234567891011121314151617181920212223242526272829303132333435363738`timescale 1ns / 1ps //unit 1ns, precision 1psmodule simulateFile(); wire [15:0] BCD; //transfer to display part wire [15:0] Cnt; //8421BCDcode output reg CLK, RST, Signal; parameter T1 = 0.1, //100Hz T2 = 0.01, //1000Hz T3 = 0.002; //5000Hz wire Cnt_EN, Cnt_CR; //control signals of the counter wire Latch_Sig; //latch singal Fre_Measure Watch(BCD, CLK, Signal, RST, Cnt_EN, Cnt_CR, Latch_Sig, Cnt); initial begin RST = 0; CLK = 0; Signal = 0; end always forever #10 CLK = ~CLK; //generate clock signal always #T1 Signal = ~Signal; //T1 or T2 or T3 always begin #10 RST = 1; #200 RST = 0; #10 RST = 1; #200 RST = 0; #10 RST = 1; #200 RST = 0; #10 RST = 1; end endmodule仿真结果以100Hz（T1）为例，输出的仿真结果如下。其中CLK是固定的1Hz基准时钟信号；RST是系统需求的复位按键，当按下复位即RST为下降沿时，可以看到计数器Cnt被清零同时第一排译码输出的BCD码也被清零；Signal为输入的信号，这里我使用的是100Hz的，由我自己设定，由于频率较快，可以看到波形图非常密集；Cnt_EN是计数使能信号，可见在它为高电平时，Cnt随着输入信号一样快速计数；Cnt_CR是清零信号，在每次计数使能的上升沿或者复位的下降沿到来时Cnt_CR置零，也就是对Cnt清零操作；此外，当时钟信号到来时，假如系统不在计数（Cnt_EN=0），那么Latch_Sig将置1，也就是把记录数值存入锁存器。实际上，这种设计方案会存在±1的计数误差，应为输入信号不一定与分频器同周期，即有可能每次测量的起始位置出于输入信号一个周期内的不同状态。timescaletimescale是Verilog中的预编译指令，指定位于它后边的module的时间单位和时间精度，直到遇到新的timescale指令或者resetall指令。它的语法如下：1`timescale time_unit / time_precision假如我们延时x个time_unit，那延时的总时间time = x * time_unit，但最后真正延时的时间是根据time_precision对time进行四舍五入后的结果。注意：time_unit和time_precision只能是1、10和100这三种整数，单位有s、ms、us、ns、ps和fs。time_precision必须小于等于time_unit。timescale的时间精度设置是会影响仿真时间的，1ps精度可能是1ns精度仿真时间的一倍还多，并且占用更多的内存，所以如果没有必要，应尽量将时间精度设置得更大一些。仿真时间之前进行仿真时，我往往是让它自动运行至系统默认时间然后停止，这就会造成出现好几次重复循环的情况。在Vivado中，窗口上方有三个类似播放器中的按钮，从左往右依次是：复位、不停运行、按指定时长（在后面的栏中设定）运行。此外，如果计算不准时间，可以直接在仿真文件末尾或者想要结束的地方使用$stop或者$finifsh来终止仿真。]]></content>
      <categories>
        <category>程序与设计</category>
      </categories>
      <tags>
        <tag>数字电路</tag>
        <tag>代码实现</tag>
        <tag>verilog</tag>
        <tag>Vivado</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[verilog笔记：运动码表的硬件描述语言实现]]></title>
    <url>%2Fverilog20200107211139%2F</url>
    <content type="text"><![CDATA[继上一篇logisim笔记：基本使用及运动码表的实现，我还使用了硬件描述语言对同样需求的运动码表进行了实现，那么就在这里一并也总结一下吧。本文参考自：https://blog.csdn.net/leon_zeng0/article/details/78441871https://www.cnblogs.com/douzi2/p/5147151.htmlhttps://blog.csdn.net/FPGADesigner/article/details/82425612整体设计由于需求与使用Logisim实现时一致，因此我的设计思路也基本沿用上一篇博文中提到的方案。但是要注意的是，这里的000状态并不在作为按键抬起之后的中间态，而是进入系统时的一个默认初始状态。Vivado中一些高亮的含义在具体的代码之前，我还想先归纳一下本次实践过程中遇到的和发现的Vivado中一些高亮提醒的含义。土黄色高亮土黄色高亮出现的原因主要可能是下面三种情况：定义重复。定义放在了调用处的后面（identifier used before its declaration）。声明残缺（empty statement）。蓝色高亮含有undeclared symbol。和上面土黄色高亮相搭配出现，有定义重复时指明重复定义的位置。16位数值比较器该模块用于比较两个16位二进制数的大小，以确定是否需要存入记录的数据。代码如下：1234567891011121314module _16bit_Comp( input [15:0] A, input [15:0] B, output reg Y ); always @ (A or B) begin if(A &lt; B) Y &lt;= 1; else Y &lt;= 0; endendmodule16位寄存器TMRecord表示码表暂停时的读数，regRecord表示寄存器中已经存储的记录，初始值为9999。控制信号有使能信号和reset信号。当收到reset信号时，直接将记录改为9999。当有使能信号时，将TMRecord记录下来。代码如下：1234567891011121314151617181920module _16bit_Reg( input enable, input [15:0] TMRecord, input [15:0] regRecord, input [15:0] maxDefault, input reset, input clock, output reg [15:0] next_record ); always @ (reset or enable) begin if(reset &amp; enable) next_record &lt;= maxDefault; else if(enable) next_record &lt;= TMRecord; else next_record &lt;= regRecord; endendmodule数码管显示驱动将BCD码转化为7位二进制数，即对应7段数码管，用于显示。12345678910111213141516171819202122module watchDrive( input [3:0] BCD, output reg [6:0] light ); always @ (BCD) begin case(BCD) 0: light = 7'B0111111; 1: light = 7'B0001001; 2: light = 7'B1011110; 3: light = 7'B1011011; 4: light = 7'B1101001; 5: light = 7'B1110011; 6: light = 7'B1110111; 7: light = 7'B0011001; 8: light = 7'B1111111; 9: light = 7'B1111011; default: light = 7'B0111111; endcase endendmodule顶层文件该部分主要包括对各个变量的定义和初始化；状态转换，即共设计了5种状态，对应不同的功能，当按下不同按键时，选择对应的状态并作为次态；数码管的显示与进位，即对数码管4个位置依次改动，从低位开始计算，当进位时产生进位信号到下一位。当有重置信号（这里使用的是reset和start的上升沿）时清零。123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212`timescale 1ns / 1ps//top modulemodule runningwatch( input start, input stop, input store, input reset, input CLK, output reg TM_EN, //enable the timer output reg SD_EN, //enable register output reg DP_SEL, //the screen show which data output reg [15:0] regRecord, //data the register storing output reg [15:0] TMRecord, //timer data output [6:0] tenmsLight, hunmsLight, onesLight, tensLight ); reg [3:0] tenMS, hunMS, oneS, tenS; //four 4bit digits from small to high and each from 0 to 9 reg tenmsup, hunmsup, onesup; //the signals if the bigger digit than itself should add //allocate state parameter S0 = 3'B000; //initial state parameter S1 = 3'B001; //TIMING:timer set as 00.00,record does not change,show timer,timer counts,TM_EN = 1, SD_EN = 0, DP_SEL = 0 parameter S2 = 3'B010; //PAUSE:timer does not change,record does not change,show timer,timer does not count,TM_EN = 0, SD_EN = 0, DP_SEL = 0 parameter S3 = 3'B011; //UPDATE:timer does not change,record set as timer,show register,timer does not count,TM_EN = 0, SD_EN = 1, DP_SEL = 1 parameter S4 = 3'B100; //KEEP:timer does not change,record does not change,show register,timer does not count,TM_EN = 0, SD_EN = 0, DP_SEL = 1 parameter S5 = 3'B101; //RESET:timer set as 00.00,record set as 99.99,show timer,timer does not count,TM_EN = 0, SD_EN = 1, DP_SEL = 0 reg [2:0] state; reg [2:0] next_state; //reg CLK; initial //only run once begin regRecord = 16'B0010011100001111; TMRecord = 16'B0000000000000000; state = S0; tenMS = 0; hunMS = 0; oneS = 0; tenS = 0; end //to judge if store the timer's data wire new; reg newRecord; _16bit_Comp comparator(TMRecord, regRecord, new); //compare always @ (new) newRecord = new; reg [15:0] MAX = 16'B0010011100001111; //sequential logic part always @ (posedge CLK) //update the state at each posedge begin state &lt;= next_state; end //combinatory logic part //state transform always @ (state or start or stop or store or reset or newRecord) begin next_state = S0; //if not press the key, back to the initial state case(state) S0: //initial state begin TM_EN = 0; SD_EN = 0; DP_SEL = 0; if(start) begin next_state = S1; TM_EN = 1; SD_EN = 0; DP_SEL = 0; end else if(stop) begin next_state = S2; TM_EN = 0; SD_EN = 0; DP_SEL = 0; end else if(store &amp; newRecord) begin next_state = S3; TM_EN = 0; SD_EN = 1; DP_SEL = 1; end else if(store &amp; ~newRecord) begin next_state = S4; TM_EN = 0; SD_EN = 0; DP_SEL = 1; end else if(reset) begin next_state = S5; TM_EN = 0; SD_EN = 1; DP_SEL = 0; end else begin next_state = S0; TM_EN = 0; SD_EN = 0; DP_SEL = 0; end end S1: //TIMING begin TM_EN = 1; SD_EN = 0; DP_SEL = 0; if(start) begin next_state = S1; TM_EN = 1; SD_EN = 0; DP_SEL = 0; end else if(stop) begin next_state = S2; TM_EN = 0; SD_EN = 0; DP_SEL = 0; end else if(store &amp; newRecord) begin next_state = S3; TM_EN = 0; SD_EN = 1; DP_SEL = 1; end else if(store &amp; ~newRecord) begin next_state = S4; TM_EN = 0; SD_EN = 0; DP_SEL = 1; end else if(reset) begin next_state = S5; TM_EN = 0; SD_EN = 1; DP_SEL = 0; end else begin next_state = S0; TM_EN = 0; SD_EN = 0; DP_SEL = 0; end end S2: //PAUSE begin TM_EN = 0; SD_EN = 0; DP_SEL = 0; if(start) begin next_state = S1; TM_EN = 1; SD_EN = 0; DP_SEL = 0; end else if(stop) begin next_state = S2; TM_EN = 0; SD_EN = 0; DP_SEL = 0; end else if(store &amp; newRecord) begin next_state = S3; TM_EN = 0; SD_EN = 1; DP_SEL = 1; end else if(store &amp; ~newRecord) begin next_state = S4; TM_EN = 0; SD_EN = 0; DP_SEL = 1; end else if(reset) begin next_state = S5; TM_EN = 0; SD_EN = 1; DP_SEL = 0; end else begin next_state = S0; TM_EN = 0; SD_EN = 0; DP_SEL = 0; end end S3: //UPDATE begin TM_EN = 0; SD_EN = 1; DP_SEL = 1; if(start) begin next_state = S1; TM_EN = 1; SD_EN = 0; DP_SEL = 0; end else if(stop) begin next_state = S2; TM_EN = 0; SD_EN = 0; DP_SEL = 0; end else if(store &amp; newRecord) begin next_state = S3; TM_EN = 0; SD_EN = 1; DP_SEL = 1; end else if(store &amp; ~newRecord) begin next_state = S4; TM_EN = 0; SD_EN = 0; DP_SEL = 1; end else if(reset) begin next_state = S5; TM_EN = 0; SD_EN = 1; DP_SEL = 0; end else begin next_state = S0; TM_EN = 0; SD_EN = 0; DP_SEL = 0; end end S4: //KEEP begin TM_EN = 0; SD_EN = 0; DP_SEL = 1; if(start) begin next_state = S1; TM_EN = 1; SD_EN = 0; DP_SEL = 0; end else if(stop) begin next_state = S2; TM_EN = 0; SD_EN = 0; DP_SEL = 0; end else if(store &amp; newRecord) begin next_state = S3; TM_EN = 0; SD_EN = 1; DP_SEL = 1; end else if(store &amp; ~newRecord) begin next_state = S4; TM_EN = 0; SD_EN = 0; DP_SEL = 1; end else if(reset) begin next_state = S5; TM_EN = 0; SD_EN = 1; DP_SEL = 0; end else begin next_state = S0; TM_EN = 0; SD_EN = 0; DP_SEL = 0; end end S5: //RESET begin TM_EN = 0; SD_EN = 1; DP_SEL = 0; if(start) begin next_state = S1; TM_EN = 1; SD_EN = 0; DP_SEL = 0; end else if(stop) begin next_state = S2; TM_EN = 0; SD_EN = 0; DP_SEL = 0; end else if(store &amp; newRecord) begin next_state = S3; TM_EN = 0; SD_EN = 1; DP_SEL = 1; end else if(store &amp; ~newRecord) begin next_state = S4; TM_EN = 0; SD_EN = 0; DP_SEL = 1; end else if(reset) begin next_state = S5; TM_EN = 0; SD_EN = 1; DP_SEL = 0; end else begin next_state = S0; TM_EN = 0; SD_EN = 0; DP_SEL = 0; end end default: begin next_state = S0; TM_EN = 0; SD_EN = 0; DP_SEL = 0; end //default initial state endcase end //reset to zero always @ (posedge reset or posedge start) begin tenMS &lt;= 0; hunMS &lt;= 0; oneS &lt;= 0; tenS &lt;= 0; TMRecord &lt;= 0; end //the followings are which have stated before: //reg [3:0] tenMS, hunMS, oneS, tenS are four 4bit digits from small to high and each from 0 to 9 //reg tenmsup, hunmsup, onesup are the signals if the bigger digit than itself should add //timer, divide into four digits //10ms always @ (posedge CLK) begin if(TM_EN) begin TMRecord = TMRecord + 1; if(tenMS &lt; 9) begin tenMS &lt;= tenMS + 1; tenmsup &lt;= 0; end else begin tenMS &lt;= 0; tenmsup &lt;= 1; end end end //100ms always @ (posedge tenmsup) begin if(TM_EN) begin if(hunMS &lt; 9) begin hunMS &lt;= hunMS + 1; hunmsup &lt;= 0; end else begin hunMS &lt;= 0; hunmsup &lt;= 1; end end end //1s always @ (posedge hunmsup) begin if(TM_EN) begin if(oneS &lt; 9) begin oneS &lt;= oneS + 1; onesup &lt;= 0; end else begin oneS &lt;= 0; onesup &lt;= 1; end end end //10s always @ (posedge onesup) begin if(TM_EN) begin if(tenS &lt; 9) tenS &lt;= oneS + 1; else oneS &lt;= 0; end end //save to the register wire [15:0] newReg; _16bit_Reg register(SD_EN, TMRecord, regRecord, MAX, reset, CLK, newReg); always @ (newReg) regRecord = newReg; //change BCD to tube lights watchDrive TENms(tenMS, tenmsLight); watchDrive HUNms(hunMS, hunmsLight); watchDrive ONEs(oneS, onesLight); watchDrive TENs(tenS, tensLight); endmodule仿真文件最后我们还需要自行编写一个仿真文件。123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354module simulateFile(); reg start, stop, store, reset; wire TM_EN, SD_EN, DP_SEL; wire [15:0] regRecord; wire [15:0] TMRecord; wire [6:0] tenmsLight; wire [6:0] hunmsLight; wire [6:0] onesLight; wire [6:0] tensLight; reg CLK; runningwatch watch(start, stop, store, reset, CLK, TM_EN, SD_EN, DP_SEL, regRecord, TMRecord, tenmsLight, hunmsLight, onesLight, tensLight); initial begin CLK = 0; start = 0; stop = 0; store = 0; reset = 0; end begin always //generate clock signal forever #10 CLK = ~CLK; //always #100 //&#123;start, stop, store, reset&#125; = 4'B0000; //begin start = 0; stop = 0; store = 0; reset = 0; end always begin #50 &#123;start, stop, store, reset&#125; = 4'B0001; #50 &#123;start, stop, store, reset&#125; = 4'B1000; #500 &#123;start, stop, store, reset&#125; = 4'B0100; #50 &#123;start, stop, store, reset&#125; = 4'B0010; #50 &#123;start, stop, store, reset&#125; = 4'B1000; #50 &#123;start, stop, store, reset&#125; = 4'B0100; #100 &#123;start, stop, store, reset&#125; = 4'B0010; #50 &#123;start, stop, store, reset&#125; = 4'B0001; #50 $stop; //stop simulation end endendmodule仿真结果Run simulation，得到如下输出波形图。遇到的问题报错：[Synth 8-462] no clock signal specified in event control我原本直接在顶层文件中定义时钟并使用forever生成连续的时钟信号，结果出现了如上报错。在仿佛调整后，最后发现解决的办法是将时钟信号放在仿真文件里生成然后作为input输入到顶层文件。使用模块的输出赋值时遇到问题这个问题的主要原因还是因为我对于verilog赋值规则以及变量性质的不熟悉，这里做一个小归纳：给wire赋值必须用assign。给reg赋值用always。使用非阻塞赋值时，reg不能给wire赋值，反之则可以。使用阻塞赋值时，reg可以给wire赋值，反之则不行。]]></content>
      <categories>
        <category>程序与设计</category>
      </categories>
      <tags>
        <tag>踩坑血泪</tag>
        <tag>数字电路</tag>
        <tag>代码实现</tag>
        <tag>verilog</tag>
        <tag>Vivado</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[logisim笔记：基本使用及运动码表的实现]]></title>
    <url>%2Flogisim20200107121101%2F</url>
    <content type="text"><![CDATA[似乎好久没有写新的博文了。由于今年的春节较往年要早，近一个月来，各门考试接踵而至让我忙得不可开交。虽然非常赶，终于还是考完了。当初想都不敢想的13天考10门的考试周也就这样熬过去了。事实证明，即便是很大的困难，逼一下自己还是能挺过来的。不过最后还是和往年的冬天一样发了个一年一度的高烧，真的难受，以后还得更加注重自己的身体。废话不多说，这几天就打算先把这学期期末阶段一些有意义、有价值的的知识与经历整理记录一下。首先是一个运动码表的大作业，本文主要是使用Logisim对运动码表进行实现的方法与过程。需求分析该项目要求我们设计一个简单的运动码表，包括四个按键组成的输入模块和四个7段数码管组成的输出模块。四个按键分别是：Start，按下时计时器归零并重新开始计时；Stop，按下时停止计时并显示计时数据；Store，按下时尝试更新系统记录，要求记录当前码表值，若已有记录，则和当前待记录的值作对比，如果计时数据比记录数据要小即用时要短，则更新系统记录并显示；Reset，按下时进行复位，将计时数据置为00.00，系统记录置为99.99。Logisim使用在具体说明实现的方法之前，我想先把之前整理的一些Logisim的基本使用做一个简短的总结。Logisim的使用其实不难，可以参考网上整理的Logisim文档翻译就可以快速入门。实际上，在具备一定数字电路知识的情况下，到处点点也能够学会Logisim的基本使用方法。Logisim为数字电路的设计提供了很大的帮助，我们可以通过填写真值表或者输入逻辑函数快速生成原本手动连线根本无法完成的复杂电路。如果想要获得Logisim最新的优化版本，可以加入华科谭志虎教授创建的计算机硬件系统设计交流群（群号：957283876），此群汇集了多个高校的学生，是一个比较活跃的技术交流群，谭教授秒回且超赞的解答真的忍不住让人点赞！下面是一些我当初刚使用时记录在note上的一些Logisim的使用与常见处理，分享在此。引脚在Logisim中，引脚即Pin，可以使用键盘上、下、左、右光标键来改变引脚的朝向。注意：这里该表朝向不是按照旋转方向来的，而是按哪个方向就是指向哪个方向。此外，根据形状的不同，引脚分为输入引脚（较方）和输出（较圆）引脚，可以使用UI左上角最左侧的手型的戳工具点击对应的引脚来修改引脚的值（高电平、低电平、未知x态）。当我们选中引脚时，按下alt+数字组合，就可以改变到对应的位宽。与门选中与门，按下数字键，就修改输入引脚个数。线颜色的含义亮绿色：高电平。深绿色：低电平。蓝色：未知状态。灰色：飞线。红色：信号冲突。黑色：多位总线。橙色：位宽不匹配。时钟ctrl+K：驱动与关闭时钟连续运行。ctrl+T：驱动时钟单步运行。其它快捷键上面列出的都是一些并比较典型的特例，别的地方使用基本类似，可以多多尝试。下面再列出两个比较常用的快捷键：ctrl+D：创建副本。ctrl+Z：撤销。整体设计为了实现运动码表的功能，我们将整个项目拆分成如下几个模块：计时模块（包括计时使能模块）。码表驱动与显示模块。系统记录数据寄存模块。码表状态功能控制模块。数值比较模块。数值选择模块。此外，根据需求分析，我们设计了如下6个状态，并构建状态机如下：000 中间态：每次按键弹起后，回到该状态。101 复位：计时变成00.00，记录变成99.99，显示计时数值，时钟不计时。001 计时：计时变成00.00，记录不变，显示计时数值，时钟计时。010 停止：计时不变，记录不变，显示计时数值，时钟不计时。011 更新（小于系统记录NewRecord=1）：计时不变，记录变成计时，显示记录数值，时钟不计时。100 不更新（大于等于系统记录NewRecord=0）：计时不变，记录不变，显示记录数值，时钟不计时。可得状态转换关系的真值表如下所示：为了实现上述状态转换与控制信号输出，我们设计了如下码表控制电路。其中SD-EN控制寄存器使能，DP-SEL控制码表显示数值的选择，TM-Reset控制是否复位。设计实现这是设计完成后最终circ文件中所包含的内容，下面我简述一下各个模块中的内容及思路。数码管驱动：即将4位2进制数转化成7个二进制信号，驱动7段数码管的亮灭。4位BCD计数器：基于下面的BCD计数器状态转换（即在0-9之间递增循环）和BCD计数器输出函数（即在达到9时输出进位信号）。码表计数器：由四个4位BCD计数器组成。码表显示驱动：即将四个4位二进制组成的数字通过四个7段数码管显示出来，内部基于上面的显示驱动转换电路即数码管驱动的封装。码表控制器：上文的设计中已经提及，基于码表控制器状态转换（输入信号+现态-&gt;次态）和码表控制器输出函数（现态-&gt;控制信号）。计时使能：这里我比较巧妙地运用了D触发器的置位清零两个端，将在后文提及。最后就是运动码表的总电路图：遇到的问题复位后自动开始计时这是最让我头疼的一个问题，由于上述状态机的设计方法和Logisim中按键在鼠标松开后自动弹起的功能，导致在我按下Reset清零后系统会自动重新开始计时（因为这时的现态已经不是复位状态），这显然是不符合需求的。为了使计时使能在下一次按键到来之前能保持现态，我将复位控制信号从状态转换电路中单独取出，并使用一个D触发器的置位清零两个端子实现了这个保持功能，效果不错。计时器遇到9时跳跃进位当最初的电路实现完成后，我兴奋地开始计时，结果计时器的花式进位方式顿时让我傻了眼。仔细研究后我发现，我起初设计的电路在进位时只考虑了低一位计数器传来的进位信号，而事实上，高位的进位条件往往是由后几位共同决定的，为此，修改电路如下： 问题解决。码表在更新数据的前一个时钟内会显示较大的记录数值这个问题其实不是非常重要，但同班的一位同学还是注意到了这点。也就是说，当计时数据比系统记录要小的时候，系统应该更新记录并显示最好的成绩，然而当按下Store进行存储更新时，在前一个时钟周期内，码表会短暂地先显示原本较大即较差地系统记录，这是不希望出现的。解决的办法可以在寄存器和显示选择器之间添加一个三态门，具体可以看上文中的总电路图。]]></content>
      <categories>
        <category>程序与设计</category>
      </categories>
      <tags>
        <tag>踩坑血泪</tag>
        <tag>Logisim</tag>
        <tag>数字电路</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[matlab笔记：一个非线性方程问题的多种求解方法]]></title>
    <url>%2Fmatlab20191116003545%2F</url>
    <content type="text"><![CDATA[上周科学计算引论结课了，借着写上机报告的机会，我把书本上所有的求解非线性方程的方法（标量型）都用matlab实现了一下，并对一个实际工程问题进行求解。关于实现过程中遇到的问题以及注意事项，我已写在matlab笔记：久未使用之后踩的一堆坑内。实际问题在电机学中，凸极同步发电机的功角特性可表示为：P_{em}=\frac{E_{0}V}{x_{d}}sin\theta +V^{2}\left ( \frac{1}{x_{q}}-\frac{1}{x_{d}} \right )sin\theta \cdot cos\theta式中，$P_{em}$表示发电机的电磁功率；$E_{0}$表示发电机电势；$V$表示发电机端电压；$x_{q}$表示横轴同步电抗；$x_{d}$表示纵轴同步电抗；$\theta$表示功率角，$\theta \in \left ( 0,\frac{\pi }{2} \right )$。如令$\frac{E_{0}V}{x_{d}}=P_{j}$，$V^{2}\left ( \frac{1}{x_{q}}-\frac{1}{x_{d}} \right )=P_{2e}$，则上式可以简化为：P_{em}=P_{j}sin\theta +P_{2e}sin\theta \cdot cos\theta在电力系统稳定计算中，我们往往要由上式求出功率角$\theta$。我们可以使用几何方法求解，也可以利用迭代法求解该非线性方程。我们将上式变为：\theta =arcsin\frac{P_{em}}{P_{j}+P_{2e}cos\theta }以许实章编《电机学习题集》第367页26-1为例，将$P_{em}=1$，$P_{j}=1.878$，$P_{2e}=0.75$代入，得到方程：\theta =arcsin\frac{1}{1.878+0.75cos\theta }问题求解在《计算方法》第二章，我们学习了一些非线性方程的数值解法，这里我们分别使用几何法和迭代法求解上述问题并进行比较。设方程求解的预定精度为$0.001^{\circ}$即$1.7\times 10^{-3}rad$，由闭区间上连续函数的性质和初步估计，可确定方程的解位于区间$[0,\frac{\pi }{6}]$。几何方法由几何法的求解方法，我们定义函数：f(\theta )=arcsin\frac{1}{1.878+0.75cos\theta }-\theta求解$\theta$即求解$f(\theta )$在$[0,\frac{\pi }{6}]$上的零点。在MATLAB中，将该函数实现如下：123456function [y] = func(x)%几何法函数方程%统一使用弧度制format long %为提高精度，保留更多位数y = asin(1 / (1.878 + 0.75 * cos(x))) - x;end由于MATLAB计算过程中默认保留$4$位小数，为了提高精度，我使用了format long保留更多有效数字。值得注意的是，计算时统一使用弧度制，待求出解之后再使用rad2deg函数转化为角度。二分法根据二分法的格式，编写二分法函数如下：123456789101112131415161718192021222324252627function [errors, ans, time] = bisection(low, high, max, stopError) %二分法%func是待求零点的函数%low，high分别是解区间的上下限%max是最多循环步数，防止死循环%stopError是预定精度，作为终止条件%errors记录每次循环的误差%ans记录最终求解结果，表示为角度%time是总的循环次数format long %为提高精度，保留更多位数fl = func(low);fh = func(high);error = high - low;for i = 1 : max mid = (low + high ) / 2; fm = func(mid); if fm * fl &gt; 0 low = mid; else high = mid; enderror = high - low;errors(i) = error; if error &lt; stopError, break, endendtime = i;ans = rad2deg(mid); %转换成角度end输入命令[errorsB, ans, time] = bisection(0, pi / 6, 50, 1.7e-5)，求得结果如下：1234567891011121314151617181920212223errorsB = 列 1 至 5 0.261799387799149 0.130899693899575 0.065449846949787 0.032724923474894 0.016362461737447 列 6 至 10 0.008181230868723 0.004090615434362 0.002045307717181 0.001022653858590 0.000511326929295 列 11 至 15 0.000255663464648 0.000127831732324 0.000063915866162 0.000031957933081 0.000015978966540ans = 22.909240722656250time = 15使用二分法共迭代$15$次，求得结果为$22.909240722656250^{\circ}$。此外，迭代误差为$0.000015978966540$，符合预设精度要求。然而，此种方法计算次数较多，因此我又尝试了下面的方法。弦截法根据弦截法的计算方法，编写弦截法函数如下：1234567891011121314151617181920212223242526function [errors, ans, time] = linecut(a, b, max, stopError)%弦截法%func是待求零点的函数%a，b是在解的领域取的两点，这里取解区间的两个端点%max是最多循环步数，防止死循环%stopError是预定精度，作为终止条件%errors记录每次循环的误差%ans记录最终求解结果，表示为角度%time是总的循环次数format long %为提高精度，保留更多位数fa = func(a);fb = func(b);error = abs(a - b); for i = 1 : max x = b - (b - a) * fb / (fb - fa); a = b; b = x; fa = func(a); fb = func(b); error = abs(a - b); errors(i) = error; if error &lt; stopError, break, endendtime = i;ans = rad2deg(b); %转换成角度end输入命令[errorsL, ans, time] = linecut(0, pi / 6, 50, 1.7e-5)，求得结果如下：12345678910111213errorsL = 0.120609794441825 0.003164447033051 0.000026217912123 0.000000005427336ans = 22.909760215805360time = 4与上面的二分法相比，弦截法只需计算$4$次，效率大大提高。计算所得结果为$22.909760215805360^{\circ}$，迭代误差为$0.000000005427336$，符合预设精度要求，而且比二分法的最终迭代误差更小，显然可以发现弦截法的收敛速度要快于二分法。下面我再用弦截法的改造方法Steffensen方法进行试验。Steffensen方法根据Steffensen方法的计算方法，编写函数如下：12345678910111213141516171819202122function [errors, ans, time] = steffensen(x, max, stopError)%Steffensen方法%func是待求零点的函数%x是初始点%max是最多循环步数，防止死循环%stopError是预定精度，作为终止条件%errors记录每次循环的误差%ans记录最终求解结果，表示为角度%time是总的循环次数format long %为提高精度，保留更多位数f = func(x);for i = 1 : max o = x - f ^ 2 / (f - func(x - f)); error = abs(x - o); x = o; f = func(o); errors(i) = error; if error &lt; stopError, break, endendtime = i;ans = rad2deg(o); %转换成角度end选取初始点为$0$，输入命令[errorsS1, ans, time] = steffensen(0, 50, 1.7e-5)，求得结果如下：12345678910111213errorsS1 = 0.381516143583955 0.018291709371805 0.000042893415627 0.000000000236814ans = 22.909760215804820time = 4发现计算次数没有比弦截法少，因此修改初值为$\frac{\pi }{6}$，再次计算，得结果：12345678910111213errorsS2 = 0.125855705283236 0.002107105082521 0.000000571210576ans = 22.909760215802425time = 3一般而言，Steffensen方法的收敛速度要快于弦截法，但这与初始点的选取有关。对于这个问题，当设初始点为$0$时，Steffensen方法的迭代次数与弦截法持平；当设初始点为$\frac{\pi }{6}$时，迭代次数才小于弦截法。因此，想让Steffensen方法更快地收敛，需选取合适的初始点。在我看来，Steffensen方法的一大优势就是它是一种单步迭代方法，相比二步迭代方法的弦截法，Steffensen方法只需要一个初值就可以开始迭代。Picard迭代法上述的方法都是基于几何图形的求解方法，而下面的Picard迭代法则是基于不动点原理给出的。首先，我们编写迭代函数：123456function [y] = interation(x)%迭代函数%统一使用弧度制format long %为提高精度，保留更多位数y = asin(1 / (1.878 + 0.75 * cos(x)));end我们使用如下命令查看函数图像：123&gt;&gt; x = 0 : pi / 36 : pi / 6;&gt;&gt; y = asin(1 * (1.878 + 0.75 * cos(x)) .^ (-1));&gt;&gt; plot(x, y)得到：易验证，该函数在$[0,\frac{\pi }{6}]$上满足Picard迭代条件。Picard迭代法根据Picard迭代法原理，定义Picard迭代函数：1234567891011121314151617181920function [errors, ans, time] = picard(x, max, stopError)%Picard迭代法%interation是迭代函数%x是初始点%max是最多循环步数，防止死循环%stopError是预定精度，作为终止条件%errors记录每次循环的误差%ans记录最终求解结果，表示为角度%time是总的循环次数format long %为提高精度，保留更多位数for i = 1 : max o = interation(x); error = abs(x - o); x = o; errors(i) = error; if error &lt; stopError, break, endendtime = i;ans = rad2deg(o); %转换成角度end输入命令[errorsP, ans, time] = picard(0, 50, 1.7e-5)求解：12345678910111213errorsP = 0.390355832559508 0.009044503640668 0.000428788831489 0.000020583068808 0.000000988625736ans = 22.909757357777192time = 5Picard迭代结果为$22.909757357777192^{\circ}$，且精度符合要求。下面使用Picard迭代法的改进Aitken加速迭代法进行试验。Aitken加速迭代法编写Aitken加速迭代法函数代码如下：12345678910111213141516171819202122function [errors, ans, time] = aitken(x, max, stopError)%Aitken迭代法%interation是迭代函数%x是初始点%max是最多循环步数，防止死循环%stopError是预定精度，作为终止条件%errors记录每次循环的误差%ans记录最终求解结果，表示为角度%time是总的循环次数format long %为提高精度，保留更多位数for i = 1 : max x1 = interation(x); x2 = interation(x1); x3 = x2 - (x2 - x1) ^ 2 / (x2 - 2 * x1 + x); error = abs(x3 - x); x = x3; errors(i) = error; if error &lt; stopError, break, endendtime = i;ans = rad2deg(x); %转换成角度end输入命令[errorsA, ans, time] = aitken(0, 50, 1.7e-5)求解得到：12345678910111213errorsA = 0.399614867056990 0.000235879375045 0.000000000176165ans = 22.909760215804827time = 3可以看到，Aitken加速迭代法仅需3次迭代就得到了符合条件的解，且它的迭代误差只用$0.000000000176165$，小于上述所有的方法，由此该方法的优势得以体现。Newton迭代法Newton迭代法也是一种求解非线性方程的高效算法，因此我也对其进行实现。这里要用到func的导数，经计算，编写func的导函数为程序df：123456function [y] = df(x)%求导数%统一使用弧度制format longy = (0.75 * sin(x) / (1 - (1 / (1.878 + 0.75 * cos(x))) ^ 2) ^ 0.5) / (1.878 + 0.75 * cos(x)) ^ 2 - 1;endNewton迭代法编写Newton迭代法的计算程序如下：1234567891011121314151617181920function [errors, ans, time] = newton(x, max, stopError)%Newton迭代法%func是待求零点的函数%x是初始点%max是最多循环步数，防止死循环%stopError是预定精度，作为终止条件%errors记录每次循环的误差%ans记录最终求解结果，表示为角度%time是总的循环次数format long %为提高精度，保留更多位数for i = 1 : max o = x - func(x) / df(x); error = abs(o - x); x = o; errors(i) = error; if error &lt; stopError, break, endendtime = i;ans = rad2deg(x); %转换成角度end输入命令[errorsN, ans, time] = newton(0, 50, 1.7e-5)进行计算，得解：12345678910111213errorsN = 0.390355832559508 0.009488988787132 0.000005925259246ans = 22.909760215672179time = 3Newton下山法为尽可能避免因初值选取不当导致计算过程缓慢收敛或者发散（经上面计算，此问题不存在这种情况），引入下山因子$\lambda \in (0,1]$，得到改进后的Newton下山法，其计算程序如下：12345678910111213141516171819function [errors, ans, time] = hill(x, max, stopError)%Newton下山法% 此处显示详细说明format long %为提高精度，保留更多位数l = 1;for i = 1 : max o = x - l * func(x) / df(x); while abs(func(o)) &gt; abs(func(x)) %不满足下山条件 l = l / 2; o = x - l * func(x) / df(x); end error = abs(o - x); x = o; errors(i) = error; if error &lt; stopError, break, endendtime = i;ans = rad2deg(x); %转换成角度end计算得到结果如下：12345678910111213errorsH = 0.390355832559508 0.009488988787132 0.000005925259246ans = 22.909760215672179time = 3由于此问题初值选取得当，即初值取$0$时使用一般Newton迭代法不存在缓慢收敛或者发散的问题，因此Newton下山法在这里并没有发挥作用。可以看到，Newton迭代法仅$3$步就完成了求解的任务，非常高效。问题的解以上方法都得到了一致的答案，根据精度要求，我们得出此条件下功率角$\theta$为$22.910^{\circ}$，与许实章编《电机学习题集》中的例题答案$22.9^{\circ}$相吻合。方法比较将上述各种方法的误差记录绘制成图表，由于Newton下山法在该问题中没有发挥作用，因此仅作出Newton迭代法的迭代误差图像。根据图片，我们可以观察到以上各个方法均收敛。其中，表现较为优越的有Aitken加速迭代法、Newton迭代法和Steffensen迭代法，均用了$3$次迭代就达到了精度要求。然而这里Steffensen法的收敛速度更依赖于初值的选取，当初值选为$0$时，它的收敛速度就类似于弦截法在该问题上的收敛速度了。因此，总的来说，对于这个问题，最高效的算法是Aitken加速迭代法和Newton迭代法。另外，二分法最简单却也最低效，迭代了$15$次才达到预设的精度要求。可见，各种算法的效率大致上与它们的复杂度和高级程度成正比关系。]]></content>
      <categories>
        <category>程序与设计</category>
      </categories>
      <tags>
        <tag>matlab</tag>
        <tag>代码实现</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[verilog笔记：数值比较器实现与vivado使用]]></title>
    <url>%2Fverilog20191115233116%2F</url>
    <content type="text"><![CDATA[本周数字电路课程老师布置了一个利用verilog语言进行数值比较器波形仿真的作业。可以利用Modelsim或者Vivado实现。由于Vivado默认安装大小就有将近30个GB（2018版好像是27GB左右，2014版是12.68GB，这版本的容量增速跟maltab有得一拼啊），因此之前装了之后不太会使用便又卸了。最近刚好趁着双十一降价给自己的laptop加了一个SSD，因此正好赶快学习一下如何使用。有关如何给笔记本加装SSD的问题，这里有两个视频可以解决，安装准备与步骤、安装后点亮磁盘。本文参考自：https://blog.csdn.net/qq_41154156/article/details/80989125https://wenku.baidu.com/view/0294cbb3bb4cf7ec4bfed01a.html关于vivado相比于Modelsim，Vivado的UI还是要舒服许多的，有点像Multisim之于Pspice。关于Vivado的使用，上面参考的文章中的步骤比较详细，照做一遍之后基本就会了。1位数值比较器1位数值比较器的逻辑图如下：使用verilog代码实现：1234567891011121314module _1bit_Comp( input A, input B, output AGB, output AEB, output ALB ); wire Anot, Bnot; not n0(Anot, A), n1(Bnot, B); and n2(AGB, A, Bnot), n3(ALB, Anot, B); nor n4(AEB, AGB, ALB);endmodule为了输出仿真波形，新建一个仿真文件：123456789module simulateFile(); reg A, B; wire AGB, AEB, ALB; _1bit_Comp u1(A, B, AGB, AEB, ALB); initial begin A = 0; B = 0; end always #50 &#123;A, B&#125; = &#123;A, B&#125; + 1;endmodule其中，过程赋值语句always只能给寄存器类型变量赋值，因此，在这里A、B要定义为reg类型。这里“#50”表示延时，使用{A, B}使AB变成二进制数，方便生成所有不同的输入，在这里即00、01、10、11。Run Simulation，输出波形：2位数值比较器2位数值比较器的逻辑图如下：使用verilog代码，调用1位数值比较器，实现2位数值比较器如下：12345678910111213141516171819module _2bit_Comp( input A1, input A0, input B1, input B0, output FAGB, output FAEB, output FALB ); wire AGB1, AEB1, ALB1, AGB0, AEB0, ALB0; //signal inside wire G1O, G2O; //the output of and gate G1, G2 _1bit_Comp C1(A1, B1, AGB1, AEB1, ALB1); //Instantiate 1-bit Comparator _1bit_Comp C0(A0, B0, AGB0, AEB0, ALB0); and G1(G1O, AEB1, AGB0), G2(G2O, AEB1, ALB0), G3(FAEB, AEB1, AEB0); or G4(FAGB, AGB1, G1O); or G5(FALB, ALB1, G2O);endmodule可以使用RTL ANALYSIS来仿真出2位数值比较器的RTL schematic电子原理图。类似的，编写仿真文件：123456789module simulateAgain(); reg A1, A0, B1, B0; wire FAGB, FAEB, FALB; _2bit_Comp u2(A1, A0, B1, B0, FAGB, FAEB, FALB); initial begin A1 = 0; A0 = 0; B1 = 0; B0 = 0; end always #30 &#123;A1, A0, B1, B0&#125; = &#123;A1, A0, B1, B0&#125; + 1;endmoduleRun Simulation，输出波形：出现的问题报错：[Synth 8-439] module’xxx’not found当初遇到这个问题后，我的第一反应是上网搜索原因。得到的解释有模块未添加、IP未正确设置等。对照网上的解决方案之后，我发现除了我无法理解的，网上所述的问题我都不存在。于是我只好独立进行思考。果然，我还犯不了像网上那样“高级”的错误。错误的原代码如下：12not n0(Anot, A); n1(Bnot, B);报错：[Synth 8-439] module’n1’not found。当我调用门的时候，由于内部变量换行导致我将逗号误用成了分号，因此导致分号之后的变量not found，修改后错误即可解决。其实，这个问题仔细观察即可发觉，相比于n1，同样格式的n0就没有报错，那么很有可能错误就在两者之间。ERROR: [Common 17-39] ‘xxx’ failed due to earlier errors这是我在执行仿真文件时遇到的error。仔细检查后，发现错误也与上一个问题相同。由于我在设计完电路后没有Run Synthesis综合并生成网表文件来进行检验，也没有进行其它的仿真操作，因此之前并没能发现这个问题。于是最后当调用该电路的仿真文件开始运行时就会报告这样的错误。要注意的点和matlab中函数文件的要求类似，verilog定义模块时，需要新建的模块文件名称与模块的文件名称一致。例如，我上面的1位数值比较器module名为_1bit_Comp，那么对应的文件名就应该是_1bit_Comp.v。此外，每个模块应使用一个文件来表示，且一个文件最多能表示一个模块（可以在其中调用其它模块，这点和matlab很像），两者呈一一对应关系。新建project时，如要从RTL代码开始综合，就选择RTL project（默认的这个）。要注意的是，下面的“Do not specify sources at this time”（此时不定义源文件）可以勾上。否则，下一步会进入添加source file。如果在一个project中已经建立了一个仿真文件，那么当你新建一个仿真文件时，需要建立在create的new file内，这样在后面对不同的仿真文件进行仿真时可以将对应的文件夹依次分别激活。在source窗口中，一般情况下，Vivado会自动加粗识别出来的top module，同时对应module名称前面也会有一个二叉树状的图标表示这是顶层模块。有时候，软件也会识别错误或者与实际需求不符，这时候我们可以右键想要置顶的module，在弹出的菜单中点击Set As Top将其设为顶层。当在同一个project中创建了多个仿真文件时，如要在进行完一次仿真之后对另一个仿真文件，需要对对应的文件夹进行激活。方法是右键仿真文件，然后在弹出的菜单中点击Make Active即可。加装固态盘前文提到给笔记本加装SSD，给了两个示范视频，这里我还是想再稍稍补充一下关于加装固态盘的一些事情。首先必须确保自己的本有空位。我使用的是小电池版本，因此有一整个2.5英寸7mm的硬盘位，这个请在决定购买新的硬盘前和卖家自己核对确认。如果还是不放心，那么最好亲自拆开查看，眼见为实嘛。要注意的是，必须使用完全对应规格的螺丝刀（比如我使用的是梅花T5螺丝刀），否则很容易发生滑丝，即螺牙连接处由于受力过大或其它原因导致螺牙磨损而使螺牙无法咬合，这会为今后的拆机带来不必要的麻烦。上面是我买的固态盘和数据线，相同或者类似机型的可以参考一下。在到货之后我发现，我所买的闪迪SSD较7mm稍薄些，因此平时拿动时（一般较大幅度翻动laptop时）会感到里面有东西松动的响声，不过使用至今没遇到任何问题。另外，为了适配各类硬盘，数据线的长度也有可能不能完全匹配，其实也没有关系，稍稍用力将数据线对应接头按入SATA3接口并用架子将固态盘固定即可。相比移动硬盘的USB接口，内置固态的SATA3读取速度还是相当不错的。]]></content>
      <categories>
        <category>程序与设计</category>
      </categories>
      <tags>
        <tag>踩坑血泪</tag>
        <tag>数字电路</tag>
        <tag>代码实现</tag>
        <tag>verilog</tag>
        <tag>Vivado</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[markdown笔记：写报告时的一些应用]]></title>
    <url>%2Fmarkdown20191110194256%2F</url>
    <content type="text"><![CDATA[在上一篇matlab笔记：久未使用之后踩的一堆坑中，我用matlab完成了实验，接下来就是写报告了。然而写博客用惯了markdown，现在极其嫌弃word文档。考虑到这次报告中大量的公式及代码，word文档的观感可想而知，因此果断决定使用markdown来书写实验报告。那么这篇文章就及时跟进一下我在写报告的时候发现的一些之前没注意的新应用吧。本文参考自：https://blog.csdn.net/m0_37925202/article/details/80461714https://www.w3school.com.cn/tags/att_p_align.asp居中文字之前写markdown的时候一直没有考虑到要把文字居中，而这回报告的标题就有这个要求了。实现的方法有如下两种。1&lt;center&gt;这样就居中了&lt;/center&gt;1&lt;p align="center"&gt;这样就居中了&lt;/p&gt;此外html中的&lt;p&gt;标签的align属性还有其它的用法：1234&lt;p align="left"&gt; &lt;!--左对齐--&gt;&lt;p align="right"&gt; &lt;!--右对齐--&gt;&lt;p align="center"&gt; &lt;!--居中--&gt;&lt;p align="justify"&gt; &lt;!--对行进行伸展，使每行都可以有相等的长度（就像在报纸和杂志中）--&gt;转pdf文件考虑到markdown文件不能作为最终的报告提交，我就想办法将markdown转化为pdf。我首先找到了VScode里面的markdown PDF扩展，然而安装之后有一个东西一直安装失败。这里我想起了之前我在markdown笔记：markdown的基本使用中强烈推荐过的typora（我的报告最后就是用它书写的），打开之后，果然没有让我失望。直接点击“文件”“导出”选择PDF，瞬间就完成了pdf文件的转换。此外，typora提供的导出格式实在是丰富，虽然导出的word与原markdown文件的颜值有少许降低，但还是不得不赞叹typora的实用！行间公式刚开始使用typora时，我遇到了一个疑惑：似乎typora不能插入行内公式块。在查找相关资料之后，我终于找到了解决的方案。点击“文件”“偏好设置”，把markdown扩展语法中的内联公式项打上勾。补充：当使用$夹着文字而非LaTex格式的公式时，字体会发生变化，如$我会变$我没变，呈现的效果是：$我会变$我没变。插入图片typora插入图片的方法与hexo中类似，也是可以创建一个文件夹存放图片，然后在偏好设置里进行设置。当指定路径之后，typora存放图片的文件夹名可以与markdown文件的名字不一致，而hexo中则需要一致才能够直接用图片名调用。此外，还是在偏好设置中，我们可以调整字体，我一般使用的是16px。]]></content>
      <categories>
        <category>操作和使用</category>
      </categories>
      <tags>
        <tag>markdown</tag>
        <tag>typora</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[matlab笔记：久未使用之后踩的一堆坑]]></title>
    <url>%2Fmatlab20191110193640%2F</url>
    <content type="text"><![CDATA[本周科学计算引论结课了，就花了一整天时间把要求的实验报告写了。根据考核说明，算法可以使用各种工具、语言来实现，但由于这门课程的上机实验统一使用的是matlab，再加上我上一次使用matlab大量实践是在劳动节的数学建模华中赛了，还是很有必要重拾起来再熟悉一下。于是乎，一大波坑就等着我去填了。本文参考自：https://blog.csdn.net/zhanshen112/article/details/79728887报错：未找到具有匹配签名的构造函数我编写了一个二分法的函数求解非线性方程，然而当我调用的时候，却遇到报错：“未找到具有匹配签名的构造函数”，这是怎么回事呢？我们来冷静分析一下。识破！原来是我定义的函数名为half，而half也是matlab自带的函数之一，可以使用help functionname查看函数具体的使用方法与功能。调用时默认优先使用自带的函数，因此修改函数名为matlab自带函数之外的即可。报错：矩阵维度必须一致这是我在画图时遇到的一个问题，首先先补充一下matlab中作函数图像的方法，如下图所示。当我尝试使用上述方法作简单的函数图像时，并没有报错，而当我想要作出我实验所需函数（如下所示）的图像时，却出现了“矩阵维度必须一致”的错误信息。1y = asin(1 / (1.878 + 0.75 * cos(x)))查阅相关资料，我才发现是乘除的时候出现的问题，为此我将其中的除/修改为乘*，并用.^代替^作乘方计算，即将原函数式修改为：1y = asin(1 * (1.878 + 0.75 * cos(x)) .^ (-1))这样，问题就解决了。这里我想强调一下在matlab中^与.^的区别：.^是点乘，而^是乘法。直接用^进行乘法的话，在这里即矩阵乘法，也就是说，必须满足前一个矩阵的列数等于后一个矩阵的行数。而使用.^点乘操作，是使每一个元素相乘，也就是向量或者矩阵中对应元素相乘，也很好记忆，加个点就是点乘。函数输出只有一个这是一个很愚蠢的问题，显然我好久没用了，因为matlab不必使用return返回结果，在函数声明的第一句就确定了返回值的数量和顺序。因此在调用函数的时候，必须也提供对应的变量去接收返回值，否则只能得到第一个返回的元素。使用diff求导不是导数值用惯了pytorch，总想着能够自动求导，一查matlab还真有这么一个函数，即diff函数。然而，事实证明它不是我想要的。我们可以在命令行中使用这一个函数：声明变量x：syms x。它代表着声明符号变量x，只有声明了符号变量才可以进行符号运算，包括求导。定义一个需要求导的函数：f(x) = sin(x) + x ^ 2。使用diff函数求导：diff(f(x))。也可以对已经定义好的m文件中的函数直接求导。这里，我们会得到ans为2*x + cos(x)。如果想pretty一些，可以使用pretty函数将结果转化成书面格式：pretty(ans)。然而，当我代入不同的具体数值想得到函数的导数值的时候，发现输出的结果却是0。使用help查阅diff函数的用法，得到的说明是：1234此MATLAB函数计算沿大小不等于1的第一个数组维度的X相邻元素之间的差分： Y = diff(X) Y = diff(X,n) Y = diff(X,n,dim)原来这个函数的主要用法是对向量或者矩阵中的元素进行差分计算，当用它来求导时，得到的只是一个表达式，且函数一但复杂，得到的就是一个参数众多的逼近格式。唉，总而言之，还是老老实实地拿起笔自己算出导函数吧。都用矩阵实验室（matlab）了，手动求个导还是得会的呀。角度制弧度制互换无论使用计算器还是编程计算，这都是一个需要注意的点，matlab默认使用的是弧度制，在计算出结果之后，可以使用rad2deg函数进行转换。同样的，我推测角度制转弧度制的函数名为deg2rad，一试，果不其然。这个函数还是挺好记的，英文里有许多同音词与字符的妙用，比如这里的to和2的two，还有at和@，感觉既方便又高级。保留更多位数matlab默认是保留4位有效数字，为了提升计算精度，可以使用format long来增加计算过程中保留的位数。常用操作难得开一篇写matlab使用的博文，那就在这补上几个我记忆中的较为常用的命令或操作。Ctrl+C终止操作。这跟许多地方都一样，在matlab中，Ctrl+C平时可以用来粘贴剪切板上的内容，而在程序运行时，可以使用它来终止运行，这在死循环的时候非常有用。clc清空命令行。bench测试性能。这其实不是一个很常用的命令，可以跟朋友输这个命令看看自己电脑的性能，下图是我的结果。需要注意的是，笔记本电脑电池使用模式的不同对这个排名影响还是挺大的，如果想让排名高一些的话，请确保电池开在最佳性能的模式。另外，不同的电脑的比较对象可能会不一样，比如学校的台式机和我的笔记本在这里比较的对象就不一样。 常用的还有许多，以后在使用中不断增加，先在这占个位。]]></content>
      <categories>
        <category>程序与设计</category>
      </categories>
      <tags>
        <tag>踩坑血泪</tag>
        <tag>matlab</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[front end笔记：使用div实现居中显示]]></title>
    <url>%2Ffront-end20191110165901%2F</url>
    <content type="text"><![CDATA[最近心血来潮花了好久给自己的博客添加了一个粒子时钟，最后想要使它在sidebar中居中显示废了我好大功夫，为了以后不在这上面浪费时间，我决定浪费现在的时间把这个问题记下来。本文参考自：https://www.jianshu.com/p/f0bffc42c1cehttps://blog.csdn.net/chwshuang/article/details/52350559swig由于我要将我的时钟显示在侧边栏，需要插入到header.swig文件中。hexo博客的源码中有大量这个格式的文件，然而具体的使用方法我也不是很清楚。在查阅了一些文章之后，我对swig有了一个初步的认知。swig是一个JS前端模板引擎，它有如下特点：支持大多数主流浏览器。表达式兼容性好。面向对象的模板继承。将过滤器和转换应用到模板中的输出。可根据路劲渲染页面。支持页面复用。支持动态页面。可扩展、可定制。可以通过VSchart将swig与其它前端模板框架进行对比，这个网站由维基支持，可以在里面进行各种对比，非常有意思，在这里推荐一下。关于swig的基本用法，可以在我文首的第一个参考链接中找到，个人认为不搞前端的话大概率是用不到的。原本的方法当我添加完时钟本地测验时，我发现添加的时钟在侧边栏的位置没有居中。根据之前学习html的记忆，我尝试了使用&lt;p align=center&gt;&lt;/p&gt;标签包裹我的插入语句，然而并没有达到想要的效果。使用div实现居中为了达到上述的目的，我使用&lt;div&gt;标签来分割出块，并使用div的属性来实现居中显示的效果。123&lt;div style="Text-align:center;width:100%;"&gt; &#123;% include '../_custom/clock.swig' %&#125;&lt;/div&gt;使用nav实现隐藏本以为大功告成，结果在移动端查看时，发现竖屏显示效果非常煞风景。其实在电脑端浏览器也可以预览移动端的效果，方法很简单，就是直接将浏览器窗口小化，减小两边间距，网页就会自动变成竖屏显示的状态（除非没有）。此外，当我们F12检查元素或者查看源时，网页也会被挤到一侧从而变成竖屏显示的状态。注：这里补充一下检查元素和查看源之间的区别，一般的浏览器右键都会有这两个功能，表面上看起来似乎也差不多，但是它们还是有区别的。检查元素看的是渲染过的最终代码，可以做到定位网页元素、实时监控网页元素属性变化的功能，可以及时调试、修改、定位、追踪检查、查看嵌套，修改样式和查看js动态输出信息。这让我想起了自己当初就是这样直接修改四级成绩，然后骗朋友的，不知道的人还真的想不出这原因，就以为的确是真的啦哈哈。另一方面，查看源只是把网页输出的源代码，即就是别人服务器发送到浏览器的原封不动的代码直接打开，既不能动态变化，也不能修改。为了解决这个问题，追求美观，我就想到可以把时钟和标签、归档、分类等菜单中的索引一起，在竖屏状态下不点击时就不显示。在分析了header.swig中菜单部分的源码之后，我注意到一个标签&lt;nav&gt;，它是是HTML5的新标签，可以标注一个导航链接的区域。于是，我将插入时钟的语句移入nav所包裹的块中，就完美达到了我的需求。意外的问题在我写这篇博文的时候，出现了一个奇怪的问题。每当我想要本地预览（部署应该也会出现这个问题），都会报错：“Nunjucks Error: [Line 17, Column 239] unexpected token: }”，这就让我非常的苦恼。根据错误信息，我开始一个一个寻找我文中的花括号。在反复删减和搜索相关问题之后，我发现是我插入在行间的一个include的swig语句惹的祸（我要是写出来又报错，插入在段间就没问题，可以到上文找）。这类异常一般是文章中使用了大括号{}这个特殊字符，且没有转义导致编译不通过，解决的办法是使用&amp;#123; &amp;#125;对大的花括号进行转换。补充：小的圆括号可用&amp;#40; &amp;#41;进行转换。没有这类问题当然再好不过啦，如果出现了，可以试试上面的方法。这类涉及转义的符号还是得熟悉其规则，避免老是出错。]]></content>
      <categories>
        <category>程序与设计</category>
      </categories>
      <tags>
        <tag>前端</tag>
        <tag>踩坑血泪</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[kaggle笔记：手写数字识别——使用KNN和CNN尝试MNIST数据集]]></title>
    <url>%2Fkaggle20191102112435%2F</url>
    <content type="text"><![CDATA[kaggle是一个著名的数据科学竞赛平台，暑假里我也抽空自己独立完成了三四个getting started级别的比赛。对于MNIST数据集，想必入门计算机视觉的人应该也不会陌生。kaggle上getting started的第一个比赛就是Digit Recognizer：Learn computer vision fundamentals with the famous MNIST data。当时作为入门小白的我，使用了入门级的方法KNN完成了我的第一次机器学习（自认为KNN是最最基础的算法，对它的介绍可见我的另一篇博文machine-learning笔记：机器学习的几个常见算法及其优缺点，真的非常简单，但也极其笨拙）。而最近我又使用CNN再一次尝试了这个数据集，踩了不少坑，因此想把两次经历统统记录在这，可能会有一些不足之处，留作以后整理优化。本文参考自：https://blog.csdn.net/gybinlero/article/details/79294649https://blog.csdn.net/qq_43497702/article/details/95005248https://blog.csdn.net/a19990412/article/details/90349429KNN首先导入必要的包，这里基本用不到太多：1234567import numpy as npimport csvimport operatorimport matplotlibfrom matplotlib import pyplot as plt%matplotlib inline导入训练数据：12345678910trainSet = []with open('train.csv','r') as trainFile: lines=csv.reader(trainFile) for line in lines: trainSet.append(line) trainSet.remove(trainSet[0])trainSet = np.array(trainSet)rawTrainLabel = trainSet[:, 0] #分割出训练集标签rawTrainData = trainSet[:, 1:] #分割出训练集数据我当时用了一种比较笨拙的办法转换数据类型：12345678910111213rawTrainData = np.mat(rawTrainData) #转化成矩阵，或许不需要m, n = np.shape(rawTrainData)trainData = np.zeros((m, n)) #创建初值为0的ndarrayfor i in range(m): for j in range(n): trainData[i, j] = int(rawTrainData[i, j]) #转化并赋值rawTrainLabel = np.mat(rawTrainLabel) #或许不需要m, n = np.shape(rawTrainLabel)trainLabel = np.zeros((m, n))for i in range(m): for j in range(n): trainLabel[i, j] = int(rawTrainLabel[i, j])这里我们可以查看以下数据的维度，确保没有出错。为了方便起见，我们把所有pixel不为0的点都设置为1。12345m, n = np.shape(trainData)for i in range(m): for j in range(n): if trainData[i, j] != 0: trainData[i, j] = 1仿照训练集的步骤，导入测试集并做相同处理：12345678910111213141516171819202122testSet = []with open('test.csv','r') as testFile: lines=csv.reader(testFile) for line in lines: testSet.append(line) testSet.remove(testSet[0])testSet = np.array(testSet)rawTestData = testSetrawTestData = np.mat(rawTestData)m, n = np.shape(rawTestData)testData = np.zeros((m, n))for i in range(m): for j in range(n): testData[i, j] = int(rawTestData[i, j])m, n = np.shape(testData)for i in range(m): for j in range(n): if testData[i, j] != 0: testData[i, j] = 1同样的，可使用testData.shape查看测试集的维度，保证它是28000*784，由此可知操作无误。接下来，我们定义KNN的分类函数。12345678910111213141516def classify(inX, dataSet, labels, k): inX = np.mat(inX) dataSet = np.mat(dataSet) labels = np.mat(labels) dataSetSize = dataSet.shape[0] diffMat = np.tile(inX, (dataSetSize, 1)) - dataSet sqDiffMat = np.array(diffMat) ** 2 sqDistances = sqDiffMat.sum(axis = 1) distances = sqDistances ** 0.5 sortedDistIndicies = distances.argsort() classCount=&#123;&#125; for i in range(k): voteIlabel = labels[0, sortedDistIndicies[i]] classCount[voteIlabel] = classCount.get(voteIlabel, 0) + 1 sortedClassCount = sorted(classCount.iteritems(), key = operator.itemgetter(1), reverse = True) return sortedClassCount[0][0]为了更好地分类，这里我们需要选择合适的k值，我选取了4000个样本作为验证机进行尝试，找到误差最小的k值并作为最终的k值输入。1234567891011121314151617181920212223242526272829303132333435trainingTestSize = 4000#分割出验证集m, n = np.shape(trainLabel)trainingTrainLabel = np.zeros((m, n - trainingTestSize))for i in range(m): for j in range(n - trainingTestSize): trainingTrainLabel[i, j] = trainLabel[i, j] trainingTestLabel = np.zeros((m, trainingTestSize))for i in range(m): for j in range(trainingTestSize): trainingTestLabel[i, j] = trainLabel[i, n - trainingTestSize + j] m, n = np.shape(trainData)trainingTrainData = np.zeros((m - trainingTestSize, n))for i in range(m - trainingTestSize): for j in range(n): trainingTrainData[i, j] = trainData[i, j] trainingTestData = np.zeros((trainingTestSize, n))for i in range(trainingTestSize): for j in range(n): trainingTestData[i, j] = trainData[m - trainingTestSize + i, j]#使k值为3到9依次尝试training = []for x in range(3, 10): error = 0 for y in range(trainingTestSize): answer = (classify(trainingTestData[y], trainingTrainData, trainingTrainLabel, x)) print 'the classifier came back with: %d, %.2f%% has done, the k now is %d' % (answer, (y + (x - 3) * trainingTestSize) / float(trainingTestSize * 7) * 100, x) #方便知道进度 if answer != trainingTestLabel[0, y]: error += 1 training.append(error)这个过程比较长，结果会得到training的结果是[156, 173, 159, 164, 152, 155, 156]。可以使用plt.plot(training)更直观地查看误差，呈现如下：注意：这里的下标应该加上3才是对应的k值。可以看图手动选择k值，但由于事先无法把握训练结束的时间，可以编写函数自动选择并使程序继续进行。123456theK = 3hasError = training[0]for i in range(7): if training[i] &lt; hasError: theK = i + 3 hasError = training[i]在确定k值后，接下来就是代入测试集进行结果的计算了。由于KNN算法相对而言比较低级，因此就别指望效率了，跑CPU的话整个过程大概需要半天左右。123456m, n = np.shape(testData)result = []for i in range(m): answer = (classify(testData[i], trainData, trainLabel, theK)) result.append(answer) print 'the classifier came back with: %d, %.2f%% has done' % (answer, i / float(m) * 100)最后，定义一个保存结果的函数，然后saveResult(result)之后，再对csv文件进行处理（后文会提到），然后就可以submit了。1234567def saveResult(result): with open('result.csv', 'w') as myFile: myWriter = csv.writer(myFile) for i in result: tmp = [] tmp.append(i) myWriter.writerow(tmp)最终此方法在kaggle上获得的score为0.96314，准确率还是挺高的，主要是因为问题相对简单，放到leaderboard上，这结果的排名就要到两千左右了。CNN在学习了卷积神经网络和pytorch框架之后，我决定使用CNN对这个比赛再进行一次尝试。首先还是导入相关的包。1234567891011121314151617import torchimport torch.nn as nnimport torch.nn.functional as Fimport torch.optim as optimimport pandas as pdimport numpy as npfrom math import *%matplotlib inlineimport matplotlib.pyplot as pltimport matplotlib.cm as cmimport torch.utils.data as Datafrom torch.autograd import Variableimport csv导入训练数据，可以使用train.head()查看导入的结果，便于后续的处理。1train = pd.read_csv("train.csv")对数据进行处理，由于要使用的是CNN，我们必须要把数据整理成能输入的形式，即从数组变成高维张量。12345train_labels = torch.from_numpy(np.array(train.label[:]))image_size = train.iloc[:, 1:].shape[1]image_width = image_height = np.ceil(np.sqrt(image_size)).astype(np.uint8)train_data = torch.FloatTensor(np.array(train.iloc[:, 1:]).reshape((-1, 1, image_width, image_height))) / 255 #灰度压缩，进行归一化注：reshape中的-1表示自适应，这样我们能让我们更好的变化数据的形式。我们可以使用matplotlib查看数据处理的结果。123plt.imshow(train_data[1].numpy().squeeze(), cmap = 'gray')plt.title('%i' % train_labels[1])plt.show()可以看到如下图片，可以与plt.title进行核对。注：可以用squeeze()函数来降维，例如：从[[1]]—&gt;[1]。与之相反的是便是unsqueeze(dim = 1)，该函数可以使[1]—&gt;[[1]]。以同样的方式导入并处理测试集。12test= pd.read_csv("test.csv")test_data = torch.FloatTensor(np.array(test).reshape((-1, 1, image_width, image_height))) / 255接下来我们定义几个超参数，这里将要使用的是小批梯度下降的优化算法，因此定义如下：1234#超参数EPOCH = 1 #整个数据集循环训练的轮数BATCH_SIZE = 10 #每批的样本个数LR = 0.01 #学习率定义好超参数之后，我们使用Data对数据进行最后的处理。1234567trainData = Data.TensorDataset(train_data, train_labels) #用后会变成元组类型train_loader = Data.DataLoader( dataset = trainData, batch_size = BATCH_SIZE, shuffle = True)上面的Data.TensorDataset可以把数据进行打包，以方便我们更好的使用；而Data.DataLoade可以将我们的数据打乱并且分批。要注意的是，这里不要对测试集进行操作，否则最终输出的结果就难以再与原来的顺序匹配了。接下来，我们定义卷积神经网络。1234567891011121314151617181920212223242526272829303132333435#build CNNclass CNN(nn.Module): def __init__(self): super(CNN, self).__init__() #一个卷积层 self.conv1 = nn.Sequential( nn.Conv2d( #输入(1, 28, 28) in_channels = 1, #1个通道 out_channels = 16, #输出层数 kernel_size = 5, #过滤器的大小 stride = 1, #步长 padding = 2 #填白 ), #输出(16, 28, 28) nn.ReLU(), nn.MaxPool2d(kernel_size = 2), #输出(16, 14, 14) ) self.conv2 = nn.Sequential( #输入(16, 14, 14) nn.Conv2d(16, 32, 5, 1, 2), #这里用了两个过滤器，将16层变成了32层 nn.ReLU(), nn.MaxPool2d(kernel_size = 2) #输出(32, 7, 7) ) self.out = nn.Linear(32 * 7 * 7, 10) #全连接层，将三维的数据展为2维的数据并输出 def forward(self, x): #父类已定义，不能修改名字 x = self.conv1(x) x = self.conv2(x) x = x.view(x.size(0), -1) output = F.softmax(self.out(x)) return outputcnn = CNN()optimzer = torch.optim.Adam(cnn.parameters(), lr = LR) #define optimezerloss_func = nn.CrossEntropyLoss() #loss function使用交叉嫡误差print(cnn) # 查看net architecture完成以上的操作之后，就可以开始训练了，整个训练时间在CPU上只需要几分钟，这比KNN算法要优越许多。123456789101112for epoch in range(EPOCH): for step, (x, y) in enumerate(train_loader): b_x = Variable(x) b_y = Variable(y) output = cnn(b_x) loss = loss_func(output, b_y) #cross entropy loss #update W optimzer.zero_grad() loss.backward() optimzer.step() print('epoch%d' % (epoch + 1), '-', 'batch%d' % step, '-', 'loss%f' % loss) #查看训练过程 print('No.%depoch is over' % (epoch + 1))代入测试集求解：12345output = cnn(test_data[:])#print(output)result = torch.max(output, 1)[1].squeeze()#print(result)仿照KNN中的结果转存函数，定义saveResult函数。1234567def saveResult(result): with open('result.csv', 'w') as myFile: myWriter = csv.writer(myFile) for i in result: tmp = [] tmp.append(i) myWriter.writerow(tmp)最后使用saveResult(result.numpy())把结果存入csv文件。改进然而，若使用上述的CNN，得出的结果在leaderboard上会达到两千三百多名，这已经进入所有参赛者的倒数两百名之内了。为什么这个CNN的表现甚至不如我前面的KNN算法呢？我觉得主要有下面三个原因。首先，由于CNN的参数较多，仅经过1轮epoch应该是不足够把所有参数训练到最优或者接近最优的位置的。个人认为，靠前的数据在参数相对远离最优值时参与训练而在之后不起作用，很有可能导致最后顾此失彼，因此有必要增加epoch使之前的数据多次参与参数的校正。同时，也要增大batch size使每次优化参数使用的样本更多，从而在测试集上表现更好。训练结束后，我发现我的C盘会被占用几个G，不知道是不是出错了，也有可能是参数占用的空间，必须停止kernel才能得到释放（我关闭了VScode后刷新，空间就回来了）。关于内存，这里似乎存在着一个问题，我将在后文阐述。注：由于VScode前段时间也开始支持ipynb，喜欢高端暗黑科技风又懒得自己修改jupyter notebook的小伙伴可以试一试。学习率过大。尽管我这里的学习率设置为0.01，但对于最后的收敛来说或许还是偏大，这就导致了最后会在最优解附近来回抖动而难以接近的问题。关于这个问题，可以到deep-learning笔记：学习率衰减与批归一化中看看我较为详细的分析与解决方法。由于训练时间和epoch轮数相对较小，我推测模型可能会存在过拟合的问题。尤其是最后的全连接层，它的结构很容易造成过拟合。关于这个问题，也可以到machine-learning笔记：过拟合与欠拟合和machine-learning笔记：机器学习中正则化的理解中看看我较为详细的分析与解决方法。针对上述原因，我对我的CNN模型做了如下调整：首先，增加训练量，调整超参数如下。1234#超参数EOPCH = 3BATCH_SIZE = 50LR = 1e-4引入dropout随机失活，加强全连接层的鲁棒性，修改网络结构如下。1234567891011121314151617181920212223242526272829303132333435#build CNNclass CNN(nn.Module): def __init__(self): super(CNN, self).__init__() #一个卷积层 self.conv1 = nn.Sequential( nn.Conv2d( #输入(1, 28, 28) in_channels = 1, #1个通道 out_channels = 16, #输出层数 kernel_size = 5, #过滤器的大小 stride = 1, #步长 padding = 2 #填白 ), #输出(16, 28, 28) nn.ReLU(), nn.MaxPool2d(kernel_size = 2), #输出(16, 14, 14) ) self.conv2 = nn.Sequential( #输入(16, 14, 14) nn.Conv2d(16, 32, 5, 1, 2), #这里用了两个过滤器，将16层变成了32层 nn.ReLU(), nn.MaxPool2d(kernel_size = 2) #输出(32, 7, 7) ) self.dropout = nn.Dropout(p = 0.5) #每次减少50%神经元之间的连接 self.fc = nn.Linear(32 * 7 * 7, 1024) self.out = nn.Linear(1024, 10) #全连接层，将三维的数据展为2维的数据并输出 def forward(self, x): x = self.conv1(x) x = self.conv2(x) x = x.view(x.size(0), -1) x = self.fc(x) x = self.dropout(x) output = F.softmax(self.out(x)) return output本想直接使用torch.nn.functional中的dropout函数轻松实现随机失活正则化，但在网上看到这个函数好像有点坑，因此就不以身试坑了，还是在网络初始化中先定义dropout。注：训练完新定义的网络之后我一直在思考dropout添加的方式与位置。在看了一些资料之后，我认为或许去掉全连接层、保持原来的层数并在softmax之前dropout可能能达到更好的效果。考虑到知乎上有知友提到做研究试验不宜在MNIST这些玩具级别的数据集上进行，因此暂时不再做没有太大意义的调整，今后有空在做改进试验。经过上面的改进后，我再次训练网络并提交结果，在kaggle上的评分提高至0.97328，大约处在1600名左右，可以继续调整超参数（可以分割验证集寻找）和加深网络结构以取得更高的分数，但我的目的已经达到了。与之前的KNN相比，无论从时间效率还是准确率，CNN都有很大的进步，这也体现了深度学习相对于一些经典机器学习算法的优势。出现的问题由于这个最后的网络是我重复构建之后完成的，因此下列部分问题可能不存在于我上面的代码中，但我还是想汇总在这，以防之后继续踩相同的坑。报错element 0 of tensors does not require grad and does not have a grad_fnpytorch具有自动求导机制，这就省去了我们编写反向传播的代码。每个Variable变量都有两个标志：requires_grad和volatile。出现上述问题的原因是requires_grad = False，修改或者增加（因为默认是false）成True即可。RuntimeError: Dimension out of range (expected to be in range of [-1, 0], but got 1)这个好像是我在计算交叉熵时遇到的，原因是因为torch的交叉熵的输入第一个位置的输入应该是在每个label下的概率，而不是对应的label，详细分析与举例可参考文首给出的第三个链接。AttributeError: ‘tuple’ object has no attribute ‘numpy’为了查看数据处理效果，我在数据预处理过程中使用matplotlib绘制出处理后的图像，但是却出现了如上报错，当时的代码如下：123plt.imshow(trainData[1].numpy().squeeze(), cmap = 'gray')plt.title('%i' % train_labels[1])plt.show()查找相关资料之后，我才知道torch.utils.data会把打包的数据变成元组类型，因此我们绘图还是要使用原来train_data中的数据。转存结果时提醒DefaultCPUAllocator: not enough memory由于当初在实现KNN算法转存结果时使用的函数存入csv文件后还要对文件进行空值删除处理，比较麻烦（后文会写具体如何处理），因此我想借用文章顶部给出的第二个链接中提供的方法：123out = pd.DataFrame(np.array(result), index = range(1, 1 + len(result)), columns = ['ImageId', 'Label'])#torch和pandas的类型不能直接的转换，所以需要借助numpy中间的步骤，将torch的数据转给pandasout.to_csv('result.csv', header = None)结果出现如下错误：我好歹也是八千多买的DELL旗舰本，8G内存，它居然说我不够让我换块新的RAM？什么情况…尝试许久，我怀疑是训练得到的参数占用了我的内存，那只好先把训练出的result保存下来，再导入到csv文件。最后我还是选择自己手动处理csv文件中的空值，应该有其它的转存csv文件的方法或者上述问题的解决措施，留待以后实践过程中发现解决，也欢迎大家不吝赐教。excel/csv快速删除空白行如果你使用的是我的saveResult函数或者类似，你就很有可能发现更新后的csv文件中数据之间双数行都是留空的，即一列数据之间都有空白行相隔，那么可以使用如下方法快速删除空白行。选中对应列或者区域。在“开始”工具栏中找到“查找与选择”功能并点击。在下拉菜单中，点击“定位条件”选项。在打开的定位条件窗口中，选择“空值”并确定。待电脑为你选中所有空值后，任意右键一个被选中的空白行，在弹出的菜单中点击“删除”。如果数据量比较大，这时候会有一个处理时间可能会比较长的提醒弹出，确认即可。等待处理完毕。]]></content>
      <categories>
        <category>人工智能</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>深度学习</tag>
        <tag>踩坑血泪</tag>
        <tag>代码实现</tag>
        <tag>pytorch</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[hexo笔记：SEO优化]]></title>
    <url>%2Fhexo20191101212014%2F</url>
    <content type="text"><![CDATA[在toefl笔记：首考考托福——记一次裸考经历文章末尾我曾提到在被百度收录之后要好好做SEO，这段时间我也的确有所尝试与改进，因此在本文中将一些我认为比较有效的或者依旧存疑的SEO优化方法写下来，供日后参考深究。本文参考自：https://blog.csdn.net/lzy98/article/details/81140704https://www.jianshu.com/p/86557c34b671https://baijiahao.baidu.com/s?id=1616368344109675728&amp;wfr=spider&amp;for=pchttps://www.jianshu.com/p/7e1166eb412ahttps://baijiahao.baidu.com/s?id=1597172076743185609&amp;wfr=spider&amp;for=pcSEO之前在知乎上碰巧看到一篇别人是如何推广自己的博客的文章，里面就提到了SEO这个概念。我当时也很好奇，百度之后才发现它完全不同于CEO、CTO等概念。SEO（Search Engine Optimization），汉译为搜索引擎优化。它是一种方式，即利用搜索引擎的规则提高网站在有关搜索引擎内的自然排名。通俗的讲就是post的内容更容易被搜索引擎搜索到或者收录，且在搜索结果列表中显示靠前。看了一圈，SEO的办法真的是多种多样，下面我就简单记录一部分我试过的方法。优化url同样在站点配置文件下面，可以找到站点的url设置。如果你尚未更改过，你会发现默认的url是http://yoursite.com，我在这里吃了不少亏，之前苹果上add to favorites、RSS订阅后点开的链接以及copyright的链接都会直接跳转到yoursite而非我的博文链接。SEO搜索引擎优化认为，网站的最佳结构是用户从首页点击三次就可以到达任何一个页面，但是我们使用hexo编译的站点默认打开文章的url是：sitename/year/mounth/day/title四层的结构，这样的url结构很不利于SEO，爬虫就会经常爬不到我们的文章，于是，我们可以将url直接改成sitename/title的形式，并且title最好是用英文（中文的url会出现好多乱码，我这方面还有待改进）。基于以上原因，我在根目录的站点配置文件下修改url设置如下（注释中是默认的）：如此，再次添加RSS订阅，就可以跟yoursite这个鬼地方say goodbye啦。对permalink的修改将会是你的站点的一次巨大的变动，会造成大量的死链。死链会造成搜索引擎对网站的评分降低并有可能会降权。我们可以直接百度搜索“site:url”（url即你的站点网址）查看已经被搜索引擎收录的网址。如下图所示，目前我已被收录了四个，其中前两个经此番调整已成为死链。这时我们可以在百度站长平台中提交死链，由于死链文件制作稍较复杂，我们可以选择规则提交的方式提交死链（处理死链过程较长，我提交的死链目前还在处理中）。很重要的是，我们需要在自己的所有博文中修改链接，我使用了VScode的搜索关键字符功能对所有markdown文件进行了修改，效率相对较高。此外，如果使用了leancloud等第三方服务，那么也需要修改对应的url与新的相匹配，否则会造成原来数据的丢弃，还是挺可惜的。压缩文件关于压缩的方法，网上有很多，可以选择简易的应用。我选择的是用hexo-neat，安装插件后在站点配置文件添加如下设置，效果不错。12345678910111213141516171819202122# hexo-neat# 博文压缩neat_enable: true# 压缩htmlneat_html: enable: true exclude:# 压缩css neat_css: enable: true exclude: - &apos;**/*.min.css&apos;# 压缩jsneat_js: enable: true mangle: true output: compress: exclude: - &apos;**/*.min.js&apos; - &apos;**/jquery.fancybox.pack.js&apos; - &apos;**/index.js&apos;添加完成之后，每次generate你就会在git bash终端看到neat压缩的反馈信息。另外也有和很多网友使用的是gulp压缩，设置也很简便且有效。压缩网站文件不仅可以提高访问加载的速度，同时减少了大量空白符，对SEO也是有不小的帮助的，推荐尝试。主动推送首先在根目录下安装插件npm install hexo-baidu-url-submit --save。在根目录站点配置文件中新增如下字段：12345baidu_url_submit: count: 100 # 提交最新的一个链接 host: gsy00517.github.io # 在百度站长平台中注册的域名 token: lY..........Fk # 请注意这是您的秘钥，所以请不要把博客源代码发布在公众仓库里! path: baidu_urls.txt # 文本文档的地址，新链接会保存在此文本文档里域名和秘钥可以在站长工具平台的连接提交中的接口调用地址中找到，即对应host与token后面的字段。再把主题配置文件中的deploy修改如下：123456789# Deployment## Docs: https://hexo.io/docs/deployment.htmldeploy:- type: git repo: github: git@github.com:Gsy00517/Gsy00517.github.io.git coding: git@git.dev.tencent.com:gsy00517/gsy00517.git branch: master- type: baidu_url_submitter注意：必须严格按照上述格式，否则无法deploy。这样以后每次执行hexo d，新的链接就会主动推送给百度，然后百度就会更快地派爬虫来发现你站点中的新链接，可以在第一时间收录新建的链接。自动推送自动推送是百度搜索资源平台为提高站点新增网页发现速度推出的工具，安装自动推送JS代码的网页，在页面被访问时，页面url将立即被推送给百度。详情可以查看百度的站长工具平台使用帮助。事实上，如果已经实行了主动推送，那么自动推送其实不是那么必要，因为主动推送是在生成url的时候第一时间进行推送，之后访问页面时进行的自动推送就显得晚了一步。不同推送方式的效果大概是：主动推送&gt;自动推送&gt;sitemap。下面还是写一下自动推送的实现方法。在blog\themes\next\source\js\src目录下，创建名为bai.js的文件，并根据百度提供的自动推送功能方法添加以下代码：1234567891011121314&lt;script&gt;(function()&#123; var bp = document.createElement(&apos;script&apos;); var curProtocol = window.location.protocol.split(&apos;:&apos;)[0]; if (curProtocol === &apos;https&apos;) &#123; bp.src = &apos;https://zz.bdstatic.com/linksubmit/push.js&apos;; &#125; else &#123; bp.src = &apos;http://push.zhanzhang.baidu.com/push.js&apos;; &#125; var s = document.getElementsByTagName(&quot;script&quot;)[0]; s.parentNode.insertBefore(bp, s);&#125;)();&lt;/script&gt;此外，还可以blog\scaffolds目录下的模板文件post.md的分隔线之后添加这么一行：1&lt;script type=&quot;text/javascript&quot; src=&quot;/js/src/bai.js&quot;&gt;&lt;/script&gt;这样以后每次创建新的文章就会自动在文末添加这行代码，即在生成的模板中包含这行代码。如此，只要访问你的这个页面，它就会自动向百度推送你的这个网页。sitemap首先需要安装sitemap站点地图自动生成插件。windows下打开git bash，输入安装命令：12npm install hexo-generator-sitemap --savenpm install hexo-generator-baidu-sitemap --save然后在站点配置文件_config.yml中找到如下对应的位置（一般默认有，没有的话可以添加），修改如下：12345# 自动生成sitemapsitemap: path: sitemap.xmlbaidusitemap: path: baidusitemap.xml特别要注意的是，上面的path一定要缩进，否则在hexo generate时会无法编译导致报错。（似乎有一些版本的hexo不存在这样的问题，关于版本可以使用hexo version命令查看）这样以后每次generate后都会在public目录下面生成sitemap.xml和baidusitemap.xml两个文件，即你的站点地图。也可以deploy后直接在域名后面加上这两个文件名查看你的站点地图。在百度站长平台中，有sitemap提交的选项，由于我当初提交的网站协议前缀是http，因此xml文件所在的https前缀的链接不属于我提交的网站，而我的github page和coding page都设置了强制https访问。这个问题以后有机会再做解决，不存在这个问题的可以试试提交sitemap。疑惑在优化的过程中，我发现我的post模板也被改变了（原因目前未知），从原本的：12345title: &#123;&#123; title &#125;&#125;date: &#123;&#123; date &#125;&#125;tags:copyright: truetop:变成了：1234567891011---noteId: &quot;9bfafbb............dd5a3&quot;tags: []title: [object Object]: nulldate: [object Object]: nullcopyright: truetop: null---更奇怪的是，我无法删除noteId并恢复到原来的样式，每次更改保存之后又会自动给我换回来，为了方便使用，我将其修改为：1234567891011---noteId: &quot;55c6d..............d6fcf&quot;title: &#123;&#123; title &#125;&#125;date: &#123;&#123; date &#125;&#125;copyright: truetop:categories:tags:---这样就可以直接提取文章标题和创建时间了。对于noteId的作用，网上也找不到相关信息，可能是类似于网站的ID标识的一个代号吧，我对它之后的改进以及用法可见后文。使用noteId改进url今天看了几个url中含有noteId的网站，立马想到其实noteId其实可以用来替代url的中文等符号从而消除乱码，这更方便了爬虫的抓取。于是，我把站点配置文件下的url设置修改如下：同时我把模板文件post.md修改为：12345678910111213---noteId: &apos;prefix+time remember to change!!!&apos;title: &#123;&#123; title &#125;&#125;date: &#123;&#123; date &#125;&#125;copyright: truetop:categories:tags:---&lt;script type=&quot;text/javascript&quot; src=&quot;/js/src/bai.js&quot;&gt;&lt;/script&gt;这就把我的博文网址修改成了“关键词+创建时间”的形式，当然要手动更改。同样的，以上的操作也带来了巨大的麻烦。我需要给之前没有生成noteId的博文一一加上noteId，同时也免不了对外部辅助平台和网站内链的大幅度修改。对于检查网站死链，我推荐一个挺实用的轻量工具Xenu，下载安装之后，选择file，然后check URL，输入网站地址，即可检查站内所有的连接中是否存在死链。下面是我仅修改了url设置而未更改内链时检测的情况，其中红色的就是死链。修改url之后大约是在我修改了url格式的两天后，当我再用“site:url”查询收录情况时，我发现被收录的死链已经减少了一个（似乎不是提交死链的原因，因为规则提交的死链还在处理中），然而我之前被收录、修改后原url依然可用的主页和分类页面却也消失了，这就使我非常得纳闷。这几日也查找了许多资料寻找原因，总结如下。首先，网站url的变动产生大量死链，很有可能会导致网站排名消失，原来积累的权重大大减少甚至清除。还好目前我只是一个新站，倘若已运行并被收录了一段时间，应该要慎重考虑是否是因为网址必须得精简等原因从而放弃网站的排名。要注意的是，如果网站url链接过深，会影响搜索引擎蜘蛛抓取，时间久了，蜘蛛来的次数就会减少，最后导致网站不收录。一般建议扁平化结构，url在三层以内方便蜘蛛爬行，这在上文也提到过。此外，如果是新站的话，收录之后消失也是正常的。事实上，上线6个月之内的网站都可以被称为新站。因为搜索引擎蜘蛛对新站有一个好奇心，发现新鲜的事物喜欢去抓取一下，这就是收录，但是收录之后会有一个审核期，包括这个收录之后又消失的问题，审核期过后如果在数据库找不到相同的信息就会认为这是一篇原创，这个时候再去看收录就又会恢复了。值得注意的是，新站上线短期内，只新增更新内容就行了，不要去改动以前的内容（特别是标题、url等，搜索引擎对这些内容很敏感）以免延长新站考核时间，当网站索引趋于稳定状态后可以适当改动。总而言之，目前没什么好担心的，担心也没有用，还是认认真真好好地写笔记好啦！添加robots文件Robots协议（也称为爬虫协议、机器人协议等）的全称是“网络爬虫排除标准”（Robots Exclusion Protocol），网站可以通过Robots协议告诉搜索引擎哪些页面可以抓取，哪些页面不能抓取。如果一个网站使用大量的js、flash、ifrmae等内容，或者如果一个网站结构混乱，那么整个网站就会是乱七八糟、毫无章法，不仅用户体验极差，更重要的是蜘蛛也不会喜欢，也没有心思去抓取网站的内容了。robots.txt是搜索引擎蜘蛛访问网站时要查看的第一个文件，并且会根据robots.txt文件的内容来爬行网站。在某种意义上说，它的一个任务就是指导蜘蛛爬行，减少搜索引擎蜘蛛的工作量。当搜索引擎蜘蛛访问网站时，它会首先检查该站点根目录下是否存在robots.txt文件，如果该文件存在，搜索引擎蜘蛛就会按照该文件中的内容来确定爬行的范围；如果该文件不存在，则所有的搜索引擎蜘蛛将能够访问网站上所有没有被口令保护的页面。通常搜索引擎对网站派出的蜘蛛是有配额的，多大规模的网站放出多少蜘蛛。如果我们不配置robots文件，那么蜘蛛来到网站以后会无目的地爬行，造成的一个结果就是，需要它爬行的目录，没有爬行到，不需要爬行的，也就是我们不想被收录的内容却被爬行并放出快照。所以robots文件对于SEO具有重要的意义。如果网站中没有robots.txt文件，则网站中的程序脚本、样式表等一些和网站内容无关的文件或目录即使被搜索引擎蜘蛛爬行，也不会增加网站的收录率和权重，只会浪费服务器资源。此外，搜索引擎派出的蜘蛛资源也是有限的，我们要做的应该是尽量让蜘蛛爬行网站重点文件、目录，最大限度的节约蜘蛛资源。在站点根目录的source文件下添加robots.txt文件，加入如下内容：1234567891011121314User-agent: * Allow: /Allow: /archives/Disallow: /categories/Disallow: /tags/Disallow: /vendors/Disallow: /js/Disallow: /css/Disallow: /fonts/Disallow: /vendors/Disallow: /fancybox/Sitemap: https://gsy00517.github.io/sitemap.xmlSitemap: https://gsy00517.github.io/baidusitemap.xml注意sitemap中要修改成自己的url。另外，可以在站长工具平台检测robots文件。]]></content>
      <categories>
        <category>操作和使用</category>
      </categories>
      <tags>
        <tag>hexo</tag>
        <tag>配置优化</tag>
        <tag>SEO</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[machine learning笔记：机器学习的几个常见算法及其优缺点]]></title>
    <url>%2Fmachine-learning20191101192042%2F</url>
    <content type="text"><![CDATA[接触机器学习也有一段较长的时间了，不敢说自己全部掌握甚至精通，但是期间也了解或者尝试了许多机器学习的算法。这次就结合参考资料和我自己的感受小结一下几种机器学习的常见算法及其优点和缺点。决策树算法学过数据结构中的树应该对这个算法不会感到困惑，下面就简单介绍一下其优缺点。优点易于理解和解释，可以可视化分析，容易提取出规则。可以同时处理标称型和数值型数据。测试数据集时，运行速度比较快。决策树可以很好的扩展到大型数据库中，同时它的大小独立于数据库大小。缺点对缺失数据处理比较困难。容易出现过拟合问题，容易受到例外的干扰，对测试集非常不友好。忽略数据集中属性的相互关联。ID3算法计算信息增益时结果偏向数值比较多的特征。改进措施对决策树进行剪枝。可以采用交叉验证法和加入正则化的方法。较为理想的决策树是叶子节点数少且深度较小。使用基于决策树的combination算法，如bagging算法，randomforest算法，可以解决过拟合的问题。常见算法C4.5算法ID3算法是以信息论为基础，以信息熵和信息增益度为衡量标准，从而实现对数据的归纳分类。ID3算法计算每个属性的信息增益，并选取具有最高增益的属性作为给定的测试属性。C4.5算法核心思想是ID3算法，是ID3算法的改进，改进方面有：用信息增益率来选择属性，克服了用信息增益选择属性时偏向选择取值多的属性的不足。在树构造过程中进行剪枝。能处理非离散的数据。能处理不完整的数据。优点产生的分类规则易于理解，准确率较高。缺点在构造树的过程中，需要对数据集进行多次的顺序扫描和排序，因而导致算法的低效。C4.5只适合于能够驻留于内存的数据集，当训练集大得无法在内存容纳时程序无法运行。CART分类与回归树这是一种决策树分类方法，采用基于最小距离的基尼指数估计函数，用来决定由该子数据集生成的决策树的拓展形。如果目标变量是标称的，称为分类树；如果目标变量是连续的，称为回归树。分类树是使用树结构算法将数据分成离散类的方法。优点非常灵活，可以允许有部分错分成本，还可指定先验概率分布，可使用自动的成本复杂性剪枝来得到归纳性更强的树。在面对诸如存在缺失值、变量数多等问题时CART显得非常稳健。下面对决策树的各种算法做一个小结：算法支持模型树结构特征选择ID3分类多叉树信息增益C4.5分类多叉树信息增益比CART分类、回归二叉树基尼系数、均方差补充：信息熵：表示随机变量的不确定性，熵越大，不确定性越大。这与物理中的熵性质类似。信息增益：即不确定性减小的幅度。信息增益=信息熵（前）-信息熵（后）。在构造决策树的时候往往选择信息增益大的特征优先作为节点分类标准。信息增益比：由于仅根据信息增益构建决策树，那么三叉树以及多叉树比二叉树的效果一般来说分类效果要好，然而这很有可能会导致过拟合的问题。因此定义信息增益比=惩罚参数*信息增益。当特征个数较多时，惩罚参数较小；当特征个数较少时，惩罚参数较大，从而使信息增益比较大，进而克服信息增益偏向于选取取值较多的特征的问题。总的来说，信息增益比相对于信息增益更客观。基尼系数：表示集合的不确定性，基尼系数越大，则表示不平等程度越高。分类算法KNN算法优点KNN是一种在线技术，新数据可以直接加入数据集而不必进行重新训练。KNN理论简单，容易实现。实际上，KNN没有训练过程，或者说，它的训练过程就是导入数据集。缺点KNN对于样本容量大的数据集计算量比较大，极易引发维度灾难。样本不平衡时，预测偏差比较大。如：某一类的样本比较少，而其它类样本比较多。KNN每一次分类都会重新进行一次全局运算，耗时久。这在实践中会非常有体会，可以参考kaggle笔记：手写数字识别——使用KNN和CNN尝试MNIST数据集。在CV领域，KNN已经被完全弃用。这是因为它不适合用来表示图像之间的视觉感知差异，如下图所示，这是CS231n中提到的一个例子，后三张图片经过不同的变换，结果与第一张原图的L2距离居然是一样的，而显然对我们而言这三张图是有很大区别的，在实际应用中往往应该区分开。应用领域文本分类。模式识别。聚类分析。多分类领域。支持向量机（SVM）支持向量机是一种基于分类边界的方法。其基本原理是（以二维数据为例）：如果训练数据分布在二维平面上的点，它们按照其分类聚集在不同的区域。基于分类边界的分类算法的目标是：通过训练，找到这些分类之间的边界（直线的称为线性划分，曲线的称为非线性划分）。对于多维数据（如N维），可以将它们视为N维空间中的点，而分类边界就是N维空间中的面，称为超面（超面比N维空间少一维）。线性分类器使用超平面类型的边界，非线性分类器使用超曲面。支持向量机的原理是将低维空间的点映射到高维空间，使它们成为线性可分，再使用线性划分的原理来判断分类边界。在高维空间中是一种线性划分，而在原有的数据空间中，是一种非线性划分。在我的博文machine-learning笔记：一个支持向量机的问题中，我提及了SVM的简介与一个问题，感兴趣的话可以了解一下。优点解决小样本下机器学习问题，相对于其他训练分类算法不需要过多样本。解决非线性问题。擅长应付线性不可分，主要用松弛变量（惩罚变量）和核函数来实现。无局部极小值问题。（相对于神经网络等算法）引入了核函数，可以很好的处理高维数据集。泛化能力比较强。结构风险最小，指分类器对问题真实模型的逼近与真实解之间的累计误差。缺点对于核函数的高维映射解释力不强，尤其是径向基函数。对缺失数据敏感。应用领域：文本分类。图像识别。主要二分类领域。朴素贝叶斯算法朴素贝叶斯，即naive bayes。说白了就是要“sometimes naive”。优点对大数量训练和查询时具有较高的速度。即使使用超大规模的训练集，针对每个项目通常也只会有相对较少的特征数，并且对项目的训练和分类也仅仅是特征概率的数学运算而已。支持增量式运算。即可以实时的对新增的样本进行训练。朴素贝叶斯对结果解释容易理解。缺点由于使用了样本属性独立性的假设，所以如果样本属性有关联时其效果不好。应用领域文本分类。欺诈检测。Logistic回归算法优点计算代价不高，易于理解和实现。缺点容易产生欠拟合。分类精度不高。应用领域用于二分类领域，可以得出概率值，适用于根据分类概率排名的领域，如搜索排名等。Logistic回归的扩展softmax可以应用于多分类领域，如手写字识别等。聚类算法K-means算法K-means算法，即K均值算法，是一个简单的聚类算法，把n的对象根据他们的属性分为k个分割，k小于n。算法的核心就是要优化失真函数J，使其收敛到局部最小值但不是全局最小值。它比较适合凸数据集，即任意两个数据点之间的连线都在数据集内部。算法流程随机选择k个随机的点（称为聚类中心）。对数据集中的每个数据点，按照距离k个中心的距离，将其与最近的中心点关联起来，与同一中心点关联的点聚成一类。计算每一组的均值，将该组所关联的中心点移到平均值的位置。重复第2、3两步，直到中心点不再变化。优点算法速度很快。缺点分组的数目k是一个输入超参数，不合适的k可能返回较差的结果。EM最大期望算法EM算法是基于模型的聚类方法，是在概率模型中寻找参数最大似然估计的算法，其中概率模型依赖于无法观测的隐藏变量。E步估计隐含变量，M步估计其他参数，交替将极值推向最大。EM算法比K-means算法计算复杂，收敛也较慢，不适于大规模数据集和高维数据，但比K-means算法计算结果稳定、准确。EM经常用在机器学习和计算机视觉的数据集聚（Data Clustering）领域。集成算法（AdaBoost）俗话说的好“三个臭皮匠，顶个诸葛亮”，集成算法就是将多个弱分类器集成在一起，构建一个强分类器。事实上，它可能不属于算法，而更像一种优化手段。优点很好的利用了弱分类器进行级联。可以将不同的分类算法作为弱分类器。AdaBoost具有很高的精度。相对于bagging算法和randomforest算法，AdaBoost充分考虑的每个分类器的权重。缺点AdaBoost迭代次数也就是弱分类器数目不太好设定，可以使用交叉验证来进行确定。数据不平衡导致分类精度下降。训练比较耗时，每次重新选择当前分类器最好切分点。应用领域模式识别。计算机视觉领域。二分类和多分类场景。神经网络算法优点分类准确度高，学习能力极强。对噪声数据鲁棒性和容错性较强。有联想能力，能逼近任意非线性关系。缺点神经网络参数较多，权值和阈值。我在训练一个只有四层的CNN时，C盘就被占用了几个G，详细情况可见kaggle笔记：手写数字识别——使用KNN和CNN尝试MNIST数据集。黑盒过程，不能观察中间的结果，甚至无法完全理解其是怎么达到效果的。学习过程比较长，有可能陷入局部极小值。应用领域计算机视觉。自然语言处理。语音识别等。]]></content>
      <categories>
        <category>人工智能</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[toefl笔记：首考考托福——记一次裸考经历]]></title>
    <url>%2Ftoefl20191019150438%2F</url>
    <content type="text"><![CDATA[今天上午终于把自己开学以来耿耿于怀的托福考完了。目前来看是铁定要二战了，因此下午抽空把这次考试的经历总结一下，方便再战的时候可以吸取经验和教训。前期大一下学期的时候去听了一个学姐的出国分享会，当然是新东方支持的，讲座已结束就被新东方的老师搞了一波传销。当时也不知道托福的有效期是两年，头脑一热就报了班和十月份的托福。不过后来想想可能暑研和暑校也用得上，或许不亏。于是我抽了个周末去新东方校区做了一个入班小测，很幸运的是我的分能进入强化班，毕竟高中的底子还可以。然而很坑的是，我从同学那得知去年同期的同一个班报名费比我少了将近一千（我报的强化班4400rmb），简直坐地起价啊！可能时间比较早校方不是很担心没人报。然后到了暑假，我开始零零散散地准备。在家做了一套听力一套阅读，心态就崩了，怎么这么难！感觉就是跟四六级不在一个档次上。于是三分钟热度就被浇没了，之后就只是背单词了，打算等八月上了课听了老师的解题方法再强化练习。到了八月中旬，开始上课了，听了几节课明白题型之后，感觉托福或许也没有想象得那么可怕，熟悉就好。那段时间回去后断断续续刷了几套TPO的阅读和听力部分。然后开学一周，很快就到了国庆。根据我的原计划，身为拖延症重度患者的我打算在国庆力挽狂澜，为此我早早地准备好了新东方的《7天搞定托福高频核心词》，当时觉得时间充裕，计划合理，未来充满希望。然而…等到国庆结束，我也明白了一个道理：不能被事物的表面现象所迷惑。不过，乐观的我依然觉得剩下的两周足以完成复习。However，国庆上来劈头盖脸砸过来的课程和任务让我分身乏术。周三晚上的实验课，我做到十一点才回寝室，这更是对我的致命一击。因为回寝太晚，没时间更换被子，导致挨冻一晚上（武汉的天气太怪了）。最后，又是喉咙痛，又是犯鼻炎，那时就感觉托福基本要凉。考前第二天，我又刷了一套新托福的阅读和听力，成绩不是特别理想。由于新东方的TPO加载速度感人，也可能是学校网络的问题，总之直到考试之前，我只在小站刷过三套TPO，在新东方刷过两套老TPO和一套新TPO。甚至直到写这篇文章的标题之前，我都不知道托福的英文拼写是“TOEFL”（之前一直觉得是“TOFEL”）。考试前一天，病情加重，我也就不刷题了，把上课的笔记和题型又好好熟悉了一下就早点休息了。考前考前问了新东方替我报名的老师，她说要打印确认信。不知道为什么我无法下载确认信成pdf格式的，最后屏幕截图打算打印，早上去考场的时候却忘记了。不过还好最后发现根本不需要确认信。早上提早一个小时出寝室，结果发现找不到租八戒了，不知道为什么周六大家起这么早。最后租了辆摩拜拖着病体艰难地骑到了考场。到了考试的楼下，有一个小姐姐热心地给我指路，不过我马上就发现她别有目的。她让我填一个貌似是培训机构的表格，善良易上当的我稀里糊涂地填了，本来想写个假的电话，结果感冒头很晕也没多想就如实写了，反正我平时也不怎么接陌生电话。坐电梯到了考试的楼层，碰到我们学院一个经常见到的学长在做志愿者。他总是活跃在各种场合，好像是英语协会的，总之看到他也是开心了一小下。之后就是看序号，签到，领钥匙，去存背包。在储物室的时候，一边的考试人员一直重复说“A考场的人可以把水拿出来”，我没听太懂。由于之前问过她我鼻炎犯了可不可以带餐巾纸（她说考场会提供），就不想再问第二次了。之前看别人的考试经历，说中途休息时可以出来喝水吃零食，还可以看写作模板，我以为是可以回到储物室的，后来才发现不能。放完东西，我本来想再看会英语进入一下状态，结果过安检之后就只能一直在里边等了。我们来到一个签承诺书的房间，大家都一排排坐在一种比较矮的长凳上，我拿了一张承诺书和一支笔就往后坐了。其实还可以拿个写字的时候用来垫的板子，我没注意，不过好多人和我一样都是在腿上或者趴凳子上写的。写承诺书的时候我没仔细看黑板上的要求，写错了一次，只好挺无奈地找工作人员换了一张。不一会人就好多了，我发现这次考托福的女生比较多，大约是男生的两倍，这在我们学校很不常见啦。考试的也有大人，在我观察是不是还有培训机构的老师的时候，我的隔壁也是实验班的一个朋友也来考试了。他在C考场，那个考场更大。我的A考场人最少，相对来说环境要理想一些。不过不同考场的同学还是在同一个房间等待的。我继续观察，发现还是有几个大二面孔的，和几个人说了几句，发现还是有不少首考的人的。入场过了一会有一个男老师进来说一些有关考试的注意事项，说完没多久大家就到隔壁的考试教室刷脸入场了。考场的教室和等候的教室一样，也是黄色的日光灯，看着也挺舒适。入场顺序是按照姓氏的首字母顺序的，我进去的比较早。尽管A考场大概也就二三十个人，但整个入场过程还是挺久的。考官把我领到座位上，虽然是随机抽的但好像我的考位还是我的序号。为我把身份证插在旁边的卡槽里之后，考官又为我输了激活码进入考试界面，然后没说什么就走了。考试的隔间挺好，靠桌子往里坐一点就完全看不到旁边了。首先是确认姓名的界面，然而考官走了我也没处问，担心确认了就直接开始考试了因此久久没敢点。由于别人还在入场，因此我不敢太早开始考试。我回头看了一眼，发现是我进候考室以来就注意到的那个男生。虽然没问过他，但看上去他这次绝对不是一战了。过了一会，我听到有人点鼠标的声音，于是我也鼓足勇气开始点。前面大概有七八的页面都是只需continue的direction界面，而且这个界面是不会自动跳转的，我在这里停留了很久。终于，大概第十个人入场的时候，我听到有人开始试音了。意外的是，第一个开始试音的人居然真的在介绍他生活的城市。哈哈哈看来也是首考的，不知道待会整个考场一齐开始诠释人类的本质的时候，他有什么感想。这里我暗暗庆幸自己报了班。当大家都在诠释人类的本质时，我心里觉得还是挺可乐的。不过就在这时，我听到前面提到的那位久经沙场的老哥也开始复读了，于是我又点了一个continue。每个continue我的拖好久才点，不过入场真的是挺久的。大概有十个人完成试音之后，我才看到了describe the city you live in，心想这个时间还是可以的。阅读直到考试，我才知道review的用法，点击之后，会出现一个表格，可以看到哪些题已经answered了。阅读第一篇是关于研究者根据化石推断远古的自然环境，第二篇是什么记不太清了，好像也是历史相关的，第三篇是北美西海岸土著人的一些文化，还有配图。没有遇到加试。总的来说，考场里效率还是比寝室要好一些的。我大概还有40分钟时做完了第一篇，到最后一篇时时间还有20多分钟，相对来说还是比较宽裕的。听力接下来就是听力了，我的听力是加试，有3个section。第一个section的对话我考虑太久，导致最后答lecture三道题要在一分钟之内答完。当时也只能以这个section只有50%的概率计入成绩来安慰自己。第二个section做得还行，一些笔记还是没记到要点上，还是得多练。遗憾的是，我听力有好几篇都没听懂听力开头“you will hear part of a lecture in a ……gy class”中学科具体是什么，如果能听懂的话肯定是有一定帮助的，词汇量还不够啊！到了第三个section，我之前在考试教室门外抽的餐巾纸用完了，我只能忍着做题，结果第三个section的lecture的conversation中的男老师似乎有异国口音，说得很不清楚，在lecture部分我也走了一会神。同样的，我发现我不是很善于掌握1个conversation+1个lecture情况下的时间，lecture的时间又分配得不多。当时也只能又以这个section只有50%的概率计入成绩来安慰自己，好吧其实两个section都凉了。考试前两天对自己的listening还是最自信的，现在看来还是得花真功夫才行。休息终于休息了，我出门的时候拿到张纸条，上面提醒我11：04返场。我本来想喝口热水缓解一下我喉咙的疼痛，却被告知不能回储物室了。我这时候看到别的同学放在楼梯口的一个大桌子上的水和零食，心里真的拔凉拔凉的。我5块钱买的士力架啊！我的口语写作模板啊！不过好像大多数人都没怎么吃东西，要是我不生病的话应该也没什么问题。考场外面的钟不是很准，我一直担心里面开始口语了我还没进去，后来发现开始第二部分的考试也是需要考官输入验证码的，因此完全不必担心。什么都没带，我那十分钟也就上了个厕所并且补充了餐巾纸。口语之前开始的晚，因此我休息的时间也差不多在大家的中间。口语部分一开始的continue就比较少了，又试了一次音。这回就没有人真正介绍自己的城市了，大家又当了一回复读机。可能由于感冒造成鼻音太重的问题，我试音的音量偏低，得说得用点劲才行。我在这里也停顿得有点久，因为待会等大家都开始说了，我就可以偷偷混投入其中以掩盖我拙劣的口语哈哈。事实上，和大家一起说真的能说得更开更自信，当大部分人说完之后，我们考场里有一个女生还在说，我明显地听到她顿了一下，然后声音顿时小了很多。之前超牛的老哥老早就进去了（他很早就完成了听力），我进去之后本想偷听他在说什么，因为口语题都一样，结果…天呐竟然跟不上他的语速！还好这时候有一个水平不高但的确可以帮到我的吞吞吐吐的小哥开始讲了，我听到他在说work什么的，自己在脑子里构思了一下便也开始答题了。然而，我的提前构思反倒先入为主了。当题目放出来时，我花了好久才读清题目，因为跟我想的太不一样了。以后还是不能太期望于听到别人的答案。最后，第一部分比较凉。其实整个口语都比较凉，因为我感冒鼻音简直太重了，就像蒙着几个口罩一样，特别是其中有个录音我还咳嗽了几声。写作最后两部分考试感觉时间飞快，既然口语凉了，也听说写作给分还可以，我就放飞自我开始写了。在我听听力的时候，就听到劈里啪啦的打字声了。一直对自己打字速度很有自信的我还是小惊讶了一下。两篇作文都不是很难，我第二篇大概只写了三百词出头一些，细节还是写的有点少，都是论述的，这点下回要改进。两篇作文都是到点自动保存提交的，没有整体检查拼写，打字速度还是得练，盲打还是得加强。考场的机械键盘相对来说比较扁平，跟笔记本手感还是比较相近的。考完最后还有一个report成绩还是cancel的选项，考官明确说过这个不能提问，于是我看的很仔细。还好我的水平还是无压力看懂了，砸了两千块当然要report啦！出考场后还是有点不放心特地查了一下，发现还是有网友选cancel的，不过好像可以付额外的费用解决。出考场才知道已经十二点半了，由于中途没补充零食，肚子也是饿得咕咕叫（写大作文的时候开始明显感到饿）。从储物柜拿手机的时候，不小心带了出来，掉在地上了，心里一惊，还好只在钢化膜上留下一条线，当时也觉得无所谓了。考完也挺平静的，感觉托福考试也就这样，只可惜这次时运不济，命途多舛。以后考托福一定要在学期初考，并且好好准备，关键是要注意身体的健康！早上起来的时候百度了一下自己的博客，发现已经被李彦宏爸爸的百度收录了，可以直接百度到我的博客和文章，也是今天比较开心的一件事吧，以后会好好做SEO的。结果出成绩了，更新一下。10月19号上午考完的试，31号凌晨3：51终于刷新的成绩。查询页面显示的是“2019年10月23日的MyBest Scores”，不知道是不是23号就批好了成绩。考完后一直关注贴吧和公众号，似乎我那周是最后一次最多两周出成绩的考试，以后托福的成绩好像都会考后10天就出结果。雅思更狠，马上跟着改成了6天出成绩，它们是不是也在竞争呢…查分的时候还是很忐忑的，没想到这次首考的成绩能到90+，虽然完全不够，但还是比我想象得要好一些的。口语果然离20还是差了一点，或许有生病的影响，但的确能体现我的水平，还是得加强练习！别人口中的提分项——写作，我也没有取得高分，看来还是不能大意，平时需要熟能生巧。但愿二战能够取得一定的进步吧！今天去听了我们学校的海外交流项目介绍的讲座，大体上的语言成绩要求是CET4&gt;550，CET6&gt;500，TOEFL&gt;80，IELTS&gt;6.0，否则要电话面试，但这些基本都是相对来说比较中规中矩的科研项目或者学分项目，还是得努力提高英语水平啊！]]></content>
      <categories>
        <category>英语</category>
      </categories>
      <tags>
        <tag>个人经历</tag>
        <tag>托福</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[artificial intelligence笔记：吴恩达——阅读论文的建议]]></title>
    <url>%2Fartificial-intelligence20191007232512%2F</url>
    <content type="text"><![CDATA[接触科研，读paper是一件很头疼的事情。本文就来写一下吴恩达对于阅读ML、DL相关方面论文的建议，方便参考。建议首先要说明的是，这里的建议不是我想出来的，仅仅是对吴恩达提供的建议做搬运及整理。如果你读到这里，应该也知道这个领域的先驱+巨佬Andrew Ng的大名。吴恩达（Andrew Ng），著名的美籍华裔计算机科学家，曾担任百度首席科学家，任教于Stanford，大家刚入门的时候想必都了解过或者看过由吴恩达老师讲授的斯坦福的经典课程CS229机器学习、CS230深度学习，此外，Andrew Ng还特地在网易云上为中国学生提供了中文字幕的课程（Andrew Ng英语说得比中文溜好多了哈哈）。另外，他还是著名教育平台Coursera的创始人，那里的课程更新鲜更优质，而且还可以锻炼英语能力，旁听即可。呃放错了，不是上面那张，是这张。对于如何阅读论文，Andrew Ng的建议是：不要从头读到尾。相反，需要多次遍历论文。具体有如下几个注意点：阅读文章标题、摘要和图通过阅读文章标题、摘要、关键网络架构图，或许还有实验部分，你将能够对论文的概念有一个大致的了解。在深度学习中，有很多研究论文都是将整篇论文总结成一两个图形，而不需要费力地通读全文。尤其是在描述网络架构的时候，作者一般会采用比较通用的格式，读多了就会熟悉起来，比如下面DenseNet的结构：读介绍、结论、图，略过其他介绍、结论和摘要是作者试图仔细总结自己工作的地方，以便向审稿人阐明为什么他们的论文应该被接受发表。此外，略过相关的工作部分（如果可能的话），这部分的目的是突出其他人所做的工作，这些工作在某种程度上与作者的工作有关。因此，阅读它可能是有用的，但如果不熟悉这个主题，有时会很难理解。通读全文，但跳过数学部分这里我说一下我对于数学部分的处理：一般我会把重要的公式等略读一遍，然后参照着CSDN博客等网站上其他网友的解释与详解进行理解。通读全文，但略过没有意义的部分Andrew Ng还解释说，当你阅读论文时（即使是最有影响力的论文），你可能也会发现有些部分没什么用，或者没什么意义。因此，如果你读了一篇论文，其中一些内容没有意义（这并不罕见），那么你可以先略读。除非你想要掌握它，那就花更多的时间。确实，当我在阅读ILSVRC、COCO等顶级比赛许多获奖模型的论文时，其中都有对比赛情况的详细结果介绍，我觉得这些部分一定程度上是可以扫读和跳读的。分享关于论文，我之前也做过一些分享，详情可以看看我之前的文章。在deep-learning笔记：开启深度学习热潮——AlexNet一文中，我提到了刚开始阅读英文论文的比较有效的方法。在deep-learning笔记：使网络能够更深——ResNet简介与pytorch实现一文中，我也提供了许多经典模型论文的英文版、中文版、中英对照的链接。最后要说明的是，本篇文章中Andrew Ng的建议有部分摘自公众号Datawhale的推送文章。我关注了不少这方面的公众号，删选了几个比较优质的，在今后也会一一放到博客中推荐。]]></content>
      <categories>
        <category>人工智能</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>深度学习</tag>
        <tag>论文分享</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[欢迎到访：写在前面]]></title>
    <url>%2Fpreface20191007202443%2F</url>
    <content type="text"><![CDATA[不知不觉，建站已有不少日子了，无论是内容还是界面，都逐渐丰富了起来。觉得有必要补充一篇类似于导言的文字，今天抽出点时间写一下，日后继续完善。关于我我来自浙江，就读于华科，目前是一名电子信息工程专业的大二本科生。从大学开始真正比较全面地接触信息技术，一年下来，在课余接触并尝试过的方面主要有编程语言python与R、linux操作系统、前端设计、机器学习与深度学习。我的博客也主要围绕这几个方面展开。入门没多久，许多理解也还比较浅薄，博客内容主要是一些干货的搬运分享并结合自己积累的一些理解与经验，会有不足与疏漏，如果大家能给予指导，我将非常感激！今后我会尽量陆续加入更多深层次的内容。对于这个博客网站，可以把它看作一个技术博客，而我更多的把它看成一个自己的空间，因此偶尔也会加入一些学习生活的元素，请别见怪！此外，我会尽我所能提升文章的质量，在发布后也会不断地查漏补缺，小幅修正与大幅补充结合，使每篇文章尽可能更规范易懂、更丰富充实。或许在这个移动端主导的时代，搭网站写博客的价值有所降低。但当我写了一段时间之后，发现驱使我写博客的动力不减反增。最大的收获就是我每写一篇文章就会不断地想把相关的知识彻底搞懂，这就不断促使我去深究（哪怕钻牛角尖），因此总能收获好多写之前根本没想到的内容。当然如果能成为鸟哥那样的博主那就再好不过啦，哈哈我说着玩的，向大佬们看齐！关于博客为了提高访问速度，我对我的博客进行了github+coding双线部署，url如下：github page：https://gsy00517.github.io/coding page：http://gsy00517.coding.me/大家可以择优访问，事实上我并没有感到github的速度比coding慢。此外，由于我是同一个源文件双线部署，因此选择了把所有功能优先应用在github page上（比如文章打分、文章链接），但其实coding page也没有太大的区别。由于我的文章主要是按时间顺序由新到老排列的，多了之后不方便查找和浏览，因此我新增了标签和分类，或许可以帮助你更快地查看想看的内容。此外，也推荐使用我页面上的搜索功能利用关键词查找，非常便捷。注：PC体验更佳~关于订阅首先，我想说的是，如果你觉得我写的内容，或者方向对你有一点用处的话，非常欢迎收藏或订阅我的博客！如果你也在写博客的话，我们可以互相关注！在侧栏，你可能会发现这样一个图标。点击之后，你会进入一个看不懂的atom.xml文件。其实，看不懂是正常的，因为这个是给电脑看的。一个便捷的办法就是使用chrome的扩展程序添加feed，然后打开网页时，就可以直接点击浏览器右上角的图标（会显示加号）进行订阅。这样以后每次更新了新的博文，你就可以收到提醒。此外还可以像收藏其他网站一样进行收藏，这里不详述了。欢迎大家常来踩踩，也欢迎大家留下评论。评论很简单，无需登录任何账号直接评论即可~我目前也还在学习的过程中，欢迎大家和我交流，也欢迎各种批评与建议，我会努力改进！Good News好消息！好消息！本站已和百度、谷歌等世界知名搜索引擎达成战略合作关系！如果我写的文章有不足或疏漏之处、看完后有费解有困惑，都可以直接问度娘和谷哥就好啦~哈哈哈皮这一下很开心。]]></content>
  </entry>
  <entry>
    <title><![CDATA[calculus笔记：分部积分表格法]]></title>
    <url>%2Fcalculus20191007184856%2F</url>
    <content type="text"><![CDATA[假期里想着不能让b站收藏夹里的学习资源一直吃灰，于是又刷了一遍b站的收藏夹。碰巧就看到了自己之前收藏的一种积分方法，那么这篇文章就来搬运一下这种方法的计算流程。表格法事实上，这种方法说白了还是分部积分法，但使用起来却要方便好多。我们直接看例子：求解$ \int \left ( x^{2}+x \right )e^{x}dx $。画一个两行的表格。把多项式部分写在第一行，然后把剩余的部分写在第二行。$ x^{2}+x\ $$ e^{x}\ $接下来，我们对第一行求导，直到导数为零为止。对第二行积分，直到与第一行的0对齐为止。$ x^{2}+x\ $$ 2x+1\ $20$ e^{x}\ $$ e^{x}\ $$ e^{x}\ $$ e^{x}\ $第三步就是交叉相乘，在本题即为第一行第一列与第二行第二列相乘，第一行第二列与第二行第三列相乘，第一行第三列与第二行第四列相乘。要注意的是，这里的交叉相乘还需要带符号，依次为正负正负正…以此类推。最后，将相乘结果相加，整理即可得到最终的解。+\left ( \left ( x^{2}+x \right )*e^{x} \right )-\left ( \left ( 2x+1 \right )*e^{x} \right )+\left ( 2*e^{x} \right )=\left ( x^{2}-x+1 \right )*e^{x}+C\注意：别忘了加上常数C。下面再来看一个例子熟悉一下：求解$ \int xsinxdx $。画表格：$ x\ $$ 1\ $$ 0\ $$ sinx\ $$ -cosx\ $$ -sinx\ $求解：+\left ( x*\left ( -cosx \right ) \right )-\left ( 1*\left ( -sinx \right ) \right )=-xcosx+sinx+C\其实b站上还是有挺多这样的干货的，此生无悔入b站！其它运算终止情况看完上面的部分，细心的你肯定会想到以上的方法并不普适，仅仅适用于导数能求导至零及含有多项式因式的情况。因此，为了能更灵活地运用分部积分表格法，下面补充其它两种运算可以终止的情况。第一行出现零元素这就是上面所说的含多项式的情况，也一并列写在这里，方便总览归纳。某列函数的乘积（或它的常数倍）等于第一列按照分部积分的一般做法，当出现之后的某一项恰好是原来积分或者是原来积分的常数倍时，计算进入循环。这时就可以把两者移到等式的同一侧，计算出结果，这在表格法的分部积分中也是类似的。来看看例子：求解$ \int e^{3x}sin2xdx $。$ e^{3x}\ $$ 3e^{3x}\ $$ 9e^{3x}\ $$ sin2x\ $$ -\frac{cos2x}{2}\ $$ -\frac{sin2x}{4}\ $可见，第三列的乘积和第一列的乘积相差一个常数（这里是$ -\frac{9}{4} $），因此仿照之前的方法交叉相乘列出积分：\int e^{3x}sin2xdx=e^{3x}(-\frac{cos2x}{2})-3e^{3x}(-\frac{sin2x}{4})+9(-\frac{1}{4})\int e^{3x}sin2xdx\移项化简可得：\int e^{3x}sin2xdx=\frac{1}{13}e^{3x}(3sin2x-2cos2x)+C\即为所求。看完这种情况，你一定会敏锐地发现，其实分部积分表格法本质上和一般的分部积分法一模一样，不过的确在使用上还是有一定的优势的。某列的两个函数乘积（记为$ f(x) $）是一个容易计算的积分这种情况下，先把之前的项用之前的方法类似列出，再在结果后加上不定积分$ (-1)^{k-1}\int f(x)dx $。来看例子：求解$ \int x^{2}arctanxdx $。$ arctanx\ $$ \frac{1}{1+x^{2}}\ $$ x^{2}\ $$ \frac{1}{3}x^{3}\ $可得解：\int x^{2}arctanxdx=\frac{1}{3}x^{3}arctanx-\frac{1}{3}\int \frac{x^{3}}{1+x^{2}}dx=\frac{1}{3}\left \{ x^{3}arctanx-\frac{1}{2}[x^{2}-ln(1+x^{2})] \right \}+C\另外，当表中的第一行的某列出现多项之和，而再求导无法改变该函数或者该函数中某一项的属性，则终止表格，后再重新组合，另建表格求解。这种情况一般不会出现在题目中，如遇到再做补充。]]></content>
      <categories>
        <category>知识点与小技巧</category>
      </categories>
      <tags>
        <tag>微积分</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[game笔记：海贼王燃烧之血键盘操作]]></title>
    <url>%2Fgame20191007172952%2F</url>
    <content type="text"><![CDATA[这是一篇关于海贼王也关于游戏的文章，出于对海贼王狂热的喜爱，我决定还是在博客里添一篇这样的文章，感兴趣可以看看。画展国庆期间，我留在了武汉。幸运的是，海贼王官方在大陆的首次巡展“路飞来了”正好此时也在武汉开展。作为一名海贼铁粉，我当然是毫不犹豫地买了票。其实根据我博客网站的icon以及我目前的个人头像，应该很容易看出我对海贼王的热爱哈哈。去的时候快接近饭点了，人还是不少，都是真爱啊~不过像我这样我单身一人的占比不大，但也有。让我惊讶的是当我看着日文的原稿时竟能直接反应出中文，果然那么多年来全套漫画没白买。期间跟一位貌似是艺术生的小哥聊得挺开的，只可惜最后没留联系方式，有缘再见吧。整个展看下来还是挺震撼的，尤其是刚进去的时候，激动地鸡皮疙瘩都起来了。不过跟我在东京塔下面的海贼王主题乐园激动得哭出来还是有一定差距的。看完展之后我买了几张原稿的复刻版，花了不少钱，但觉得挺值，珍藏了。我是一个漫画党，除了剧场版或特别篇之外我看的都是漫画（不过是通过动画入坑的，星空卫视司法岛，一代人的记忆哈哈）。说实话，尾田构思之精巧，漫画史上无人能及，感兴趣可以看看知乎上关于尾田构思的讨论，漫画真的埋了很多神一般的线索，这是动画里办不到的，细细看很有意思。燃烧之血看完展，我心中对于海贼王的热血再一次得到激发，回学校就打开燃烧之血，回到海贼世界过把瘾。海贼王燃烧之血（One Piece：Burning Blood）是16年发行的一款海贼王题材的格斗游戏，个人觉得其中的自然系元素化以及霸气设定真的太棒了！另外各种招式都还原得很全很细致，简直就是一边玩一边享受精彩的画面。文末提供了一些图，可以欣赏一下，真的很赞。由于steam版价格原因（加上全部DLC需两三百rmb）以及原本这游戏好像是在游戏机上的（PC版是移植的），导致PC键盘操作方式的教程不是很全，因此本篇文章主要就是对该款游戏的按键操作做一个补充。按键操作十几个小时玩下来，基本的按键摸得比较熟了，其实键盘操作也有键盘的优势，熟练就好。首先是很普适的移动方式：前进 W后退 S左行 A右行 D下面是一些基本的战斗操作：攻击 K重击 O跳跃 L防御 ；（分号键）往后换人 E往前换人 I突破极限状态 右ctrl必杀技（突破极限状态下） 右ctrl如果要使用招式，那么按下Q，在战斗界面的左侧就会出现招式列表，即三个招式的名称及按键操作，按住Q不松，再配合对应按键，就可以使出对应的招式。招式一 （招式列表情况下） K招式二 （招式列表情况下） O招式三 （招式列表情况下） ；一般情况下，长按对应键不松可以延迟招式的释放时间（比如在对手倒地时可以尝试）。此外，一些招式延迟附带蓄力效果，可以打出更强的攻击（附带破防效果）。接下来是一些组合按键的操作：破防 K+L重击破防 O+；侧步闪躲 W、S、A、D+；范围攻击 S+K范围重击 S+O跳跃攻击 L+K跳跃重击 L+O有些角色还拥有特殊的衍生技能，需通过一定的按键组合释放，这里举两个例子，别的可以参考收藏图鉴：艾斯 神火•不知火 L+O白胡子 垂直跳跃攻击 L+O接下来就是非常有特色的能力啦，按住P键就可以开启自然系能力者的元素化，可以轻松躲掉普攻并适时反击。如果有霸气的话，按住P也可开启霸气，在期间进行攻击就可以造成更大伤害，也不用惧怕自然系了。此外，一些角色开启能力时还可以实现特定的能力，作为海贼迷真的感动到哭，下面举几个例子：女帝 快速后闪 P+L黄猿 瞬移 P+L+方向大熊 瞬移 P+L白胡子 双震 P+招式一白胡子的双震是我最喜欢的技能，真的非常有打击感和冲击力，上图截自b站up主的操作教程，我的许多操作都是从那学来的，他在b站和爱奇艺上都有很详细的连招教程，同时配音也很逗，感兴趣的话可以去观摩一下。角色特点这里补充一些目前我发现的角色的特点，也是只有海贼迷才懂的，可以说这游戏做得真的赞，后续我发现更多的话会继续补充。1.众所周知，路飞无法被女帝石化。2.山治对抗女性角色时，只能对女性示爱，因此只有挨打的份。画面欣赏静态无声的画面比起动态有声的还是差多了，但依旧不影响其魅力，看着就觉得很兴奋啦~当然，游戏仅是起娱乐作用，劳逸结合是关键。如果你也热爱海贼王的话，欢迎和我交流！]]></content>
      <categories>
        <category>游戏</category>
      </categories>
      <tags>
        <tag>海贼王</tag>
        <tag>个人经历</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[hexo笔记：ssh与https以及双线部署的一些注意点]]></title>
    <url>%2Fhexo20191006232704%2F</url>
    <content type="text"><![CDATA[不久前，我对本博客网站做了一些优化，其中包括将网站同时部署到github和coding上。关于双线部署如何具体操作，网上有许多较为详尽的教程可以参考，如果有问题的话可以参考多篇不同的教程找出原因解决。在这篇文章中，我主要想讲讲我这期间遇到的一些小事项。github与coding考虑到每次打开博客的加载速度问题，我前几天尝试了把博客部署到coding上，实现了coding+github双线部署。coding现已经被腾讯云收购，可以直接用微信登录。部署完成后，为了看一看效果，我使用了站长工具分别对coding和github上的网站速度进行了测试。测试结果如下：可见，部署在coding上确实能提高一点速度。不过事实上，在实际使用中，并没有感到coding更快，搜索之后发现似乎是coding的服务器也不在内地而在香港的原因。这里附上我的两个链接，可以看看效果，择优访问：github page：https://gsy00517.github.io/coding page：https://gsy00517.coding.me/ssh与https在网上的一个教程中，作者提到使用ssh比https更加稳定，尝试后暂时没有发现明显的区别，但是另一个直观的改变就是在push代码时，使用ssh url就不需要输入账号和密码。下面是我在hexo配置文件中的设置，也就是位于站点根目录下的_config.yml文件，其中后面注释中的https://github.com/Gsy00517/Gsy00517.github.io.git是原本的https url。上面对应的ssh url一般可以从平台上直接复制获取，也可以参照我的格式进行设置。这里简要说一说ssh与https的区别。一般默认情况下使用的是https，除了需要在fetch和push时使用密码之外，使用https的配置比较方便。然而，使用ssh url却需要先配置和添加好ssh key，并且你必须是这个项目的拥有或者管理者，而https就没有这些要求。其实，配置ssh key也并没有那么繁琐，而且这是一劳永逸的，所以推荐还是使用ssh。要注意的是，ssh key保存的默认位置或许会不同于网上的教程，不过可以自行更改。我的默认地址是在用户文件夹下的AppData\Roaming\SPB_16.6的ssh文件夹中。AppData文件夹默认是隐藏的，可以通过查看隐藏的项目打开。此外，如果需要经常清理temp文件的话，不妨取消这个文件夹的隐藏，这在释放windows空间中还是挺有效的，可以参见windows笔记：释放空间。key所在的文件是上图所示的第二个publisher文件，然而似乎无法直接用office打开，选择打开方式为记事本即可。当然，如果实在找不到key所在的文件，也可以直接使用文件资源管理器的搜索功能查找名为.ssh的文件夹即可。注：http与https的区别在于，http是明文传输的，而https是使用ssl加密的，更加安全。若要将连接提交百度站点验证，就需要使用https协议，这个在github和coding都有强制https访问的选项。双线部署注意事项LeanCloud这里主要针对hexo博客双线部署后可能会出现的几个问题说明一下注意点。首先，如果之前使用的是LeanCloud来接受记录评论和统计阅读量的，那么为了共享数据，必须在LeanCloud控制台设置的安全中心中，添加新增的web安全域名，保存后即可解决问题。Widget如果使用的是基于Widget的评分系统，那么必须更改Widget设置中的domain。我是免费使用Widget，只能同时添加一个domain。我继续使用github page的域名，因此只能在我的github page中看到评分系统。文内链接因为双线部署用的依旧还是同一份本地源码文件，因此在博文中提供的链接依旧是一致的。这里我也将继续使用github page的链接，也就是文内推荐的我本人的博文链接依旧还是指向github page的。事实上，这并无任何影响。]]></content>
      <categories>
        <category>操作和使用</category>
      </categories>
      <tags>
        <tag>hexo</tag>
        <tag>配置优化</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[deep learning笔记：使网络能够更深——ResNet简介与pytorch实现]]></title>
    <url>%2Fdeep-learning20191001184216%2F</url>
    <content type="text"><![CDATA[之前我用pytorch把ResNet18实现了一下，但由于上周准备国家奖学金答辩没有时间来写我实现的过程与总结。今天是祖国70周年华诞，借着这股喜庆劲，把这篇文章补上。本文参考自：https://blog.csdn.net/weixin_43624538/article/details/85049699https://blog.csdn.net/u013289254/article/details/98785869简介ResNet残差网络是由何凯明等四名微软研究院的华人提出的，当初看到论文标题下面的中国名字还是挺高兴的。文章引入部分，作者就探讨了深度神经网络的优化是否就只是叠加层数、增加深度那么简单。显然这是不可能的，增加深度带来的首要问题就是梯度爆炸、消散的问题，这是由于随着层数的增多，在网络中反向传播的梯度会随着连乘变得不稳定，从而变得特别大或者特别小。其中以梯度消散更为常见。值得注意的是，论文中还提到深度更深的网络反而出现准确率下降并不是由过拟合所引起的。为了解决这个问题，研究者们做出了很多思考与尝试，其中的代表有relu激活函数的使用，Batch Normalization的使用等。关于这两种方法，可以参考网上的资料以及我的博文deep-learning笔记：开启深度学习热潮——AlexNet和deep-learning笔记：学习率衰减与批归一化。对于上面这个问题，ResNet作出的贡献是引入skip/identity connection。如下所示就是两个基本的残差模块。上面这个block可表示为：$ F(X)=H(X)-x $。在这里，X为浅层输出，H(x)为深层的输出。当浅层的X代表的特征已经足够成熟，即当任何对于特征X的改变都会让loss变大时，F(X)会自动趋向于学习成为0，X则从恒等映射的路径继续传递。这样，我们就可以在不增加计算成本的情况下使得在前向传递过程中，如果浅层的输出已经足够成熟（optimal），那么就让深层网络后面的层仅实现恒等映射的作用。当X与F（X）通道数目不同时，作者尝试了两种identity mapping的方式。一种即对X缺失的通道直接补零从而使其能够对齐，这种方式比较简单直接，无需额外的参数；另一种则是通过使用1x1的conv来映射从而使通道也能达成一致。论文老规矩，这里还是先呈上我用黄色荧光高亮出我认为比较重要的要点的论文原文，这里我只有英文版。如果需要没有被我标注过的原文，可以直接搜索，这里我仅提供一次，可以点击这里下载。不过，虽然没有pdf中文版，但其实深度学习CV方向一些比较经典的论文的英文、中文、中英对照都可以到Deep Learning Papers Translation上看到，非常方便。自己实现在论文中，作者提到了如下几个ResNet的版本的结构。这里我实现的是ResNet18。由于这不是我第一次使用pytorch进行实现，一些基本的使用操作我就不加注释了，想看注释来理解的话可以参考我之前VGG的实现。由于残差的引入，导致ResNet的结构比较复杂，而论文中并没有非常详细的阐述，在研究官方源码之后，我对它的结构才有了完整的了解，这里我画出来以便参考。注：此模块在2016年何大神的论文中给出了新的改进，可以参考我的博文deep-learning笔记：记首次ResNet实战。ResNet18的每一layer包括了两个这样的basic block，其中1x1的卷积核仅在X与F（X）通道数目不一致时进行操作，在我的代码中，我定义shortcut函数来对应一切通道一致、无需处理的情况。123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105import torchimport torch.nn as nnimport torch.nn.functional as Fclass ResNet(nn.Module): def __init__(self): super(ResNet, self).__init__() self.conv1 = nn.Conv2d(in_channels = 3, out_channels = 64, kernel_size = 7, stride = 2, padding = 3, bias = False) self.max = nn.MaxPool2d(kernel_size = 3, stride = 2, padding = 1) self.bn1 = nn.BatchNorm2d(64) self.bn2 = nn.BatchNorm2d(64) self.bn3 = nn.BatchNorm2d(128) self.bn4 = nn.BatchNorm2d(256) self.bn5 = nn.BatchNorm2d(512) self.shortcut = nn.Sequential() self.shortcut3 = nn.Sequential(nn.Conv2d(64, 128, kernel_size = 1, stride = 2, bias = False), nn.BatchNorm2d(128)) self.shortcut4 = nn.Sequential(nn.Conv2d(128, 256, kernel_size = 1, stride = 2, bias = False), nn.BatchNorm2d(256)) self.shortcut5 = nn.Sequential(nn.Conv2d(256, 512, kernel_size = 1, stride = 2, bias = False), nn.BatchNorm2d(512)) self.conv2 = nn.Conv2d(in_channels = 64, out_channels = 64, kernel_size = 3, stride = 1, padding = 1, bias = False) self.conv3_1 = nn.Conv2d(in_channels = 64, out_channels = 128, kernel_size = 3, stride = 2, padding = 1, bias = False) self.conv3_2 = nn.Conv2d(in_channels = 128, out_channels = 128, kernel_size = 3, stride = 1, padding = 1, bias = False) self.conv4_1 = nn.Conv2d(in_channels = 128, out_channels = 256, kernel_size = 3, stride = 2, padding = 1, bias = False) self.conv4_2 = nn.Conv2d(in_channels = 256, out_channels = 256, kernel_size = 3, stride = 1, padding = 1, bias = False) self.conv5_1 = nn.Conv2d(in_channels = 256, out_channels = 512, kernel_size = 3, stride = 2, padding = 1, bias = False) self.conv5_2 = nn.Conv2d(in_channels = 512, out_channels = 512, kernel_size = 3, stride = 1, padding = 1, bias = False) self.avg = nn.AdaptiveAvgPool2d((1, 1)) #adaptive自适应，只给定输入和输出大小，让机器自行调整选择核尺寸和步长大小 self.fc = nn.Linear(512, 1000) def forward(self, x): x = F.relu(self.bn1(self.conv1(x))) x1 = self.max(x) #layer1 x = F.relu(self.bn2(self.conv2(x1))) x = self.bn2(self.conv2(x)) x += self.shortcut(x1) #pytorch0.4.0之后这里要改为x = x + self.shortcut(x1) x2 = F.relu(x) x = F.relu(self.bn2(self.conv2(x2))) x = self.bn2(self.conv2(x)) x += self.shortcut(x2) x3 = F.relu(x) #layer2 x = F.relu(self.bn3(self.conv3_1(x3))) x = self.bn3(self.conv3_2(x)) x += self.shortcut3(x3) x4 = F.relu(x) x = F.relu(self.bn3(self.conv3_2(x4))) x = self.bn3(self.conv3_2(x)) x += self.shortcut(x4) x5 = F.relu(x) #layer3 x = F.relu(self.bn4(self.conv4_1(x5))) x = self.bn4(self.conv4_2(x)) x += self.shortcut4(x5) x6 = F.relu(x) x = F.relu(self.bn4(self.conv4_2(x6))) x = self.bn4(self.conv4_2(x)) x += self.shortcut(x6) x7 = F.relu(x) #layer4 x = F.relu(self.bn5(self.conv5_1(x7))) x = self.bn5(self.conv5_2(x)) x += self.shortcut5(x7) x8 = F.relu(x) x = F.relu(self.bn5(self.conv5_2(x8))) x = self.bn5(self.conv5_2(x)) x += self.shortcut(x8) x = F.relu(x) #ending x = self.avg(x) #变换维度，可以设其中一个尺寸为-1，表示机器内部自己计算，但同时只能有一个为-1 x = x.view(-1, self.num_flat_features(x)) x = self.fc(x) x = F.softmax(x, dim = 1) return x def num_flat_features(self, x): size = x.size()[1:] num_features = 1 for s in size: num_features *= s return num_features net = ResNet()同样的，我们可以随机生成一个张量来进行验证：123input = torch.randn(1, 3, 48, 48)out = net(input)print(out)如果可以顺利地输出，那么模型基本上是没有问题的。出现的问题在这里我还是想把自己踩的一些简单的坑记下来，引以为戒。softmax输出全为1当我使用F.softmax之后，出现了这样的一个问题：查找资料后发现，我错误的把对每一行softmax当作了对每一列softmax。因为这个softmax语句是我从之前的自己做的一道kaggle题目写的代码中ctrl+C+V过来的，复制过来的是x = F.softmax(x, dim = 0)，在这里，dim = 0意味着我对张量的每一列进行softmax，这是因为我之前的场景中需要处理的张量是一维的，也就是tensor（）里面只有一对“[]”，此时它默认只有一列，我对列进行softmax自然就没有问题。而放到这里，我再对列进行softmax时，每列上就只有一个元素。那么结果就都是1即100%了。解决的方法就是把dim设为1。下面我在用一组代码直观地展示一下softmax的用法与区别。1234567import torchimport torch.nn.functional as Fx1= torch.Tensor( [ [1, 2, 3, 4], [1, 3, 4, 5], [3, 4, 5, 6]])y11= F.softmax(x1, dim = 0) #对每一列进行softmaxy12 = F.softmax(x1, dim = 1) #对每一行进行softmaxx2 = torch.Tensor([1, 2, 3, 4])y2 = F.softmax(x2, dim = 0)我们输出每个结果，可以看到：bias或许你可以发现，在我的代码中，每个卷积层中都设置了bias = False，这是我在参考官方源码之后补上的。那么，这个bias是什么，又有什么用呢？我们在学深度学习的时候，最早接触到的神经网络应该是感知器，它的结构如下图所示。 要想激活这个感知器，就必须使x1 * w1 + x2 * w2 + ... + xn * wn &gt; T（T为一个阈值），而T越大，想激活这个感知器的难度越大。考虑样本较多的情况，我不可能手动选择一个阈值，使得模型整体表现最佳，因此我们不如使得T变成可学习的，这样一来，T会自动学习到一个数，使得模型的整体表现最佳。当把T移动到左边，它就成了bias偏置，x1 * w1 + x2 * w2 + ... + xn * wn - T &gt; 0。显然，偏置的大小控制着激活这个感知器的难易程度。在比感知器高级的神经网络中，也是如此。但倘若我们要在卷积后面加上归一化操作，那么bias的作用就无法体现了。我们以ResNet卷积层后的BN层为例。可参考我的上一篇博文，BN处理过程中有这样一步： 对于分子而言，无论有没有bias，对结果都没有影响；而对于下面分母而言，因为是方差操作，所以也没有影响。因此，在ResNet中，因为每次卷积之后都要进行BN操作，那就不需要启用bias，否则非但不起作用，还会消耗一定的显卡内存。官方源码如果你此时对ResNet的结构已经有了比较清晰的理解，那么可以尝试着来理解一下官方源码的思路。其实我觉得先看像我这样直观的代码实现再看官方源码更有助理解且更高效。123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256257258259260261262263264265266267268269270271272273274275276277278279280281282283284285286287288289290291292293294295296297298299300301302303304305306307308309310311312313314315316317318319320321322323324325326327328329330331332333334335336337338339340341342343344import torchimport torch.nn as nnfrom .utils import load_state_dict_from_url__all__ = ['ResNet', 'resnet18', 'resnet34', 'resnet50', 'resnet101', 'resnet152', 'resnext50_32x4d', 'resnext101_32x8d', 'wide_resnet50_2', 'wide_resnet101_2']model_urls = &#123; 'resnet18': 'https://download.pytorch.org/models/resnet18-5c106cde.pth', 'resnet34': 'https://download.pytorch.org/models/resnet34-333f7ec4.pth', 'resnet50': 'https://download.pytorch.org/models/resnet50-19c8e357.pth', 'resnet101': 'https://download.pytorch.org/models/resnet101-5d3b4d8f.pth', 'resnet152': 'https://download.pytorch.org/models/resnet152-b121ed2d.pth', 'resnext50_32x4d': 'https://download.pytorch.org/models/resnext50_32x4d-7cdf4587.pth', 'resnext101_32x8d': 'https://download.pytorch.org/models/resnext101_32x8d-8ba56ff5.pth', 'wide_resnet50_2': 'https://download.pytorch.org/models/wide_resnet50_2-95faca4d.pth', 'wide_resnet101_2': 'https://download.pytorch.org/models/wide_resnet101_2-32ee1156.pth',&#125;def conv3x3(in_planes, out_planes, stride=1, groups=1, dilation=1): """3x3 convolution with padding""" return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride, padding=dilation, groups=groups, bias=False, dilation=dilation)def conv1x1(in_planes, out_planes, stride=1): """1x1 convolution""" return nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=stride, bias=False)class BasicBlock(nn.Module): expansion = 1 __constants__ = ['downsample'] def __init__(self, inplanes, planes, stride=1, downsample=None, groups=1, base_width=64, dilation=1, norm_layer=None): super(BasicBlock, self).__init__() if norm_layer is None: norm_layer = nn.BatchNorm2d if groups != 1 or base_width != 64: raise ValueError('BasicBlock only supports groups=1 and base_width=64') if dilation &gt; 1: raise NotImplementedError("Dilation &gt; 1 not supported in BasicBlock") # Both self.conv1 and self.downsample layers downsample the input when stride != 1 self.conv1 = conv3x3(inplanes, planes, stride) self.bn1 = norm_layer(planes) self.relu = nn.ReLU(inplace=True) self.conv2 = conv3x3(planes, planes) self.bn2 = norm_layer(planes) self.downsample = downsample self.stride = stride def forward(self, x): identity = x out = self.conv1(x) out = self.bn1(out) out = self.relu(out) out = self.conv2(out) out = self.bn2(out) if self.downsample is not None: identity = self.downsample(x) out += identity out = self.relu(out) return outclass Bottleneck(nn.Module): expansion = 4 def __init__(self, inplanes, planes, stride=1, downsample=None, groups=1, base_width=64, dilation=1, norm_layer=None): super(Bottleneck, self).__init__() if norm_layer is None: norm_layer = nn.BatchNorm2d width = int(planes * (base_width / 64.)) * groups # Both self.conv2 and self.downsample layers downsample the input when stride != 1 self.conv1 = conv1x1(inplanes, width) self.bn1 = norm_layer(width) self.conv2 = conv3x3(width, width, stride, groups, dilation) self.bn2 = norm_layer(width) self.conv3 = conv1x1(width, planes * self.expansion) self.bn3 = norm_layer(planes * self.expansion) self.relu = nn.ReLU(inplace=True) self.downsample = downsample self.stride = stride def forward(self, x): identity = x out = self.conv1(x) out = self.bn1(out) out = self.relu(out) out = self.conv2(out) out = self.bn2(out) out = self.relu(out) out = self.conv3(out) out = self.bn3(out) if self.downsample is not None: identity = self.downsample(x) out += identity out = self.relu(out) return outclass ResNet(nn.Module): def __init__(self, block, layers, num_classes=1000, zero_init_residual=False, groups=1, width_per_group=64, replace_stride_with_dilation=None, norm_layer=None): super(ResNet, self).__init__() if norm_layer is None: norm_layer = nn.BatchNorm2d self._norm_layer = norm_layer self.inplanes = 64 self.dilation = 1 if replace_stride_with_dilation is None: # each element in the tuple indicates if we should replace # the 2x2 stride with a dilated convolution instead replace_stride_with_dilation = [False, False, False] if len(replace_stride_with_dilation) != 3: raise ValueError("replace_stride_with_dilation should be None " "or a 3-element tuple, got &#123;&#125;".format(replace_stride_with_dilation)) self.groups = groups self.base_width = width_per_group self.conv1 = nn.Conv2d(3, self.inplanes, kernel_size=7, stride=2, padding=3, bias=False) self.bn1 = norm_layer(self.inplanes) self.relu = nn.ReLU(inplace=True) self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1) self.layer1 = self._make_layer(block, 64, layers[0]) self.layer2 = self._make_layer(block, 128, layers[1], stride=2, dilate=replace_stride_with_dilation[0]) self.layer3 = self._make_layer(block, 256, layers[2], stride=2, dilate=replace_stride_with_dilation[1]) self.layer4 = self._make_layer(block, 512, layers[3], stride=2, dilate=replace_stride_with_dilation[2]) self.avgpool = nn.AdaptiveAvgPool2d((1, 1)) self.fc = nn.Linear(512 * block.expansion, num_classes) for m in self.modules(): if isinstance(m, nn.Conv2d): nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu') elif isinstance(m, (nn.BatchNorm2d, nn.GroupNorm)): nn.init.constant_(m.weight, 1) nn.init.constant_(m.bias, 0) # Zero-initialize the last BN in each residual branch, # so that the residual branch starts with zeros, and each residual block behaves like an identity. # This improves the model by 0.2~0.3% according to https://arxiv.org/abs/1706.02677 if zero_init_residual: for m in self.modules(): if isinstance(m, Bottleneck): nn.init.constant_(m.bn3.weight, 0) elif isinstance(m, BasicBlock): nn.init.constant_(m.bn2.weight, 0) def _make_layer(self, block, planes, blocks, stride=1, dilate=False): norm_layer = self._norm_layer downsample = None previous_dilation = self.dilation if dilate: self.dilation *= stride stride = 1 if stride != 1 or self.inplanes != planes * block.expansion: downsample = nn.Sequential( conv1x1(self.inplanes, planes * block.expansion, stride), norm_layer(planes * block.expansion), ) layers = [] layers.append(block(self.inplanes, planes, stride, downsample, self.groups, self.base_width, previous_dilation, norm_layer)) self.inplanes = planes * block.expansion for _ in range(1, blocks): layers.append(block(self.inplanes, planes, groups=self.groups, base_width=self.base_width, dilation=self.dilation, norm_layer=norm_layer)) return nn.Sequential(*layers) def forward(self, x): x = self.conv1(x) x = self.bn1(x) x = self.relu(x) x = self.maxpool(x) x = self.layer1(x) x = self.layer2(x) x = self.layer3(x) x = self.layer4(x) x = self.avgpool(x) x = torch.flatten(x, 1) x = self.fc(x) return xdef _resnet(arch, block, layers, pretrained, progress, **kwargs): model = ResNet(block, layers, **kwargs) if pretrained: state_dict = load_state_dict_from_url(model_urls[arch], progress=progress) model.load_state_dict(state_dict) return modeldef resnet18(pretrained=False, progress=True, **kwargs): r"""ResNet-18 model from `"Deep Residual Learning for Image Recognition" &lt;https://arxiv.org/pdf/1512.03385.pdf&gt;`_ Args: pretrained (bool): If True, returns a model pre-trained on ImageNet progress (bool): If True, displays a progress bar of the download to stderr """ return _resnet('resnet18', BasicBlock, [2, 2, 2, 2], pretrained, progress, **kwargs)def resnet34(pretrained=False, progress=True, **kwargs): r"""ResNet-34 model from `"Deep Residual Learning for Image Recognition" &lt;https://arxiv.org/pdf/1512.03385.pdf&gt;`_ Args: pretrained (bool): If True, returns a model pre-trained on ImageNet progress (bool): If True, displays a progress bar of the download to stderr """ return _resnet('resnet34', BasicBlock, [3, 4, 6, 3], pretrained, progress, **kwargs)def resnet50(pretrained=False, progress=True, **kwargs): r"""ResNet-50 model from `"Deep Residual Learning for Image Recognition" &lt;https://arxiv.org/pdf/1512.03385.pdf&gt;`_ Args: pretrained (bool): If True, returns a model pre-trained on ImageNet progress (bool): If True, displays a progress bar of the download to stderr """ return _resnet('resnet50', Bottleneck, [3, 4, 6, 3], pretrained, progress, **kwargs)def resnet101(pretrained=False, progress=True, **kwargs): r"""ResNet-101 model from `"Deep Residual Learning for Image Recognition" &lt;https://arxiv.org/pdf/1512.03385.pdf&gt;`_ Args: pretrained (bool): If True, returns a model pre-trained on ImageNet progress (bool): If True, displays a progress bar of the download to stderr """ return _resnet('resnet101', Bottleneck, [3, 4, 23, 3], pretrained, progress, **kwargs)def resnet152(pretrained=False, progress=True, **kwargs): r"""ResNet-152 model from `"Deep Residual Learning for Image Recognition" &lt;https://arxiv.org/pdf/1512.03385.pdf&gt;`_ Args: pretrained (bool): If True, returns a model pre-trained on ImageNet progress (bool): If True, displays a progress bar of the download to stderr """ return _resnet('resnet152', Bottleneck, [3, 8, 36, 3], pretrained, progress, **kwargs)def resnext50_32x4d(pretrained=False, progress=True, **kwargs): r"""ResNeXt-50 32x4d model from `"Aggregated Residual Transformation for Deep Neural Networks" &lt;https://arxiv.org/pdf/1611.05431.pdf&gt;`_ Args: pretrained (bool): If True, returns a model pre-trained on ImageNet progress (bool): If True, displays a progress bar of the download to stderr """ kwargs['groups'] = 32 kwargs['width_per_group'] = 4 return _resnet('resnext50_32x4d', Bottleneck, [3, 4, 6, 3], pretrained, progress, **kwargs)def resnext101_32x8d(pretrained=False, progress=True, **kwargs): r"""ResNeXt-101 32x8d model from `"Aggregated Residual Transformation for Deep Neural Networks" &lt;https://arxiv.org/pdf/1611.05431.pdf&gt;`_ Args: pretrained (bool): If True, returns a model pre-trained on ImageNet progress (bool): If True, displays a progress bar of the download to stderr """ kwargs['groups'] = 32 kwargs['width_per_group'] = 8 return _resnet('resnext101_32x8d', Bottleneck, [3, 4, 23, 3], pretrained, progress, **kwargs)def wide_resnet50_2(pretrained=False, progress=True, **kwargs): r"""Wide ResNet-50-2 model from `"Wide Residual Networks" &lt;https://arxiv.org/pdf/1605.07146.pdf&gt;`_ The model is the same as ResNet except for the bottleneck number of channels which is twice larger in every block. The number of channels in outer 1x1 convolutions is the same, e.g. last block in ResNet-50 has 2048-512-2048 channels, and in Wide ResNet-50-2 has 2048-1024-2048. Args: pretrained (bool): If True, returns a model pre-trained on ImageNet progress (bool): If True, displays a progress bar of the download to stderr """ kwargs['width_per_group'] = 64 * 2 return _resnet('wide_resnet50_2', Bottleneck, [3, 4, 6, 3], pretrained, progress, **kwargs)def wide_resnet101_2(pretrained=False, progress=True, **kwargs): r"""Wide ResNet-101-2 model from `"Wide Residual Networks" &lt;https://arxiv.org/pdf/1605.07146.pdf&gt;`_ The model is the same as ResNet except for the bottleneck number of channels which is twice larger in every block. The number of channels in outer 1x1 convolutions is the same, e.g. last block in ResNet-50 has 2048-512-2048 channels, and in Wide ResNet-50-2 has 2048-1024-2048. Args: pretrained (bool): If True, returns a model pre-trained on ImageNet progress (bool): If True, displays a progress bar of the download to stderr """ kwargs['width_per_group'] = 64 * 2 return _resnet('wide_resnet101_2', Bottleneck, [3, 4, 23, 3], pretrained, progress, **kwargs)pth文件在阅读官方源码时，我们会注意到官方提供了一系列版本的model_urls，其中，每一个url都是以.pth结尾的。当我下载了对应的文件之后，并不知道如何处理，于是我通过搜索，简单的了解了pth文件的概念与使用方法。简单来说，pth文件就是一个表示Python的模块搜索路径（module search path）的文本文件，在xxx.pth文件里面，会书写一些路径，一行一个。如果我们将xxx.pth文件放在特定位置，则可以让python在加载模块时，读取xxx.pth中指定的路径。下面我使用pytorch对pth文件进行加载操作。首先，我把ResNet18对应的pth文件下载到桌面。1234567891011import torchimport torchvision.models as models# pretrained = True就可以使用预训练的模型net = models.resnet18(pretrained = False)#注意，根据model的不同，这里models.xxx的内容也是不同的，比如models.squeezenet1_1pthfile = r'C:\Users\sheny\Desktop\resnet18-5c106cde.pth'#pth文件所在路径net.load_state_dict(torch.load(pthfile))print(net)输出结果如下：123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384ResNet( (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False) (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False) (layer1): Sequential( (0): BasicBlock( (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) (1): BasicBlock( (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) ) (layer2): Sequential( (0): BasicBlock( (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (downsample): Sequential( (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False) (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) ) (1): BasicBlock( (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) ) (layer3): Sequential( (0): BasicBlock( (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (downsample): Sequential( (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False) (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) ) (1): BasicBlock( (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) ) (layer4): Sequential( (0): BasicBlock( (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (downsample): Sequential( (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False) (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) ) (1): BasicBlock( (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) ) (avgpool): AdaptiveAvgPool2d(output_size=(1, 1)) (fc): Linear(in_features=512, out_features=1000, bias=True))这样你就可以看到很详尽的参数设置了。我们还可以加载所有的参数。1234567import torchpthfile = r'C:\Users\sheny\Desktop\resnet18-5c106cde.pth'net = torch.load(pthfile)print(net)输出如下：12345678OrderedDict([(&apos;conv1.weight&apos;, Parameter containing:tensor([[[[-1.0419e-02, -6.1356e-03, -1.8098e-03, ..., 5.6615e-02, 1.7083e-02, -1.2694e-02], [ 1.1083e-02, 9.5276e-03, -1.0993e-01, ..., -2.7124e-01, -1.2907e-01, 3.7424e-03], [-6.9434e-03, 5.9089e-02, 2.9548e-01, ..., 5.1972e-01, 2.5632e-01, 6.3573e-02], ...,]]></content>
      <categories>
        <category>人工智能</category>
      </categories>
      <tags>
        <tag>深度学习</tag>
        <tag>论文分享</tag>
        <tag>踩坑血泪</tag>
        <tag>代码实现</tag>
        <tag>pytorch</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[deep learning笔记：学习率衰减与批归一化]]></title>
    <url>%2Fdeep-learning20191001151454%2F</url>
    <content type="text"><![CDATA[一段时间之前，在一个深度学习交流群里看到一个群友发问：为什么他的训练误差最后疯狂上下抖动而不是一直降低。作为一个入门小白，我当时也很疑惑。但后来我结合所学，仔细思考之后，发现这是一个挺容易犯的错误。本文参考自：https://blog.csdn.net/bestrivern/article/details/86301619https://www.jianshu.com/p/9643cba47655https://www.cnblogs.com/eilearn/p/9780696.htmlhttps://blog.csdn.net/donkey_1993/article/details/81871132https://www.pytorchtutorial.com/how-to-use-batchnorm/问题事实上，这是一个在机器学习中就有可能遇到的问题，当学习速率α设置得过大时，往往在模型训练的后期难以达到最优解，而是在最优解附近来回抖动。还有可能反而使损失函数越来越大，甚至达到无穷，如下图所示。而在深度学习中，假设我们使用mini-batch梯度下降法，由于mini-batch的数量不大，大概64或者128个样本，在迭代过程中会有噪声。这个时候使用固定的学习率导致的结果就是虽然下降朝向最小值，但不会精确地收敛，只会在附近不断地波动（蓝色线）。但如果慢慢减少学习率，在初期，学习还是相对较快地，但随着学习率的变小，步伐也会变慢变小，所以最后当开始收敛时，你的曲线（绿色线）会在最小值附近的一个较小区域之内摆动，而不是在训练过程中，大幅度地在最小值附近摆动。对于这个问题，我目前收集了有下面这些解决办法。直接修改学习率在吴恩达的机器学习课程中，他介绍了一种人为选择学习率的规则：每三倍选择一个学习率。比如：我们首先选择了0.1为学习率，那么当这个学习率过大时，我们修改成0.3。倘若还是偏大，我们继续改为0.01、0.003、0.001…以此类推，当学习率偏小是也是以三倍增加并尝试检验，最终选出比较合适的学习率。但这种方法只适用于模型数量小的情况，且这种方法终究还是固定的学习率，依旧无法很好地权衡从而达到前期快速下降与后期稳定收敛的目的。学习率动态衰减学习率衰减的本质在于，在学习初期，你能承受并且需要较大的步伐，但当开始收敛的时候，小一些的学习率能让你步伐小一些，从而更稳定地达到精确的最优解。为此，我们另外增添衰减率超参数，构建函数使学习率能够在训练的过程中动态衰减。\alpha = \frac{1}{1+decayrate*epochnum}*\alpha _{0}\其中decay rate称为衰减率，epoch num是代数，$ \alpha _{0} $是初始学习率。此外还有下面这些构造方法：指数衰减：$ \alpha =0.95^{epochnum}*\alpha _{0} $其他常用方法：\alpha =\frac{k}{\sqrt{epochnum}}*\alpha _{0}\\alpha =\frac{k}{\sqrt{t}}\alpha _{0}\其中k为mini-batch的数字。几种衰减方法的实现在pytorch中，学习率调整主要有两种方式：1.直接修改optimizer中的lr参数。2.利用lr_scheduler()提供的几种衰减函数。即使用torch.optim.lr_scheduler，基于循环的次数提供了一些方法来调节学习率。3.利用torch.optim.lr_scheduler.ReduceLROnPlateau，基于验证测量结果来设置不同的学习率.下面提供几种实现方法：准备（对下列通用）：1234567891011import torchfrom torch.optim import * #包含Adam，lr_scheduler等import torch.nn as nn#生成一个简单全连接神经网络class net(nn.Module): def __init__(self): super(net, self).__init__() self.fc = nn.Linear(1, 10) def forward(self, x): return self.fc(x)手动阶梯式衰减1234567model = net()LR = 0.01optimizer = Adam(model.parameters(), lr = LR)for epoch in range(100): if epoch % 5 == 0: for p in optimizer.param_groups: p['lr'] *= 0.9 #学习率超参的位置：optimizer.state_dict()['param_groups'][0]['lr']这里是每过5个epoch就进行一次衰减。lambda自定义衰减12345678import numpy as np model = net()LR = 0.01optimizer = Adam(model.parameters(), lr = LR)lambda1 = lambda epoch: np.sin(epoch) / epochscheduler = lr_scheduler.LambdaLR(optimizer, lr_lambda = lambda1)for epoch in range(100): scheduler.step()lr_lambda会接收到一个int参数：epoch，然后根据epoch计算出对应的lr。如果设置多个lambda函数的话，会分别作用于optimizer中的不同的params_group。StepLR阶梯式衰减123456model = net()LR = 0.01optimizer = Adam(model.parameters(), lr = LR)scheduler = lr_scheduler.StepLR(optimizer, step_size = 5, gamma = 0.8)for epoch in range(100): scheduler.step()每个epoch，lr会自动乘以gamma。三段式衰减123456model = net()LR = 0.01optimizer = Adam(model.parameters(), lr = LR)scheduler = lr_scheduler.MultiStepLR(optimizer, milestones = [20,80], gamma = 0.9)for epoch in range(100): scheduler.step()这种方法就是，当epoch进入milestones范围内即乘以gamma，离开milestones范围之后再乘以gamma。这种衰减方式也是在学术论文中最常见的方式，一般手动调整也会采用这种方法。连续衰减123456model = net()LR = 0.01optimizer = Adam(model.parameters(), lr = LR)scheduler = lr_scheduler.ExponentialLR(optimizer, gamma = 0.9)for epoch in range(100): scheduler.step()这种方法就是在每个epoch中lr都乘以gamma，从而达到连续衰减的效果。余弦式调整123456model = net()LR = 0.01optimizer = Adam(model.parameters(), lr = LR)scheduler = lr_scheduler.CosineAnnealingLR(optimizer, T_max = 20)for epoch in range(100): scheduler.step()这里的T_max对应1/2个cos周期所对应的epoch数值。基于loss和accuracy123456model = net()LR = 0.01optimizer = Adam(model.parameters(), lr = LR)scheduler = lr_scheduler.ReduceLROnPlateau(optimizer, mode = 'min', factor = 0.1, patience = 10, verbose = False, threshold = 0.0001, threshold_mode = 'rel', cooldown = 0, min_lr = 0, eps = 1e-08)for epoch in range(100): scheduler.step()当发现loss不再降低或者accuracy不再提高之后，就降低学习率。注：上面代码中各参数意义如下：mode：’min’模式检测metric是否不再减小，’max’模式检测metric是否不再增大；factor：触发条件后lr*=factor；patience：不再减小（或增大）的累计次数；verbose：触发条件后print；threshold：只关注超过阈值的显著变化；threshold_mode：有rel和abs两种阈值计算模式，rel规则：max模式下如果超过best(1+threshold)为显著，min模式下如果低于best(1-threshold)为显著；abs规则：max模式下如果超过best+threshold为显著，min模式下如果低于best-threshold为显著；cooldown：触发一次条件后，等待一定epoch再进行检测，避免lr下降过速；min_lr：最小的允许lr；eps：如果新旧lr之间的差异小与1e-8，则忽略此次更新。这里非常感谢facebook的员工给我们提供了如此多的选择与便利！对于上述方法如有任何疑惑，还请查阅torch.optim文档。批归一化（Batch Normalization）除了对学习率进行调整之外，Batch Normalization也可以有效地解决之前的问题。我是在学习ResNet的时候第一次遇到批归一化这个概念的。随着深度神经网络深度的加深，训练越来越困难，收敛越来越慢。为此，很多论文都尝试解决这个问题，比如ReLU激活函数，再比如Residual Network，而BN本质上也是解释并从某个不同的角度来解决这个问题的。通过使用Batch Normalization，我们可以加快网络的收敛速度，这样我们就可以使用较大的学习率来训练网络了。此外，BN还提高了网络的泛化能力。BN的基本思想其实相当直观：首先，因为深层神经网络在做非线性变换前的激活输入值（就是x=WU+B，U是输入）随着网络深度加深或者在训练过程中，其分布逐渐发生偏移或者变动，之所以训练收敛慢，一般是整体分布逐渐往非线性函数的取值区间的上下限两端靠近（对于Sigmoid函数来说，意味着激活输入值WU+B是大的负值或正值），这就导致了反向传播时低层神经网络的梯度消失，这是训练深层神经网络收敛越来越慢的本质原因。事实上，神经网络学习过程本质上是为了学习数据的分布，而BN就是通过一定的规范化手段，把每层神经网络任意神经元这个输入值的分布强行拉回到均值为0、方差为1的标准正态分布，其实就是把越来越偏的分布强制拉回比较标准的分布，这样使得激活输入值落在非线性函数对输入比较敏感的区域，这样输入的小变化就会导致损失函数较大的变化，从而让梯度变大，避免梯度消失问题产生，而且梯度变大意味着学习收敛速度快，因此通过BN能大大加快训练速度。下面来看看BN的具体操作过程：即以下四个步骤：1.计算样本均值。2.计算样本方差。3.对样本数据进行标准化处理。4.进行平移和缩放处理。这里引入了γ和β两个参数。通过训练可学习重构的γ和β这两个参数，让我们的网络可以学习恢复出原始网络所要学习的特征分布。下面是BN层的训练流程：这里的详细过程如下：输入：待进入激活函数的变量。输出：1.这里的K，在卷积网络中可以看作是卷积核个数，如网络中第n层有64个卷积核，就需要计算64次。注意：在正向传播时，会使用γ与β使得BN层输出与输入一样。2.在反向传播时利用γ与β求得梯度从而改变训练权值（变量）。3.通过不断迭代直到训练结束，求得关于不同层的γ与β。4.不断遍历训练集中的图片，取出每个batch_size中的γ与β，最后统计每层BN的γ与β各自的和除以图片数量得到平均值，并对其做无偏估计直作为每一层的E[x]与Var[x]。5.在预测的正向传播时，对测试数据求取γ与β，并使用该层的E[x]与Var[x]，通过图中11：所表示的公式计算BN层输出。注意：在预测时，BN层的输出已经被改变，因此BN层在预测中的作用体现在此处。上面输入的是待进入激活函数的变量，在残差网络ResNet中，的确也是先经过BN层再用relu函数做非线性处理的。那么，为什么BN层一般用在线性层和卷积层的后面，而不是放在非线性单元即激活函数之后呢？因为非线性单元的输出分布形状会在训练过程中变化，归一化无法消除他的方差偏移。相反的，全连接和卷积层的输出一般是一个对称、非稀疏的一个分布，更加类似高斯分布，对他们进行归一化会产生更加稳定的分布。比如，我们对一个高斯分布的数据relu激活，那么小于0的直接就被抑制了，这样得到的结果很难是高斯分布了，这时候再添加一个BN层就很难达到所需的效果。很多实验证明，BatchNorm只要用了就有效果，所以在一般情况下没有理由不用。但也有相反的情况，比如当每个batch里所有的sample都非常相似的时候，相似到mean和variance都基本为0时，最好不要用BatchNorm。此外如果batch size为1，从原理上来讲，此时用BatchNorm是没有任何意义的。注意：通常我们在进行Transfer Learning的时候，会冻结之前的网络权重，注意这时候往往也会冻结BatchNorm中训练好的moving averages值。这些moving averages值只适用于以前的旧的数据，对新数据不一定适用。所以最好的方法是在Transfer Learning的时候不要冻结BatchNorm层，让moving averages值重新从新的数据中学习。批归一化实现这里还是使用pytorch进行实现。准备（对下列通用）：12import torchimport torch.nn as nn2d或3d输入123456# 添加了可学习的仿射变换参数m = nn.BatchNorm1d(100)# 未添加可学习的仿射变换参数m = nn.BatchNorm1d(100, affine = False)input = torch.autograd.Variable(torch.randn(20, 100))output = m(input)我们查看m，可以看到有如下形式：1BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)这里解释一下涉及到的参数：num_features：来自期望输入的特征数，该期望输入的大小为：batch_size * num_features(* width)eps：为保证数值稳定性（分母不能趋近或取0），给分母加上的值，默认为1e-5。momentum：计算动态均值和动态方差并进行移动平均所使用的动量，默认为0.1。affine：一个布尔值，当设为true时，就给该层添加可学习的仿射变换参数。仿射变换将在后文做简单介绍。BatchNorm1d可以有两种输入输出：1.输入（N，C），输出（N，C）。2.输入（N，C，L），输出（N，C，L）。3d或4d输入12345m = nn.BatchNorm2d(100)#或者m = nn.BatchNorm2d(100, affine = False)input = torch.autograd.Variable(torch.randn(20, 100, 35, 45))output = m(input)BatchNorm2d也可以有两种输入输出：1.输入（N，C，L），输出（N，C，L）。2.输入（N，C，H，W），输出（N，C，H，W）。4d或5d输入123m = nn.BatchNorm3d(100)#或者m = nn.BatchNorm3d(100, affine=False)BatchNorm3d同样支持两种输入输出：1.输入（N，C，H，W），输出（N，C，H，W）。2.输入（N，C，D，H，W），输出（N，C，D，H，W）。仿射变换这里我简单介绍一下仿射变换的概念，仿射变换（Affine Transformation或Affine Map）是一种二维坐标（x, y）到二维坐标（u, v）的变换，它是另外两种简单变换的叠加，一是线性变换，二是平移变换。同时，仿射变换保持了二维图形的“平直性”、“平行性”和“共线比例不变性”，非共线的三对对应点确定一个唯一的仿射变换。补充：共线性：若几个点变换前在一条线上，则仿射变换后仍然在一条线上。平行性：若两条线变换前平行，则变换后仍然平行。共线比例不变性：变换前一条线上两条线段的比例，在变换后比例不变。在二维图像变换中，它的一般表达如下：可以视为线性变换R和平移变换T的叠加。另外，仿射变换可以通过一系列的原子变换的复合来实现，包括平移，缩放，翻转，旋转和剪切。因此我们可以将几种简单的变换矩阵相乘来实现仿射变换。]]></content>
      <categories>
        <category>人工智能</category>
      </categories>
      <tags>
        <tag>深度学习</tag>
        <tag>代码实现</tag>
        <tag>pytorch</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[windows笔记：一些快捷的操作]]></title>
    <url>%2Fwindows20191001130531%2F</url>
    <content type="text"><![CDATA[之前在网上看到一个windows系统下的上帝模式，很好奇，尝试之后感觉不错，这里介绍一下创建的方法。除此之外附上一些类似的快捷操作。上帝模式上帝模式，即”God Mode”，或称为“完全控制面板”。它是windows系统中隐藏的一个简单的文件夹窗口，包含了几乎所有windows系统的设置，如控制面板的功能、界面个性化、辅助功能选项等控制设置，用户只需通过这一个窗口就能实现所有的操控，而不必再去为调整一个小小的系统设置细想半天究竟该在什么地方去打开设置。打开上帝模式后你将会看到如下界面：好吧我承认和想象中的上帝模式不太一样，不过下面我还是介绍一下这个略显简陋的上帝模式是怎么设置的。方式一：添加桌面快捷方式首先在桌面新建一个文件夹。将新建的文件夹命名为：GodMode.{ED7BA470-8E54-465E-825C-99712043E01C}。重命名完成后，你将看到一个类似于控制面板但没有名称的图标，双击打开，就可以看到之前所展示的上帝模式的界面了。方式二：添加到快捷菜单win+R运行，输入regedit打开注册表编辑器，允许更改。依次展开路径至HKEY_CLASSES_ROOT\DesktopBackground\Shell。点击shell后在右侧窗口鼠标右击，选择新建项。把新建的项重命名为“上帝模式”。点击上帝模式后，双击右侧窗口中的默认，在数值数据处输入上帝模式，点击确定。右击上帝模式，选择新建项。把新建的项重命名为“command”。点击command后，双击右侧窗口中的默认，在数值数据处输入：explorer shell:::{ED7BA470-8E54-465E-825C-99712043E01C}，确定。这时候在桌面空白处右键打开快捷菜单，就可以看到上帝模式已成功添加。类似的操作在上面我的快捷菜单中，可以看到还有关机、重启、锁屏等选项。其实它们添加的操作和添加上帝模式的步骤是一样的，只需把命名为“上帝模式”的地方修改成“关机”等文字，并且在上文中的第8步中，用对应的数值数据即可。这里提供四种功能对应的数值数据，其实这些和上面上帝模式的commmand命令都是可以直接在cmd中执行的：关机Shutdown -s -f -t 00注销Shutdown -l重启Shutdown -r -f -t 00锁屏Rundll32 User32.dll,LockWorkStation事实上，锁屏功能可以直接使用win+L快捷键达到目的。除此之外，win还可以搭配其他的一些按键完成一些快捷操作，比如win+D可以快速最小化一切窗口回到桌面，想知道win有哪些搭配可以右键左下角的win图标查看。]]></content>
      <categories>
        <category>操作和使用</category>
      </categories>
      <tags>
        <tag>命令操作</tag>
        <tag>配置优化</tag>
        <tag>windows</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[machine learning笔记：过拟合与欠拟合]]></title>
    <url>%2Fmachine-learning20191001104538%2F</url>
    <content type="text"><![CDATA[本文介绍在模型评估可能会出现的过拟合与欠拟合两种现象，并对解决方法做一个总结。解释我们先通过图片来直观地解释这两种现象：在上图中，右边是过拟合的情况，它指的是模型对于训练数据拟合过度，反映到评估指标上，就是模型在训练集上的表现很好，但在测试集和新数据上的表现较差。这是因为在这种条件下，模型过于复杂，导致把噪声数据的特征也学习到了模型中，导致模型的泛化能力下降，从而在后期的应用过程中很容易输出错误的预测结果。左边是欠拟合的情况，它指的是在训练和预测时的表现都不好，这样的模型没有很好地捕捉到数据地特征，从而不能够很好地拟合数据。相比而言，中间是拟合适当的情况，这种模型在应用中就具有很好的鲁棒性。解决方法针对过拟合获取更多数据。更多的样本可以让模型学到更多有效的特征，从而减小噪声的影响。当然，一般情况下直接增加数据是很困难的，因此我们需要通过一定的规则来扩充训练数据。比如，在图像分类问题上，我们可以使用数据增强的方法，通过对图像的平移、旋转、缩放等方式来扩充数据；更进一步地，可以使用生成式对抗网络来合成大量新的训练数据。降低模型复杂度。模型复杂度过高是数据量较小时过拟合的主要原因。适当降低模型的复杂度可以避免模型拟合过多的噪声。比如，在神经网络模型中减少网络层数、神经元个数等；在决策树模型中降低树的深度、进行剪枝等。注意：网络深度增加引起的准确率退化不一定是过拟合引起的，这是因为深度造成的梯度消失、梯度爆炸等问题，这在ResNet的论文中有讨论，详细可以看我的博文deep-learning笔记：使网络能够更深——ResNet简介与pytorch实现。正则化方法。这里的方法主要是权重正则化法，具体说明可以参考machine-learning笔记：机器学习中正则化的理解。集成学习。即把多个模型集成在一起，从而降低单一模型的过拟合风险。主要有Bagging（bootstrap aggregating）和Boosting（adaptive boosting）这两种集成学习方法。针对欠拟合解决欠拟合问题也可以参照解决过拟合问题的思路；添加新特征。当特征不足或者现有特征与样本标签的相关性不强时，模型容易出现欠拟合。因此，通过挖掘“上下文特征”、“组合特征”等新的特征，往往能够取得更好的效果。在深度学习中，也有很多模型可以帮助完成特征工程，比如因此分解机、梯度提升决策树、Deep-crossing等都可以成为丰富特征的方法。增加模型复杂度。当模型过于简单时，增加模型复杂度可以使模型拥有更强的拟合能力。比如，在线性模型中添加高次项，在神经网络模型中增加网络层数、神经元个数等。对于模型的选择，我在文末补充了两种模型选择的准则供参考。减小正则化系数。正则化是用来防止过拟合的，但当模型出现欠拟合现象时，我们就应该有针对性地减小正则化系数。模型选择准则模型选择的信息准则有很多，我这里介绍我知道的两个比较常用的模型选择准则：AIC准则赤池信息准则（Akaike Information Criterion，AIC）公式定义如下：AIC=2k-2ln(L)\其中k表示模型参数个数（复杂度），L表示经验误差（似然函数）。当需要从一组可供选择的模型中选择最佳模型时，通常选择AIC最小的模型。BIC准则贝叶斯信息准则（Bayesian Information Criterion，BIC）是对AIC准则的改进，定义如下：BIC=kln(n)-2ln(L)\与AIC不同，这里k的系数不再是常数。其中n代表的是样本量（数据量），这样，BIC准则就与样本量相关了。当样本量足够时，过拟合的风险变小，我们就可以允许模型复杂一些。这里再次附上这张直观的图片，方便理解与体会。简析可参考machine-learning笔记：机器学习中正则化的理解。]]></content>
      <categories>
        <category>人工智能</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>模型评估</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[artificial intelligence笔记：人工智能前沿发展情况分享]]></title>
    <url>%2Fartificial-intelligence20191001101334%2F</url>
    <content type="text"><![CDATA[这是我在一个相关的群里看到的一个论文，这篇论文比较新，看完之后觉得对目前AI发展状况的了解有一定价值，就放了上来。论文这里直接提供图片形式的原文：其他难得有一篇以AI开篇的文章，由于在我不到一年前真正接触AI相关知识时，一直疑惑人工智能、机器学习与深度学习之间的关系。直到看了台大教授李宏毅的课才知道三者之间的包含关系，这里就把课件中的一张图片放上来，一目了然：最后，再补张和deep-learning笔记：一篇非常经典的论文——NatureDeepReview文末对应的一张我觉得挺真实的图哈哈。不得不说，目前丰富的库和各种深度学习框架的确极大地方便了AI的学习与研究，许多轮子都已造好。学会运用这些工具还是很有帮助的！]]></content>
      <categories>
        <category>人工智能</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>深度学习</tag>
        <tag>论文分享</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[machine learning笔记：一个支持向量机的问题]]></title>
    <url>%2Fmachine-learning20191001093428%2F</url>
    <content type="text"><![CDATA[在学习机器学习理论的过程中，支持向量机（SVM）应该是我们会遇到的第一个对数学要求比较高的概念。理解它的原理要花费了我不少时间，写这篇博文是因为我之前看到的一个有关SVM的问题，其解答需用到SVM的相关数学原理，可以促使我思考。支持向量机的具体原理以及推导网上有大量资源，我也会在文中提供一些供参考。简介支持向量机是一种有监督的学习方法，主要思想是建立一个最优决策超平面，使得该平面两侧距离该平面最近的两类样本之间的距离最大化，从而对分类问题提供良好的泛化能力。这里有个小故事，也是我第一次看SVM课程时老师提到的，可以通过这个小故事大致理解一下SVM在做什么。它的优点主要有如下四点：1.相对于其他训练分类算法，SVM不需要过多的样本。2.SVM引入了核函数，可以处理高维的样本。3.结构风险最小。也就是说，分类器对问题真实模型的逼近与真实解之间的累计误差最小。4.由于SVM的非线性，它擅长应付线性不可分的问题。这主要是用松弛变量（惩罚变量）和核函数来实现的。这里我附上我所知的三个SVM的常用软件工具包：SVMLight、LibSVM、Liblinear。问题下面就是我在文章开头提到的问题，直接搬运：解析中提到的拉格朗日乘子法和KKT条件，也是我在看到这个问题后才尝试去理解的。能力有限，不能自己很好的解释，这里附上瑞典皇家理工学院（KTH）“统计学习基础”课程的KKT课件，个人觉得讲的很直观且详细了。]]></content>
      <categories>
        <category>人工智能</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[database笔记：范式的理解]]></title>
    <url>%2Fdatabase20190921195840%2F</url>
    <content type="text"><![CDATA[今天终于完成了计算机三级数据库的考试，这也是本学期的第一门考试。听说计算机三级中要属计算机网络最简单，然而出于学到更多有用的知识的目的，我报了数据库。然而事实证明也没学到多少，毕竟这个计算机等级考试是给非计算机专业的人设置的，现在只求能过。不过两三天书看下来，还是有些收获，现在考完了有时间就在这里记一下，方便自己和别人今后有需要看。本文参考自：https://blog.csdn.net/he626shidizai/article/details/90707037https://blog.csdn.net/u013011841/article/details/39023859范式注意：本文中的范式指的是数据库范式。在设计数据库时，为了设计一个良好的逻辑关系，必须要使关系受一定条件的约束，这种约束逐渐成为一种规范，就是我们所说的范式。目前关系数据库有六种范式：第一范式（1NF）、第二范式（2NF）、第三范式（3NF）、巴斯-科德范式（BCNF）、第四范式(4NF）和第五范式（5NF）。要求最低的是1NF，往后依次变得严格。其中最后的5NF又称完美范式。数据库一般只需满足3NF，下面我就介绍一下前三种范式。第一范式数据库考试官方教程并没有对每个范式的定义进行讲解，另外因为文字定义比较晦涩难懂，我这里通过多方参考，用图片的形式来展示各个约束条件。首先，1NF是所有关系型数据库最基本的要求，它的定义为：符合1NF的关系中的每个属性都不可再分。下图就是一个违反1NF的例子：修改如下：上面的情况就符合1NF了。我们还可以把第一范式分成两点来理解：每个字段都只能存放单一值还是上反例： 上图中，第一行的课程有两个值，这就不符合第一范式了。因此要修改成这样：每笔记录都要能用一个唯一的主键识别 这里出现了重复组，同样也不满足1NF，因为缺乏唯一的标识码。因此修改如下：第二范式第二范式是建立在第一范式的基础上的，它的改进在于：消除了非主属性对于码的部分函数依赖。第二范式消除了非主属性对于码的部分函数依赖，也就是说，第二范式中所有非主属性完全依赖于主键，即不能依赖于主键的一部分属性。为了解释明白，还是通过实例的说明：上表中，学号和课程号组合在一起是主键，但是姓名只由学号决定，这就违反了第二范式。同样的，课程名只由课程号决定，这也违反了第二范式。此外，只需要知道学号和课程号就能知道成绩。为了满足第二范式，我们就需要对上表做如下拆分：第三范式同样的，第三范式建立在第二范式的基础上。不同之处在于，在第二范式的基础之上，第三范式中非主属性都不传递依赖于主键。这是什么意思？还是看图说话：上表中，主键是学号，且已满足第二范式。然而，学校的地址也可以根据学校名称来确定，第三范式就是在这里再做一个分解：]]></content>
      <categories>
        <category>知识点与小技巧</category>
      </categories>
      <tags>
        <tag>数据库</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[hexo笔记：在linux（ubuntu）下安装使用]]></title>
    <url>%2Fhexo20190917085649%2F</url>
    <content type="text"><![CDATA[之前一直在win10下使用hexo搭建部署博客，方法参见：hexo笔记：开始创建个人博客——方法及原因。那么，如果想在linux环境下使用hexo，该如何操作呢？本文参考自：https://blog.csdn.net/y5492853/article/details/79529410原因由于在更改主题配置文件_config.yml时，长达八百多行的配置文件总是让我找得头晕目眩。由于VScode（好像）没有提供字符的快速查找匹配功能，我之前一直采用一种笨拙的办法，即在文件的空处输入想要查找的字符然后左键选中，这个时候文件中同样的字符也会被选中，这样快速拉动滚动条时就可以比较明显地发现想找的目标字符了。然而对于这种办法，我觉得主要有两大问题：忘记删除在空白处添加的文字我就犯过这样低级的错误，找到并更改之后没有删除自己添加的字符就直接快乐地ctrl+S了，于是就造成了网站能打开但是一片空白的bug。所以大家没事还是不要随意在主题配置文件中添加文字。不是长久之计虽然这个八百多行的文件已经让我够呛了，然后或许今后还会遇到更长的文件，那么这种方法就会变得极其低效（而且伤眼睛）。基于这些因素，我脑子里的第一个反映就是vim编辑器中的对文件字符的查找定位的功能（关于vim的使用，等我多多尝试并熟练之后再做小结）。好了，接下来就开始操作吧。更正最近突然发现VScode自带了搜索功能，可以直接在整个文件夹中搜索关键词。这里所给的快捷键是ctrl+shift+F，但win10用户可能会发现按了之后没有任何反应。事实上，反应还是有的，当你再次打字时，就会发现简体变成了繁体，再次按ctrl+shift+F即可恢复。直接点击搜索图标即可便捷地进行搜索，为我之前眼瞎没有发现表示无奈，但下面还是写一下怎么安装。安装首先安装node.js。这里就没windows下直接双击exe安装包那么easy啦，打开终端，老老实实输命令：123sudo apt-get install nodejssudo apt install nodejs-legacysudo apt install npm由于ubuntu源中的node.js是旧版本，下面会出现问题，我在后文解释。由于npm服务器在国外可能会影响下载速度，和windows下的步骤一样，我们换成淘宝镜像：1sudo npm config set registry https://registry.npm.taobao.org这时候如果我们直接安装hexo，会出现如下错误：因此，我们安装node升级工具n：1sudo npm install n -g并且使用sudo n stable升级版本，若看到如下输出，说明升级成功：注意：fetch可能需要花费一点时间，这时候终端不会有任何输出，不要以为出错了，耐心等待即可，不要ctrl+C中止。最后，我们安装hexo：1sudo npm install -g hexo注：-g表示安装到全局环境。接下来的初始化操作跟windows下基本一样，可以参照我之前的博文。我继续对原来的博客进行编辑，所以无需初始化一个新的，直接把windows的对应文件夹整个copy过来就行了。]]></content>
      <categories>
        <category>操作和使用</category>
      </categories>
      <tags>
        <tag>hexo</tag>
        <tag>ubuntu</tag>
        <tag>安装教程</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[machine learning笔记：机器学习中正则化的理解]]></title>
    <url>%2Fmachine-learning20190915150339%2F</url>
    <content type="text"><![CDATA[在接触了一些ml的知识后，大家一定会对正则化这个词不陌生，但是我感觉根据这个词的字面意思不能够直接地理解它的概念。因此我打算写一篇文章做个记录，方便以后回忆。线性代数中的正则化如果直接搜索正则化这个名词，首先得到的一般是代数几何中的一个概念。百度词条对它的解释是：给平面不可约代数曲线以某种形式的全纯参数表示。怎么样？是不是觉得一头雾水。这里我推荐使用谷歌或者维基百科来查询这些专业名词。对于不能科学上网的朋友，没关系，我这里提供了谷歌镜像和wikiwand，大家可以在上面得到一样的搜索结果。我们直接到维基百科搜索regularization：里面第一段是这样解释的：In mathematics, statistics, and computer science, particularly in machine learning and inverse problems, regularization is the process of adding information in order to solve an ill-posed problem or to prevent overfitting.这就和我们在机器学习应用中的目的比较相近了。机器学习中的正则化在机器学习中，正则化是一种为了减小测试误差的行为（有时候会增加训练误差）。我们在构造机器学习模型时，最终目的是让模型在面对新数据的时候，可以有很好的表现。当你用比较复杂的模型比如神经网络去拟合数据时，很容易出现过拟合现象（训练集表现很好，测试集表现较差），这会导致模型的泛化能力下降，这时候，我们就需要使用正则化，来降低模型的复杂度。为了加深印象，我下面简单介绍几种常用的机器学习正则化方法：早停法（Early Stopping）早停法，就是当训练集的误差变好，但是验证集的误差变坏（即泛化效果变差）的时候停止训练。这种方法可以一定程度上有效地防止过拟合，同时这也说明了验证集在机器学习中的重要性。权重正则化法因为噪声相比于正常信号而言，通常会在某些点出现较大的峰值。所以，只要我们保证权重系数在绝对值意义上足够小，就能够保证噪声不会被过度相应，这也是奥卡姆剃刀原理的表现，即模型不应过度复杂，尤其是当数据量不大的时候。上面是在一个网课上看到的、我觉得可以较好地呈现模型的复杂度与数据量对模型预测表现的影响的一张图片，其中向左的横轴表示数据量大小，向右的横轴表示模型复杂度，竖轴是预测表现。通过这张图，可以很明显地观察到：模型的复杂度提升需要大量的数据作为依托。权重正则化主要有两种：L1正则：$ J=J_{0}+\lambda \left | w \right |_{1} $，其中J代表损失函数（也称代价函数），$ \left | w \right |_{1} $代表参数向量w的L1范数。L2正则（weight decay）：$ J=J_{0}+\lambda \left | w \right |_{2} $，其中$ \left | w \right |_{2} $代表参数向量w的L2范数。 这里就产生了Lasso回归与岭回归两大机器学习经典算法。其中Lasso回归是一种压缩估计，可以通过构造惩罚函数得到一个较为精炼的模型，使得它可以压缩一些系数，同时设定一些系数为0，从而达到特征选择的目的。基于Lasso回归这种可以选择特征并降维的特性，它主要有这些适用情况：样本量比较小，但指标量非常多的时候（易于过拟合）。进行高维统计时。需要对特征进行选择时。对于这些回归的详细解释，大家可以到网上搜集相关信息。补充：L0范数：向量中非零元素的个数。L1范数：向量中每个元素绝对值的和。L2范数：向量元素绝对值的平方和再平方。下面我再附上一组图，希望能帮助更好地理解权重正则化：首先我们可视化一个损失函数。下面我们看一看正则化项的图示，这里使用L1范数作为正则化项。接着，我们将上面两者线性组合：我们来看看结果：可见，正则化项的引入排除了大量原本属于最优解的点，上图的情况中剩下一个唯一的局部最优解。数据增强数据增强可以丰富图像数据集，有效防止过拟合。这种方法在AlexNet中有很好的应用，大家可以看看我的博文deep-learning笔记：开启深度学习热潮——AlexNet。随机失活（dropout）dropout即随机砍掉一部分神经元之间的连接，每次只更新一部分，这可以有效地增加它的鲁棒性，提高泛化能力。这个方法在AlexNet中也有详细的解释，推荐大家去看一下。 以上就是比较常规且流行的正则化方式，今后或许会有补充，也欢迎大家提供意见~]]></content>
      <categories>
        <category>人工智能</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[deep learning笔记：开启深度学习热潮——AlexNet]]></title>
    <url>%2Fdeep-learning20190915113859%2F</url>
    <content type="text"><![CDATA[继之前那篇deep-learning笔记：着眼于深度——VGG简介与pytorch实现，我觉得还是有必要提一下VGG的前辈——具有历史意义的AlexNet，于是就写了这篇文章简要介绍一下。简介ALexNet是第一个运用大型深度卷积神经网络的模型，在ILSVRC中一下子比前一年把错误率降低了10%，这是非常惊人的，也很快引起了注意。于是，自2012年开始，深度学习热潮由此引发。根据我之前听网课的笔记以及网上的其他文章，我把AlexNet主要的进步归纳如下：使用大型深度卷积神经网络。分组卷积（groupconvolution）来充分利用GPU。随机失活dropout：一种有效的正则化方法。关于正则化，可以看我的博文machine-learning笔记：机器学习中正则化的理解。数据增强data augumentation：增大数据集以减小过拟合问题。relu激活函数：即max（0，x），至今还被广泛应用。个人思考这段时间也看了不少东西，对于如何提升神经网络的性能这个问题，我觉得主要有如下三个方面：从网络本身入手：增加深度。增加宽度。减少参数量。防止过拟合。解决梯度消失的问题。从数据集入手：尽可能使用多的数据。从硬件入手：提升GPU性能。充分利用现有的GPU性能。当你阅读完AlexNet的论文，你会发现它在这几个方面都有思考且做出了非常优秀的改进。论文在放论文之前，我还是先贴一张流程图，方便在阅读论文的时候进行对照与理解。下面奉上宝贵的论文：论文原版论文中文版从introduction第一句开始，作者就开始了一段长长的吐槽：Current approaches to object recognition make essential use of machine learning methods…吐槽Yann LeCun大佬的论文被顶会拒收了仅仅因为Yann LeCun使用了神经网络。其实，那段时间之前，由于SVM等机器学习方法的兴起，神经网络是一种被许多ml大佬们看不起的算法模型。在introduction的最后，作者留下了这样一句经典的话：All of our experiments suggest that our results can be improved simply by waiting for faster GPUs and bigger datasets to become available.有没有感觉到一种新世界大门被打开的感觉呢？有关论文别的内容，我暂不多说了，大家可以自己看论文学习与体会。附上推荐重点阅读的章节：3.1 ReLU Nonlinearity；3.5 Overall Architecture；4 Reducing Overfitting。说明同我写VGG的那篇文章中一样，我在英文原版中用黄颜色高亮了我觉得重要的内容给自己和大家今后参考。另外，我在这里推荐大家还是先尝试阅读英文原版。一方面由于一些公式、符号以及名词的原因，英文原版叙述更精准，中文翻译有缺漏、偏颇之处；另一方面更重要的，接触这些方面的知识仅参考中文是远远不够的。在这里我推荐一个chrome英文pdf阅读插件，大家可以自己到chrome里面搜索安装：有了这个插件，遇到不认识的单词，只需双击单词，就可以看到中文释义，一定程度上可以保证阅读的流畅性。但是如果想从根本上解决问题，只有好好背单词吧（我也在朝这个方向努力…）。另外，iPad的上也有好多强大的app，在这里不一一推荐了。补充：最近又发现一款特别好用且美观的查词插件，功能非常强大，推荐一下：沙拉查词。]]></content>
      <categories>
        <category>人工智能</category>
      </categories>
      <tags>
        <tag>深度学习</tag>
        <tag>论文分享</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[markdown笔记：公式插入和代码高亮]]></title>
    <url>%2Fmarkdown20190915095628%2F</url>
    <content type="text"><![CDATA[在上一篇文章deep-learning笔记：着眼于深度——VGG简介与pytorch实现中，我用到了markdown其他的一些使用方法，因此我想在此对之前的一篇文章markdown笔记：markdown的基本使用做一些补充。本文参考自：https://www.jianshu.com/p/25f0139637b7https://www.jianshu.com/p/fd97e1f8f699https://www.jianshu.com/p/68e6f82d88b7https://www.jianshu.com/p/7c02c112d532公式插入无论是学习ml还是dl，我们总是离不开数学的，于是利用markdown插入数学公式就成了一个的需求。那么怎么在markdown中插入公式呢？markdown中的公式分为两类，即行内公式与行间公式。它们对应的代码如下：12$ \Gamma(z) = \int_0^\infty t^&#123;z-1&#125;e^&#123;-t&#125;dt\,. $$$\Gamma(z) = \int_0^\infty t^&#123;z-1&#125;e^&#123;-t&#125;dt\,.$$让我们来看一下效果：行内公式：$ \Gamma(z) = \int_0^\infty t^{z-1}e^{-t}dt\,. $行间公式：\Gamma(z) = \int_0^\infty t^{z-1}e^{-t}dt\,.注意：使用单个$时，需要在公式两边与$之间空一格，我之前没空就一直转换不成公式的形式。如果你是使用hexo编写博客，那么默认的设置是无法转义markdown公式的，解决这个问题的配置方法可以参考本文顶部给出的第三个链接。另外要注意，在使用公式时，对应文件需开启mathjax选项。补充更新：在查看next主题配置文件时，我注意到next好像自带mathjax支持，设置如下，这样就无需在每个文件中添加开启mathjax的选项。markdown公式的具体语法可以参照本文的第一个链接，你可以在typora中根据它的官方文档进行尝试。注意：在typora中，只需输入$或者$$就可直接进入公式编辑，无需输入一对。有机会我再对上面提到的语法进行搬运。下面介绍一种更简单省力的方法（也是我在用的方法）：打开在线LaTex公式编辑器。在上方的框框中输入你想要的公式： 你可以在下方的GIF图中随时观察你的输入时候符合预期，如在书写word文档等类似文本时需要插入公式，也可以直接复制图片。拷贝下方黄颜色方框中的代码到markdown文件。 你可以选择去掉两边的“\”和方括号，否则你的公式两侧将会套有方括号，另外你还需要使用上文提到的$来确定公式显示方式。这里我们这样输入：$ x+y=z $。得到：$ x+y=z $。以上就是使用LaTex给markdown添加公式的方法。你也可以使用黄颜色框中的URL选项来添加代码，格式是![](URL)。例如，输入：![](https://latex.codecogs.com/gif.latex?x&amp;plus;y=z)可以看到：这种方法就不需要文章顶部链接三中的配置了，也是一种推荐的方法。代码高亮markdown中使代码高亮的格式如下：三个反引号+语言名代码…三个反引号例如，输入：可以看到：1print("hello world!")同样的，在typora中，你也不必输入成对的三个反引号。这里我要提醒一个我以前用Rmarkdown时踩过的坑：注意！他俩是不一样的！真正的“`”在这里：]]></content>
      <categories>
        <category>操作和使用</category>
      </categories>
      <tags>
        <tag>markdown</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[deep learning笔记：着眼于深度——VGG简介与pytorch实现]]></title>
    <url>%2Fdeep-learning20190915073809%2F</url>
    <content type="text"><![CDATA[VGG是我第一个自己编程实践的卷积神经网络，也是挺高兴的，下面我就对VGG在这篇文章中做一个分享。本文参考自：https://blog.csdn.net/xiaohuihui1994/article/details/89207534https://blog.csdn.net/sinat_33487968/article/details/83584289简介VGG模型在2014年取得了ILSVRC竞赛的第二名，第一名是GoogLeNet。但是VGG在多个迁移学习任务中的表现要优于googLeNet。相比之前的神经网路，VGG主要有两大进步：其一是它增加了深度，其二是它使用了小的3x3的卷积核，这可以使它在增加深度的时候一定程度上防止了参数的增长。缺点是它的参数量比较庞大，但这并不意味着它不值得我们仔细研究。下图展示的是VGG的结构。为了通过对比来对VGG的一些改进进行解释，VGG的作者在论文中提供了多个版本。论文要详细分析VGG，我可能不能像网上写的那样好，更不可能像论文一样明白。那么我在这里就先附上论文。论文原版论文中文版我在英文原版中用黄颜色高亮了我觉得比较重要的内容，大家可以参考一下。大家也可以自己到网上进行搜索，这类经典的网络网上有许多介绍与分析。通过论文或者网上的资源对这个网络有一定理解之后，你可以看看我下面的代码实现。自己实现这里我使用pytorch框架来实现VGG。pytorch是一个相对较新的框架，但热度上升很快。根据网上的介绍，pytorch是一个非常适合于学习与科研的深度学习框架。我尝试了之后，也发现上手很快。在pytorch中，神经网络可以通过torch.nn包来构建。这里我不一一介绍了，大家可以参考pytorch官方中文教程来学习，照着文档自己动手敲一遍之后，基本上就知道了pytorch如何使用了。为了方便直观的理解，我先提供一个VGG16版本的流程图。下面是我实现VGG19版本的代码：首先，我们import所需的包。12import torchimport torch.nn as nn接下来，我们定义神经网络。1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980class VGG(nn.Module): def __init__(self, num_classes = 1000): #imagenet图像库总共1000个类 super(VGG, self).__init__() #先运行父类nn.Module初始化函数 self.conv1_1 = nn.Conv2d(in_channels = 3, out_channels = 64, kernel_size = 3, padding = 1) #定义图像卷积函数：输入为图像（3个频道，即RGB图）,输出为64张特征图,卷积核为3x3正方形，为保留原空间分辨率，卷积层的空间填充为1 self.conv1_2 = nn.Conv2d(in_channels = 64, out_channels = 64, kernel_size = 3, padding = 1) self.conv2_1 = nn.Conv2d(in_channels = 64, out_channels = 128, kernel_size = 3, padding = 1) self.conv2_2 = nn.Conv2d(in_channels = 128, out_channels = 128, kernel_size = 3, padding = 1) self.conv3_1 = nn.Conv2d(in_channels = 128, out_channels = 256, kernel_size = 3, padding = 1) self.conv3_2 = nn.Conv2d(in_channels = 256, out_channels = 256, kernel_size = 3, padding = 1) self.conv3_3 = nn.Conv2d(in_channels = 256, out_channels = 256, kernel_size = 3, padding = 1) self.conv3_4 = nn.Conv2d(in_channels = 256, out_channels = 256, kernel_size = 3, padding = 1) self.conv4_1 = nn.Conv2d(in_channels = 256, out_channels = 512, kernel_size = 3, padding = 1) self.conv4_2 = nn.Conv2d(in_channels = 512, out_channels = 512, kernel_size = 3, padding = 1) self.conv4_3 = nn.Conv2d(in_channels = 512, out_channels = 512, kernel_size = 3, padding = 1) self.conv4_4 = nn.Conv2d(in_channels = 512, out_channels = 512, kernel_size = 3, padding = 1) self.conv5_1 = nn.Conv2d(in_channels = 512, out_channels = 512, kernel_size = 3, padding = 1) self.conv5_2 = nn.Conv2d(in_channels = 512, out_channels = 512, kernel_size = 3, padding = 1) self.conv5_3 = nn.Conv2d(in_channels = 512, out_channels = 512, kernel_size = 3, padding = 1) self.conv5_4 = nn.Conv2d(in_channels = 512, out_channels = 512, kernel_size = 3, padding = 1) self.relu = nn.ReLU(inplace = True) #inplace=TRUE表示原地操作 self.max = nn.MaxPool2d(kernel_size = 2, stride = 2) self.fc1 = nn.Linear(512 * 7 * 7, 4096) #定义全连接函数1为线性函数:y = Wx + b，并将512*7*7个节点连接到4096个节点上。 self.fc2 = nn.Linear(4096, 4096) self.fc3 = nn.Linear(4096, num_classes) #定义全连接函数3为线性函数:y = Wx + b，并将4096个节点连接到num_classes个节点上，然后可用softmax进行处理。 #定义该神经网络的向前传播函数，该函数必须定义，一旦定义成功，向后传播函数也会自动生成（autograd） def forward(self, x): x = self.relu(self.conv1_1(x)) x = self.relu(self.conv1_2(x)) x = self.max(x) #输入x经过卷积之后，经过激活函数ReLU，循环两次，最后使用2x2的窗口进行最大池化Max pooling，然后更新到x。 x = self.relu(self.conv2_1(x)) x = self.relu(self.conv2_2(x)) x = self.max(x) x = self.relu(self.conv3_1(x)) x = self.relu(self.conv3_2(x)) x = self.relu(self.conv3_3(x)) x = self.relu(self.conv3_4(x)) x = self.max(x) x = self.relu(self.conv4_1(x)) x = self.relu(self.conv4_2(x)) x = self.relu(self.conv4_3(x)) x = self.relu(self.conv4_4(x)) x = self.max(x) x = self.relu(self.conv5_1(x)) x = self.relu(self.conv5_2(x)) x = self.relu(self.conv5_3(x)) x = self.relu(self.conv5_4(x)) x = self.max(x) x = x.view(-1, self.num_flat_features(x)) #view函数将张量x变形成一维的向量形式,总特征数并不改变,为接下来的全连接作准备。 x = self.fc1(x) #输入x经过全连接1，然后更新x x = self.fc2(x) x = self.fc3(x) return x def num_flat_features(self, x): size = x.size()[1:] #all dimensions except the batch dimension num_features = 1 for s in size: num_features *= s return num_featuresvgg = VGG()print(vgg)我们print网络，可以看到输出如下：1234567891011121314151617181920212223VGG( (conv1_1): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (conv1_2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (conv2_1): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (conv2_2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (conv3_1): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (conv3_2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (conv3_3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (conv3_4): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (conv4_1): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (conv4_2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (conv4_3): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (conv4_4): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (conv5_1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (conv5_2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (conv5_3): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (conv5_4): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (relu): ReLU(inplace=True) (max): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False) (fc1): Linear(in_features=25088, out_features=4096, bias=True) (fc2): Linear(in_features=4096, out_features=4096, bias=True) (fc3): Linear(in_features=4096, out_features=1000, bias=True))最后我们随机生成一个张量来进行验证。123input = torch.randn(1, 3, 224, 224)out = vgg(input)print(out)其中(1, 3, 224, 224)表示1个3x224x224的矩阵，这是因为VGG输入的是固定尺寸的224x224的RGB（三通道）图像。如果没有报错，那么就说明你的神经网路可以运行通过了。我们也可以使用torch.nn.functional来实现激活函数与池化层，这样的话，你需要还需要多引入一个包：123import torchimport torch.nn as nnimport torch.nn.functional as F #新增同时，你不需要在init中实例化激活函数与最大池化层，相应的，你需要对forward前馈函数进行更改：1234567891011121314151617181920212223242526272829303132333435def forward(self, x): x = F.relu(self.conv1_1(x)) x = F.relu(self.conv1_2(x)) x = F.max_pool2d(x, kernel_size = 2, stride = 2) #输入x经过卷积之后，经过激活函数ReLU，循环两次，最后使用2x2的窗口进行最大池化Max pooling，然后更新到x。 x = F.relu(self.conv2_1(x)) x = F.relu(self.conv2_2(x)) x = F.max_pool2d(x, kernel_size = 2, stride = 2) x = F.relu(self.conv3_1(x)) x = F.relu(self.conv3_2(x)) x = F.relu(self.conv3_3(x)) x = F.relu(self.conv3_4(x)) x = F.max_pool2d(x, kernel_size = 2, stride = 2) x = F.relu(self.conv4_1(x)) x = F.relu(self.conv4_2(x)) x = F.relu(self.conv4_3(x)) x = F.relu(self.conv4_4(x)) x = F.max_pool2d(x, kernel_size = 2, stride = 2) x = F.relu(self.conv5_1(x)) x = F.relu(self.conv5_2(x)) x = F.relu(self.conv5_3(x)) x = F.relu(self.conv5_4(x)) x = F.max_pool2d(x, kernel_size = 2, stride = 2) x = x.view(-1, self.num_flat_features(x)) #view函数将张量x变形成一维的向量形式,总特征数并不改变,为接下来的全连接作准备。 x = self.fc1(x) #输入x经过全连接1，然后更新x x = self.fc2(x) x = self.fc3(x) return x如果你之前运行通过的话，那么这里也是没有问题的。这里我想说明一下torch.nn与torch.nn.functional的区别。这两个包中有许多类似的激活函数与损失函数，但是它们又有如下不同：首先，在定义函数层（继承nn.Module）时，init函数中应该用torch.nn，例如torch.nn.ReLU，torch.nn.Dropout2d，而forward中应该用torch.nn.functionl，例如torch.nn.functional.relu，不过请注意，init里面定义的是标准的网络层。只有torch.nn定义的才会进行训练。torch.nn.functional定义的需要自己手动设置参数。所以通常，激活函数或者卷积之类的都用torch.nn定义。另外，torch.nn是类，必须要先在init中实例化，然后在forward中使用，而torch.nn.functional可以直接在forward中使用。大家还可以通过官方文档torch.nn.functional与torch.nn来进一步了解两者的区别。大家或许发现，我的代码中有大量的重复性工作。是的，你将在文章后面的官方实现中看到优化的代码，但是相对来说，我的代码更加直观些，完全是按照网络的结构顺序从上到下编写的，可以方便初学者（including myself）的理解。出现的问题虽然我的代码比较简单直白，但是过程中并不是一帆风顺的，出现了两次报错：输入输出不匹配当我第一遍运行时，出现了一个RuntimeError： 这是一个超级低级的错误，经学长提醒后我才发现，我两个卷积层之间输出输入的channel数并不匹配： 唉又是ctrl+C+V惹的祸，改正后的网络可以参见上文。在这里，我想提醒我自己和大家注意一下卷积层输入输出的维度公式：假设输入的宽、高记为W、H。超参数中，卷积核的维度是F，stride步长是S，padding是P。那么输出的宽X与高Y可用如下公式表示：X=\frac{W+2P-F}{S}+1\Y=\frac{H+2P-F}{S}+1\然而，当我在计算ResNet的维度的时候，发现套用这个公式是除不尽的。于是我搜索到了如下规则：1.对卷积层操作，除不尽时，向下取整。2.对池化层操作，除不尽时，向上取整。没有把张量转化成一维向量上面的问题解决了，结果还有错误： 根据报错，可以发现3584x7是等于25088的，结合pytorch官方文档，我意识到我在把张量输入全连接层时，没有把它拍扁成一维。因此，我按照官方文档添加了如下代码：12345678x = x.view(-1, self.num_flat_features(x))def num_flat_features(self, x): size = x.size()[1:] #all dimensions except the batch dimension num_features = 1 for s in size: num_features *= s return num_features再运行，问题解决。另外，我原本写的代码中，在卷积层之间的对应位置都加上了relu激活函数与池化层。后来我才意识到，由于它们不具有任何需要学习的参数，我可以直接把它们拿出来单独定义：12self.relu = nn.ReLU(inplace = True)self.max = nn.MaxPool2d(kernel_size = 2, stride = 2)虽然是一些很低级的坑，但我还是想写下来供我自己和大家今后参考。官方源码由于VGG的结构设计非常有规律，因此官方源码给出了更简洁的版本：123456789101112131415161718192021222324252627282930313233343536373839import torch.nn as nnimport mathclass VGG(nn.Module): def __init__(self, features, num_classes=1000, init_weights=True): super(VGG, self).__init__() self.features = features self.classifier = nn.Sequential( nn.Linear(512 * 7 * 7, 4096), nn.ReLU(True), nn.Dropout(), nn.Linear(4096, 4096), nn.ReLU(True), nn.Dropout(), nn.Linear(4096, num_classes), ) if init_weights: self._initialize_weights() def forward(self, x): x = self.features(x) x = x.view(x.size(0), -1) x = self.classifier(x) return x def _initialize_weights(self): for m in self.modules(): if isinstance(m, nn.Conv2d): n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels m.weight.data.normal_(0, math.sqrt(2. / n)) if m.bias is not None: m.bias.data.zero_() elif isinstance(m, nn.BatchNorm2d): m.weight.data.fill_(1) m.bias.data.zero_() elif isinstance(m, nn.Linear): m.weight.data.normal_(0, 0.01) m.bias.data.zero_()因为VGG中卷积层的重复性比较高，所以官方使用一个函数来循环产生卷积层：1234567891011121314def make_layers(cfg, batch_norm=False): layers = [] in_channels = 3 for v in cfg: if v == 'M': layers += [nn.MaxPool2d(kernel_size=2, stride=2)] else: conv2d = nn.Conv2d(in_channels, v, kernel_size=3, padding=1) if batch_norm: layers += [conv2d, nn.BatchNorm2d(v), nn.ReLU(inplace=True)] else: layers += [conv2d, nn.ReLU(inplace=True)] in_channels = v return nn.Sequential(*layers)接下来定义各个版本的卷积层（可参考上文中对论文的截图），这里的“M”表示的是最大池化层。1234567891011121314151617181920212223242526272829303132333435363738394041424344cfg = &#123; 'A': [64, 'M', 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512, 'M'], 'B': [64, 64, 'M', 128, 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512, 'M'], 'D': [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 'M', 512, 512, 512, 'M', 512, 512, 512, 'M'], 'E': [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 256, 'M', 512, 512, 512, 512, 'M', 512, 512, 512, 512, 'M'],&#125;def vgg11(**kwargs): model = VGG(make_layers(cfg['A']), **kwargs) return modeldef vgg11_bn(**kwargs): model = VGG(make_layers(cfg['A'], batch_norm=True), **kwargs) return modeldef vgg13(**kwargs): model = VGG(make_layers(cfg['B']), **kwargs) return modeldef vgg13_bn(**kwargs): model = VGG(make_layers(cfg['B'], batch_norm=True), **kwargs) return modeldef vgg16(**kwargs): model = VGG(make_layers(cfg['D']), **kwargs) return modeldef vgg16_bn(**kwargs): model = VGG(make_layers(cfg['D'], batch_norm=True), **kwargs) return modeldef vgg19(**kwargs): model = VGG(make_layers(cfg['E']), **kwargs) return modeldef vgg19_bn(**kwargs): model = VGG(make_layers(cfg['E'], batch_norm=True), **kwargs) return modelif __name__ == '__main__': # 'VGG', 'vgg11', 'vgg11_bn', 'vgg13', 'vgg13_bn', 'vgg16', 'vgg16_bn', 'vgg19_bn', 'vgg19' # Example net11 = vgg11() print(net11)附上pytorch官方源码链接。可以在vision/torchvision/models/下找到一系列用pytorch实现的经典神经网路模型。好了，以上就是VGG的介绍与实现，如有不足之处欢迎大家补充！]]></content>
      <categories>
        <category>人工智能</category>
      </categories>
      <tags>
        <tag>深度学习</tag>
        <tag>论文分享</tag>
        <tag>踩坑血泪</tag>
        <tag>代码实现</tag>
        <tag>pytorch</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[deep learning笔记：一篇非常经典的论文——NatureDeepReview]]></title>
    <url>%2Fdeep-learning20190914142553%2F</url>
    <content type="text"><![CDATA[这是一篇非常经典的有关深度学习的论文，最近在看一个网课的时候又被提到了，因此特地找了pdf文档放在这里和大家分享。简述这篇文章首先介绍了深度学习的基本前期储备知识、发展背景，并对机器学习范畴内一个重要方向——监督学习进行完整介绍，然后介绍了反向传播算法和微积分链式法则等深度学习基础内容。文章的接下来重点介绍了卷积神经网络CNN的实现过程、几个非常重要的经典卷积神经网络以及深度卷积神经网络对于视觉任务理解的应用。文章最后探讨了分布表示和语言模型，循环神经网络RNN原理以及对未来的展望和现实的实现。总而言之，我觉得这是一篇值得逐字逐句反复阅读咀嚼的文章，读完这篇文章，大概就相当于打开了深度学习的大门了吧。这篇文章的个人理解与感悟或许我以后会补上，在接触还不深的情况下我不说废话啦，先附上原文，其中黄色高亮部分是一些比较重要的内容，大家有时间的话可以认真看一下。下面附上原文链接。最后贴一张我觉得挺搞笑的图。这张图片还有张兄弟图，可以看看我的另一篇论文分享artificial-intelligence笔记：人工智能前沿发展情况分享。]]></content>
      <categories>
        <category>人工智能</category>
      </categories>
      <tags>
        <tag>深度学习</tag>
        <tag>论文分享</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ubuntu笔记：重装ubuntu——记一段辛酸血泪史]]></title>
    <url>%2Fubuntu20190914100050%2F</url>
    <content type="text"><![CDATA[这是不久前我踩过的一个巨坑，在这里我想先强调一下：不要升级linux发行版！！！重要的事情说三遍！！！不要升级linux发行版！！！重要的事情说三遍！！！不要升级linux发行版！！！重要的事情说三遍！！！为什么？不要问我为什么。我按系统提示升级ubuntu到18.04LTS后，就再也进不去系统了。不信你可以尝试一下，你将会看到如下界面：注：图片来自网络，我就不再为了截图而踩一次坑了。不仅是图形界面，命令行界面也进不去了（据说可以在重启时选择recovery mode并且狂按回车强行进入界面，但我失败了）。不过如果你真的尝试了并且掉坑里了的话，没关系，你获得了一个很好的重装系统的锻炼机会。下面我们就按步骤锻炼一下。本文参考自：https://blog.csdn.net/Spacegene/article/details/86659349准备U盘准备一个2G以上的无用的U盘，或者备份好里面的文件。然后将其格式化。镜像下载ubuntu16.04LTS镜像到本地。删除分区你可以通过控制面板中的创建并格式化硬盘分区来看到你的windows与ubuntu分区的情况。（以下操作都是针对重装，不再重新分区，需要的可以自行上网查找教程）在重新安装ubuntu16.04之前我们需要删除原先Ubuntu的EFI分区及启动引导项，这里推荐直接使用windows下的diskpart来删除。使用win+R输入diskpart打开diskpart.exe，允许其对设备进行更改。接下来使用如下命令查看分区，我的笔记本只有一块SSD，两个系统都装在上面，故进入disk 0。其中类型未知的便是分给ubuntu的分区，我这里有一块8G的swap分区和60G的/分区。接下来执行如下命令：1234select partition 7delete partition override #删除该分区select partition 8delete partition override #删除该分区注意：以上命令是针对我的情况，具体请按照对应ubuntu分区的序号删除。现在你可以在控制面板中的创建并格式化硬盘分区中看到你删除的分区已经合并成一块未分配的空间，这也意味着你与你原来ubuntu上的数据彻底说再见了。删除ubuntu启动引导项首先下载EasyUEFI，使用免费试用版即可。下载完成后安装，打开EasyUEFI如图： 选择管理EFI启动选项Manage EFI Boot Option，然后选择ubuntu启动引导项，点击中间的删除按钮来删除该引导项。现在重新启动，你会发现已经没有让你选择系统的引导界面，而是直接进入windows系统。制作启动U盘首先我们下载一个免费的U盘制作工具rufus。此时插入已经格式化的U盘，打开rufus，一般情况下它会自动选择U盘，你也可以在device选项下手动选择或确认。点击select，选择之前下载好的镜像文件。其他设置保留默认即可，不放心的话可以比对下图： 然后start开始制作。如果此时rufus提示需要下载一些其它文件，选择Yes继续即可。没有问题的话制作完的U盘会如图所示： 在下面的步骤中，请一直插着U盘不要拔。安装现在重新启动电脑，开机的过程中不停地快按F12进入bios界面（我的是戴尔的电脑，不同电脑按键或有不同，自行百度；如果快按不行的话再次重启试一试长按，因为网上有些教程说的是长按，而我是长按不行而快按可以）。随后选择U盘启动（不同电脑这个界面也可能不一样，具体可以百度，印象中是选择UEFI BOOT中UEFI：U盘名那项）。接下来就进入了紫红色的GNU GRUB界面，选择install ubuntu。随后就是些比较简单的安装过程，基本上可以按默认进行，因为是重装，好像不需要联网安装且有汉化包。接下来是比较重要的部分：进入安装类型installation type界面后，选择其他选项something else，这样我们就可以自己配置ubuntu分区了，点击继续。接下来会进入一个磁盘分区的界面，选中之前清出来的未分配分区（名为“空闲”，也可以通过大小来判断），点击下方+号，新建一个swap分区，大小为8G左右（一般和电脑的内存相当即可，具体还有待研究，不分这个区会有警告）。再次双击空闲分区，挂载点下拉，选择/。在安装启动引导器的设备选项中，选择Windows boot manager。结果可以参考下图：确认无误后点击现在安装，然后就一路默认直到安装完成。后期别忘了把U盘格式化回来，可以继续使用，留着做纪念也行，说不定哪天又要重装。下面我展示一下我目前的一部分美化效果，亲测发现这只会牺牲一点点儿CPU，所以并不用担心，大胆地美化就是，可能这也是使用linux发行版不多的几种乐趣之一吧。以上就是重装ubuntu的全部内容，由于是基于几天前的回忆可能会有疏漏，欢迎补充！我也会在新问题出现时及时更新。]]></content>
      <categories>
        <category>操作和使用</category>
      </categories>
      <tags>
        <tag>踩坑血泪</tag>
        <tag>个人经历</tag>
        <tag>ubuntu</tag>
        <tag>安装教程</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ubuntu笔记：释放空间]]></title>
    <url>%2Fubuntu20190914094853%2F</url>
    <content type="text"><![CDATA[前一篇讲了如何清理windows下的空间，然而虽然ubuntu中垃圾文件没win10那么多，可是我给ubuntu分配的空间比win10少得多了，于是我又找了些清理ubuntu的方法。方法删除apt-get下载的软件包直接在终端执行sudo apt-get autoclean删除缓存的所有软件包sudo apt-get clean删除其他软件依赖的但现在已不用的软件包sudo apt-get autoremove这条命令执行后，软件的配置文件还是会保留的。清除所有已删除包的残余配置文件dpkg -l |grep ^rc|awk &#39;{print $2}&#39; |sudo xargs dpkg -P这时候如果出现如下错误，那无需担心，因为已经不存在残余的配置文件了。 可以把上面四个命令按顺序执行一遍，就完成了对ubuntu系统的空间释放。]]></content>
      <categories>
        <category>操作和使用</category>
      </categories>
      <tags>
        <tag>命令操作</tag>
        <tag>ubuntu</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[windows笔记：释放空间]]></title>
    <url>%2Fwindows20190914091023%2F</url>
    <content type="text"><![CDATA[暑假里想跑CVPR中的代码，发现作者提供的环境配置都是基于linux终端的，这样windows的git bash就满足不了我了。二话不说我花了两天时间装了个ubuntu+windows双系统，好不容易装好了，却发现我的硬盘空间已经岌岌可危（理论上要留内存的三倍左右可以保证系统顺畅运行，我的内存是8G，也就是说我C盘应空出20G左右为宜）。于是我就找了些释放空间的办法，分享在这里。本文参考自：http://www.udaxia.com/wtjd/9147.html利用磁盘属性进行清理这是最稳的方法，但释放的空间也相对较少，不过还是有效的。选择“此电脑”，右键C盘，属性，然后就可以在常规下面看到磁盘清理。一般按默认选择的进行清理，当然全点上勾也无所谓。注意：千万不能选择压缩此驱动器以节约磁盘空间！另外在工具下面你可以看到一个优化的选项，一般系统会定期自动执行优化，如果你是强迫症，时时刻刻都容不得一点冗余的话，可以手动优化。据我们数据结构的老师说，由于数据在存储时大多是稀疏矩阵，存在许多的空间浪费，而磁盘碎片整理优化的就是这个。清理系统文件在前面的磁盘清理界面中我们还可以看到清理系统文件这一选项，可以选择它进行进一步清理，这里面有一项是以前的windows安装文件，如果不打算回退的话清理无妨。有可能你还会注意到一个名为“系统错误内存转储文件”的选项，这个也可以大胆清除，对一般使用者（基本不需排查系统问题）这个文件完全没有任何作用。对于防止系统错误内存转储文件占用空间，还有一劳永逸直接禁止生成的办法，首先打开高级系统设置，来到如图所示选项卡。打开“启动和故障修复”设置窗口，在写入调试信息的下拉列表中选择“无”并确定即可。当然，如果你觉得你或许用得上系统错误内存转储文件，那么也可以选择这里的小内存转储或者核心内存转储，这样同样也能节省空间。清理系统文件是给windows10瘦身最有效的办法之一，事实上，若无特殊需求，扫描结果中的文件均可以勾选清除。删除临时文件这里有两个临时文件中的全部文件可以删除，一个是C:\Users\用户名\AppData\Local\Temp目录下的文件，这里是临时文件最多的地方，可以上到几个G；另一个是C:\Windows\Temp，这里文件大小相对较小，可以忽略不计。另外我也找到了一些其他的临时文件，但似乎它们的体积都是0，可能是系统自动清理了。注意：千万不要误删上一级目录！如果担心删除出错，可先放到回收站，重启之后看有无异常再做决定。亲测上述两个文件夹中的所有文件均可删除。删除冗余更新众所周知，windows系统会自动更新，这也是许多人弃windows的一大原因，然而windows还是要用的，于是我找到了一种可以清理多余的windows更新文件的方法。首先右键左下角开始菜单，选择“Windows PowerShell（管理员）” 弹出是否允许进行更改选择“是”。输入命令dism.exe /Online /Cleanup-Image /AnalyzeComponentStore并执行。这时候会显示“推荐使用组件存储清理：是or否”，因为我前不久清过，所以这里显示为“否”，那么就别清理了。如果显示为“是”，那么进行第四步。输入命令dism.exe /online /Cleanup-Image /StartComponentCleanup并执行。这样电脑就会开始清理啦。这个过程会比较长，不要着急和担心，在这期间你可以做些别的事情，比如看看我其他的几篇博客。重装系统俗话说得好“大力出奇迹”，重装系统无疑是最有效清理空间的一种方法。只要备份好数据，重装其实没有想象的那么困难。我本人这一年多来就重装过3次系统(两次是被迫的…)，其实装了几次就熟练了，我曾看到某linux大牛（忘了是谁）总共重装了19次linux。你可以在我的另一篇博文ubuntu笔记：重装ubuntu——记一次辛酸血泪史中看到如何在双系统情况下重装ubuntu的过程。]]></content>
      <categories>
        <category>操作和使用</category>
      </categories>
      <tags>
        <tag>命令操作</tag>
        <tag>windows</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[front end笔记：制作web时的一些小技巧与小问题]]></title>
    <url>%2Ffront-end20190914080224%2F</url>
    <content type="text"><![CDATA[大一有一段时间，我沉迷于web前端制作网页，比较熟练地掌握了html的语法，还根据需要接触了一些CSS以及js的内容。说白了，html只是一种标记语言（不属于编程语言），但是它简单易学，且很容易获得可视化的效果，对于培养兴趣而言我感觉是很有帮助的。油管up主，现哈佛在读学霸John Fish（请科学上网）当初就是从html进入计算机世界的。下面贴一个我自己做的网页，是综合web三大语言编写的，大一的时候把自己需要的网站都放上面了，也有一种归属感吧。主页上那个是python之禅，也是我很喜欢的一段文字，在python环境下import this就可以看到。由于上面的网页是我学web的时候边看书边编的，各种元素都尝试了一下，最后也没有美化一直到现在，所以大佬们勿喷哈。小技巧我这里强烈推荐使用VScode写前端，它有很多强大的插件，我这里推荐其中一个吧。如介绍所写，使用alt+B快捷键可以直接在默认浏览器下查看你写的网页，而shift+alt+B可以选浏览器查看，因为有些时候microsoft自带的edge浏览器无法实现你编写的效果（巨坑），推荐使用chrome打开浏览。使用这个插件能让你更快捷地预览你编写的效果并进行修改，大大提高了效率。其他的插件网上有很多推荐，也等待着你自己去发现，这里就不一一列出了。还有一个快捷的操作就是快速生成代码块，在VScode中是这样操作的（其他编辑器也应该类似）：输入一个！：按tab键或者回车： 这样就可以节省很多时间，非常方便。小问题在我想使用web来打开我本地的txt文件时，我遇到过这样一个问题：打开的中文文档在浏览器中显示为乱码。在尝试其他浏览器后，我发现这不是浏览器的问题。最后我大致找到了两种解决办法：一种是找到head下面的meta charset，修改代码如下：另一种是另存为文件时修改一下格式，这里我们修改成“UTF-8”。另外我室友在使用python导入文件的时候也因为格式导致报错，修改成ANSI后即可。 希望通过上面两种方法的尝试能让你解决乱码问题，几种编码格式的区别在这里暂不说明，以后有空补上。]]></content>
      <categories>
        <category>环境配置</category>
      </categories>
      <tags>
        <tag>前端</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[anaconda笔记：conda的各种命令行操作]]></title>
    <url>%2Fanaconda20190913231748%2F</url>
    <content type="text"><![CDATA[anaconda是一个开源的包、环境管理器，可以比较有效地配置多个虚拟环境，当python入门到一定程度时，安装anaconda是很必要的。前段时间室友学习python的时候问到过我一些相关的问题，我就在这里简单写一些我知道的以及我搜集到的知识。环境变量安装anaconda过程中一个很重要的步骤就是配置环境变量，网上有很多手动添加环境变量的教程，其实很简单，只需添加三个路径，当然更简单的是直接在安装的时候添加到path（可以无视warning）。我想在这里写的是环境变量的概念问题，其实直到不久前帮同学安装我才明白。环境变量是指在操作系统中用来指定操作系统运行环境的一些参数。当要求系统运行一个程序而没有告诉它程序所在的完整路径时，系统除了在当前目录下面寻找此程序外，还会到path中指定的路径去找。这就是为什么不添加C:\Users\用户名\Anaconda3\Scripts到path就无法执行conda命令，因为此时conda.exe无法被找到。conda与pip利用conda install与pip install命令来安装各种包的过程中，想必你也对两者之间的区别很疑惑，下面我就总结一下我搜集到的相关解答。简而言之，pip是python包的通用管理器，而conda是一个与语言无关的跨平台环境管理器。对我们而言，最显着的区别可能是这样的：pip在任何环境中安装python包，conda安装在conda环境中装任何包。因此往往conda list的数量会大于pip list。要注意的是，如果使用conda install多个环境时，对于同一个包只需要安装一次，有conda集中进行管理。但是如果使用pip，因为每个环境安装使用的pip在不同的路径下，故会重复安装，而包会从缓存中取。总的来说，我推荐尽早安装anaconda并且使用conda来管理python的各种包。升级我们可以在命令行中或者anaconda prompt中执行命令进行操作。123conda update conda #升级condaconda update anaconda #升级anaconda前要先升级condaconda update --all #升级所有包在升级完成之后，我们可以使用命令来清理一些无用的包以释放一些空间：12conda clean -p #删除没有用的包conda clean -t #删除保存下来的压缩文件（.tar）虚拟环境conda list命令用于查看conda下的包，而conda env list命令可以用来查看conda创建的所有虚拟环境。下面就简述一下如何创建这些虚拟环境。使用如下命令，可以创建一个新的环境：conda create -n Python27 python=2.7其中Python27是自定义的一个名称，而python=2.7是一个格式，可以变动等号右边的数字来改变python环境的kernel版本，这里我们安装的是python2.7版本（将于2020年停止维护）。在anaconda prompt中，我们可以看到我们处在的是base环境下，也就是我安装的python3环境下，我们可以使用下面两个命令来切换环境：在创建环境的过程中，难免会不小心取了个难听的环境名，别担心，我们有方法来删除环境。conda remove -n 难听的名字 --all有时候一个环境已经配置好了，但我们想要重命名，这怎么办呢？可以这样办：12conda create -n 新名字 --clone 老名字conda remove -n 老名字 --all把环境添加到jupyter notebook首先通过activate进入想要添加的环境中，然后安装ipykernel，接下来就可以进行添加了。12pip install ipykernelpython -m ipykernel install --name Python27 #Python27可以取与环境名不一样的名字，但方便起见建议统一我们可以使用如下命令来查看已添加到jupyter notebook的kernel：jupyter kernelspec list显示如下：我们也可以在jupyter notebook中的new或者kernel下查看新环境是否成功添加。若想删除某个指定的kernel，可以使用命令jupyter kernelspec remove kernel_name来完成。在这里我想说明一下为什么要分开python的环境。由于python是不向后兼容的，分开环境可以避免语法版本不一引起的错误，同时这也可以避免工具包安装与调用的混乱。]]></content>
      <categories>
        <category>环境配置</category>
      </categories>
      <tags>
        <tag>命令操作</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[jupyter notebook笔记：一些细小的操作]]></title>
    <url>%2Fjupyter-notebook20190913225022%2F</url>
    <content type="text"><![CDATA[首先强烈安利jupyter notebook，它是一种交互式笔记本，安装anaconda的时候会一并安装，下载VS的时候也可以选择安装。我是在初学机器学习的时候接触jupyter notebook的，立刻就被它便捷的交互与结果呈现方式所吸引，现在python编程基本不使用其他的软件。其实完全可以在初学python的时候使用jupyter notebook，可以立即得到反馈以及分析错误，可以进步很快！打开操作当初安装好之后还不了解，每次打开jupyter notebook都会先弹出一个黑框框，这时候千万别关掉，等一会就能来到网页。另外打开之后也别关掉，使用的时候是一直需要的，因为只有开着才能访问本机web服务器发布的内容。另外你也可以不通过快捷方式，直接在命令行中直接输入jupyter notebook来打开它。有些时候，当你插入硬盘或者需要直接在特定的目录下打开jupyter notebook（它的默认打开是在“usr/用户名/”路径下），那你可以在输入命令的后面加上你想要的路径。123jupyter notebook D:\ #打开D盘，于我是我的移动硬盘jupyter notebook E:\ #我的U盘jupyter notebook C:\Users\用户名\Desktop #在桌面打开快捷键操作在jupyter notebook中可以通过选中cell然后按h的方式查询快捷键。其他在jupyter notebook中可以直接使用markdown，这对学习可以起到很大的辅助作用，markdown的基本操作可以看我的另一篇博文markdown笔记：markdown的基本使用此外，在我的博文anaconda笔记：conda的各种命令行操作中，也介绍了如何将python的虚拟环境添加到jupyter notebook中，欢迎阅读。VScode前段时间也开始支持ipynb，喜欢高端暗黑科技风又懒得自己修改jupyter notebook的小伙伴可以试一试，不过我在kernel配置方面似乎还有一些小问题有待解决。]]></content>
      <categories>
        <category>环境配置</category>
      </categories>
      <tags>
        <tag>命令操作</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[markdown笔记：markdown的基本使用]]></title>
    <url>%2Fmarkdown20190913211144%2F</url>
    <content type="text"><![CDATA[既然要写技术博客，那么markdown肯定是必备的了，这篇文章就来介绍一下markdown的基本使用操作。本文参考自：https://www.jianshu.com/p/191d1e21f7ed介绍Markdown是一种可以使用普通文本编辑器编写的标记语言，其功能比纯文本更强，因此许多程序员用它来写blog。在这里我先推荐一款markdown编辑器——typora，大家可以免费下载使用。注意在我刚开始使用markdown的时候总是跳进这个坑，在这里提上来提醒一下，在使用markdown标记后要添加文字时，需要在相应标记后空一格，否则标记也会被当作文本来处理，例如我输入“#####错误”时：错误正确的做法是输入“##### 正确”：正确一种简单的判别方法就是使用IDE，这样对应的标记就会有语法高亮。使用标题话不多说，直接示范：123456# 这是一级标题## 这是二级标题### 这是三级标题#### 这是四级标题##### 这是五级标题###### 这是六级标题效果如下：这是一级标题这是二级标题这是三级标题这是四级标题这是五级标题这是六级标题字体还是直接示范：1234**这是加粗的文字***这是倾斜的文字*`***这是斜体加粗的文字***~~这是加删除线的文字~~这是加粗的文字这是倾斜的文字这是斜体加粗的文字这是加删除线的文字引用1234&gt;我引用&gt;&gt;我还引用&gt;&gt;&gt;我再引用&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;扶我起来，我还能继续引用！我引用我还引用我再引用扶我起来，我还能继续引用！引用是可以嵌套的，可以加很多层，我一般使用一个&gt;来表示额外的需要注意的内容。另外，如果想让下一段文字不被引用，需要空一行。分割线分割线使用三个及以上的-或*就可以。有时候用---会造成别的文字的格式变化，因此我在使用VScode编辑时，如果看到---被高亮（分割线正常其作用时应该不高亮），就会改用***。12---***效果如下：图片markdown中添加图片的语法是这样的：1![显示在图片下方的文字](图片地址 &quot;图片title&quot;)其中title可加可不加，它就是鼠标移动到图片上时显示的文字。然而我在使用hexo搭建我的个人博客的过程中，遇到了使用上述语法图片却无法显示的情况，因此我改用了下列标签插件：1&#123;% asset_img xxxxx.xxx 图片下方的名字 %&#125;其中xxxxx.xxx只需直接输入图片名称以及格式即可，因为我使用了hexo-asset-image插件，它可以在_posts文件中创建与博文名称相同的对应的文件夹，只需把图片移入即可。其安装命令：npm install hexo-asset-image --save也可用cnpm更快地安装：cnpm install hexo-asset-image --save补充：后来发现在关于本人中无法使用上述asset_img标签插件来对图片进行插入，故又尝试了![显示在图片下方的文字](图片地址 &quot;图片title&quot;)的方法，发现可行！原因可能是之前误用了中文括号导致的。可以参考一下Hexo文章中插入图片的方法。在插入图片的后面，会留有一小段空白区，看着不舒服的话可以不要回车，即直接在插入图片的语句后面跟进下一段的文字或者图片等，这样行间隙就会小很多。其实在hexo中可以直接使用img标签，它会自行处理，并且这样还更方便调整高度和宽度。1&lt;img src=&quot;&quot; width=&quot;50%&quot; height=&quot;50%&quot;&gt;超链接由于我希望在新的页面打开链接，而似乎markdown本身的语法不支持在新标签页打开链接，因此我推荐直接使用html语言来代替。1&lt;a href=&quot;超链接地址&quot; target=&quot;_blank&quot;&gt;超链接名&lt;/a&gt;列表无序列表123- 列表内容+ 列表内容* 列表内容列表内容列表内容列表内容有序列表1231. 列表内容2. 列表内容3. 列表内容列表内容列表内容列表内容可以看到，上面显示的列表是有嵌套的，方法就是敲三个空格缩进。表格1234表头|表头|表头---|:--:|---:内容|内容|内容内容|内容|内容其中第二行的作用分割表头和内容，-有一个就行，为了对齐可多加几个。此外文字默认居左，有两种改变方法：两边加：表示文字居中。右边加：表示文字居右。然而我在hexo使用表格时，出现了无法正常转换的问题，因此我改用了如下HTML的表格形式。12345678910&lt;table border=&quot;1&quot;&gt;&lt;tr&gt;&lt;td&gt;第一行第一列&lt;/td&gt;&lt;td&gt;第一行第二列&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;第二行第一列&lt;/td&gt;&lt;td&gt;第三行第二列&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;效果如下：第一行第一列第一行第二列第二行第一列第二行第二列代码最后的最后，是我最喜欢ctrl+C+V的代码了。单行或句中代码输入方式：1`来复制我呀`显示：来复制我呀其中`在键盘的左上角，我当初找了好久。多行代码块的写法就是用上下两对```围住。好了于是你现在就可以自由的复制粘贴啦。]]></content>
      <categories>
        <category>操作和使用</category>
      </categories>
      <tags>
        <tag>markdown</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[hexo笔记：开始创建个人博客——方法及原因]]></title>
    <url>%2Fhexo20190913153310%2F</url>
    <content type="text"><![CDATA[大家好，这是我的第一篇博文，这也是我的第一个自己搭建的网站，既然搭了，那第一篇就讲讲我搭建的过程吧。安装步骤安装node.js进入官网。选择对应系统（我这里用win10），选择LTS（长期支持版本）安装，安装步骤中一直选择next即可。安装完后就可以把安装包删除了。安装git进入官网。选择对应系统的版本下载，同样也是按默认安装。安装成功后，你会在开始菜单中看到git文件夹。 其中Git Bash与linux和mac中的终端类似，它是git自带的程序，提供了linux风格的shell，我们可以在这里面执行相应的命令。注意：bash中的复制粘贴操作与linux中类似，ctrl+C用于终止进程，可以用鼠标中键进行粘贴操作。不嫌麻烦的话可以使用ctrl+shift+C和ctrl+shift+V进行复制粘贴操作。安装hexohexo是一个快速、简洁且高效的博客框架，在这里我们使用hexo来搭建博客。首先，新建一个名为“blog”的空文件夹，以后我们的操作都在这个文件夹里进行，可以在bash中使用pwd命令查看当前所处位置。创建这个文件夹的目的是万一因为创建的博客出现问题或者不满意想重来等原因可以直接简单地把文件夹删除，也方便了对整个网站本地内容的移动。打开新建的文件夹，右键空白处，选择Git Bash Here。 接下来我们输入两行命令来验证node.js是否安装成功。 如出现如图所示结果，则表明安装成功。为了提高以后的下载速度，我们需要安装cnpm。cnpm是淘宝的国内镜像，因为npm的服务器位于国外有时可能会影响安装。继续在bash中输入如下命令安装cnpm：npm install -g cnpm --registry=https://registry.npm.taobao.org检验安装是否成功，输入cnpm： 接下来我们安装hexo，输入命令：cnpm install -g hexo-cli和上面一样，我们可以用hexo -v来验证是否成功安装hexo，这里就不贴图了。接下来我们输入如下命令来建立整个项目：hexo init你会发现你的文件夹中多了许多文件，你也可以用ls -l命令来看到新增的文件。 完成本地环境的搭建至此，我们已经完成了本地环境的搭建，在这里，我想先介绍hexo中常用的命令。hexo n &quot;文章标题&quot;用于创建新的博文（欲删除文章，直接删除md文件并用下面的命令更新即可）。hexo shexo会监视文件变动并自动更新，通过所给的localhost:4000/就可以直接在本地预览更新后的网站了。部署到远端服务器三步曲：123hexo clean #清除缓存，网页正常情况下可以忽略此条命令，执行该指令后，会删掉站点根目录下的public文件夹。hexo g #generate静态网页（静态网页这里指没有前端后端的网页而不是静止），该命令把md编译为html并存到public文件目录下。hexo d #将本地的更改部署到远端服务器（需要一点时间，请过一会再刷新网页）。此外，上面最后两步也可以使用hexo g -d直接代替。如果出现ERROR Deployer not found: git报错，可以使用npm install --save hexo-deployer-git命令解决。注意：由于部署到远端输入密码时密码不可见，有时候会导致部署失败，只有出现INFO Deploy done: git的结果才表明部署成功，否则再次部署重输密码即可。现在我们在bash中运行hexo s，打开浏览器，输入localhost:4000/，就可以看到hexo默认创建的页面了。部署到远端服务器为了让别人能访问到你搭建的网站，我们需要部署到远端服务器。这里有两种选择，一种是部署到github上，新建一个repository，然后创建一个xxxxx.github.io域名（这里xxxxx必须为你的github用户名）。另一种选择是部署到国内的coding，这是考虑到访问速度的问题，不过我选择的是前者，亲测并没感觉有速度的困扰。个人比较推荐用github pages创建个人博客。部署这块网上有许多教程，这里不详细解释了，以后有机会补上。在部署的时候涉及到对主题配置文件的操作，linux和mac用户可以使用vim进行编辑，不过也可以使用VScode、sublime等代码编辑器进行操作。注：为了国内的访问速度，我最后添加了coding/github双线部署，两者的操作方式大同小异。值得注意的是，如果使用的是leancloud的第三方阅读量与评论统计系统，那么还得在leancloud的安全中心中添加coding的web域名。创建原因首先说明，我只是一个刚起步的入门级小白，懂得不多，别喷我哈~步入大二，虽然我是大学才算真正接触编程，但一年多下来我也接触并且学习了不少技术知识。接触的多了、遇到的问题也复杂了起来，导致每次百度到的答案不一定能够解决我遇到的问题。此外，之前在学习编程语言、操作系统、ml、dl等知识的时候，为方便起见利用文本记了些笔记。然而笔记分散在四处，不方便管理与查看，因此就萌生了写博客的想法。由于个人比较喜欢自由DIY，所以没有使用CSDN、博客园等知名技术博客网站。最后还是非常感谢我们华科的校友程序羊在b站和其他站点上分享的各种经验，我就是通过他的视频来搭建起自己的第一个博客网站的。他的其他视频也给了我很多启迪。最后，最关键的原因，还是因为今天中秋节有空闲的时间哈哈，祝大家节日快乐！]]></content>
      <categories>
        <category>操作和使用</category>
      </categories>
      <tags>
        <tag>hexo</tag>
        <tag>安装教程</tag>
      </tags>
  </entry>
</search>
