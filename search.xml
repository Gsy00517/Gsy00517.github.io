<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[computer vision笔记：SSD和DSSD]]></title>
    <url>%2Fcomputer-vision20200201113416%2F</url>
    <content type="text"><![CDATA[最近看了目标检测中比较经典的SSD，觉得有不少挺好的创新。之前的文章中也提到过，由于one-stage的检测方法和two-stage的检测方法都存在着速度与精度平衡的问题，所以SSD在借鉴YOLO的网络架构和Faster RCNN中的anchor box实现多尺度的思想的基础上，设计出了效果更好的算法。而DSSD是SSD众多改进版本中比较突出的一支，主要是牺牲了fps来换取精度。本篇文章我就不花时间去写两个模型的流程了，主要是记录一些我觉得比较好的点。References：电子文献：https://zhuanlan.zhihu.com/p/33544892https://www.cnblogs.com/edbean/p/11335139.htmlhttps://www.zhihu.com/question/58200555SSD和DSSD背景在SSD之前，目标检测主要有两种思路，一种是以YOLO为代表的基于一体化卷积网络的检测，即使用端到端的one-stage检测方法，其主要思路是均匀地在图片的不同位置进行密集抽样，抽样时可以采用不同尺度和长宽比，然后利用CNN提取特征后直接进行分类与回归，整个过程只需要一步，可参考deep-learning笔记：端到端学习；另一种思路是以RCNN系列为代表的先提取候选区域再进行分类与回归的two-stage检测方法，详见computer-vision笔记：RPN与Faster RCNN。前者的优势在速度，但是均匀的密集采样的一个重要缺点是训练比较困难，这主要是因为正样本与负样本（背景）极其不均衡，导致模型准确度稍低；后者的优势在精度，但总是无法取得平衡且更好的效果。SSD本文的SSD算法，其英文全名是Single Shot MultiBox Detector，其中Single shot指明了SSD算法属于one-stage方法，MultiBox指明了SSD使用了多框预测。上图是几种方法的基本框架对比图，对于Faster RCNN，其先通过CNN得到候选框，然后再进行分类与回归，而YOLO与SSD可以一步到位完成检测。相比YOLO，SSD采用CNN来直接进行检测，而不是像YOLO那样在全连接层之后做检测，这是SSD相比YOLO的其中一个不同点。另外还有两个重要的改变：一是SSD提取了不同尺度的特征图来做检测，大尺度特征图（较靠前的特征图）可以用来检测小物体，而小尺度特征图（较靠后的特征图）用来检测大物体（见下图）；二是SSD采用了不同尺度和长宽比的先验框（Prior boxes，Default boxes，在Faster RCNN中叫做锚，Anchors）。YOLO算法缺点是难以检测小目标，而且定位不准，而上面这几点重要的改进使得SSD在一定程度上克服这些缺点。DSSDDSSD其实就是D加SSD。其中D代表反卷积，其重要意义就是可以提升分辨率，而提升分辨率的重要效果就是小物体检测性能提升；SSD代表其使用的backbone，这部分的与SSD的结构上无较大差异，主要是将VGG换为了更深的ResNet-101来获得更高的准确度，并在其后加入数个卷积层。单纯加卷积层并不能直接提升精度，但是在这之后加入后文提到的prediction module后就能极大地提升精度。由于利用single-scale输出预测multi-scale物体在精度方面具有一定劣势，因此SSD利用各层的feature map的输出感受野各不相同的特性，用大的感受野检测大型物体，用小的感受野检测小型物体。但是若直接利用浅层的输出检测小型物体，由于浅层网络语义信息较少，所以最终表现还不是很好。于是DSSD从中得到启发，使用反卷积和skip connection扩大图像，这样除了能提升分辨率，还能保证语义信息充足。这样就形成了一个“宽-窄-宽”沙漏型的结构，其中网络的中间层用于编码输入图像的信息，然后再逐渐用更大的层来自上而下地解码在这整个图像上的图。要注意的是，这里的反卷积并不能复原原来的图像。类似的思想可以看一下我写的computer-vision笔记：图像金字塔与高斯滤波器。空洞卷积（atrous convolutions）在SSD中，作者采用了一种名为空洞卷积（atrous convolutions，又名扩张卷积（dilated convolutions））的卷积方式，向卷积层引入一个称为“扩张率（dilation rate）”的新参数，该参数定义了卷积核处理数据时选取各值之间的间距。空洞卷积的有效性基于一个假设，即紧密相邻的像素几乎相同，全部纳入会产生冗余，不如每隔H（hole size）个选取一个。在相同的计算条件下，空洞卷积提供了更大的感受野。它经常用在实时图像分割中。当网络层需要较大的感受野，但计算资源有限而无法提高卷积核数量或大小时，可以考虑使用空洞卷积。难例挖掘（hard negative mining）首先介绍一下什么是难例。根据IoU我们可以把采样获得的正负样本分成难易样本。简单负样本：与GT没有任何交集。这种样本是最多的，在图像中随意采集往往都属于这一类。简单正样本：与GT的交集远远大于阈值。也就是非常接近正确结果的。困难负样本：与GT有交集，但小于阈值。困难正样本：与GT有交集，且仅略大于阈值。后面两种就是我们所说的难例，其loss较大。若在采样时难例占比太小，那么总体的loss就不大，这对训练来说是不利的。因为本身正样本比较稀少，因此难例挖掘主要关注的是困难负样本，其基本策略是：选取所有的正样本，数量记为k个。对所有的负样本求loss，递减排序，选取前3k个。用这k个正样本，3k个负样本参与损失计算与反向传播。（SSD和DSSD都采取了这样的比例）smoothL1在使用损失函数时，我们以往一般使用的是L1损失或者L2损失，而在Faster RCNN和SSD中都使用了smoothL1来作为损失函数。为了更清楚地探究smoothL1的作用，我们将三者对x求导。可以看到，对L1，其损失函数的导数随着x的增大而增大，这就导致了在训练初期，预测值与ground truth差异过于大时，损失函数对预测值的梯度十分大，训练不稳定。对L2，其损失函数的导数为常数，这就导致了在训练后期，预测值与ground truth差异已经很小时，损失函数的导数的绝对值仍然为1，此时如果learning rate不变，那么模型参数将在稳定值附近波动，难以继续收敛以达到更高精度。一种思路是可以让学习率动态衰减，方法及实现详见deep-learning笔记：学习率衰减与批归一化。而smoothL1则从两个方面限制了梯度：（1）当预测框与ground truth差别过大时，梯度值不至于过大。也就是避免了梯度爆炸，使得模型更加健壮。（2）当预测框与ground truth差别很小时，梯度值足够小。可以说smoothL1很好地解决了L1和L2的缺陷，且比较简便。残差模块在预测模块中，DSSD也对SSD进行了改进，在feature map之后还引入了残差模块来提升性能，如下图所示。经比较可以发现，引入1个残差单元（即上方图c）的效果最好。移去批归一化在DSSD中，为了提高测试速度，作者还通过去掉BN层来提高模型的速度。根据论文中的一组公式，我们可以简单地了解一下作者的方法。实际上，作者的方法就是把批归一化进行变换拆分，从而将去掉的BN层加入到卷积层中。具体就是rewrite一个卷积层中如等式2所示的weight和如等式3所示的bias，从而就不需要等式4中相关的批归一化变量了。作者通过实验也证明了这种方法对速度的提升（尽管还是很低）。网络训练因为在Fast RCNN和Faster RCNN中没有特征或者像素重新采样阶段，所以它们依赖于数据增强（data augmentation）。主要的方法有随机剪裁（random cropping），光度失真（random photometric distortion）和随机翻转（random flipping）。在SSD中还包括了一种随机扩展（random expansion）的数据增强trick，这种trick对小检测对象比较有用。DSSD在SSD的基础上进行训练，即先引入预训练得比较好的SSD，然后冻结SSD部分训练后面的部分，最后以较小的学习率训练整个网络来进行微调（fine-tuning）。]]></content>
      <categories>
        <category>人工智能</category>
      </categories>
      <tags>
        <tag>深度学习</tag>
        <tag>计算机视觉</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[computer vision笔记：将检测问题变回归问题——YOLO]]></title>
    <url>%2Fcomputer-vision20200130224438%2F</url>
    <content type="text"><![CDATA[在许多新的算法里，涉及到bounding box时，往往会转化成回归问题。而做出这种转变的开山之作，据我所知应该是YOLO。关于YOLO，网上有很多很好的解读，我也在之前的文章中多次提到过YOLO的一些要素，这里不一一列举了，也不重复了，可以使用我网站内的搜索功能看一下相关内容。Andrew Ng在卷积神经网络的公开课上好像也特地围绕YOLO的相关内容讲了一周吧，也可以去看看。本文主要是想记录一下YOLO设计损失函数的一点trick和一些理解。注意，本文讨论的是YOLOv1。损失函数我们直接看YOLO的损失函数。可以看到，这里所有的损失都是利用ground truth和预测作差。熟悉机器学习相关知识的读者应该会知道，一般我们计算分类的损失时，一般会使用交叉熵或者逻辑回归，而YOLO的类别损失（上图最后一项）依旧使用了直接作差的方法，所以我们说，YOLO把整个检测问题作为一个回归问题来处理。暂时不关注其它损失项，我觉得这里比较巧妙的是第二项也就是宽高误差。善于观察的话肯定会发现，宽高误差在这里所有的损失项中略显突兀，即它在作差之前分别对ground truth和预测值开了根号，这是为什么呢？实际上，YOLO的一个motivation就是解决密集物体和小物体的检测（虽然还是没能很好的解决）。相比于大的Bbox产生预测误差，作者更加关注小的Bbox产生的误差。而平方误差损失（sum-squared error loss）中对同样的偏移loss值是一样的。因此这里用了一种比较巧妙的办法，就是用长和宽的平方根来代替原值，这时小的Bbox发生偏移时，其横轴上反应的loss比大的Bbox发生偏移时要大。这为我们提供了一种很好的思路，若我们对大跨度的目标更感兴趣或者说更希望较大的目标预测更精确时，我们不妨也可以使用平方或是立方来增强大尺度时的反应。其实我觉得这里的思想其实和Gamma校正是一致的。YOLO优点速度快且容易优化YOLO（You Only Look Once）只需读取一次图像就可以进行端到端的优化，这使得它的速度可以很快。它将检测问题转化为回归问题，可以满足实时性的要求。我手机上的一款APP用的就是YOLO的改进版。 背景误识别率低YOLO对全图进行卷积学习，综合考虑了全图的上下文信息，因此背景的误检测较少。泛化能力强这也是因为综合考虑了图片全局，因此能够更好地学习数据集的本质表达，从而使得泛化性能更好。YOLO缺点定位精度不够这尤其是对于小目标，因为网络较深，使得细粒度特征和小物体特征不明显。因为它是端到端的检测算法（one-stage），没有Faster RCNN那样提取候选框的操作，这也导致了它的精度不高。补充：由于one-stage和two-stage检测都存在着速度与精度平衡的问题，所以后来的SSD（也是端到端）在借鉴YOLO的网络架构和Faster RCNN中的anchor box实现多尺度的思想的基础上，设计出了更好的算法。可以看一下computer-vision笔记：SSD和DSSD。对密集目标的识别存在不足这主要是因为一个grid cell只能预测一个物体。如下图所示，当两个bicycle靠得比较近时，只预测出了一个bicycle。异常长宽比的目标识别不佳因为YOLOv1是最后直接学习框的值，这导致了它对异常长宽比的目标识别不佳。对目标个数有限制由于YOLOv1最后的输出是固定的7x7，也就是说它最多只能预测49个目标。在我前面APP的截图中可以看到标注了max为100，这说明该版本的YOLO固定输出10x10。]]></content>
      <categories>
        <category>人工智能</category>
      </categories>
      <tags>
        <tag>深度学习</tag>
        <tag>计算机视觉</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[computer vision笔记：AP和mAP]]></title>
    <url>%2Fcomputer-vision20200130130437%2F</url>
    <content type="text"><![CDATA[本文紧接前一篇文章machine-learning笔记：准确率和召回率，具体到目标检测中来谈一谈两个指标：AP和mAP。AP即average precision平均准确率，而mAP即mean average precision，翻译成平均平均准确率…不太好听，我且称它为均匀平均准确率吧。目标检测中的P和R在目标检测中，TP、FP、FN、TN有更加具体的定义，比如我们可以这样定义：TP：IoU大于或等于阈值的检测框。FP：IoU小于阈值的检测框。FN：未被检测到的（没有被标注的）ground truth。TN：由于在实验中一般不会去标注负类，因此不关心这项，可以忽略不计。关于IoU，也就是交集比上并集的比值，如果没理解可以看看computer-vision笔记：non-max suppression这篇文章。AP比如有这样一个例子，总共有7个图像，其中用绿色框表示15个ground truth，用红色框表示24个我们预测的对象，并用字母标记和给出confidence。补充：在YOLO中，confidence是这样定义的：confidence=Pr(Object)*IoU_{pred}^{truth}这里的$Pr(Object)$指的是物体先验概率，如果框内包含物体则其值为1，否则其值为0。$IoU_{pred}^{truth}$为预测框与GTbox的交并比，取值在0到1之间。因此confidence的取值范围也在0到1之间。由该公式可知confidence反应了两个信息：预测框内包含物体的置信度。预测框预测的准确度。然后我们用上面的方法判断每一个标注出来的预测对象预测得是正确的（TP）还是不正确的（FP），并根据confidence从大到小进行排列。注意，这里的Acc TP和Acc FP指的是累计的正确个数和错误个数。准确率是利用累计的正确个数比上已经统计的个数，召回率是利用累计的正确个数去比上总的正确个数（也就是15）。至于为什么这么计算，可以看看我前一篇文章的理解。在统计完所有预测红框的准确率和召回率之后，接下来我们计算AP。利用前面计算得出的数据，我们可以画出如下统计图。上面这种方法称为11点插值法，就是从召回率为0开始，以0.1为间隔计算准确率，因此共有11项。如上图右下角的公式所示，我们定义这11个点中每一个点的准确率等于大于该召回率的所有点中准确率的最大值。最后累加求平均即为该分类的AP值。这种方法的缺点是它的估算有点粗糙，因此后来作了如下改进。在2012年之后（还好没有世界末日），又出现了用面积来逼近AP值的方法。如上图中的公式所示，这里做得改变就是把插值点的精确度替换成了当前点的精确度，也就是把离散变成连续，从而可以计算面积，使得最后的平均更加有意义。mAP以上就是AP的两种计算方法，那么怎么来计算mAP呢？其实AP针对的是某一类比如说人，而数据集中往往还有其他的许多类别比如说汽车、汽车人。那么分别计算每一个类的AP再求平均就得到了mAP。注意，mAP针对的是多分类任务。mAP越高，代表精度越高。]]></content>
      <categories>
        <category>人工智能</category>
      </categories>
      <tags>
        <tag>计算机视觉</tag>
        <tag>模型评估</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[machine learning笔记：准确率和召回率]]></title>
    <url>%2Fmachine-learning20200130121842%2F</url>
    <content type="text"><![CDATA[在二分类问题中，我们常用准确率（precision）和召回率（recall）来进行评价。实际上，许多问题都可以转变为二分类问题。对于这其中的一些概念，之前都是死记硬背，今天在看目标检测的评价指标时突然有一点理解，赶快写下来。References：参考文献：[1]统计学习方法（第2版）4种情况通常我们把关注的类称为正类，其它类称为负类。而分类器在测试数据集上的预测可能是正确的也可能是不正确的。由此就出现了四种情况。TP：True Positive（真的正类），即把正类预测为正类的个数。FN：False Negative（假的负类），即把正类预测为负类的个数。FP：False Positive，即把负类预测为正类的个数。TN：True Negative，即把负类预测为负类的个数。一般不关注这一项。准确率和召回率由此我们可定义准确率为$P=\frac{TP}{TP+FP}$。定义召回率为$R=\frac{TP}{TP+FN}$。此外，还定义了$F_{1}$为准确率和召回率的调和均值，即$\frac{2}{F_{1}}=\frac{1}{P}+\frac{1}{R}$。可变换定义式求得$F_{1}$值。易得，当准确率和召回率都高时，$F_{1}$也会高。我们可以结合下图来直观地理解一下。结合上图和准确率与召回率地定义，由于一般在问题中我们只对我们预测的正类做标注，因此我觉得可以这样理解准确率和召回率：准确率：预测出的结果中，有多少是正确的。召回率：所有属于正类的目标中，有多少被正确地预测出来了。]]></content>
      <categories>
        <category>人工智能</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>模型评估</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[computer vision笔记：RPN与Faster RCNN]]></title>
    <url>%2Fcomputer-vision20200129102927%2F</url>
    <content type="text"><![CDATA[最近在看SiamRPN系列，结果看着看着就看到Faster RCNN上面去了。尽管这个模型已经有一段时间了，我还是想把通过这两天学习的理解写下来。RPN全称是Region Proposal Network，这里Region Proposal翻译为“区域选取”，也就是“提取候选框”的意思，所以RPN就是用来提取候选框的网络。在Faster RCNN这个结构中，RPN专门用来提取候选框，在RCNN和Fast RCNN等物体检测架构中，用来提取候选框的方法通常是比较传统的方法，而且比较耗时。而RPN一方面耗时较少，另一方面可以很容易结合到Fast RCNN中，成为一个整体。我们可以认为Faster RCNN所做的创新与改进就是用RPN结合Fast RCNN。它们三者都是based on regional proposal，即预先提取出候选区域，再通过CNN对候选区域进行样本分类（two-stage），这会影响它的速度，达不到YOLO那样的实时性，但从另一方面也保证了它的定位精度。References：电子文献：http://www.telesens.co/2018/03/11/object-detection-and-classification-using-r-cnns/https://zhuanlan.zhihu.com/p/31426458https://blog.csdn.net/lanran2/article/details/54376126建议在正文开始之前，推荐可以先了解一下相关的概念，比如1x1卷积、bounding box、anchor box和NMS等。我之前写过几篇相关的文章，可以先速览一下以防概念不清。Bbox和anchor：computer-vision笔记：anchor-box。非极大抑制：computer-vision笔记：non-max suppression。1x1卷积：deep-learning笔记：着眼于深度——VGG简介与pytorch实现。此外也推荐b站上的两个视频图解RCNN和FastRCNN和图解FasterRCNN，讲得挺不错的，就是声音有延迟，也可以直接看我的文章。视频中有地方讲得比较模糊，我在查阅资料之后更正了。附faster RCNN的pytorch实现。RCNN和Fast RCNN它们两者是Faster RCNN的先驱和基础，在这里简单介绍一下。首先先说明，无论是RCNN，还是Fast RCNN，还是Faster RCNN，它们的目的都是一样的，就是对一个图片，找出其中的目标物体，并用bounding box框出。RCNNRCNN即Region CNN，可以说是利用深度学习进行目标检测的开山之作，它用CNN（AlexNet）代替了之前sliding window的方法。上面是RCNN的基本结构（图源自上面推荐的视频）。首先我们通过Selective Search去生成大量的认为可能存在目标物体的小图块。然后将所有的这些图块通过预先训练得很完美的模型进行特征提取，比如AlexNet、VGG等。然后我们再对这些convNet卷积网络的输出进行两个操作：（1）Bbox回归，确定Bbox框出的目标位置即用回归的方式确定Bbox的4个参数；（2）分类，即用SVM判断Bbox标注的目标是什么物体。RCNN的缺点就是计算成本非常巨大，这里会用上千张小图块去通过一个同样的卷积网络，即进行约2000次左右的串行式前向传播，而在之后又要分别经过回归和支持向量机两个模型，也就是要重复地执行以上操作上千遍，这就严重影响了速度。此外，Selective Search去生成这些图像块的过程也是非常expensive和slow的。Fast RCNNFast RCNN主要针对RCNN的问题进行了改进。它首先直接对原始图像用卷积网络去提取特征，然后再在这张feature map上使用Selective Search。这样就使得只需要一张图像经过一遍卷积网络而不是将上千张图像去经过上千遍网络。然而Selective Search在这里还是没有得到改进，这是后面Faster RCNN使用RPN替换Selective Search的突破点。然后通过RoI Pooling Layer使各个图像块的大小统一，以方便后面的回归和分类操作，这点后面还会讲到。最后Fast RCNN还做了一项改进就是使用两个并行的层代替了原本的SVM和Bbox回归两个模型，减少了模型的复杂度和参数量。Faster RCNN基本结构 如图所示，Faster RCNN可以分为4个主要部分：conv layers作为一种CNN网络目标检测方法，Faster RCNN首先使用一组基础的conv卷积+relu激活+pooling池化提取image的feature maps。该feature maps被共享用于后续RPN层和全连接层。Region Proposal NetworkRPN网络用于生成region proposals。该层通过softmax判断anchors属于positive或者negative，再利用bounding box regression修正anchors获得精确的proposals。RoI poolingRoI即Region of Interest，RoI pooling是池化层的一种。该层收集输入的feature maps和proposals，综合这些信息后，提取proposal feature maps，送入后续全连接层判定目标类别。classifier利用proposal feature maps计算proposal的类别，即确定是什么物体。同时再次进行bounding box regression以获得检测框最终的精确位置。流程预处理首先对输入的图像进行预处理（常规操作），即减去均值并缩放成固定大小MxN。这个预处理过程对training和inference都是identical的。注意，这里的mean指的是对于整个训练集/测试集的均值而不是单张图片本身。 特征提取接收了处理后固定大小的图像后，使用一个卷积网络去提取特征，这个网络包含conv，pooling，relu三种层。以使用VGG16的Faster RCNN版本的网络结构为例，如图所示。 这里的conv layers部分共有13个conv层，13个relu层和4个pooling层。这里的参数值得注意：所有的conv层都设置kernel size为3，padding为1，stride为1；而所有的pooling层都设置kernel size为2且stride为2。这样设置有什么目的呢？可以结合下面的示意图来看，首先对所有的卷积都做了扩边处理（padding为1，即填充一圈0），使原图变为(M+2)x(N+2)的大小，然后再做3x3卷积，输出大小仍为MxN。正是这种设置，使得conv层不改变输入和输出矩阵大小。 再来看池化层，设kernel size为2且stride为2。这样每个经过pooling层的MxN矩阵，都会变为(M/2)x(N/2)大小。综上所述，在通过的整个卷积网络中，conv层和relu层不改变输入输出大小，只有pooling层使输出长宽都变为输入的1/2。那么4个pooling层就使得MxN大小的矩阵经过特征提取后固定变为(M/16)x(N/16)的feature maps。提取候选框（RPN）接下来就是最重要的Region Proposal Network。经典的检测方法生成检测框都非常耗时，如OpenCV adaboost使用滑动窗口+图像金字塔生成检测框；或如RCNN使用SS（Selective Search）方法生成检测框。而Faster RCNN则抛弃了传统的滑动窗口和SS方法，直接使用RPN生成检测框，这也是Faster RCNN的巨大优势，能极大提升检测框的生成速度。这里还是先借用一张图来明确一下Faster RCNN的流程，整个RPN网络其实相当于这里的Anchor Generation Layer和Region Proposal Layer。这里的Anchor Target Layer是用于识别出一系列与ground truth box的分数达到一定阈值的较好的foreground anchors前景（物体）锚框和低于一定阈值的background anchors背景锚框，以及其对应的regression coefficients来训练RPN网络。这里的RPN Loss就是识别标记的foreground/background labels中的正确率与predicted和target regression coefficients之间的定义距离的组合。最后的Classification Loss也与RPN Loss定义类似，也是组合了正确率和系数距离。补充：这里我想先结合上面讲一下训练的过程，也可以跳过这一块继续看RPN。上面所述的Anchor Target Layer的输出并不用于后面分类器的训练。用于后面分类器训练的是Proposal Target Layer的输出。也就是说RPN层和分类器是分开训练的，先用预训练好的模型（比如VGG、ResNet和作者论文中用的ZF）训练RPN，再把训练好的RPN放到Faster RCNN中走上面流程图中的另一条路径训练分类器也就是整个Fast RCNN网络。根据这种思路，实际的训练过程分为4步：（1）在已经预训练好的model上，第一次训练RPN网络。（2）第一次训练整个Fast RCNN网络。（3）再第二次单独训练训练RPN网络。（4）再次利用步骤3中训练好的RPN网络，收集proposals，并第二次训练Fast RCNN网络。之所以只进行了类似的“循环”两次，是因为循环更多次并没有negligible improvements。好的还是先回到RPN模块。还是参照上一节提到的使用VGG16的Faster RCNN版本的网络结构，可以看到RPN网络实际分为2条线，上面一条通过softmax分类anchors获得positive anchors（存在目标的，也就是foreground anchors）和negative anchors两类，下面一条用于计算对于anchors的bounding box regression偏移量，以获得精确的proposal。而最后的Proposal Layer则负责综合positive anchors和对应bounding box regression偏移量获取proposals，同时剔除太小和超出边界的proposals。其实整个网络到了Proposal Layer这里，就相当于完成了目标定位的功能。下面更细地讲一下这里具体是怎么做的。anchor作者是这样生成anchor box的：对输入的feature map上的每一个点（pixel），都设置9个不同尺度和形状的anchor box，如下图所示。这样通过anchor box引入了目标检测中多尺度的方法，可以看到基本上整张图片上的各种尺度和形状都被cover到了。当然，这样做获得box很不准确，不用担心，后面还有2次bounding box regression可以修正检测框的位置。补充：下面是原论文中的一张图，在这里做一些简单的解释。这里的256-d指的是之前用于提取特征的卷积网络生成的feature maps的数量，其具体维度视使用的卷积网络而定。原文中使用的是ZF model中，其最后一层conv层输出维度为256，即生成256张feature maps，也相当于得到的一个feature map中每个点都是256维的。结合前文中使用VGG16的Faster RCNN版本的网络结构，可以看到，在卷积网络提取出feature map之后，又做了3x3卷积且输出依旧是256维的，相当于每个点又融合了周围3x3的空间信息，也许这样做会是模型更鲁棒。图中的k表示的不是千，而是每个点对应的anchor的个数，这里默认是9，而每个anhcor要分positive和negative，所以每个点cls分类需要两个分数，一个是前景（物体）分数，一个是背景分数，即图中所示2k scores；而每个anchor box又需要4个偏移量来定位，所以这里reg回归为4k coordinates。在生成anchor的示意图中可以看到，显然anchors太多了，因此训练时会在合适的anchors中随机选取128个postive anchors与128个negative anchors进行训练。分类为了便于分析，我还是再把上面那张图拿下来。在经过3x3的卷积之后，又经过18个1x1的卷积核，这里的卷积主要是为了改变维数。比如我们输入一张WxH的feature map，那么经过该卷积输出就为WxHx18大小（输出图像通道数总是等于卷积核数量）。那么为什么是18呢？容易发现，18等于2（positive/negative）乘上9（anchors），也就是因为feature maps每一个点都有9个anchors，同时每个anchors又有可能是positive和negative，所以利用WxHx(9x2)大小的矩阵来保存这些信息。这里的softmax就是用于分类获得positive anchors，也就相当于初步提取了检测目标候选区域的Bbox。而softmax前后的两个reshape其实就是为了变换输入的张量以便于softmax分类（有点类似于一些网络在最后的卷积层和全连接层之间将张量拍扁成一维的），后面的reshape就是恢复原状的作用。其实RPN在这里就是在原图尺度上，设置了密密麻麻的候选anchor box。然后判断哪些是里面有目标的positive anchor，哪些是没目标的negative anchor。回归接下来我们来看看RPN模块下面那一条路径在做什么。如图所示，绿色框为事先标注的飞机的ground truth box（GT），红色框为前面提取的positive anchors，虽然红色框被分类器识别为飞机，但是由于红色框定位不准，依旧没有达到正确地检测出飞机的目标。所以我们希望采用一种方法对红色框进行微调，使得positive anchors和GT更加接近。对于一个box，我们一般使用一个四维向量$\left ( x,y,w,h \right )$来表示，即标注中心点的坐标和box的宽度和高度。在下图中，红色框A代表原始的positive anchors，绿色框G代表目标的GT，我们的目标是寻找一种关系，使得输入原始的A经过映射得到一个跟真实窗口G更接近的回归窗口G’，即寻找一种变换$F$，s.t.$F\left ( A_{x},A_{y},A_{w},A_{h} \right )=\left ( G_{x}^{‘},G_{y}^{‘},G_{w}^{‘},G_{h}^{‘} \right )$且$\left ( G_{x}^{‘},G_{y}^{‘},G_{w}^{‘},G_{h}^{‘} \right )\approx \left ( G_{x},G_{y},G_{w},G_{h} \right )$。那么这个变换$F$如何选择呢？一种简单的思路就是先做平移后做缩放，即：G_{x}^{'}=A_{w}\cdot d_{x}\left ( A \right )+A_{x}G_{y}^{'}=A_{h}\cdot d_{y}\left ( A \right )+A_{y}G_{w}^{'}=A_{w}\cdot exp\left ( d_{w}\left ( A \right ) \right )G_{h}^{'}=A_{h}\cdot exp\left ( d_{h}\left ( A \right ) \right )因此我们需要学习的是$d_{x}\left ( A \right )$，$d_{y}\left ( A \right )$，$d_{w}\left ( A \right )$，$d_{h}\left ( A \right )$这四个变换。当输入的A与GT相差较小时，可以认为这种变换是一种线性变换，那么就可以用线性回归来进行微调。注：线性回归就是给定输入的特征向量$X$，学习一组参数$W$，使得经过线性回归后的值跟真实值$Y$非常接近，即$Y=WX$。对于该问题，输入$X$是cnn feature map，定义为$\phi$；同时还有训练传入A与GT之间的变换量，即$\left ( t_{x},t_{y},t_{w},t_{h} \right )$。输出是上面所说的四个变换。则目标函数可表示为：d_{\ast }\left ( A \right )=W_{\ast }^{T}\cdot \phi \left ( A \right )为了让预测值$d_{\ast }\left ( A \right )$与真实值$t_{\ast }$差距最小，设计L1损失函数：Loss=\sum_{i}^{N}\left | t_{\ast }^{i}-W_{\ast }^{T}\cdot \phi \left ( A^{i} \right ) \right |得到优化目标为：\widehat{W}_{\ast }=argmin_{W_{\ast }}\sum_{i}^{n}\left | t_{\ast }^{i}-W_{\ast }^{T}\cdot \phi \left ( A^{i} \right ) \right |+\lambda \left \| W_{\ast } \right \|这里的$argmin$表示的是给定参数的表达式达到最小值。补充：这里positive anchor与ground truth之间的平移量$\left ( t_{x},t_{y} \right )$和尺度因子$\left ( t_{w},t_{h} \right )$定义如下：t_{x}=\frac{\left ( T_{x}-O_{x} \right )}{O_{w}}t_{y}=\frac{\left ( T_{y}-O_{y} \right )}{O_{h}}t_{w}=log\left ( \frac{T_{w}}{O_{w}} \right )t_{h}=log\left ( \frac{T_{h}}{O_{h}} \right )请结合下图理解。之所以这样定义，是为了回归系数在图片进行仿射变换之后依旧能够保持不变。有关仿射变换，可以看一下之前的文章linear-algebra笔记：二维仿射变换。Proposal LayerProposal Layer有3个输入：positive anchors分类器结果，Bbox reg的变换量以及生成的anchor。如图所示，在选择最优的多个box，然后对这些box作NMS，结果作为proposals输出。RPN小结以上就是RPN网络提取候选框的大致介绍，总结起来就是：首先，生成anchors；然后，用softmax分类器提取positvie anchors；接着，Bbox reg回归positive anchors；最后，通过Proposal Layer生成proposals。RoI pooling先来看一个问题：对于传统的CNN（如AlexNet和VGG），当网络训练好后输入的图像尺寸必须是固定值，同时网络输出也是固定大小的vector或者matrix。如果输入图像大小不定，这个问题就变得比较麻烦。有2种解决办法：从图像中crop一部分传入网络，或者将图像warp成需要的大小后传入网络。 问题是，无论采取那种办法都不是很好，要么crop后破坏了图像的完整结构，要么warp后破坏了图像原始形状信息。于是，Faster RCNN就提出了RoI pooling来解决这个问题。其步骤如下：由于proposal对应的尺度是MXN，所以首先将其映射回(M/16)X(N/16)尺度大小的feature map。再将每个proposal对应的feature map区域水平分为WxH的网格。接着对网格中的每一份都进行max pooling处理。如此就得到了WxH固定大小的输出。分类器最后的分类部分利用已经获得的proposal feature maps，通过full connect层与softmax计算每个proposal具体属于那个类别（如人，车，电视等），输出含有各个类别的概率向量；同时再次利用bounding box regression获得每个proposal的位置偏移量，用于回归更加精确的目标检测框。]]></content>
      <categories>
        <category>人工智能</category>
      </categories>
      <tags>
        <tag>深度学习</tag>
        <tag>计算机视觉</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[computer vision笔记：non-max suppression]]></title>
    <url>%2Fcomputer-vision20200128162422%2F</url>
    <content type="text"><![CDATA[Non-max Suppression即非极大值抑制，可简写为NMS，顾名思义就是抑制不是极大值，可以理解为局部最大搜索。由于在目标检测时，我们的算法可能会对同一个对象做出多次检测。我们的目标就是要去除冗余的检测框，仅保留最好的一个。这时就可以采用非极大值抑制的方法来确保算法对每个对象只检测一次。如果本文阅读时有不懂之处，可以先看一下computer-vision笔记：anchor-box。References：电子文献：https://www.cnblogs.com/makefile/p/nms.html交并比（IoU）或许在之前你已经看到过这个名词，比如computer-vision笔记：anchor-box。这里就简单介绍一下交并比。可直接根据字面意思理解，下面直接通过一张图介绍，看完就懂。非极大值抑制（NMS）非极大值抑制主要可以分为如下几步：抛弃概率很低的预测 这一步在网上大多数的文章中都没有被提及，但我认为是有必要的。因为有可能会存在着孤立的bounding box，它不会被抑制掉但它的概率很低，而它的内部的确没有框出目标，这是我们不希望的情况。因此非极大值抑制的第一步就是抛弃概率很低的预测，因为它们很有可能不包含任何目标。比如，我们可以抛弃$p_{c}&lt; 0.5$的所有box。选取概率最大的box并对其它box进行抑制 在剩余的一系列box（记为$B$）中，选取概率$p_{c}$（这里是0到1之间的一个数）最大的box，并把它作为最终要输出的一个预测，从$B$中移除。同时我们从$B$中移除和刚刚选出的box的IoU达到一定阈值的box，因为它们很有可能在标注同一个目标。比如，我们可以把IoU大于阈值0.4的box都舍去。重复直到列表为空 重复第二步操作，也就是寻找剩余$B$中的下一个概率最大的box，并把它作为输出从$B$中移除，同时用它对它周围的box进行抑制。以此类推，直到$B$中不含未处理的box，即所有的有效预测均已输出。在图片所示的例子中，我们共需要进行两次循环，最终输出两个预测：人和汽车。可用如下伪代码表示NMS的处理过程。]]></content>
      <categories>
        <category>人工智能</category>
      </categories>
      <tags>
        <tag>计算机视觉</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[computer vision笔记：anchor box]]></title>
    <url>%2Fcomputer-vision20200128162333%2F</url>
    <content type="text"><![CDATA[之前在deep-learning笔记：着眼于深度——VGG简介与pytorch实现一文中提到了anchor box这一个概念，在我阅读YOLO的论文时，也遇到了这个名词。搞懂之后觉得还是写一下比较好。bounding box的向量表示在目标检测和目标跟踪等领域中，对于目标物体的位置和大小我们往往会使用bounding box来框出表示（有时为方便也简写为Bbox，注意不是那个打节拍的口技虽然我也在学哈哈）。下面以目标检测为例，介绍一下如何用向量表示bounding box。我们一般取图片的左上角为（0，0），取图片的右下角为（1，1）。假如现在我们要对下面这个图像中的目标进行检测，检测该图中是否存在三大将即黄猿、赤犬、青雉。我们可以使用这样一个8维的向量来表示图片中红色框即输出的bounding box。y=\begin{bmatrix} p_{c}\\ b_{x}\\ b_{y}\\ b_{h}\\ b_{w}\\ c_{1}\\ c_{2}\\ c_{3} \end{bmatrix}其中$p_{c}$表示的是识别目标存在的概率，这里可以简化成1（存在目标）和0（不存在目标）。倘若$p_{c}=0$，也就是认为识别区域中不存在目标，那么向量中后面的7个参数就都是无意义项（don’t cares）。注意：因为没有进行分割，这里的识别区域是整个图像，而在实际应用中往往进行了较为精细地分割以提高检测效果。$b_{x}$和$b_{y}$指的是目标所在中心点的坐标，也就是bounding box的中点坐标。注意，这两个坐标的取值必须为0到1之间的数，且坐标系取图片的左上角为（0，0），取图片的右下角为（1，1），千万别搞错了。$b_{h}$和$b_{w}$指的是bouding box占识别区域总长、宽的比值，在这个例子中取值在0到1之间，但要注意的是，它们的取值也可以超过1，也就是物体的大小超出了识别区域（这在分割图像后可能发生，可以看一下后文图例）。这就相当于以每个识别区域为单位1，我觉得这也是为什么之前设定坐标系时取右下角为（1，1）而不是其他数值的原因之一吧。这个用于表示bounding box的向量的长度可以这样来计算：length = 5 + 待检测目标类别的总数。这时因为这里的$c_{1}c_{2}c_{3}$采用的是one-hot编码，当$p_{c}=1$时，这三个参数有且仅有一个值为1，即一个bounding box只能表示一个目标。比如bounding box中圈出的是赤犬，那么我们就可以将$c_{1}c_{2}c_{3}$表示为010。以上文中的图片为例，其bounding box的向量表示应该如下所示。y=\begin{bmatrix} 1\\ 0.35\\ 0.2\\ 0.4\\ 0.3\\ 0\\ 0\\ 1 \end{bmatrix}anchor box上文提到，我们可以将图像进行分割，以提高检测的效果。注意，这些分割是隐式的。在YOLO等algorithm中，一般有这样的规则，即物体的中心点在哪一个格子内，哪一个格子就负责检测这个物体。这就会导致一个问题。由于我们的检测方式是每一个识别区域（即分割的格子）都只输出一个向量表示其中的bounding box或者不存在目标（$p_{c}=0$），因此当检测格子分割得较少时，可能会存在两个目标物体的中点同属一个格子而只能表示出一个物体的问题。这就需要引入anchor box。比如在上图中，我们对图像进行3x3的分割，假如我们要识别汽车、人、汽车人这三类物体（也就是说要用到8维的向量），而汽车和人的中点都位于同一个格子中。这时再用之前的方法是无法同时识别人和汽车的。这时，我们可以预先定义两个不同形状的anchor box。注意：在实际应用中往往会指定更多个anchor box。此时，每个格子的输出向量就要表示成如下形式。此时向量的长度就要这样来计算：length = (5 + 待检测目标类别的总数) x anchor数。y=\begin{bmatrix} p_{c}\\ b_{x}\\ b_{y}\\ b_{h}\\ b_{w}\\ c_{1}\\ c_{2}\\ c_{3}\\ p_{c}\\ b_{x}\\ b_{y}\\ b_{h}\\ b_{w}\\ c_{1}\\ c_{2}\\ c_{3} \end{bmatrix}为了好看，我们用转置表示。y=\left [ p_{c}\;b_{x}\;b_{y}\;b_{h}\;b_{w}\;c_{1}\;c_{2}\;c_{3}\;p_{c}\;b_{x}\;b_{y}\;b_{h}\;b_{w}\;c_{1}\;c_{2}\;c_{3} \right ]^{T}这里前8个参数是和竖着的anchor box1相关联的，后8个参数是和横着的anchor box2相关联的。这样，anchor box1与人更近似，我们就用前8个参数标注人的bounding box，anchor box2与汽车更接近，我们就用后8个参数标注汽车的bounding box。此时这里的$b_{h}$和$b_{w}$指的就是anchor box之于格子的比值。因此，我们可以通过增加不同长宽比和尺寸的anchor box使得检测更加具有针对性。倘若这张图片种只有汽车，那么我们选择与汽车的IoU（交并比）最大的anchor box。比如这里我们就是将第一个$p_{c}$设为0，随后的7个参数都成了无意义项，第二个$p_{c}$设为1，用最后的7个参数标注汽车也就是anchor box2。一般情况下，我们可以通过更精细地分割图片来大大降低同一格子中同时出现两个物体的中点的可能。不过anchor box还有更多不错的效果值得借鉴，比如可以在目标跟踪中应对目标尺度大小的变化。选取关于如何选取anchor box，主要有三种方法：人为经验选取当我们知道需要预测的目标类型时，我们往往可以合理地使用人工指定的方法设置一系列anchor box。比如设置扁而宽的用于检测汽车，设置长而窄的用于检测人，设置高而大的用于检测汽车人。聚类这种方法在后期YOLO论文中有很好的使用，即利用机器学习中的k-means算法，将两类对象形状进行聚类，选择最具有代表性的一组anchor box来代表我们试图检测的一组对象类别。作为参数学习即作为参数学习。]]></content>
      <categories>
        <category>人工智能</category>
      </categories>
      <tags>
        <tag>计算机视觉</tag>
        <tag>海贼王</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[deep learning笔记：迁移学习]]></title>
    <url>%2Fdeep-learning20200128143652%2F</url>
    <content type="text"><![CDATA[迁移学习（Transfer Learning），又称预训练。即利用社区内开源的权重参数更快更好地训练自己的网络。然而，我一直纳闷的是，别人训练好的参数是怎么直接用到自己的网络上来的，倘若网络结构内部有一点不同那岂不是完全不一样了吗？Andrew Ng的课程给了我很大的启发，结合上自己的一些想法，写下来。References：电子文献：http://www.telesens.co/2018/03/11/object-detection-and-classification-using-r-cnns/#Network_Architecture原理这里摘录上面参考资料中的一段话，我觉得说得很明白了。Using a network trained on one dataset on a different problem is possible because neural networks exhibit “transfer learning”. The first few layers of the network learn to detect general features such as edges and color blobs that are good discriminating features across many different problems. The features learnt by the later layers are higher level, more problem specific features. These layers can either be removed or the weights for these layers can be fine-tuned during back-propagation.如何使用迁移学习迁移学习主要有如下三种策略。下面我们具体情况具体分析。训练数据较少首先，我们把开源的代码（即网络结构）和对应的权重都下载下来。当手头的训练集较小时，我们可以冻结所有层的参数，去掉网络中的softmax层或者其它与最后输出相联系的层，并且创建自己的softmax单元。这主要是考虑到下载的模型所对应的输出类别或者其他的需求与我们的不一致。在训练过程中，我们保持之前所有层冻结（用于特征提取等），只训练和我们自己设计的softmax层有关的参数。其实这就相当于用预训练的网络构成一个映射关系，对每一个输入都能产生一个特征向量。然后用自己设计的一个很浅的softmax网络对这些特征向量做预测。注：这些特征向量可以存到硬盘中，以节约每次都要遍历训练集重新计算这个激活值的时间。这在用Siamese网络进行人脸识别时是一个较为常用的操作。训练数据中等如果数据较多，我们可以冻结较少的层。根据上文所述的原理，我们一般冻结较为基础的前面的几层。对于后面的层，我们有两种方法。（1）可以加载权重作为初始化，然后用同样的结构和自己的数据集继续训练。（2）也可以直接去掉这几层，换成我们自己的隐藏单元和自己的输出层。其实我觉得这里的基本思想还是相当于把冻结的那几层看成一个关于特征的映射。训练数据较多若有足够多的数据用来作训练集，我们这时就可以把所有的参数都仅用来初始化，然后训练整个网络。因为此时不用担心过拟合的问题。其实规律就是：拥有越多的数据，我们需要冻结的层数（参数）越少，我们能够训练的层数（参数）就越多。除了训练数据量之外，新数据集与原数据集的相似度也是一个需要考虑的点。倘若我们拥有较多的数据，而新数据集与原数据集的相似度却很低时，我们最好不要使用迁移学习进行预训练，而是从头开始训练整个网络。为什么要迁移学习关于使用迁移学习的原因，Andrew Ng没有提及，不多简单想来主要的原因主要有如下几点：迁移学习可以弥补训练数据的不足。迁移学习可以大大减少训练时间。迁移学习（特别是同一领域相关的参数）可以有效地防止梯度下降卡在局部最优解处。]]></content>
      <categories>
        <category>人工智能</category>
      </categories>
      <tags>
        <tag>深度学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ubuntu笔记：权限管理]]></title>
    <url>%2Fubuntu20200127185146%2F</url>
    <content type="text"><![CDATA[之前在windows下面从来没想过权限的事情，而在ubuntu中这点就很受重视，可谓时时都会遇到权限管理的问题。即使如此，搞崩ubuntu的次数还是比windows要多的，不过好像心也没那么痛，可能ubuntu就是拿来玩的。虽然不是每次搞崩都是因为权限，但还是有必要理一理。References：参考文献：[1]完美应用ubuntu（第3版）权限在linux系统中，我们可以在终端使用ls -l查看目录下所有子目录和文件的权限属性。其输出结果中每一列的含义如下：第一列：文件类型和权限。第二列：i节点，即硬链接数。第三列：文件的属主，即文件的所有者。第四列：文件的属组。第五列：文件的大小。第六列：mtime，即最后一次修改时间。第七列：文件或者目录名。其实不做服务器的话没必要搞那么懂，我就讲一下我认为最重要的第一列。首先，第一个字母表示的是文件类型，主要有下面几种：-：表示普通文件。d：表示目录。l：表示链接文件。b：表示块设备文件，比如硬盘的存储设备等。c：表示字符设备文件，比如键盘。s：表示套接字文件，主要跟网络程序有关。p：表示管道文件。其次，之后的九个字母三个为一组，分别表示的是文件所有者（u）的权限、同组用户（g）的权限和其他用户（o）的权限。这里属主一般就是sudo赋权进入的那个用户，一般在个人系统中就是特权用户root。另外，可以用“a”表示all users。在每个三个字母组成的一组中，依次分别为读（r）、写（w）和执行（x）权限。若是字母，则表示可；若是“-”，则表示不可。例如“rw-”表示的是“可读可写不可执行”。为了方便，还可以用数字代表权限：用4代表读权限，用2代表写权限，用1代表执行权限。可以发现，这样的三个数字之和（0-7）可以表示任何一种权限组合。较为常用权限组合的有：7（可读可写可执行——rwx——4+2+1=7）6（可读可写不可执行——rw-——4+2+0=6）4（可读不可写不可执行——r———4+0+0=4）chmod一般通过chmod命令来修改权限，主要有两种方法。数字法这种方法最简洁，其基本格式是chmod (-R) 模式 文件名。这里的-R可以用来进行多级目录的权限设定，也就是将指定文件夹内的所有文件都修改权限。以两个较为常用的使用为例。12345sudo chmod 666 文件名#赋予所有用户读和写的权限，一般没有权限时我都会使用这个命令sudo chmod 600 文件名#赋予文件所有者读和写的权限，给group和other只读权限参数法这种方法适用于只需要改变单个用户的权限而又不想考虑或者计算别的用户的权限情况，其基本格式是chmod [u/g/o/a] [+/-/=] (rwxst) 文件名。这里先解释一下几个重要的参数和符号。u：所属用户。g：同组用户。o：其他用户。a：所有用户，相当于ugo。+：原权限基础上增加权限。-：原权限基础上减少权限。=：无论原权限是什么，最后的权限都修改为这里指定的权限。r：不解释，不懂的话没好好看前文。w：不解释，不懂的话没好好看前文。x：不解释，不懂的话没好好看前文。s：运行时可置UID。t：运行时可置GID。来看例子：12345678sudo chmod u+rw 文件名#给用户增加读写权限sudo chmod o-rwx 文件名#不允许其他用户读写执行sudo chmod g=rx 文件名#使同组用户只能读和执行]]></content>
      <categories>
        <category>操作和使用</category>
      </categories>
      <tags>
        <tag>命令操作</tag>
        <tag>ubuntu</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ubuntu笔记：移动文件的妙用]]></title>
    <url>%2Fubuntu20200127184352%2F</url>
    <content type="text"><![CDATA[有时候会遇到文件无法重命名的问题，这里介绍一种很神奇的方法，亲测有效。方法首先，在终端把目录切到要重命名的文件目录下，或者直接在对应目录中打开终端。接下来就是神奇的地方了，为了防止权限不够加个sudo赋个权。1sudo mv 原文件名 新文件名]]></content>
      <categories>
        <category>知识点与小技巧</category>
      </categories>
      <tags>
        <tag>ubuntu</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ubuntu笔记：snap软件包管理及问题]]></title>
    <url>%2Fubuntu20200127123832%2F</url>
    <content type="text"><![CDATA[在我安装ubuntu18.04LTS的时候，由于下载语言包是真的久，我就翻了一下ubuntu安装界面的介绍，其中一开始就是对snap store的介绍。这篇文章就说说snap和我之前遇到的问题。References：电子文献：https://www.jb51.net/article/128368.htmhttps://blog.csdn.net/u011870280/article/details/80213866snapsnap是ubuntu母公司Canonical于2016年4月发布ubuntu16.04时候引入的一种安全的、易于管理的、沙盒化的软件包格式，与传统的dpkg和apt有着很大的区别。在ubuntu软件中心下载安装的似乎都是snap管理的。这让一些商业闭源软件也能在linux上发布，说白了是ubuntu为了获得linux发行版霸权的一个重要举措，因此没少招黑。知乎上看到这么一句话，笑半天：“Fuck the political correct, make linux great again.(says 川·乌班图·普)”。常用命令列出已经安装的snap包1sudo snap list搜索要安装的snap包1sudo snap find &lt;text to search&gt;安装一个snap包1sudo snap install &lt;snap name&gt;更新一个snap包1sudo snap refresh &lt;snap name&gt;注：如果后面不加包的名字就更新所有的snap包。把一个包还原到以前安装的版本1sudo snap revert &lt;snap name&gt;删除一个snap包1sudo snap remove &lt;snap name&gt;查看最近的更改1snap changes终止snap进程1sudo snap abort &lt;进程序号&gt;后面两个命令将在下面的问题中发挥作用。问题当我在snap store也就是ubuntu软件中心下载pycharm和VScode时，遇到了如下报错：1snapd returned status code 409: Conflict上网查找之后，才知道这个错误码409表示的是：由于和被请求的资源的当前状态之间存在冲突，请求无法完成。即并发执行时返回的错误码。由于之前ubuntu软件中心无响应被我强制退出了，因此的确很有可能与之前进行到一半的安装冲突。于是使用snap changes查看最近的snap更改。果然看见之前的snap进程依旧在“Doing”，因此根据对应的序号使用sudo snap abort终止进程。这时再回到软件中心安装，就没有之前的报错了。然而…关于国内使用snap因为网络原因，而且也没有可用的镜像，导致snapcraft在中国大陆地区访问速度非常非常慢，下载软件需要很长的时间并且很容易中途出错。此外，由于snap软件会把主分区分成好多个loop，看起来真的不想说什么了。图源自贴吧，可以看到这挂载的snap软件包可以说是相当壮（别）观（扭）了。还有一个杀死强迫症（比如我）的问题就是，snap会在家目录（即18.04的主文件夹）中创建一个snap文件夹，里面各种快捷方式、循环嵌套的文件夹，害…无法用语言描述，看了就知道，总之就是非常不爽。主要是一些资料、文件一般也会放在家目录下面，看到了snap在那边亮眼睛真的难受。实在不知道为什么社区里有不少人推崇snap（不过国外没速度限制，snap对他们来说挺方便的）。总而言之，综合速度（硬伤）和美观舒适度考虑，还是尽量避免使用snap命令安装软件，也不要下载ubuntu软件商店中的snap格式的软件包（基本都是）。甚至有些“安装ubuntu之后必做的…件事”等诸如此类的ubuntu配置或者美化的教程内直接把卸载snap列作其中一项哈哈。总之管理软件还是apt优先，详见我的博文ubuntu笔记：apt包管理以及如何更新软件列表。]]></content>
      <categories>
        <category>操作和使用</category>
      </categories>
      <tags>
        <tag>命令操作</tag>
        <tag>ubuntu</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ubuntu笔记：安装与卸载deb软件包]]></title>
    <url>%2Fubuntu20200126083448%2F</url>
    <content type="text"><![CDATA[似乎是为了支持由武汉深之度科技开发的国产linux系统Deepin，近年来许多常用软件都提供了linux客户端，比如QQ for linux，baidunetdisk for linux。然而我安装百度网盘后发现打不开，一打开就报错，后来才知道百度网盘仅支持ubuntu18之后的版本。于是就又涉及到deb包的卸载问题了。References：电子文献：https://askubuntu.com/questions/18804/what-do-the-various-dpkg-flags-like-ii-rc-meanhttps://blog.csdn.net/sun2333/article/details/82707362dpkg flag我们可以使用dpkg -l | grep &#39;软件名&#39;来查看相应软件的安装状态，这时一般会出现有两个字母组成的一个flag。具体可以看后文中的截图。这里我想先整理一下这两个字母的含义。第一个字母：所需的状态desired package state（”selection state”）u——未知unknowni——安装installr——删除/卸载remove/deinstallp——清除（除包含配置文件）purge（remove including config files）h——保持hold第二个字母：当前包状态current package staten——未安装not-installedi——已安装installedc——仅安装配置文件config-files（only the config files are installed）U——解包unpackedF——由于某种原因配置失败half-configured（configuration failed for some reason）h——由于某种原因安装失败half-installed（installation failed for some reason）W——包正在等待来自另一个包的触发器triggers-awaited（package is waiting for a trigger from another package）t——包已被触发triggers-pending（package has been triggered）第三个字母：错误状态error state第三个字母通常情况下是一个空格，一般不会看到。R——包破损，需要重新安装reinst-required（package broken, reinstallation required）安装使用如下命令进行安装。1sudo dpkg -i package-file-name这里的-i表示的是install。注意，这里的package-file-name包括后缀如“.deb”。卸载下面这张图就是我卸载的过程。首先我使用了dpkg -l | grep &#39;软件名&#39;命令来查看我系统上百度网盘的安装状态。结果显示为“ii”，表示“installed ok installed”即它应该被安装并且已安装。随后，利用-r参数，使用下面命令进行移除。1sudo dpkg -r 软件名注意，这里的软件名不需要添加引号。移除之后，我们可以再次使用dpkg -l | grep &#39;软件名&#39;来查看百度网盘的安装状态。结果显示为“rc”，表示“removed ok config-files”即它已经被移除/卸载，但它的配置文件仍然存在。这时我们也是使用如下命令来彻底卸载软件包（包括配置文件）。1sudo dpkg -P 软件名在ubuntu笔记：释放空间一文中，有一次性清理所有残留配置文件的方法，可以看一下。]]></content>
      <categories>
        <category>操作和使用</category>
      </categories>
      <tags>
        <tag>命令操作</tag>
        <tag>ubuntu</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ubuntu笔记：安装typora]]></title>
    <url>%2Fubuntu20200123103954%2F</url>
    <content type="text"><![CDATA[今天ubuntu系统又双叒叕被我搞崩了，折腾一个大半天之后还是无解，没办法只好根据之前博文ubuntu笔记：重装ubuntu——记一段辛酸血泪史中的方法重装系统。心里还是非常庆幸还好当初留心写了一下。痛定思痛，由于之前没有系统地学习linux操作系统，鸟哥的书也就看了一部分，因此觉得自己以后应该更加谨慎小心一些，每一步命令都要看明白再执行，不然再翻车的话真的要心态爆炸的。之前在markdown笔记：markdown的基本使用中介绍过typora，这里主要是以它为例，仔细地分析一下安装软件时每一步命令的作用。References：电子文献：https://support.typora.io/Typora-on-Linux/安装过程信任软件包密匙12sudo apt-key adv --keyserver keyserver.ubuntu.com --recv-keys BA300B7755AFCFAE#optional, but recommended这条命令应该就是添加新的密匙并信任，一般在配置apt-get源之前运行。对apt-key的描述如下：“apt-key is used to manage the list of keys used by apt to authenticate packages. Packages which have been authenticated using these keys will be considered trusted.”由于每个发布的Debian软件包都是通过密钥认证的，而apt-key命令正是用来管理Debian软件包密钥的。添加软件库由于默认的软件仓库里是没有typora的，所以要添加对应的软件仓库。12sudo add-apt-repository &apos;deb https://typora.io/linux ./&apos;#add Typora&apos;s repository更新软件列表在添加了新的软件仓库之后，我们需要更新软件列表使得后面的操作能找到对应的软件包。1sudo apt-get update安装更新apt-get之后，就可以安装前面添加的库中的软件包了。12sudo apt-get install typora#install typora有软件包无法下载在install的过程中，提示我：“有几个软件包无法下载”。于是我照着提示执行了下面的命令：1sudo apt-get update --fix-missing然后再sudo apt-get install typora，就可以了。如果还是有问题的话，可能需要更换软件源，换成国内的镜像比较好。更新安装后的typora由apt-get管理，因此可以用以下命令来更新软件包。1sudo apt-get upgrade软连接痛定思痛，还是决定把这回翻车的地方写一下。本来用命令行打开matlab挺好的，我自作自受想转个matlab-support想着用图标打开，结果报错：MATLAB is selecting SOFTWARE OPENGL rendering。到网上查资料后找到一个貌似可行的方法。根据他所说，这是因为matlab的libstdc++库和系统库不匹配造成的，所以需要用如下命令建立一个连接。1ln -s /usr/lib/x86_64-linux-gnu/libstdc++.so.6.0.21 /usr/local/MATLAB/R2015b/sys/os/glnxa64/libstdc++.so.6注意，这里是R2015b。由于我下载的是R2018b，显然这里的地址是不一样的，当时比较心急直接回车了。结果还是没有解决问题。这句命令其实就是建立一个软连接，其基本格式是ln –s 源文件目录 目标文件目录。它只会在选定的位置上生成一个文件的镜像，不会占用磁盘空间，类似于windows中的快捷方式。若没加-s，就是硬链接，即会在选定的位置上生成一个和源文件大小相同的文件。不过，无论是软链接还是硬链接，文件都保持同步变化。讲道理即使目录出错也是不会有问题的，然而当我再次开机尝试进入系统时，就出现了卡在recovering journal的情况。卡住的位置仅有两行，第一行是recovering journal，第二行我在ubuntu社区里找到了一个比较类似的，如下图所示。他后面解答的方法如下。可以试一试，我也照着做下来了，但是没起作用。我也在网上看了其它的一些办法，有先进入recovery mode然后选择resume normal boot就好了的（就是返回正常启动，很玄学），然而我没用；也有check all file systems的，我也尝试了but failed。]]></content>
      <categories>
        <category>操作和使用</category>
      </categories>
      <tags>
        <tag>命令操作</tag>
        <tag>踩坑血泪</tag>
        <tag>ubuntu</tag>
        <tag>安装教程</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[deep learning笔记：端到端学习]]></title>
    <url>%2Fdeep-learning20200122164503%2F</url>
    <content type="text"><![CDATA[以前觉得深度学习就是有很多层的神经网络，或者周志华提出的深度随机森林，总之只要是有很“深”的结构就是深度学习。直到不久前一位计科大佬告诉我深度学习是end-to-end（也表示成“e2e”）的，当时听的也是一知半解，回去查了一下后终于恍然大悟。本文主要基于Andrew Ng的课程中“What is end-to-end deep learning?”和“Whether to use end-to-end learning?”两节。推荐可以去看一看，讲得可以说是很浅显易懂了。什么是end-to-end learning传统机器学习的流程往往由多个独立的模块组成，比如在一个典型的自然语言处理（Natural Language Processing）问题中，包括分词、词性标注、句法分析、语义分析等多个独立步骤，每个步骤是一个独立的任务，其结果的好坏会影响到下一步骤，从而影响整个训练的结果，这就是非端到端的。而深度学习模型在训练过程中，从输入端（输入数据）到输出端得到一个预测结果，该结果与真实结果相比较会得到一个误差，这个误差将用于模型每一层的调整（比如反向传播），这种训练直到模型收敛或达到预期的效果才结束，这就是端到端（end-to-end）的。相比传统方法每一个模块都有较为明确的输出，端到端的深度学习更像是一个神秘的整体。通俗的说，端到端的深度学习能够让“数据说话”。不过这种方法是很吃数据的，因此还不至于在每个领域都胜过甚至代替传统的机器学习方法。由于以前被标注的数据没有那么丰富，因此经典机器学习方法始终占据主流。随着近年来一个又一个数据集的出现，这种状况发生了转变。一个重要的转折点就是AlexNet的横空出世，详见deep-learning笔记：开启深度学习热潮——AlexNet。在目标跟踪领域，继相关滤波大火之后，也出现了很多优秀的深度学习算法。其中，孪生网络充分借鉴了两者的优势，取得了不错的成绩。上面是SiameseFC的主体架构，它借用了神经网络去提取特征，而不是利用一些较为经典的特征。实际上，也可以认为它是端到端的，在调整了相关滤波的形式之后，使相关滤波的操作过程可求导，从而实现了整个模型内部的前向传播和反向传播，实现端到端。那么，这样做有什么意义呢？误差理论告诉，误差传播的途径本身会导致误差的累积，多个阶段大概率会导致误差累积，而端到端的训练就能减少误差传播的途径，实现联合优化。何时该用end-to-end learning相比之下，端到端学习省去了每一步中间的数据处理和每一步模型的设计（这往往会涉及相当多的专业知识），但是端到端学习也有两个重要的缺点。缺点一：需要大量的数据Andrew Ng在视频课程中举了一个例子：百度的门禁系统可以识别靠近的人脸并放行。如果直接使用端到端学习，那么需要训练的数据集就是一系列照片或者视频，其中人会随机出现在任何位置、任何距离等等，而这样标注好的数据集是很匮乏的。但是，如果我们把这个任务拆解成两个子任务。首先，在照片或者视频中定位人脸，然后放大（使人脸居中等）；其次，对放大好的人脸再进行检验。这两种任务都有非常丰富的数据集或者方法可供使用。实际上，我觉得可以应用两个端到端的模型来解决这两个问题，但合起来就不是端到端的了。但在目前现有数据量的情况下，这依然能比直接端到端的方法表现得好。缺点二：可能排除有用的人工设计前面提到，人工设计的模块往往是基于知识的。而知识的注入有时候会大大简化模型（尤其是数据不足的时候）。这里Andrew Ng又举了一个例子：通过X光片来估计年龄。传统的方法就是照一张图片，然后分割出每一块骨头并测量长度，然后通过这些长度结合理论和统计来估计年龄。而若是使用端到端的模型，就是直接建立图片与年龄之间的联系，这显然是很难且很复杂的，训练结果的表现也可想而知。端到端学习确实在很多领域都能取得state-of-the-art的表现，但何时使用还是要具体问题具体分析。神经网络学到了什么我在前文中写了这样一句话：“相比传统方法每一个模块都有较为明确的输出，端到端的深度学习更像是一个神秘的整体”。但实际上一些研究者通过分离观察每一层，发现e2e的神经网络的确还是学到了一点东西的。一般而言，神经网络前几层学到的内容包含的信息比较丰富具体。越到后面越抽象，即越到后面包含的语义信息越多。下面是对每一个卷积核（神经元）做可视化处理，左图为靠前的某层的可视化结果，而右图为靠后的某层的可视化结果。可以看到，相较于后面的层，前几层的卷积核似乎呈现出更明确的任务或者说功能。它们通常会找一些简单的特征，比如说边缘或者颜色阴影。我们可以对第一层卷积层做特征可视化来看一下。从特征可视化结果中看，出第一层卷积提取出了不同的特征，有些突出了斑马的形状，有些突出了背景，有些突出了斑马的斑纹等。下面是Andrew Ng在课程中举得一个可视化例子，他所采用的方法是对每层中的隐藏单元用数据集去遍历，并且寻找出9个使得隐藏单元有较大的输出或是较大的激活的图片或者图像块。注意网络层数越深其感受野会越大。详见deep-learning笔记：着眼于深度——VGG简介与pytorch实现。实际上，实现可视化的方法有多种，可以利用反卷积，也可以对一系列图像求响应值，上面的两个例子就是用了不同的方法（前者是针对一张图，而Andrew Ng用了一个数据集）。在Visualizing and Understanding Convolutional Networks一文中，作者也提出了一些更复杂的方式来可视化卷积神经网络的计算。在NLP领域，也有类似的发现，比如一些训练过后的神经元对特定标点的响应特别强烈，而有一些训练过后的神经元对一些特定语气词的响应特别强烈。]]></content>
      <categories>
        <category>人工智能</category>
      </categories>
      <tags>
        <tag>深度学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[matlab笔记：MEX文件函数使用中的问题]]></title>
    <url>%2Fmatlab20200121194751%2F</url>
    <content type="text"><![CDATA[之前在matlab笔记：安装MinGW编译器一文中已经介绍过，MEX文件函数是Matlab提供的一种混合编程方式。通过MEX，用户可以在matlab中调用C、C++（没有C#，但我想提一下其实C#的真正含义是C++++，因为#其实就是四个+）或者Fortran编写的计算程序，加速matlab内部的矩阵运算（尤其是加速matlab代码中的for循环）。mex本质上是一个动态链接库文件（dll），可以被matlab动态加载并执行。然而在使用的过程中，我又碰到了许多问题。References：电子文献：https://ww2.mathworks.cn/help/matlab/call-mex-file-functions.htmlhttps://blog.csdn.net/hijack00/article/details/52228253https://jingyan.baidu.com/article/3a2f7c2ea00a9c66aed61163.html安装版本适配的MinGW编译器根据之前文章中写的配置方法，在编译MEX的时候，虽然没有问题，但是却出现了警告：使用的是不受支持的MinGW编译器版本。于是我先查看了当前使用的编译器的版本。方法如下：在MinGW-w64编译器的安装目录中，找到gcc.exe可执行文件的存在位置。打开命令行，切换到刚刚找到的gcc.exe文件所在的目录。键入gcc -v即可查看当前编译器的版本。这时我使用的是5.1.0版本，于是我又到mathworks的网站上看了一下各个matlab版本适配的编译器版本。这里我把图片截过来了，就不用去找了。我使用的是matlab R2019a，因此适配的是MinGW GCC 6.3（似乎高一点或者低一点都不行）。于是我根据它所提供的SourceForge网址去找新版本的安装包，下载解压之后是一个不含任何可执行文件（exe）的文件夹，而且是7.0.0版本的，无法自主地选择。寻找良久之后，我终于发现了一个在线安装文件，也建议下载这个，因为后面需要选择特定版本来安装。下载完成后直接双击安装，这里会有一个安装设置界面。这个要注意一下，别点过去了。版本号一定要设置成对应的，比如我是6.3.0；另外，由于安装在windows 64位系统上，所以选择x86_64以及win32；至于其它的选项可以任选，一般默认就好了。之后就是一路“下一步”，记得记住安装路径。之后就是用和matlab笔记：安装MinGW编译器中所写的相同的方式添加环境变量。可以直接把之前已有的MW_MINGW64_LOC的值替换成刚刚记下的路径，最后别忘了在matlab中setenv。这时也可以把之前的编译器删了，如果是TDM-GCC的话那很方便，直接在它的一个管理界面中uninstall就行了，另外还会剩下一个空文件夹，手动删除就行。连接外部库在我使用的过程中，我还遇到了如下ERROR: Unable to compile MEX function: “MEX 找不到使用 -l 选项指定的库 ‘ut’。上网搜了一下后，我才知道MEX命令可以用-L选项指定第三方库的路径，用-l来连接第三方库文件。值得注意的是，这里使用的是该库文件的文件名，不包含其文件扩展名。其基本格式如下：1mex -L&lt;library_path&gt; -l&lt;library&gt;可以发现，-L与&lt;library_path&gt;、-l与&lt;library&gt;之间是没有加空格的。可是看了这些，我还是解决不了我的问题。这里说一下我出现这个问题的背景，最近接触计算机视觉中的目标跟踪这一块，正在学习vot-toolkit的使用。我既问了度娘又问了谷哥，可是没有看到任何这个问题及其解决方法。于是我缩小范围，看了看Github上vot-toolkit的issues和VOT Challenge technical support的Google groups，惊喜的是的确都找到了同样的问题。然而都只有问题没有解答。无奈，还是自己想办法吧。其实跟着报错的提示来修改并不难，关键是要找到该修改哪里。由于报错提示的函数中根本没有MEX连接库文件的指令（我一行一行代码找的），于是我想能不能找到MEX的编译文件。最终我在vot-toolkit-master\utilities中找到了一个名为compile_mex.m的matlab文件，其中有这样的一串代码。12345if is_octave() arguments&#123;end+1&#125; = '-DOCTAVE';else arguments&#123;end+1&#125; = '-lut';end我用的是matlab，不是octave（后者相当于轻量级的免费matlab，语法什么的基本一致），那么执行的应该是else后面的语句，而在这里可以看到调用ut的命令。于是我就把这里的-lut改成了-Lut试了一下，果然成功了。其实用有搜索功能的IDE的话或许能够更快地解决这个问题。]]></content>
      <categories>
        <category>环境配置</category>
      </categories>
      <tags>
        <tag>安装教程</tag>
        <tag>matlab</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[computer vision笔记：相关滤波与KCF]]></title>
    <url>%2Fcomputer-vision20200120120823%2F</url>
    <content type="text"><![CDATA[相关滤波（cross-correlation）是在目标跟踪领域一种非常强大的方法，主打简洁和高速，各种基于相关滤波的算法层出不穷。其中，KCF（不是肯德基）是一个非常经典的算法，在目标跟踪领域虽说它不是最早运用相关滤波的算法（MOSSE要早于它），但是它对之后运用相关滤波进行目标跟踪的这一系列算法有重要的奠基作用。本文就最近对这些方面的了解，结合自己的思考，做一个简单的整理归纳，如有疏漏之处还请多多指教。References：电子文献：https://blog.csdn.net/fhcfhc1112/article/details/83783588https://blog.csdn.net/weixin_39467358/article/details/83304082https://www.cnblogs.com/jins-note/p/10215511.htmlhttps://blog.csdn.net/li_dongxuan/article/details/70667137?locationNum=5&amp;fps=1参考文献：[1]High-Speed Tracking with Kernelized Correlation Filters[2]Visual Object Tracking using Adaptive Correlation Filters建议由于KCF这篇文章主要是从理论上面来论述相关滤波来做tracking，其中涉及数学、理论的东西还是挺繁琐的。因此在正文开始之前，推荐可以先看一下我之前总结的几篇有关的博文，后文涉及到的话就不详细写了。循环矩阵：linear-algebra笔记：循环矩阵。HOG特征：computer-vision笔记：HOG特征。正负样本：machine-learning笔记：数据采样之正样本和负样本。闭式解：machine-learning笔记：闭式解。此外，还可以先看看b站up主桥本环关于相关滤波两个算法的讲解视频目标跟踪：相关滤波算法MOSSE实现代码讲解和目标跟踪：相关滤波算法KCF实现代码讲解，个人觉得讲得挺不错的。论文这里分别是MOSSE和KCF两个算法的论文，我在阅读时已用黄色高亮一部分重点。同时本篇文章也主要参考了这两篇paper，可以先看，也可以看完本文再看。相关滤波先来看一个公式：\left ( f\star g \right )\left ( \tau \right ):=\int_{-\infty }^{\infty }f^{\ast }\left ( t \right )g\left ( t+\tau \right )dt这里$:=$表示“定义为”（等效于等号上加个delta或者def），$\ast$表示复数共轭（complex-conjugate），而这里的$\star$表示的就是相关滤波操作。你可能会觉得这个式子非常熟悉，是的，它非常像卷积的式子。但是不同的是，一般的卷积操作为$\int_{-\infty }^{\infty }f\left ( t \right )g\left ( t-\tau \right )dt$，而在相关滤波这里是加号。这就是说，在卷积的时候，我们需要把模板先进行翻转，再进行卷积，而相关操作就不需要了。其实相关操作就是用来衡量两个信号是否相关，当两个信号越相似、相关性越强的时候，他们做相关操作输出的响应就会越强。用到目标跟踪里面，当做相关操作的两个框中的目标越相似，我们就会获得越高的响应。相关滤波的实际意义是把输入图像映射到一个理想响应图，将这个响应图当中的最高峰与目标中心点对应起来，也就是我们预测的目标接下来的位置。它一个最主要的优点就是能够借助于傅里叶变换，从而快速计算大量的候选样本的响应值。循环矩阵在论文High-Speed Tracking with Kernelized Correlation Filters的introduction部分，有这样一句话：“we argue that undersampling negatives is the main factor inhibiting performance in tracking.”也就是说，负样本的欠采样是阻碍跟踪效果的主要因素。这在之前的文章中介绍过，这里就不在细述了。这句话主要针对的问题是我们可以从一张图像中获得几乎无限的负样本。但由于跟踪的时间敏感性，我们的跟踪算法只能在尽可能多地采集样本和维持较低的计算需求之间取得一个平衡。之前通常的做法是从每一帧中随机选择几个样本。KCF的一大贡献就是采用了一种更方便的方法迅速获取更多的负样本，以便于能够训练出一个更好的分类器。作者发现，在傅里叶域中，如果我们使用特定的模型进行转换，一些学习算法实际上变得更容易（in the Fourier domain, some learning algorithms actually become easier as we add more samples, if we use a speciﬁc model for translations）。具体的做法如下，首先利用一个n维列向量来表示目标，记为$x$，然后利用$x$和一个循环位移矩阵$P$生成一个循环矩阵，其目的是使用一个base sample（正样本）和生成多个虚拟样本（负样本）来训练一个分类器。根据循环特性可以推出下面两点：可以周期性的获得同样的信号。同样的，我们可以把上面的变换等效成将base sample即生成向量正向移动一半长度和反向移动一半长度组合而成。这里是一个循环图片的示例，使用base sample，若我们向下移动15个像素，也就是从下面剪切15个像素拼到上面，就会变成左二图，若移动30个就可以生成左一图，右侧的图片是上移生成的。这就是在做tracking时循环采样的样本，一般会在目标周围取一个比目标更大的一个框，然后对大框框取的图像区域进行循环采样，那么就会生成这样一些新的样本来模拟我们的正样本并用于训练。获得了这样一个循环矩阵之后，作者接下来说：“all circulant matrices are made diagonal by the Discrete Fourier Transform(DFT), regardless of the generating vector x.”就是说循环矩阵的生成向量是完全指定的，且循环矩阵有一个非常好的性质：对任意生成向量$\widehat{x}$，我们都可以通过离散傅立叶变换（具有线性性）对循环矩阵进行对角化。X=C(x)=F\cdot diag(\widehat{x})\cdot F^{H}这里的$F$是一个与向量$\widehat{x}$无关的常数矩阵，如果这里看得不懂的话，可以参照linear-algebra笔记：循环矩阵。如此，在傅里叶域内，用离散傅里叶变换来做之后的计算，对速度会有非常大的提升。用到循环矩阵后，有两个常用的公式，可以参考我之前的文章。训练KCF做训练时所用的是岭回归。在线性情况下，岭回归的优化目标方程如下所示：f\left ( z \right )=w^{T}z\underset{w}{min}\sum_{i}\left ( f\left ( x_{i}-y_{i} \right ) \right )^{2}+\lambda \left \| w \right \|^{2}其闭式解为：w=\left ( X^{T}X+\lambda I \right )^{-1}X^{T}y此时就可以利用循环矩阵在傅里叶域计算的性质来求解了。最后求出如下式子：F\left ( w \right )=\frac{\widehat{x}}{\widehat{x}\odot \widehat{x}^{\ast }+\lambda \delta }\odot F\left ( y \right )=\frac{\widehat{x}\odot \widehat{y}}{\widehat{x}\odot \widehat{x}^{\ast }+\lambda \delta }从原来的矩阵相乘和求逆，转换到傅里叶域的点乘和点除（这里的除号是点除），一下子运算就简单了许多。在非线形的情况下，也可以得到一个同样的情况，这里需要引入一个满足条件的核，例如高斯核、线性核等，最后可计算得出一个闭式解。非线性情况下，引入核可得到一个类似的岭回归的优化目标方程：w=\sum_{i}\alpha _{i}\varphi \left ( x_{i} \right )f\left ( z \right )=w^{T}z=\sum_{i=1}^{n}\alpha _{i}\kappa \left ( z,x_{i} \right )\underset{w}{min}\sum_{i}\left ( f\left ( x_{i}-y_{i} \right ) \right )^{2}+\lambda \left \| w \right \|^{2}这里我们定义核函数$\kappa$为基向量$\varphi \left ( x \right )$之间的点积，即$\varphi^{T} \left ( x \right )\varphi \left ( x{}’ \right )=\kappa \left ( x,x{}’ \right )$。在岭回归/脊回归（Ridge Regression）中，闭式解的基本形式如下：\alpha =\left ( K+\lambda I \right )^{-1}y这里$K$表示核空间的核矩阵，由核函数得到$K_{ij}=\kappa \left ( x_{i},x_{j} \right )=\varphi \left ( X \right )\varphi \left ( X \right )^{T}$。最终可得：\widehat{\alpha }=\frac{\widehat{y}}{\widehat{k}^{xx}+\lambda }这里的除号也是点除，此时求出来的$K$和$\alpha$就可以来做tracking了。同样的，我们还是利用循环矩阵的性质并且在傅里叶域内来做计算。快速检测我们很少希望单独来评估一个图像块的回归函数$f\left ( z \right )$。为了检测感兴趣的目标对象，我们通常希望在几个图像位置上评估$f\left ( z \right )$，这几个候选块（candidate patches）可以通过循环位移来建模。定义$K^{z}$表示所有训练样本和所有候选块之间的核矩阵$K^{z}=\varphi \left ( X \right )\varphi \left ( Z \right )^{T}$。由于样本和图像块都是分别通过基础样本$x$和基础图像块$z$的循环移位组成的，因此矩阵$K^{z}$的每个元素可以表示为：$K_{i,j}=k(P^{i-1}z,P^{j-1}x)$。这里的$P$表示的是位移矩阵。易验证，$K^{z}$也是循环矩阵。可计算得到各测试样本的响应值：f\left ( z \right )=\left ( K^{z} \right )^{T}\alpha\widehat{f}\left ( z \right )=\widehat{k}^{xz}\odot \widehat{\alpha }最后，我们可以求得一张feature map，也就是一张二维的响应图。附录这里补充在原论文的appendix中提到的两个问题。余弦窗如果不加余弦窗，我们可以想象，除了那个最原始样本，其他循环生成的样本的边缘都比较突兀，也就说这些样本数据是比较差的，会干扰训练的结果。而如果加了余弦窗，由于图像边缘像素值就都接近0了，循环移位过程中只要目标保持完整那这个样本就是合理的，不过加了余弦窗也会弱化掉目标的背景信息，对训练也有一定的影响，不过总的来看应该是利大于弊。regression target y这个y是高斯加权后的值。初始目标的位置在padding后的search window的中心，循环移位得到的多个样本反应的是背景信息，而且离中心越远，就越不是目标，所以我们对标签进行高斯加权就刚好可以体现这种可能性准则。KCF里的输出是一个二维response矩阵，里面元素的大小代表该位置下的目标为预测目标的可能性，因此，在训练的时候就是输入是特征，而输出是一个gaussian_shaped_label，一般分类的标签是二值的，或者多值离散的，但是这个高斯标签反应的是由初始目标移位采样形成的若干样本距离初识样本越近可能性越大的准则，在代码中，高斯的峰值被移动到了左上角（于是四个角的值偏大），原因在论文的附录中进行了解释：“after computing a cross-correlation between two images in the Fourier domain and converting back to the spatial domain, it is the top-left element of the result that corresponds to a shift of zero”，也就是说目标零位移对应的是左上角的值。这样一来，我们在预测目标位置的时候，只需要pos=pos+find(response==max(response(:)))就好。如果把峰值放在中心点的话，就会“unnecessarily cause the detection output to be shifted by half a window”。补充：在代码中对目标进行padding是为了能让样本中含有特别需要学习的背景信息，而且可以尽量保证样本中目标的完整性，这是考虑循环移位将目标打散了。另外有关高斯加权，可以看一下我的文章computer-vision笔记：图像金字塔与高斯滤波器。效果这是KCF在OTB2013上面做的一个实验，由于当时效果比较好的是struck（所以逃不了被针对的命运）。可以看到，KCF（使用HOG特征+高斯核函数）和DCF（也是同一个作者同一篇论文提出的，使用HOG特征+线性核函数，称为对偶相关滤波器）相比于struck来说，精度取得了显著的提升，从0.656提升到了0.732/0.728。我们还可以根据这个统计图来看一下速度，即使使用了HOG特征和高斯核，KCF的速度还能达到172帧每秒。此外，用了多通道扩展的DCF取得了更快的速度，但就精度而言较KCF稍差，但也是质的飞跃了。相较而言，即使用了非常朴素的raw pixels，尽管效果比HOG特征差好多，但是速度并没有提高。这里也证明了HOG特征的强大。另外可以看到KCF的祖宗MOSSE速度非常亮眼，但这是因为MOSSE它只用了简单的灰度特征，而不是HOG这样高维的特征，可想而知精确度总体效果还是要差一大截的。缺点缺点一对尺度变化的适应性不强。解决办法是加一个尺度变化的比例系数进行多次检测，代价是牺牲一些速度。缺点二对目标快速变形（假设用的是HOG特征）或颜色快速变化（假设用的是颜色特征）不鲁棒。毕竟HOG描述的就是形状信息，变化得太快必然会导致效果变差。缺点三对物体快速运动或者低帧率视频不太鲁棒。这两种情况都是意味着在跟踪过程中下一帧图像中目标的位置偏离search window中心太远（要么靠近边缘，要么出去一半，要么全出去）。由于我们是给样本加了余弦窗的，也就是说目标位置靠近边缘会由于余弦窗的存在损失了部分目标信息，更不用说那些目标超出search window一半或者全超出去的情况了，这也就是CF类算法中的边界效应（Boundary Effets）。其他头一回看这么“理论”的论文我真的头都大了，要全部搞懂的话估计要花整整一天还不够。真的不得不佩服科研工作者们的智慧，我还是老老实实打基础吧。原文的理论性、数学性更强，本文把主要的几个核心公式整理了一下，有些许修改和添加，如有疏漏还请多多指教。]]></content>
      <categories>
        <category>人工智能</category>
      </categories>
      <tags>
        <tag>论文分享</tag>
        <tag>计算机视觉</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[computer vision笔记：图像金字塔与高斯滤波器]]></title>
    <url>%2Fcomputer-vision20200119231033%2F</url>
    <content type="text"><![CDATA[在computer-vision笔记：HOG特征一文中，我曾提及了Image Pyramid。那么，这个图像金字塔究竟是一个什么名胜古迹呢？References：电子文献：https://www.cnblogs.com/ronny/p/3886013.htmlhttps://www.cnblogs.com/wynlfd/p/9704770.htmlhttps://www.cnblogs.com/herenzhiming/articles/5276106.htmlhttps://www.jianshu.com/p/73e6ccbd8f3fhttps://baike.baidu.com/item/%E9%AB%98%E6%96%AF%E6%BB%A4%E6%B3%A2/9032353?fr=aladdinhttps://blog.csdn.net/lvquanye9483/article/details/81592574https://zhuanlan.zhihu.com/p/94014493原因当用一个机器视觉系统分析未知场景时，计算机没有办法预先知识图像中物体尺度，因此，我们需要同时考虑图像在多尺度下的描述，获知感兴趣物体的最佳尺度。在很多时候，我们会将图像构建为一系列不同尺度的图像集，在不同的尺度中去检测我们感兴趣的特征。比如：在Haar特征检测人脸的时候，因为我们并不知道图像中人脸的尺寸，所以需要生成一个不同大小的图像组成的“金字塔”，扫描其中每一幅图像来寻找可能的人脸。图像金字塔我们可以这样得到一个图像金字塔：首先，图像经过一个低通滤波器进行平滑处理（这个步骤会使图像变模糊，类似远处的物体没有近处的清晰），然后，对这个平滑处理后的图像进行抽样（一般抽样比例在水平和竖直方向上都为1/2），从而得到一系列缩小的图像。假设高斯金字塔的第$l$层图像为$G_{l}$，则有：G_{l}\left ( i,j \right )=\sum_{m=-2}^{2}\sum_{n=-2}^{2}\omega \left ( m,n \right )G_{l-1}\left ( 2i+m,2j+n \right )\left ( 1\leq l\leq N,0\leq i\leq R_{l},0\leq j\leq C_{l} \right )其中，$N$为高斯金字塔的层数，$R_{l}$和$C_{l}$分别为高斯金字塔第$l$层的行数和列数，$\omega \left ( m,n \right )$是一个二位可拆的5x5窗口函数，其表达式为：\omega =\frac{1}{256}\begin{bmatrix} 1 & 4 & 6 & 4 & 1\\ 4 & 16 & 24 & 16 & 4\\ 6 & 24 & 36 & 24 & 6\\ 4 & 16 & 24 & 16 & 4\\ 1 & 4 & 6 & 4 & 1 \end{bmatrix}=\frac{1}{16}\begin{bmatrix} 1 & 4 & 6 & 4 & 1 \end{bmatrix}\times \frac{1}{16}\begin{bmatrix} 1\\ 4\\ 6\\ 4\\ 1 \end{bmatrix}上式说明，2维窗口的卷积算子，可以写成两个方向上的1维卷积核的乘积。上面卷积形式的公式实际上完成了两个功能：（1）高斯模糊；（2）降维。按上述步骤生成的$G_{0}$，$G_{1}$，…，$G_{N}$就构成了图像的高斯金字塔，其中$G_{N}$为金字塔的底层（与原图像相同），$G_{N}$为金字塔的顶层。可见高斯金字塔的当前层图像是对其前一层图像进行高斯低通滤波、然后做隔行和隔列的降采样（去除偶数行与偶数列）生成的。其中每一层都是前一层图像大小的1/4。高斯滤波器本文讨论图像金字塔，怎么说着说着就变成高斯金字塔了呢。事实上，上面所提到的$\omega$其实是一个整数值的高斯核函数，进行了高斯滤波的平滑处理。高斯滤波这里先引入两个问题：为什么要对图像滤波？主要有两个目的：（1）消除图像在数字化过程中产生或者混入的噪声；（2）提取图片对象的特征作为图像识别的特征模式。如何理解滤波器？这与电路中的滤波器类似但又不同，在图像处理中，滤波器可以想象成一个包含加权系数的窗口，当使用滤波器去处理图像时，输出就相当于通过这个窗口去看这个图像。滤波的方式有很多种，而高斯滤波是一种线性平滑滤波，适用于消除高斯噪声。在图像处理中，高斯滤波一般有两种实现方式，一是用离散化窗口滑窗卷积，另一种通过傅里叶变换。最常见的就是第一种滑窗实现，只有当离散化的窗口非常大，用滑窗计算量非常大（尽管用了可分离滤波器依旧大）的情况下，可能会考虑基于傅里叶变化的实现方法。本文介绍的是滑窗卷积法。高斯噪声首先，噪声在图像中常表现为一引起较强视觉效果的孤立像素点或像素块。而高斯噪声，就是噪声的概率密度函数服从正态分布。高斯函数我们首先回顾一下概率论中的高斯函数。一维高斯分布：$G\left ( x \right )=\frac{1}{\sqrt{2\pi }\sigma }e^{-\frac{x^{2}}{2\sigma ^{2}}}$。二维高斯分布：$G\left ( x,y \right )=\frac{1}{\sqrt{2\pi }\sigma ^{2}}e^{-\frac{x^{2}+y^{2}}{2\sigma ^{2}}}$注：$\sigma$越大，高斯函数的高度越小，宽度越大。如上图所示，正态分布是一种钟形曲线，越接近中心，取值越大，越远离中心，取值越小。我们只需要将“中心点”作为原点，其他点按照其在正态曲线上的位置，分配权重，就可以得到一个加权平均值。这也是高斯核函数的构造原理。理论上，高斯分布在所有定义域上都有非负值，这就需要一个无限大的卷积核。实际上，仅需要取均值周围3倍标准差内的值（$3\sigma$准则），以外的部分可以直接去掉。这里取$\sigma=1.5$，注意，由于要对这9个点计算加权平均，因此必须让它们的权重之和等于1，上图是最终所得的高斯模板。高斯模糊模糊处理的过程其实就是卷积的过程，使用上面的高斯模板，对图像的每一个像素进行卷积，就能使图像产生模糊平滑的效果。上图最左边是原图9个像素点的灰度值，最右边是输出的结果。要注意的是，一次这样的操作实际上只得到了中心点的输出，即所求的加权平均结果为中心点的高斯滤波输出值。可分离滤波器如上面图像金字塔一节所述，2维窗口的卷积算子，可以写成两个方向上的1维卷积核的乘积。由于高斯函数可以写成可分离的形式，因此可以采用可分离滤波器实现来加速。所谓的可分离滤波器，就是可以把多维的卷积化成多个一维卷积。具体到二维的高斯滤波，就是指先对行做一维卷积，再对列做一维卷积。这样就可以将计算时间复杂度从$O\left ( M\ast M\ast N\ast N \right )$降到$O\left ( 2\ast M\ast M\ast N \right )$，这里的$M$和$N$分别是图像和滤波器的窗口大小。性质旋转对称性二维高斯函数具有旋转对称性，即滤波器在各个方向上的平滑程度是相同的。一般一幅图像的边缘方向事先是不知道的，因此，在滤波前是无法确定一个方向上比另一方向上需要更多的平滑。而旋转对称性意味着高斯平滑滤波器在后续边缘检测中不会偏向任一方向。平滑程度易调高斯滤波器宽度（决定着平滑程度）是由参数$\sigma$表征的，而且$\sigma$和平滑程度的关系是非常简单的。$\sigma$越大，高斯滤波器的频带就越宽，平滑程度就越好。通过调节平滑程度参数$\sigma$，可在图像特征过分模糊（过平滑）与平滑图像中由于噪声和细纹理所引起的过多的不希望突变量（欠平滑）之间取得balance。此外，高斯函数是单值函数。这表明，高斯滤波器用像素邻域的加权均值来代替该点的像素值，而每一邻域像素点权值是随该点与中心点的距离单调增减的。这一性质很重要，因为边缘是一种图像局部特征，如果平滑运算对离算子中心很远的像素点仍然有很大作用，则平滑运算会使图像失真。平滑可以层叠由性质：两个高斯核的卷积等同于另外一个不同核参数的高斯核卷积。可以推得：不同的高斯核对图像的平滑是连续的。g\left ( \mu ,\sigma _{1} \right )\ast g\left ( \mu ,\sigma _{2} \right )=g\left ( \mu ,\sqrt{\sigma _{1}^{2}+\sigma _{2}^{2}} \right )可分离性根据上文分析，由于高斯函数的可分离性，大高斯滤波器可以高效地实现。二维高斯函数卷积可以分两步来进行，首先将图像与一维高斯核进行卷积，然后将卷积结果与方向垂直且函数形式相同一维高斯核再进行卷积。如此，二维高斯滤波的计算量随滤波模板宽度$N$成线性增长而不是成平方增长。值得一提的是，在Young对生理学的研究中发现，哺乳动物的视网膜和视觉皮层的感受区域可以很好地用4阶以内的高斯微分来建模。拉普拉斯金字塔由于高斯金字塔用于图片下采样（即减小图片的尺寸），是从金字塔的底层到上层自下而上的。而高斯滤波构造的图像金字塔具有局部极值递性，即图像的特征是在减少的。那么我们自然而然会想到，需不需要一个自上而下的金字塔用于上采样，和高斯金字塔配合使用。这里就引入了拉普拉斯金字塔，它可以认为是一个残差金字塔，用来存储下采样后图片与原始图片的差异。其每一层的图像为同一层高斯金字塔的图像减去上一层的图像进行上采样并高斯模糊的结果。注意，高斯金字塔的下采样是不可逆的，可以这样理解：下采样过程丢失的信息不能通过上采样来完全恢复，即高斯金字塔中任意一张图$G_{i}$先进行下采样得到图$Down(G_{i})$，再进行上采样得到图$Up(Down(G_{i}))$，此时的$Up(Down(G_{i}))$与原本的$G_{i}$是存在差异的。而拉普拉斯金字塔的作用，就是记录高斯金字塔每一层下采样后再上采样得到的结果与下采样前的原图之间差异，其目的是为了能够完整的恢复出每一层的下采样前图像。可以用下面这个公式来简单表述：L_{i}=G_{i}-Up\left ( Down\left ( G_{i} \right ) \right )若将第$i+1$层的高斯金字塔从顶层开始依次加上第$i$层拉普拉斯金子塔，那么就几乎可以复原原来图像（有点绕，建议结合上图体会）。拉普拉斯的具体构造过程如下：内插将$G_{l}$进行内插（这里是用与降维时相同的滤波核而不是双线性插值），得到放大的图像$G_{l}^{\ast }$，使$G_{l}^{\ast }$的尺寸与$G_{l-1}$的尺寸相同，表示为：G_{l}^{\ast }\left ( i,j \right )=4\sum_{m=-2}^{2}\sum_{n=-2}^{2}\omega \left ( m,n \right )G_{l}\left ( \frac{i+m}{2},\frac{j+n}{2} \right )\left ( 1\leq l\leq N,0\leq i\leq R_{l},0\leq j\leq C_{l} \right )G_{l}\left ( \frac{i+m}{2},\frac{j+n}{2} \right )=\left\{\begin{matrix} G_{l}\left ( \frac{i+m}{2},\frac{j+n}{2} \right ),when\,\frac{i+m}{2},\frac{j+n}{2}\,are\,integers\\ 0,others \end{matrix}\right.这边的参数就不说明了，与上文相同。需要注意的是，这里的系数取$4$，是因为每次能参与加权的项的权值之和为4/256，这与$\omega$的选取有关。相减原理上文已经说了，接下来我们自上而下构造拉普拉斯金字塔。\left\{\begin{matrix} L_{l}=G_{l}-G_{l+1}^{\ast },when\,0\leq l< N\\ L_{N}=G_{N},when\,l=N \end{matrix}\right.如下图所示，此为文章前面小猫的图像金字塔所生成的拉普拉斯金字塔。不要以为这张图搞错了，细看的话就会发现，除了顶层的图片之外，下面的图片（大尺寸的）中仅有一些零散的点（不是屏幕上的灰尘）和淡淡的线，也就是残差。可以通过Gamma校正使这些残差特征更加清晰。关于Gamma校正，可以看一看上一篇文章computer-vision笔记：HOG特征，关于图像金字塔和高斯滤波器就说到这里了。]]></content>
      <categories>
        <category>人工智能</category>
      </categories>
      <tags>
        <tag>计算机视觉</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[computer vision笔记：HOG特征]]></title>
    <url>%2Fcomputer-vision20200119195116%2F</url>
    <content type="text"><![CDATA[这几天感觉知识的海洋真的是无边无际的，一旦查资料就意味着要查更多的资料，不懂的东西简直一个接着一个。希望某天能对这些知识有个大概的把握吧，废话不多说，开始积累！References：电子文献：https://www.cnblogs.com/hrlnw/archive/2013/08/06/2826651.htmlhttps://www.cnblogs.com/zyly/p/9651261.htmlhttps://baike.baidu.com/item/HOG/9738560?fr=aladdinhttps://zhuanlan.zhihu.com/p/40960756https://www.jianshu.com/p/354acdcbae3fhttps://blog.csdn.net/xjp_xujiping/article/details/89430002https://www.leiphone.com/news/201708/ZKsGd2JRKr766wEd.html参考文献：[1]Histograms of Oriented Gradients for Human Detection[2]Understanding and Diagnosing Visual Tracking Systems特征描述子特征描述子就是图像的一种表示，这种表示抽取了有用的信息，丢掉了不相关的信息。通常特征描述子会把一个WxHx3（3个channel）的图像转换成一个向量或者一个矩阵。HOG特征HOG（Histogram of Oriented Gradient），中文译为方向梯度直方图。它是一种在计算机视觉和图像处理中用来进行物体检测的特征描述子，通过计算像局部区域中不同方向上梯度的值，然后进行累积，得到代表这块区域特征的直方图，可将其输入到分类器里面进行目标检测。下面是论文中所呈现的进行人物检测时所采用的特征提取与目标检测的流程，本文将主要分析特征提取的部分（论文中对运用SVM分类器做检测的部分分析甚少，因此不做重点）。思想HOG特征的主要思想是：在图像中，局部目标的表象和形状能够被梯度或边缘的方向密度分布很好地描述。由于，梯度主要存在于边缘的地方，因此其密度分布能一定程度上描述目标物体的形状等特征。思路在介绍HOG特征提取的基本思路之前，我先放一张图，我认为先了解block和cell有助于理解思路，否则有点抽象，可能会（像我当初一样）看得很迷。下图的分割尺度与后文的分析一致，即一个block分成2x2的cell，一个cell分成8x8的pixel，可以先记下来。HOG特征提取的基本思路是：为了减少光照因素的影响，首先需要将整个图像进行归一化，这种压缩处理能够有效地降低图像局部的阴影和光照变化。将图像分成很多小的cell，采集cell中各像素点梯度的幅值和方向，然后在每个cell中统计出一个一维的梯度方向直方图。为了对光照和阴影有更好的不变性，我们可以在更大的范围内，对block进行对比度归一化。在归一化后，最终获得的即为HOG描述子。这里共做了两次normalization，不要搞混了。那么，为什么要分成很多小的cell呢？因为用特征描述子的一个主要原因是它提供了一种紧凑的压缩表示。后面我们会看到如何把一个9个bin的直方图表示成9个数的数组。此外，对每个cell统计梯度直方图不仅可以使表示压缩而紧凑，用直方图来表示一个patch还可以使其可以更加抗噪，因为一个gradient可能会有噪音，但是用直方图来表示后就不会对噪音那么敏感了。流程Normalize gamma ＆ colour由于在图像的纹理强度中，局部的表层曝光贡献的比重比较大，因此为了减少光照因素的影响，我们首先采用Gamma校正法对输入图像的颜色空间进行归一化。Gamma校正可以提高图像中偏暗或者偏亮部分的对比效果，能有效降低图像局部的阴影和光照变化。其校正公式为：f\left ( I \right )=I^{\gamma }其中$I$为图像像素值，$\gamma$为校正系数。由幂函数的性质容易推得Gamma校正的作用，这里以灰度图像（即仅有黑白，单通道）为例做一个简单解释。当$\gamma$小于1时，在低灰度值区域内，动态范围变大，图像对比度增强；在高灰度值区域，动态范围变小，图像对比度降低。图像的整体灰度值变大，如下面中间的图片（左边是原图）。当$\gamma$大于1时，在低灰度值区域内，动态范围变小，图像对比度降低；在高灰度值区域，动态范围变大，图像对比度提高。图像的整体灰度值变小，如下面右边的图片。注：灰度值也称灰度等级，范围一般从0到255，白色为255，黑色为0。然而，是否使用Gamma校正还要视具体情况而定，当涉及大量的类内颜色变化时，比如斑马等自身就颜色变化丰富的物体，不校正效果会更好。上面的例子使用的是灰度图像，事实上，RGB彩色图（三通道）的performance会更好一些。可以看一下下面的对比图。Image segmentation分割图像这一步在论文的流程图中没有，我觉得有必要说一下，因此自行添加了一步。这里先说一下为什么要进行图像分割。如果对一大幅图片直接提取特征，往往得不到好的效果。因为如果提取区域比较大，那么两个完全不同的图像，也可能提取出相似的HOG特征。但这种可能性在较小的区域就很小。此外，当我们把图像分割成很多区块（patch）然后对每个区块提取特征时，这其中也包含了几何上的位置特性。例如，正面的人脸，左上部分的图像区块提取的HOG特征一般是和眼睛的HOG特征相符合的。HOG的图像分割策略一般有overlap和non-overlap两种，如下图所示。第一张图是overlap的情况，指的是分割出的区块会有互相重合的区域。而non-overlap指的是区块不交叠，没有重合区域。 overlap的优点在于这种分割方式可以防止对一些物体的切割，例如，如果分割的时候正好把一个眼睛从中间切割分到了两个patch中，那么提取完HOG特征之后，可能会影响接下来的分类效果。但是如果两个patch之间有一个overlap，那么这里的三个patch至少有一个内会保留完整的眼睛。overlap的缺点在于计算量大，因为重叠区域的像素需要重复计算。non-overlap恰恰相反，其缺点如上所述，就是有时会将一个连续的物体切割开，得到不太好的HOG特征。但它的优点是计算量小，尤其是与图像金字塔（Image Pyramid，详见computer-vision笔记：图像金字塔与高斯滤波器）相结合时，这个优点更为明显。在这里，我们用overlap的策略将图像分割成一个个互相重叠的block。也就是说，每个cell的直方图都会被多次用于最终的特征描述子的计算。虽然这看起来有冗余，但可以显著地提升性能。 Compute gradients接下来就是计算图像每个像素点梯度。G_{x}\left ( x,y \right )=I\left ( x+1,y \right )-I\left ( x-1,y \right )$G_{y}\left ( x,y \right )$算法同理。在具体实现时，我们可以使用如下两种kernel来实现上面梯度的计算。 接着计算梯度的幅值和方向。G\left ( x,y \right )=\sqrt{G_{x}\left ( x,y \right )^{2}+G_{y}\left ( x,y \right )^{2}}\alpha =\arctan \frac{G_{y}\left ( x,y \right )}{G_{x}\left ( x,y \right )} 可以看到，图像的梯度去掉了很多不必要的信息（比如不变的背景色），并且加重了轮廓。Weighted vote into spatial ＆ orientation cells将图像划分成小的cell（矩形或者环形），然后统计每一个cell的梯度直方图，即可以得到一个cell的描述符。注意，这里我们在一个block内划分4个cell，即2×2个cell组成一个block，8x8个像素点组成一个cell。将一个block内每个cell的描述符串联起来即可得到一个block的HOG描述符。我们先来看一下各个像素点的梯度的具体计算情况。 统计梯度直方图的方式是利用刚才计算得出的cell中每一个像素点的梯度进行加权投票。我们一般考虑采用9个bin的直方图来统计一个cell中像素的梯度信息，即将cell的梯度方向0~180°（无向）或0~360°（考虑正负）分成9个bin，如下图所示： 如果cell中某一像素的梯度方向是20~40°，那么上面这个直方图第2个bin的计数就要加1，这样对cell中的每一个像素用梯度方向在直方图中进行加权投影（权值大小等于梯度幅值），将其映射到对应角度范围的bin内，就可以得到这个cell的梯度方向直方图了，即该cell对应的一个9维特征向量。若梯度方向位于相邻bin的交界处（如20°、40°等），需要对其进行方向和位置上的双线性插值。 上图是一个比较直观的加权投票的演示，这里将投影在交界处的梯度的幅值对半分至两侧的bin。要注意的是，如果一个角度大于160度，也就是在160至180度之间，我们知道这里角度0和180度是一样的，所以在下面这个例子里，像素的角度为165度时，就要把幅值按照比例放到0和160的bin里面去，也就是上面说的双线性插值。 事实上，我们可以采用幅值本身或者它的函数（幅值的平方根、幅值的平方、幅值的截断形式）来表示权值，但经实际测试表明：使用幅值来表示权值能获得最佳的效果。采用梯度幅值作为权重，可以使那些比较明显的边缘的方向信息对特征表达影响增大，这样比较合理，因为HOG特征主要就是依靠这些边缘纹理。经实验还发现，采用无向的梯度（即0~180°）和9个bin的直方图，能在行人检测试验中取得最佳的效果。Contrast normalize over overlapping spatial blocks由于局部光照的变化，以及前景背景对比度的变化，使得梯度强度的变化范围非常大，这就需要对梯度做局部对比度归一化。归一化能够进一步对光照、阴影、边缘进行压缩，使得特征向量对光照、阴影和边缘变化具有鲁棒性。我们之前已将2x2的cell组成了更大的block，现在要做的就是针对每个block进行对比度归一化。通常使用的HOG结构大致有三种：矩形HOG（R-HOG），圆形HOG（C-HOG）和中心环绕HOG。它们的单位都是block。实验证明，矩形HOG和圆形HOG的检测效果基本一致，而环绕形HOG效果相对差一些。 我们一般根据如下公式对block进行对比度归一化：v=\frac{v}{\sqrt{\left \| v \right \|_{2}^{2}+\xi ^{2}}}这里$v$是该block未经归一化的特征向量，比如这里一个block含2x2个cell，每个cell对应一个9维的特征向量，那么这个block的特征向量的长度就是2x2x9。这里$\xi$是一个很小的数，主要是为了防止分母等于零而引入的。Collect HOG’s over detection window我们先review一下，上面我们首先把样本图片分割为若干个像素的cell，然后把梯度方向划分为9个区间，在每个cell里面对所有像素的梯度方向在各个区间进行直方图统计，得到一个9维的特征向量；然后我们使每相邻4个cell构成一个block，把一个block内的特征向量串联起来得到一个36（即2x2x9）维的特征向量。接下来，我们用block对样本图像进行扫描，stride为一个cell的大小，扫描完成后，我们将扫描过程中每个block的特征向量（即上述36维的）串联起来，得到样本的特征向量，也就是可以输入到后面分类器的HOG特征描述子。如此，就完成了HOG特征的提取。我们可以可视化一下最后的HOG特征，可见主要捕捉的是博尔特的躯干和腿。 总结优点HOG特性能较好地捕捉局部形状信息，对几何和光学变化都有很好的不变性。HOG特征是在密集采样的图像块中求取的，在计算得到的HOG特征向量中隐含了该块与检测窗口之间的几何空间位置关系。不足很难处理遮挡问题，人体姿势动作幅度过大或物体方向改变时不易检测（这个问题后来在DPM中采用可变形部件模型的方法得到了改善）。没有选取主方向，也没有旋转梯度方向直方图，因而本身不具有旋转不变性（较大的方向变化），其旋转不变性是通过采用不同旋转方向的训练样本来实现的。本身不具有尺度不变性，其尺度不变性是通过缩放检测窗口图像的大小（如Image Pyramid）来实现的。由于梯度的性质，HOG对噪点相当敏感，在实际应用中，在block和cell划分之后，对于得到各个区域，有时候还会做一次高斯平滑去除噪点。但又由于HOG特征是基于边缘的，平滑操作会降低边缘信息的对比度，从而减少图像中的有用信息，因此还需好好做个balance。计算机视觉女神或许会发现本文中好多的图例都用了同一位女士的脸，这里就扯点题外话。照片中的女子名为Lena Soderberg，对计算机视觉领域有一定的接触的朋友应该对这张照片不会陌生。这张照片是标准的数字图像处理用例，各种算法研究经常会使用这张图作为模板。那为什么要用这幅图呢？David C. Munson在“A Note on Lena”中给出了两条理由：首先，该图像中各个频段的能量都很丰富，既有低频（光滑的皮肤），也有高频（帽子上的羽毛），适度地混合了细节、平滑区域、阴影和纹理，很适合来测试各种图像处理算法。其次，Lena是位迷人的女子，能有效吸引研究者（大部分为男性）做研究。该照片其实是一张于1972年11月出版的Playboy的中间插页。1973年，南加州大学信号图像处理研究所的副教授Alexander和学生一起，为了一个同事的学会论文正忙于寻找一幅好的图片。他们想要一幅具有良好动态范围的人的面部图片用于扫描。这时，不知是谁拿着一本Playboy走进研究室。由于当时实验室里使用的扫描仪（Muirhead wirephoto scanner）分辨率是100行每英寸，试验也仅仅需要一副512X512的图片，所以他们只将图片顶端开始的5.12英寸扫描下来，并切掉肩膀以下的部分。多年以来，由于图像Lena源于Playboy，将其引用于科技文章中饱受争议。Playboy杂志也将未授权的引用告上法庭。但随着时间的流逝，人们渐渐淡忘Lena的来源，Playboy也放松了对此的关注。值得一提的是，Lena也是Playboy发行的最畅销的海报，已经出售7,161,561份。其真正刊登的原图如下。1997年，在图像科学和技术协会的第50届会议上，Lenna被邀为贵宾出席。在会议上，她忙于签名、拍照以及介绍自我。说实话每次看到这张照片，想起学术界那些大牛们居然能弄出这样一个挺有意思的故事，不免觉得也挺有趣的。在我最喜欢的美剧之一《Silicon Valley》中，也有计算机视觉女神的身影。大家对计算机视觉有兴趣的话，其实自己也打印一张摆墙上，这样就可以和懂的小伙伴们一起讨论问题了。]]></content>
      <categories>
        <category>人工智能</category>
      </categories>
      <tags>
        <tag>计算机视觉</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[machine learning笔记：闭式解]]></title>
    <url>%2Fmachine-learning20200119145637%2F</url>
    <content type="text"><![CDATA[看论文总是会看到许多新奇的名词，比如“one-shot learning”（单样本学习）即仅从一个或者很少的样本中学习训练。还有诸如“closed-form”、“closed-form solution”，看的我真是很纳闷，这里就结合资料好好理理清楚。References：电子文献：https://blog.csdn.net/yy16808/article/details/76493384https://blog.csdn.net/langjueyun2010/article/details/80348449解析解与数值解这里先介绍两个相对应的数学概念：解析解与数值解。直接上例子：解$x^{2}=3$。那么这题的解析解是：$x=\sqrt{3}$。数值解为：$x=1.732$。简而言之，解析解就是给出解的具体函数形式，从解的表达式中就可以算出任何对应值，好像就是小学所谓的公式解；而数值解就是直接用数值方法求出具体的解。闭式解实际上，闭式解也被称为解析解。由于解析解为一封闭形式（closed-form）的函数，因此对任一独立变量，我们皆可将其带入解析函数求得正确的相依变量。即解可以表达为一个函数形式，带入变量即可得到解。这就写完了，似乎很简单。]]></content>
      <categories>
        <category>知识点与小技巧</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[computer vision笔记：Peak-to-Sidelobe Ratio应用于目标跟踪]]></title>
    <url>%2Fcomputer-vision20200118213942%2F</url>
    <content type="text"><![CDATA[在Visual Object Tracking using Adaptive Correlation Filters一文中，我看到这样一句话：“The Peak-to-Sidelobe Ratio(PSR), which measures the strength of a correlation peak, can be used to detect occlusions or tracking failure, to stop the online update, and to reacquire the track if the object reappears with a similar appearance.”其大意为：用来衡量相关峰值强度的Peak-to-Sidelobe Ratio可以用于检测遮挡或者跟踪失误、停止实时更新和在相似外观再次出现时重新获取跟踪路径。我读完在想：这个PSR是什么？这么有用。结果百度了半天翻了几页没发现有任何有关它介绍或者解释的文章。于是看了一些英文文献，将自己的一些浅见写下来。References：电子文献：https://baike.baidu.com/item/pslr/19735601参考文献：[1]Understanding and Diagnosing Visual Tracking Systems[2]Visual Object Tracking using Adaptive Correlation Filters[3]Adaptive Model Update via Fusing Peak-to-sidelobe Ratio and Mean Frame Difference for Visual TrackingWhat由于没有找到任何中文翻译，我只能取一个相近的（感觉指的差不多）。在百度百科中，我找到了如下解释：PSLR，峰值旁瓣比，peak side lobe ratio，定义为主瓣峰值强度对于最强旁瓣的峰值强度之比，多用于雷达信号脉冲压缩后对信号的评估。注意：百度百科内的名词与我遇到的相差了一个“to”，且缩写也不同。上面是百度百科内的一个配图，根据百科解释，最高的主瓣与第二高的主瓣之差，即是PLSR（峰值旁瓣比），在这里大小为-13.4dB。此外，我还找到了一个称为峰均比（PAPR，peak-to-average power ratio）的概念，它等于波形的振幅和其有效值（RMS）之比，主要是针对功率的，这里就不细说了。后来，我在一篇光学期刊的文章上找到了一个比较可靠的翻译，该文献中有一个名为“峰旁比”的名词，且文献内容与目标追踪相关。因此我暂且称其为峰旁比吧。个人觉得比峰值旁瓣比简洁且好听。Why其实中文翻译并不重要，重要的是它的作用。在查阅文献的过程中，我看到了这样一个公式：P_{sr}^{t}=\frac{max(f_{t})-\mu _{t}}{\sigma _{t}}其中$P_{sr}^{t}$是此时第t帧的峰旁比，$f_{t}$是对于第t帧分类器预测的响应，$\mu _{t}$和$\sigma _{t}$分别是响应图$f$的均值和方差。根据这种计算方法，我们可以大概分析一下峰旁比的作用。当初看到时，我觉得它与我在物理实验中做过的音叉共振实验中的品质因数有相似之处（品质因数$Q=\frac{f_{0}}{f_{2}-f_{1}}$）。由公式可知，当响应中的峰值较高，且响应分布相对而言集中在峰值及周围时，峰旁比就会较高；反之，峰旁比就会较低。那么什么时候会造成峰旁比较低呢？根据论文描述可以获得提示，当遇到遮挡，或者目标跟丢即响应区内不含目标主体时，就不会出现一个那么明显的峰值响应，同时响应也会较为分散了，此时分母较大、分子较小，峰旁比就会变低。下面也是Visual Object Tracking using Adaptive Correlation Filters中的一张figure（这篇文章提出了MOSSE），我们需要的是a much stronger peak which translates into less drift and fewer dropped tracks。看了这个想必应该知道峰旁比发挥的作用和大致原因了。和上面的峰值旁瓣比、峰均比相比较，显然峰旁比的定义能更好地表征响应的集中程度。How由峰旁比定义所得出的性质可知，峰旁比可以作为模型预测定位准确性和可信度的判据。我们可以利用峰旁比来调整Model Updater（The model updater controls the strategy and frequency of updating the observation model. It has to strike a balance between model adaptation and drift.）我的想法是，我们可以设置一个threshold，当峰旁比小于这个threshold时，表示模型未能准确定位到目标（可信度较低），这时我们可以停止模型的更新，或者通过减小学习率等方法减慢模型的更新速度以防止模型受背景或者遮挡物较大影响，而当目标再次出现时难以复原导致最终完全跟丢的问题；而当峰旁比大于这个threshold时，我们可以实时更新模型，或者运用较大的学习率（也可以根据峰旁比将学习率划分成几个等级）。]]></content>
      <categories>
        <category>人工智能</category>
      </categories>
      <tags>
        <tag>计算机视觉</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[machine learning笔记：数据采样之正样本和负样本]]></title>
    <url>%2Fmachine-learning20200118112156%2F</url>
    <content type="text"><![CDATA[今天在看最小二乘回归（least squares regression）时看到作者把positive examples设成1，把negative examples设成0。感觉对这个概念既熟悉又陌生，查了一下之后一下子想起来了。在机器学习中，数据预处理一般包括数据清洗、数据集成、数据采样。而正负样本涉及到了数据采样的问题，因此后面也提一下。正样本和负样本简单来说，和概率论中类似，一般我们看一个问题时，只关注一个事件（希望它发生或者成功，并对其进行分析计算），而正样本就是属于我们关注的这一类别的样本，负样本就是指不属于该类别的样本。数据采样平衡一般来说，比如我们训练分类器时，希望样本中正负样本的比例是接近于1:1的。因为如果正样本占比很大（比如90%）或者负样本占比远超正样本，那么训练结果可想而知，获得的分类器在测试中的效果会很差。针对这种数据不平衡的问题，有以下三种solution：过采样（over-sampling）这是一种较为直接的办法，即通过随机复制少数类来增加其中的实例数量，从而可增加样本中少数类的代表性。欠采样（under-sampling）这种方法也比较直接，即通过随机消除占多数类的样本来平衡类分布，直到多数类和少数类实现平衡。获取更多样本上面的两种方法比较直接方便，但也存在弊端，比如过采样可能会导致过拟合，欠采样可能无法很好地利用有限的数据（这也可能会造成过拟合）。因此最好还是获取更多的样本来补充，我认为主要有下面两种方法：采集例如在海贼王漫画的样本中，我们要进行20x20大小的海贼检测，那么为了获取尽可能多的负样本，我们可以截取一张1000x1000大小的海王类图像，将其拆分为20x20大小的片段加入到负样本中（即50x50地进行分割）。生成为了获得更多负样本，我们也可将前面1000x1000的海王类图像先拆分为10x10大小，这就比之前多出了4倍的负样本图像。不过要注意的是，为了保持大小的一致，还需进一步将其拉伸至20x20的大小。 当然，其实不需要从体积上达到这么大的比例，关键是像素尺寸的匹配。]]></content>
      <categories>
        <category>人工智能</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>海贼王</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ubuntu笔记：删除文件中的问题]]></title>
    <url>%2Fubuntu20200117213946%2F</url>
    <content type="text"><![CDATA[大家可以看看我删个文件多么曲折：献丑了哈哈哈，这里就对这个过程中涉及到的一些问题做一个总结吧。目录首先我cd /进入到根目录，然后我每一步ls列出目录中的文件及子目录，一步一个脚印找到了我要删的文件——MATLAB，emmm我不想解释为什么是它。然后我想当然的想remove掉这个文件，结果发现权限不够。这里其实可以ls -l以列表的形式查看目录中的文件及子目录并且列出每个文件拥有者、所属组、其他用户各自的权限的。后面我又使用cd ../来回到上一级目录，这是为了怕自己搞错目录，怕删高了一级酿成惨剧。权限它说我没权限，于是就sudo临时给个5分钟的root权限呗。本来还想sudo su进入root的（可以用ctrl+D退出），那简直杀鸡用牛刀了。删除文件/目录一开始用rm，它提示我是一个目录，于是我使用了rmdir，但它的作用是删除一个空目录，而我的目录内还有文件。于是我使用sudo rm folder_name -R即递归删除文件的方法来从里到外把这个目录中的文件都删了。其实好像也可以sudo rm -rf folder_name强制删除，这里-r和-R一样，都是递归的意思，-f就是强制执行无需确认。但是由于牢记linux最大禁忌rm -rf /*（真正的从删库到跑路），对这个命令还是比较怕的，于是就采取了前者。执行完之后再ls看了一下，发现已成功删除，df查看空间分配，内存使用也回来了不少。补充：-R递归也有许多别的妙用，比如可以通过sudo chmod a+rw file_name -R来一次性修改一个文件夹内所有文件的权限。]]></content>
      <categories>
        <category>操作和使用</category>
      </categories>
      <tags>
        <tag>命令操作</tag>
        <tag>ubuntu</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ubuntu笔记：apt包管理以及如何更新软件列表]]></title>
    <url>%2Fubuntu20200117094401%2F</url>
    <content type="text"><![CDATA[ubuntu有时在用户登录后会提示有软件包更新，每次更新之后按提示重启，你就会看到一个类似于安全模式下大写的GNU GRUB（一个多操作系统启动程序），虽然这没什么问题，但是我在想能不能自主地去更新呢？References：电子文献：https://birdteam.net/122231https://blog.csdn.net/a3192048/article/details/86618314apt-get这个有点类似于windows中的dism命令，可以用于安装、更新、卸载软件，大部分操作需要root权限，因此使用命令时别忘了授权。首先介绍一下它的常见用法：安装使用如下命令安装名为xxx的软件：1sudo apt-get install xxx卸载使用如下命令卸载名为xxx的软件：1sudo apt-get remove xxx注意：切忌卸载关键的软件包，比如coreutils。更新本文重点来了，apt-get相关升级更新命令有下面这四个：1234567891011sudo apt-get update#更新软件源缓存，从服务器更新可用的软件列表，一般在安装软件时引入新的软件仓库之后使用sudo apt-get upgrade#更新系统，即根据列表更新已安装的软件包，既不会删除在列表中已经不存在了的软件，也不会安装有依赖需求但尚未安装的软件sudo apt-get full-upgrade#根据列表更新已安装的软件包，可能会为了解决软件包冲突而删除一些已安装的软件sudo apt-get dist-upgrade#更新系统版本，也是根据列表更新已安装的软件包，可能会为了解决软件包冲突而删除一些已安装的软件，不同于full-upgrade的dist-upgrade也可能会为了解决软件包依赖问题安装新的软件包更新软件列表当我们想自主更新软件包时，可以依次执行下面两条命令：12sudo apt-get upgradesudo apt-get dist-upgrade //谨慎执行这两条命令其实比较类似，不同的是当相依性问题时，upgrade时此package就不会被升级而保留下来；而dist-upgrade相对“智能”，若遇到相依性问题，需要安装或者移除新的package时，dist-upgrade命令就会试着去安装或者移除它，这就可能以牺牲某些非重要软件包为代价来升级某些非常重要的软件包，个人认为存在一定风险。apt在根据各类教程安装各个软件时，我开始注意到有时候apt-get的位置被apt代替了。随着使用量的增加，这个疑惑越来越大，因此我决定搞搞清楚。其实，apt命令是在ubuntu16.04发布时引入的。它具有更精减但足够的命令选项，而且具有更为有效的参数选项的组织方式。实际上，虽然不是一个东西，但完完全全可以认为apt和apt-get是等价的，其格式语法几乎完全统一，在使用时不会出现不同。目前apt命令还在不断地发展，而apt-get比apt有更多、更细化的操作功能，有时对于一些低级操作，仍需使用apt-get。下表是apt命令与apt-get等命令的对比，可以看到在普通使用时是完全一样的。apt命令等效命令功能apt installapt-get install安装软件包apt removeapt-get remove移除软件包apt purgeapt-get purge移除软件包及配置文件apt updateapt-get update更新软件列表apt upgradeapt-get upgrade升级所有可升级的软件包apt autoremoveapt-get autoremove自动删除不需要的包apt full-upgradeapt-get full-upgrade在升级软件包时自动处理依赖关系apt searchapt-get search搜索应用程序apt showapt-get show显示软件包信息此外，apt还有一些自己的命令，比如apt list列出包含条件的包（已安装，可升级等）；apt edit-sources编辑源列表。]]></content>
      <categories>
        <category>操作和使用</category>
      </categories>
      <tags>
        <tag>命令操作</tag>
        <tag>ubuntu</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ubuntu笔记：双系统下时间差问题的解决]]></title>
    <url>%2Fubuntu20200117085337%2F</url>
    <content type="text"><![CDATA[第一次在电脑加装ubuntu双系统后，就存在ubuntu比windows系统时间慢8个小时的问题。当时搞了一会好像也解决了。然而，在我重装了ubuntu系统之后（详见ubuntu笔记：重装ubuntu——记一段辛酸血泪史），这个问题又出现了。一时间得不到很好地解决，也就没管。最近强迫症犯了，花了点功夫终于搞定了，决定记录在此。References：电子文献：http://doc.ntp.org/4.1.1/ntpdate.htmhttp://doc.ntp.org/4.1.1/ntpd.htmhttps://blog.csdn.net/vic_qxz/article/details/80344855windows、ubuntu系统时间在windows中，系统时间的设置较为简单。而且设置后，系统时间会自动保存在bios的时钟里面，当启动计算机时，系统会自动在bios里面读取硬件时间，以保证时间不间断。但在ubuntu linux默认情况下，系统时间和硬件时间，并不会自动同步。在ubuntu linux运行过程中，系统时间和硬件时间以异步的方式运行，互不干扰。硬件时间是靠bios电池来维持运行的，而系统时间是用CPU tick来维持的。在系统开机时，会自动从bios中取得硬件时间，设置为系统时间。这样一来就不奇怪了，中国的时区是东八区（GMT+8），因此ubuntu每次读入的是格林威治标准时间并直接将其设置为系统时间，而windows则会加上8:00调整。因此我解决的思路如下：考虑到windows下时间调整更方便，我就优先调整ubuntu的系统时间，将其系统时间（即本电脑的硬件时间）设为GMT+8，然后再在windows系统中取消自动添加8小时的自动调整，即让两个系统以同样的方法从硬件时间设置系统时间。ntpd这里先介绍一下ntpd（Network Time Protocol (NTP) daemon），如官方文档所说，它的作用是sets and maintains the system time of day in synchronism with Internet standard time servers。因此，我们可以通过ntpdate命令进行设置，其基本格式如下：1ntpdate [ -bBdoqsuv ] [ -a key ] [ -e authdelay ] [ -k keyfile ] [ -o version ] [ -p samples ] [ -t timeout ] server [ ... ]这里我们不需要用到上面的这些额外选项，因此不一一介绍了。solution安装如果还没有安装ntpdate的话，可以先执行该条命令。1sudo apt-get install ntpdate注意：apt-get大部分操作都需要root权限，别忘了sudo赋予权限。我有一回忘记sudo了结果搞了半天不知所以…真的太蠢了。从服务器校准时间这里我使用的时间服务器是time.windows.com，好像也可以用苹果的time.apple.com或者阿里云的time.pool.aliyun.com。1sudo ntpdate time.windows.com格式参考上文。在执行之后发现有0.005秒的微小偏差，因此感觉还是比较可靠的。把时间同步到硬件上同步系统时间和硬件时间，可以使用hwclock命令。1sudo hwclock --localtime --systohc这里的sysyohc即系统时间（sys）写到（to）硬件时间（hard clock）。这时ubuntu这边已经解决了，但如果重启打开windows，会发现时间快了8小时，原因之前解释过，因为自动加了8小时，所以还要作下面的调整。调整windows打开windows，调整日期/时间，把时区改到：(UTC)协调世界时。如此一来，windows上的系统时间也是硬件时间了。双系统的系统时间设置方式一致，时间准确，大功告成。注：如果windows中时间没有问题，那就无需调整时区。总之就是先设置好ubuntu的，然后再在windows里调整，因为windows下更好调整。似乎windows会自动更正系统时间，所以经以上4步操作后过段时间需要把时区调整回来。]]></content>
      <categories>
        <category>操作和使用</category>
      </categories>
      <tags>
        <tag>命令操作</tag>
        <tag>ubuntu</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[linear algebra笔记：循环矩阵]]></title>
    <url>%2Flinear-algebra20200116095725%2F</url>
    <content type="text"><![CDATA[循环矩阵是我在看相关滤波时遇到的一个terminology，通过一定的了解之后发现其具有许多有用的性质。在目标跟踪领域，循环矩阵的引入对速度的提升是非常大的。关于相关滤波，由于现在了解还不够全面和深入，暂时不提及。本文主要就循环矩阵的概念和性质做一个总结。References：电子文献：https://www.cnblogs.com/cj-xxz/p/10323711.htmlhttps://blog.csdn.net/shenxiaolu1984/article/details/50884830参考文献：[1]High-Speed Tracking with Kernelized Correlation Filters循环矩阵（Circulant Matrices）任意循环矩阵可以被傅里叶变换矩阵对角化。（All circulant matrices are made diagonal by the Discrete Fourier Transform (DFT), regardless of the generating vector x.）我们在文献中往往会看到这样一个变换：X=C(x)=F\cdot diag(\widehat{x})\cdot F^{H}下面的$X$它就是一个循环矩阵，它是由它的第一行$x=(x_{1},x_{2},…,x_{n})$的向量组每次经过一个循环位移，得到的一个循环矩阵。其中$\widehat{x}$（读作x hat）为原向量$x$的傅里叶变换；$F$是傅里叶变换矩阵，$F^{H}$表示共轭转置。换句话说，循环矩阵$X$相似于对角阵，其特征值是$\widehat{x}$的元素。以长度为3的$x$为例，其生成的循环矩阵为：X=C(x)=\begin{bmatrix} x_{1} & x_{2} & x_{3}\\ x_{3} & x_{1} & x_{2}\\ x_{2} & x_{3} & x_{1} \end{bmatrix}这样的一个矩阵它有一个特别好的性质，就是能够通过它的第一行的生成向量来做来进行对角化。通过这个式子，我们能够把$X$循环矩阵进行对角化，如此把它转换到傅里叶域之后，用离散傅里叶变化来做运算时，对速度的提升是非常大的，这将在后文进一步说明。关于这里的对角化、傅里叶变换矩阵可以看后文，在此先跳过。这里有必要列出循环矩阵的两个重要公式，这两个性质是比较常用和有用的：卷积循环矩阵乘向量等价于生成向量的逆序和该向量卷积，可进一步转化为傅里叶变换相乘。 这里$\overline{x}$表示$x$的逆序排列，$*$表示共轭。注意：卷积本身也包含逆序操作。此外，这里最后一个等号利用了信号与系统中的“时域卷积，频域相乘”，即时域卷积定理，它表明两信号在时域的卷积积分对应于在频域中该两信号的傅里叶变换的乘积。相乘循环矩阵的乘积仍是循环矩阵，所以我们只要维护循环矩阵的第一行，就可以以较低的复杂度维护循环矩阵的乘积。 公式中最终所得的乘积也是循环矩阵，其生成向量是原生成向量对位相乘的傅里叶逆变换。用了上述循环矩阵的性质之后，我们就可以使得原来两个矩阵相乘的时间复杂度$O(K^{3})$能够降到$O(Klog(K))$（反向傅里叶的复杂度（$O(Klog(K))$）加上向量点乘的复杂度（$K$）），速度的提升是非常明显的。注：这里K表示的是矩阵的尺寸。在非线形的情况下，当引入了核之后，也可以得到同样的一个情况。此时需要这个核满足一定的条件，它是可以具备循环矩阵的一些性质的，例如常用的高斯核、线性核都满足这个条件，因此可以直接拿来用。傅里叶变换矩阵（DFT matrix）关于离散傅里叶矩阵$F$这里涉及较多的数学，想看详细推导可以参考文首给出的第二个参考链接。这里把比较关键的结论部分截了过来。$F$在这里是一个奇异矩阵（方阵且行列式等于零），它可以对任意输入向量进行傅里叶变换，这是因为傅里叶变换具有线性性。矩阵快速幂在本文前的第一个参考链接中，作者是用矩阵快速幂引入的，那么这里我也简单谈一下快速幂。顾名思义，快速幂就是快速算某个数的多少次幂。我们知道，对于任何一个整数，都能用二进制来表示。那么对于$a^{n}$，$n$也一定可以用二进制来表示。那么问题来了，如何计算某个数较大的次幂呢？比如计算$a^{156}$，我们可以利用除二取余、倒序排列、高位补零的方法得到$(156)_{10}=(10011100)_{2}$。如此可以推导：Ans=a^{(156)_{10}}=a^{(10011100)_{2}}=a^{2^{7}*1+2^{6}*0+2^{5}*0+2^{4}*1+2^{3}*1+2^{2}*1+2^{1}*0+2^{0}*0}=(a^{2^{7}})*(a^{2^{4}})*(a^{2^{3}})*(a^{2^{2}})这样一来，原本要进行$156-1=155$次乘法运算，现在运算量级相当于该幂的二进制数表示中1的个数。其时间复杂度为$O(log_{2}n)$，与朴素的$O(n)$相比，效率有了极大地提高。以上就是一般的快速幂的基本套路。相对于一般的快速幂，矩阵快速幂仅仅是把他的底数和乘数换成了矩阵形式。其主要方法就是：通过把数放到矩阵的不同位置，然后把普通递推式构造成类似于“矩阵的等比数列”，最后快速幂求解递推式。矩阵快速幂主要用于求一个很复杂的递推式中的某一项问题。递推矩阵（关系矩阵）的构造，也是矩阵快速幂的难点，一般是由原始的递推公式推导或者配凑得出，网上有许多ACM的赛题解答，可以看几道理解一下思路。]]></content>
      <categories>
        <category>知识点与小技巧</category>
      </categories>
      <tags>
        <tag>线性代数</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[linear algebra笔记：二维仿射变换]]></title>
    <url>%2Flinear-algebra20200116084728%2F</url>
    <content type="text"><![CDATA[之前在deep-learning笔记：学习率衰减与批归一化一文中，我已经对仿射变换作了简单的介绍。但这里我想提出来单独对其做一个小归纳。应用仿射变换在计算机科学中有丰富的运用。例如，在计算机图形学中，它可以用于在较小或较大的屏幕上显示图形内容时简单地重新缩放图形内容。此外，它也可以应用于扭曲一个图像到另一个图像平面。另一个重要的应用是训练深层神经网络时用于扩充数据集。训练深度模型需要大量的数据。在几乎所有的情况下，模型都受益于更高的泛化性能，因为有更多的训练图像。人工生成更多数据的一种方法就是对输入数据随机应用仿射变换（数据增强）。仿射变换二维仿射变换可以用下面这个公式来表示：x'=Ax其中$A$是在齐次坐标系中的3x3矩阵，$x$是在齐次坐标系中$(x，y，1)$形式的向量。这个公式表示$A$将一个任意向量$x$映射到另一个向量$x’$。一般来说，仿射变换有6个自由度。根据参数的值，它将在矩阵乘法后扭曲任何图像。变换后的图像保留了原始图像中的平行直线（考虑剪切）。本质上，满足这两个条件的任何变换都是仿射的。它保持了二维图形的“平直性”、“平行性”和“共线比例不变性”，非共线的三对对应点可以确定一个唯一的仿射变换。下面一些特殊形式的$A$，如下图所示，从上到下分别是：缩放、平移和旋转。上述仿射变换的一个非常有用的性质是它们是线性函数。它们保留了乘法和加法运算，并遵循叠加原理。换言之，我们可以组合2个或更多的变换：向量加法表示平移，矩阵乘法表示线性映射，只要我们用齐次坐标表示它们。即利用这个性质，我们可以将二维仿射变换视为线性变换R和平移变换T的叠加，具体可以看一下之前的文章。举个例子，我们可以将旋转和平移如下表示：123A = array([[cos(angle), -sin(angle), tx], [sin(angle), cos(angle), ty], [0, 0, 1]])如果理解了的话，你会发现各种各样的变化其实还挺繁琐的。不过请放心，大多数开发人员和研究人员通常省去了编写所有这些变换的麻烦，而只需依赖优化的库来执行任务。在OpenCV中进行仿射变换非常简单，如果今后遇到的比较多再做整理。分享之前在artificial-intelligence笔记：吴恩达——阅读论文的建议一文中提到以后会分享几个觉得有价值的公众号的，这里就再分享一个吧。本文中的图片均来自该公众号的推文。]]></content>
      <categories>
        <category>知识点与小技巧</category>
      </categories>
      <tags>
        <tag>线性代数</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[matlab笔记：安装MinGW编译器]]></title>
    <url>%2Fmatlab20200115222641%2F</url>
    <content type="text"><![CDATA[因为目标追踪领域最著名的比赛VOT（Visual Object Tracking），同时也拥有一个非常重要的数据集和一套比较权威的评价指标，基于的是matlab，因此我又开始用起了matlab（这么看下来貌似matlab要成我本学期用的最多的语言了）。当我download官方的toolkit之后，按着document一路比较顺利地操作了下来，结果突然遇到一个报错，说我没有C和C++编译器。WTF？我用了这么久居然都没发现这个问题…本以为按照提示就能很快解决，结果这个问题折腾了我一整个晚上。好吧既然被折磨得这么惨那我还是本着逢血泪必写博的原则在这里写一下吧。不过不得不说，最后成功的时候真的还是挺爽的哈哈！References：电子文献：https://ww2.mathworks.cn/help/matlab/matlab_external/compiling-c-mex-files-with-mingw.html?requestedDomain=uk.mathworks.comhttps://ww2.mathworks.cn/matlabcentral/fileexchange/52848-matlab-support-for-mingw-w64-c-c-compilerhttps://www.cnblogs.com/Vae1990Silence/p/10102375.htmlhttps://blog.csdn.net/fly910905/article/details/86222946MinGWMinGW，是Minimalist GNU for Windows的缩写。它是一个可自由使用和自由发布的Windows特定头文件和使用GNU工具集导入库的集合，允许你在GNU/Linux和Windows平台生成本地的Windows程序而不需要第三方C运行时（C Runtime）库。当初报错的时候，我也是很诧异，因为之前使用CodeBlocks和Visual Studio的时候明明是有的。而这次在matlab中编译C/C++时怎么就找不到了。因为之前使用CodeBlocks也有找不到的情况，我当时是重装了一遍CodeBlocks解决问题的，因此我上网开了一下有没有类似的方法。按道理来说已经装了VS是可以找到的，但好像也存在即使安装了VS、matlab还是找不到编译器的情况。可以使用mex看一下具体是哪些路径没有匹配上，似乎可以通过修改注册表的方法解决，但我没有尝试。注：matlab调用C/C++的方式主要有两种：利用MEX技术和调用C/C++动态连接库。MEX是Matlab Executable的缩写，它是一种“可在matlab中调用的C（或Fortran）语言衍生程序”。后文中还会用到。失败的方法毕竟是花了一个晚上，看了mathworks上网友们的各种solution，试错了许多方法，这里记录两个貌似要成功的方法（其实最后还是失败了），或许会有参考价值。这里有一个被许多网友强调、要注意的是：下载后的mingw文件（没错它只有15kb看上去好假）要在打开的matlab中，找到相应的下载目录，右键点击然后选择下载并安装（download and install），否则似乎会出错。禁用IPv6IPv6，顾名思义，就是IP地址的第6版协议。我们现在用的是IPv4，它的地址是32位，总数有43亿个左右，还要减去内网专用的192、170地址段，这样一来就更少了。然而，IPv6的地址是128位的，大概是43亿的4次方，地址极为丰富。网友Kshitij Mall给出了这样一个解决思路：首先打开控制面板中的编辑系统环境变量。在高级选项中，点击环境变量。在系统变量栏中添加如下两个变量：（1）variable name：“JAVA_TOOL_OPTIONS”；value：“-Djava.net.preferIPv4Stack=true”；（2）variable name：“JAVA_OPTIONS”；value：“-Djava.net.preferIPv4Stack=true”。另外根据该网友所述，安装完成后可以删去这两个环境变量。注意：仅输入引号内部的内容。java文档指示，设置jvm属性java.net.preferIPv4Stack为true时，就可以禁用IPv6。反之，若设为0，则启用。注：禁用设置时不需要重启系统。由于该网友的系统配置和我相同（MATLAB 2019a on Windows 10 system with 64 bits），于是我毫不犹豫地首先尝试了他的方法，不知道是卡进去的原因还是为何，我成功看到了协议界面（原本一直卡在附加功能管理器加载的空白界面），但是最终开始下载支持包失败。关闭防火墙失败之后，我继续看评论，看到一个网友感谢另一个网友提供的solution，我非常激动，感觉自己也要跟着解决了。似乎好像取得了一定的进展，但是在下载第三方包的时候还是卡住了（3rt party download error），我浏览了大多数网友的评论，貌似大家基本上都卡在了这里。成功的方法当我快要绝望的时候，突然看到了评论区感谢三连，都是感谢同一个人的。这已经是2018年的一个回复了，看评论发现他们使用的是Windows 7系统，但我决定还是试一下，结果真的成功了，非常感谢最初的solution提供者pawan singh！具体方法如下：到这个网址下载合适的TDM-GCC。下载之后，create一个新的到设定的安装路径中。注意：根据matlab文档（文首第一个参考链接）。MinGW的安装文件夹名称不能包含空格。例如，不要使用：C:\Program Files\mingw-64。应改用：C:\mingw-64。我建议直接装在C盘下面，默认似乎也是这样，维持不变即可。与之前修改系统变量方式类似。添加新的系统变量名为MW_MINGW64_LOC，值为MinGW-w64编译器的安装位置，于我是C:\TDM-GCC-64。最后别忘了确定设置。在matlab命令行内执行命令：setenv(&#39;MW_MINGW64_LOC&#39;, &#39;path&#39;)，folder为TDM-GCC的安装位置，要加单引号。例如我是：setenv(&#39;MW_MINGW64_LOC&#39;, &#39;C:\TDM-GCC-64&#39;)。可以继续在命令行中执行命令：mex -setup。若没有报错，则表明成功了。然而英语老师说，however后面的往往是重点，那么我这里就however一下。上面的方法问题是没有，但是使用的时候有可能会收到警告：使用的是不受支持的MinGW编译器版本。如果没有收到这个警告，那么就万事大吉，如果有的话，能运行的话依旧还是万事大吉。But如果真的因此而运行出错，或者看着warning心里实在不舒服的话，可以看一下我后来写的matlab笔记：MEX文件函数使用中的问题。]]></content>
      <categories>
        <category>环境配置</category>
      </categories>
      <tags>
        <tag>安装教程</tag>
        <tag>matlab</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[python笔记：pandas基本使用]]></title>
    <url>%2Fpython20200113202851%2F</url>
    <content type="text"><![CDATA[在python中，Pandas可以说是最实用的库之一，它提供了非常丰富的数据读写方法。可以看一下Pandas中文网提供的Pandas参考文档中对所有I/O函数的总结。Pandas是一个开源的，BSD许可的库，为python提供高性能、易于使用的数据结构和数据分析工具。它的使用基础是Numpy（提供高性能的矩阵运算）；可以用于数据挖掘和数据分析，同时也提供数据清洗的功能。本文就对Pandas的基本使用做一个简单的归纳，所有代码可以从上往下按顺序依次执行。References：电子文献：https://www.cnblogs.com/chenhuabin/p/11477076.htmlhttps://blog.csdn.net/weixin_39791387/article/details/81487549https://kanoki.org/2019/09/16/dataframe-visualization-with-pandas-plot/https://pandas.pydata.org/pandas-docs/stable/getting_started/10min.htmlhttps://blog.csdn.net/weixin_41712499/article/details/82719987csv这里我想介绍一下一种新的数据格式：csv。它和excel很像，但又不同于excel。csv主要有如下特点：纯文本，使用某个字符集，比如ASCII、Unicode、EBCDIC或GB2312（简体中文环境）等；由记录组成（典型的是每行一条记录）；每条记录被分隔符（英语：Delimiter）分隔为字段（英语：Field (computer science)）（典型分隔符有逗号、分号或制表符；有时分隔符可以包括可选的空格）；每条记录都有同样的字段序列。在Pandas的使用以及AI相关竞赛数据集、结果的存储与使用中，csv文件往往承担着主角的位置。准备在具体使用之前，别忘了先导入所需相应的库。12import pandas as pdimport numpy as np可以使用pd.__version__来输出版本号，注意，这里的“__”是两个“_”，这个很容易搞错且难以发现。两大利器Pandas中文网首页，在介绍完Pandas之后，就重点介绍了一下Pandas的两大利器。分别是DataFrame和Series。这里我先介绍一下Seires，DataFrame在后面有更详细的操作。Series简介Series是一种类似于一维数组的对象，是由一组数据（各种NumPy数据类型）以及一组与之相关的数据标签（即索引）组成。仅由一组数据也可产生简单的Series对象。我们可以通过传入一个list的数值来创建一个Series，Pandas会创建一个默认的整数索引：12345678910s = pd.Series([1, 3, 5, np.nan, 6, 8])s&gt;&gt;&gt; 0 1.0 1 3.0 2 5.0 3 NaN 4 6.0 5 8.0 dtype: float64注：这里用np.nan来产生NaN，但要注意的是np.nan不是一个“空”对象，即使用np.nan == np.nan来判断将返回False，np.nan的类型为基本数据类型float。若要对某个值进行空值判断，如对np.nan，需要用np.isnan(np.nan)，此时返回为True。另外，也可以从字典创建Series。DataFrame简介DataFrame是Pandas中的一个表格型的数据结构，包含有一组有序的列，每列可以是不同的值类型（数值、字符串、布尔型等），DataFrame即有行索引也有列索引，可以被看做是由Series组成的字典。我们可以通过传入一个numpy数组来创建一个DataFrame，如下面带有一个datetime的索引以及被标注的列：1234567891011121314151617dates = pd.date_range('20130101', periods = 6)dates&gt;&gt;&gt; DatetimeIndex(['2013-01-01', '2013-01-02', '2013-01-03', '2013-01-04', '2013-01-05', '2013-01-06'], dtype = 'datetime64[ns]', freq = 'D')df1 = pd.DataFrame(np.random.randn(6, 4), index = dates, columns = list('ABCD'))df1&gt;&gt;&gt; A B C D2013-01-01 0.469112 -0.282863 -1.509059 -1.1356322013-01-02 1.212112 -0.173215 0.119209 -1.0442362013-01-03 -0.861849 -2.104569 -0.494929 1.0718042013-01-04 0.721555 -0.706771 -1.039575 0.2718602013-01-05 -0.424972 0.567020 0.276232 -1.0874012013-01-06 -0.673690 0.113648 -1.478427 0.524988注：上面用pd.data_range()生成了一个时间频率freq = &#39;D&#39;（即天）的日期序列。我们也可以通过传入一个可以转换为类Series（series-like）的字典对象来创建一个DataFrame：12345678910111213df2 = pd.DataFrame(&#123;'A': 1., 'B': pd.Timestamp('20130102'), 'C': pd.Series(1, index = list(range(4)), dtype = 'float32'), 'D': np.array([3] * 4, dtype = 'int32'), 'E': pd.Categorical(["test", "train", "test", "train"]), 'F': 'foo'&#125;)df2&gt;&gt;&gt; A B C D E F0 1.0 2013-01-02 1.0 3 test foo1 1.0 2013-01-02 1.0 3 train foo2 1.0 2013-01-02 1.0 3 test foo3 1.0 2013-01-02 1.0 3 train foo这里可以使用df2.dtypes来查看不同列的数据类型。读取无论是txt文件还是csv文件，在Pandas中都使用read_csv()读取，当然也使用同一个方法写入到文件，那就是to_csv()方法。1df = pd.read_csv(abs_path) #此为绝对路径为了提供更加多样化、可定制的功能，read_csv()方法定义了数十个参数，还在大部分参数并不常用，以下是几个比较常用的参数：filepath_or_buffer：文件所在路径，可以是一个描述路径的字符串、pathlib.Path对象、http或ftp的连接，也可以是任何可调用read()的对象。这是唯一一个必传的参数，也就是上面的abs_path。encoding：编码，字符型，通常为utf-8，如果中文读取不正常，可以将encoding设为gbk。当然，也可以直接将对应文件改成utf-8编码。header：整数或者由整数组成的列表，用来指定由哪一行或者哪几行作为列名，默认为header = 0，表示用第一列作为列名。若设置header = 0，则指定第二列作为列名。要注意的是，当指定第一行之后的数据作为列名时，前面的所有行都会被略过。也可以传递一个包含多个整数的列表给header，这样每一列就会有多个列名。如果中间某一行没有指定，那么该行会被略过。例如header = [0, 2]，则原本的第二行会被省去。而当文件中没有列名一行数据时，可以传递header = None，表示不从文件数据中指定行作为列名，这时Pandas会自动生成从零开始的序列作为列名。names：接着上面的header，很快就想到是不是可以自己设置列名。names就可以用来生成一个列表，为数据额外指定列名。例如：df = pd.read_csv(&#39;abs_path, names=[&#39;第一列&#39;, &#39;第二列&#39;, &#39;第三列&#39;, &#39;第四列&#39;])。在数据读取完毕之后，我们可以使用如下代码来快速查看数据是否正确地导入了。1234df.head() #看一下导入后df（DataFrame）的前几行，可在括号内输入数字来设定具体显示几行，默认5行df.tail() #类似，查看后几行type(df) #查看类型，DataFrame的输出应该是pandas.core.frame.DataFrameDataFrameDataFrame的介绍在前面的简介已经写过，这里就不赘述了。事实上，Pandas中的DataFrame的操作，有很大一部分跟numpy中的二维数组的操作是近似的。在上面的读取处理之后，我们下面对其进行一些简单的操作：查看123456789101112131415161718df.head() #上文已提及df.tail()#查看列名print(df.columns)#查看索引print(df.index)#查看各列的数据格式print(df.dtypes)#查看整个DataFrame的属性信息print(df.info())#访问对应行df.loc[0] #这里访问了第一行，将显示列名和对应每一列第一行的数据#具体有关索引请看后文筛选在numpy中，我们可以这样判断一个数组中每一个数和对应数值的比较结果：12a = np.array(range(10))a &gt; 3输出将是一串布尔型（True、False）的array。而在DataFrame中，我们可以用类似的方法通过指定列来进行筛选：12#筛选第二列中数值大于80df[df.第二列 &gt; 80]这样就会得到只用符合条件数据的对应行的一个DataFrame。我们也可以使用df[(df.第一列 &gt; 80) &amp; (df.第二列 &gt; 80) &amp; (df.第三列 &gt; 80)]来进行多条件的复杂筛选。此外，我们可以直接根据列名提取出一个新的表格：1new = df[['第一列', '第二列']] #new为仅由第一列和第二列组成的一个新的DataFrame排序可以使用如下代码根据单列或者多列的值对数据进行排序：12df.sort_values(['第二列', '第一列', '第三列'], ascending = [True, True, True])#使用df.sort_values(['第二列', '第一列', '第三列']).head()查看排序完后前几行的结果这里排序的规则是：根据设置的顺序（这里是先按第二列排），从小到大升序对所有数据进行排序。其中ascending是设置升序（默认True）和降序（False）。若仅选择单列，则无需添加[]，这里[]的作用是把选择的行列转换为列表。重命名如果觉得我前面取得列名称不好听，可以使用下面这个代码来改成需要的名字：1df.rename(columns = &#123;'第一列': '好听的第一列', '第二列': '好听的第二列', '第三列': '好听的第三列', '第四列': '好听的第四列',&#125;, inplace = True)这里用到了字典。索引前面提到了使用索引来查看第一行，可当没有数字索引，例如我们通过df = pd.DataFrame(scores, index = [&#39;one&#39;, &#39;two&#39;, &#39;three&#39;])把index设为one、two、three时，df.loc[0]就失效了。因此有下面几种处理方法：123456789#访问index为“one”的行df.loc['one']#访问实实在在所谓的第几行（无论index为何）df.iloc[0] #注意0指的是第一行#ix合并了loc和iloc的功能，当索引为数字索引的时候，ix和loc是等价的df.ix[0] #访问第一行df.ix['one'] #访问“one”行，这里也指的是第一行切片类似的，DataFrame也支持切片操作，但还是需要注意的。这里总结两种切片方式：利用索引即使用df.loc[:2]ordf.ix[:2]等索引方式，这里这样的话输出为前三行。直接切片这种方法只能在访问多行数据时使用，例如df[:2]将输出前两行，注意，这里比上面的方法要少一行。此外，值得强调的是，用这种方法访问单行数据是禁止的，例如不能使用df[0]来访问第一行数据。插入上面的索引还有一种用途，就是可以用于插入指定index的新行。1df.loc['new_index'] = ['one', 'piece', 'is', 'true']删除上面插入的那行中我说了“大秘宝是真实存在的”（海贼迷懂），下面我想把这句话所在的行删了，可以使用df.dtop()来完成。1df = df.drop('new_index')数组我们可以使用df.第一列.values以array的形式输出指定列的所用值。基于此，我们可以使用df.第一列.value_counts()来做简单的统计，也就是对该列中每一个出现数字作频次的统计。我们还可以直接对DataFrame做计算，例如：df * n（n为具体数值），结果就是对表中的每一个数值都乘上对应的倍数。元素操作map函数map()是python自带的方法, 可以对DataFrame某列内的元素进行操作。下面是一种使用实例：1234567891011def func(grade):if grade &gt;= 80: return "A"elif grade &gt;= 70: return "B"elif grade &gt;= 60: return "C"else: return "D"df['评级'] = df.第一列.map(func)这样DataFrame后面会自动添加一列名为“评级”，并根据第一列来生成数据填入。apply函数当我们需要进行根据多列生成新的一个列的操作时，就需要用到apply。其用法简单示例如下：1df['求和'] = df.apply(lambda x: x.第一列 + x.第二列, axis = 1)applymap函数applymap时对dataframe中所有的数据进行操作的一个函数，非常重要。例如，我要让之前所用的score和grade都变成scroe+或者grade+，那么我就可以这样：1df.applymap(lambda x: str(x) + '+')如果是成绩单的话，那么这样操作之后打出来就会好看些啦，哈哈。plot数据可视化本来是一个非常复杂的过程，但Pandas数据帧plot函数的出现，使得创建可视化图形变得很容易。这个函数的具体使用可以访问文首给出的第三个参考链接，为一个印度小哥利用kaggle上的数据对df.plot()做的一个非常详尽的介绍。有机会的话我会结合matplotlib对其做一个搬运与总结，先留个坑。统计我们可以使用df.describe()对数据进行快速统计汇总。输出将包括：count、mean、std、min、25%、50%、75%和max。通过df.mean()我们可以按列求均值，如果想要按行求均值，可以使用df.mean(1)。高阶此外还有使用df.T进行转置，df.dropna(how = &#39;any&#39;)删除所有具有缺失值的数据，df.fillna(value = 5)填充所有缺失数据等高阶用法。详细的可以查阅前面给的参考链接10 minutes to pandas，至于更高阶的，可以看一下cookbook，不过一般还是在运用的过程中遇到需求再查找，一下子记不住那么多的。写入通过to_csv()可以将Pandas数据写入到文本文件中，和读取read_csv()类似，它也有几个常用参数：path_or_buf：表示路径的字符串或者文件句柄，也是必需的。例如：df.to_csv(abs_path)。要注意的是，这里如果abs_path对应的文件不存在，则会新建abs_path的同名文件后再写入，如果本来已存在该文件，则会自动清空该文件后再写入。sep：分隔符，默认为逗号。当写入txt文件时，就需要这个参数来确定数据之间的分隔符了。header：元素为字符串的列表或布尔型数据。当为列表时表示重新指定列名，当为布尔型时，表示是否写入列名。这和读取时的使用基本类似。columns：后接一个列表，用于重新指定写入文件中列的顺序。例如：df.to_csv(abs_path, columns = [&#39;第四列&#39;, &#39;第二列&#39;, &#39;第三列&#39;, &#39;第一列&#39;])。index_label：字符串或布尔型变量，设置索引列列名。原本的索引是空的，使用这个参数就可以给索引添加一个列名。如果觉得不需要添加，同时空着不好看（空的话还是会有分隔符），可以设置为False去掉（同时也将不显示分隔符）。index：布尔型，是否写入索引列，默认为True。encoding：写入时所用的编码，默认是utf-8。这个和上述的许多参数其实保持默认即可。匿名函数在上面DataFrame一节的最后，我用到了两个匿名函数，这里我想举个例子来简单展示一下匿名函数的使用方法，的确很好用！当我们对一个数进行操作时，若使用函数，一般会：12def func(number): return number + 10这样看上去就有点费代码了，因此有下面的等价匿名函数可以替代：1func = lambda number: number + 10当然假如想追求代码行数的话也不拦着你~推荐最近看到了DataWhale的一篇文章，也总结的挺好，在这里推荐一下。]]></content>
      <categories>
        <category>程序与设计</category>
      </categories>
      <tags>
        <tag>代码实现</tag>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[deep learning笔记：记首次ResNet实战]]></title>
    <url>%2Fdeep-learning20200113174731%2F</url>
    <content type="text"><![CDATA[实践出真知。在之前的博文deep-learning笔记：使网络能够更深——ResNet简介与pytorch实现中，我对何凯明大神的CVPR最佳论文中提出的残差网络做了简单介绍。而就在第二年（2016年），何凯明的团队就发表了“Identity Mappings in Deep Residual Networks”这篇文章，分析了ResNet成功的关键因素——residual block背后的算法，并对residual block以及after-addition activation进行改进，通过一系列的ablation experiments验证了，在residual block和after-addition activation上都使用identity mapping（恒等映射）时，能对模型训练产生很好的效果。不知道为什么，我今天从arXiv上download这篇paper的时候发现上不去了，莫非现在上arXiv也要科学上网了？本次实战主要是基于之前的ResNet实现和flyAI平台，并结合上面提到的何凯明团队分析ResNet的论文做出一些改进，并检验效果。References：电子文献：https://blog.csdn.net/Sandwichsauce/article/details/89162570https://www.jianshu.com/p/184799230f20https://blog.csdn.net/wspba/article/details/60572886https://www.cnblogs.com/4991tcl/p/10395574.htmlhttps://blog.csdn.net/DuinoDu/article/details/80435127参考文献：[1]Identity Mappings in Deep Residual Networksablation experiments在上面我提到了这个名词，中文翻译是“消融实验”。或许在阅读论文的过程中会接触到这个名词，如果仅根据字面翻译的话或许会很纳闷。在查找了一定的资料后，我对这种方法有了大致地了解。ablation的原本释义是通过机械方法切除身体组织，如手术，从身体中去除尤指器官以及异常生长的有害物质。事实上，这种方法类似于物理实验中的控制变量法，即当在一个新提出的模型中同时改变了多个条件或者参数，那么为了分析和检验，在接下去的消融实验中，会一一控制每个条件或者参数不变，来根据结果分析到底是哪个条件或者参数对模型的优化、影响更大。在机器学习、特别是复杂的深度神经网络的背景下，科研工作者们已经采用“消融研究”来描述去除网络的某些部分的过程，以便更好地理解网络的行为。ResNet的分析与改进残差单元在2015年ResNet首次发布的时候，设计的残差单元在最后的输出之前是要经过一个激活函数的。而在2016年新提出的残差单元中，去掉了这个激活函数，并通过实验证明新提出的残差单元训练更简单。 这种新的构造的关键在于不仅仅是在残差单元的内部，而是在整个网络中创建一个“直接”的计算传播路径来分析深度残差网络。通过构造这样一个“干净”的信息通路，可以在前向和反向传播阶段，使信号能够直接的从一个单元传递到其他任意一个单元。实验表明，当框架接近于上面的状态时，训练会变得更加简单。shortcut对于恒等跳跃连接$h(x_{l})=x_{l}$，作者设计了5种新的连接方式来与原本的方式作对比，设计以及实验结果如下所示： 其中fail表示测试误差超过了20%。实验结果表明，原本的连接方式误差衰减最快，同时误差也最低，而其他形式的shortcut都产生了较大的损失和误差。作者认为，shortcut连接中的操作 (缩放、门控、1×1的卷积以及dropout) 会阻碍信息的传递，以致于对优化造成困难。此外，虽然1×1的卷积shortcut连接引入了更多的参数，本应该比恒等shortcut连接具有更加强大的表达能力。但是它的效果并不好，这表明了这些模型退化问题的原因是优化问题，而不是表达能力的问题。激活函数对于激活函数的设置，作者设计了如下几种方式进行比较： 在这里，作者将激活项分为了预激活（pre-activation）和后激活（post-activation）。通过实验可以发现，将ReLU和BN都放在预激活中，即full pre-activation最为有效。ResNet实战根据论文中的实验结果，我使用了新的残差模块进行实践。并结合在deep-learning笔记：学习率衰减与批归一化中的分析总结对BN层的位置选取作了简单调整。在本次实验中，我尝试使用了StepLR阶梯式衰减和连续衰减两种学习率衰减方式，事实证明，使用StepLR阶梯式衰减的效果在这里要略好一些（连续衰减前期学得太快，后面大半部分都学不动了…）。首次训练的结果并不理想，于是我加大了学习率每次衰减的幅度，即让最后阶段的学习率更小，这使我的模型的评分提高了不少。由于训练资源有限，我没能进行更深（仅设置了10层左右）、更久（每次仅进行20个epoch）的训练，但在每个batch中，最高的accuracy也能达到65%左右，平均大约能超过50%。相比之前使用浅层网络仅能达到20%左右的accuracy，这已经提升不少了。然而最终的打分还是没有显著提高，因此我思考是否存在过拟合的问题。为此我尝试着在全连接层和捷径连接中加入dropout正则化来提高在测试集中的泛化能力，结果最终打分仅提高了0.1，而训练时间稍短。由于我除了dropout之外并没有改变网络的层数等影响参数量的因素，因此似乎与何大神在论文中original版和dropout版shortchut的比较有一些矛盾，但的确还是说明了dropout在这里的作用微乎其微，优化模型时可以排除至考虑范围之外了。遇到的问题TabError: inconsistent use of tabs and spaces in indentation当我在flyAI提供的窗口中修改代码并提交GPU训练时，就出现了这个报错。它说我在缩进时错误的使用了制表符和空格。于是我只好把报错处的缩进删除并重敲tab缩进，问题就得到了解决。如果使用PyCharm等IDE的话，这个错误会直接显示出来，即在缩进处会有灰色的颜色警告，将光标移过去就会有具体报错。这就省得提交GPU之后才能收到报错，所以以后写代码、改代码能用IDE还是用起来好啦。RuntimeError: one of the variables needed for gradient computation has been modified by an inplace operation这是在shortcut残差连接时遇到的一个报错，上网后发现原因很简单：版本问题。在新版的pytorch中，由于0.4.0之后把Varible和Tensor融合为一个Tensor，因此inplace操作在之前对Varible时还能用，但现在只有Tensor，就会出错了。解决的办法是将x += self.shortcut(x1)替换成x = x + self.shortcut(x1)。若网络很大，找起来很麻烦，可以在网络的中间变量加一句x.backward()，看会不会报错，如果不会的话，那就说明至少这之前是没毛病的。张量第一维是batch size起初，我根据输入的torch.Size([64, 1, 128, 128])，使用如下函数将输出拍平成1维的：123456def num_flat_features(self, x): size = x.size()[0:] num_features = 1 for s in size: num_features *= s return num_features同时，为了匹配，我将第一个全连接层的输入乘上了64。其实这个时候我已经开始怀疑这个64是哪来的了，为什么这个张量第一维尺度有64。直到后来平台报错，我才意识到这个表示的不是数据的维度，而是我设计的batch size。为此我将上面的代码调整如下：123456def num_flat_features(self, x): size = x.size()[1:] num_features = 1 for s in size: num_features *= s return num_features如此，问题得到解决，最终的输出应该是batch size乘上总类别数的一个张量。arXiv文前提到了上arXiv下论文要科学上网的事情，后来我发现了一个中科院理论物理所的一个备选镜像，但是好像不是特别稳定，不过还是先留在这里吧，万一的话可以拿来试试。一般一些科研工作者会在论文发布之前上传到arXiv以防止自己的idea被别人用了。估计主要是为了防止类似牛顿莱布尼兹之争这种事吧。]]></content>
      <categories>
        <category>人工智能</category>
      </categories>
      <tags>
        <tag>深度学习</tag>
        <tag>论文分享</tag>
        <tag>踩坑血泪</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[python笔记：打印进度]]></title>
    <url>%2Fpython20200108214052%2F</url>
    <content type="text"><![CDATA[在我们训练模型的时候，我们总希望能够直接看到训练的进度，下面我就总结几个我收集的打印进度的方法。References：电子文献：https://blog.csdn.net/u013985241/article/details/86653356https://blog.csdn.net/zkp_987/article/details/81748098利用回车符打印百分比应该是最常见的方法，也是我一直使用的。不过如果简单地逐次打印百分比的话，就会占据大量的屏幕空间，甚至装不下而需要手动拖动滚动条，让人眼花缭乱。这时我就想到了利用转义符“\r”，在print完本次的进度之后，下一次直接回车将其清除覆盖，这样就达到了既不占用屏幕又清晰的目的。大致的方法如下：12345import time #这里是为了用来延时，代替训练的时间numOfTimes = 200 #总循环次数，可以是总训练数据量等，这里设为200for i in range(numOfTimes): print("\r", "progress percentage:&#123;0&#125;%".format((round(i + 1) * 100 / numOfTimes)), end = "", flush = True) time.sleep(0.02) #若前面from time import sleep，这里直接sleep(0.02)即可这里用到了python的format格式化函数，format中计算出的数值对应的位置是{0}，将在实际print的过程中被替换。此外，这里还用到了round()函数，其作用是返回浮点数的四舍五入值。关于上面在print()函数中出现的flush，文首的参考链接中已给出解释，这里做个搬运：因为print()函数会把内容放到内存中，内存中的内容并不一定能够及时刷新显示到屏幕中。而当我们使用flush = True之后，会在print结束之后，立即将内存中的东西显示到屏幕上，清空缓存。基于上述原理，flush大致有下面两个使用场景：在循环中，要想每进行一次循环体，在屏幕上更新打印的内容就得使用flush = True的参数。（我这里就是这种情况）打开一个文件，向其写入字符串，在关闭文件f.close()之前 打开文件是看不到写入的字符的。因此，如果要想在关闭之前实时地看到写入的字符串，那么就应该使用flush = True。利用tqdm库有需求就有市场，一搜果然还是有库能满足我的需求的。tqdm就是其中之一，它是一个快速，可扩展的python进度条，可以在python长循环中添加一个进度提示信息。大致用法如下：123456import tqdmimport timenumOfTimes = 200 #总循环次数，可以是总训练数据量等，这里设为200for i in tqdm.tqdm(range(numOfTimes)): time.sleep(0.02) #代替训练等耗时过程 pass也可以直接from tqdm import tqdm，这样后面就不需要tqdm.tqdm了。利用progressbar库如其名，这个库就是用来做进度条的。如果没有的话，它和tqdm都可以使用pip来安装。123456import progressbarfrom time import sleepnumOfTimes = 200 #总循环次数，可以是总训练数据量等，这里设为200progress = progressbar.ProgressBar()for i in progress(range(numOfTimes)): sleep(0.02)]]></content>
      <categories>
        <category>程序与设计</category>
      </categories>
      <tags>
        <tag>代码实现</tag>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[circuit笔记：有线双工对讲机的电子线路设计]]></title>
    <url>%2Fcircuit20200107222439%2F</url>
    <content type="text"><![CDATA[模拟电路实验是我进入大学本科以来第一个付出大量课外时间的实验课，其最后的大项目是让我们自行设计一个电路，而我们小组选择的是有线双工对讲机。今天我就简单对我们小组的设计方案做一个整理。在这里还是非常感谢组员们的共同努力和巨大帮助！References：电子文献：http://www.ttic.cc/file/TDA1013B_76329.html.comhttps://wenku.baidu.com/view/c8b016e7ed630b1c58eeb520.htmlhttps://tech.hqew.com/fangan_1909806设计目的有线对讲机是用导线直接连接进行通话，而双工通信则是像电话机一样同时进行双方的“听”和“讲”。此外，我们希望可以具有音量可调节、消侧音等一些对讲机需要的功能。设计思路利用驻极体话筒将声音信号转化为微弱的电信号。通过反相比例放大器将微弱的电信号放大。利用相位抵消法实现消侧音。只使用一根传输线进行信号互传。利用电压跟随器避免远距离导线传输时衰减过大的问题。添加了低通滤波器电路，滤去高频的噪音信号。使用TDA1013B进行功率放大并将信号传输到扬声器。最后由扬声器将电信号转化成声音信号，发出声音。设计有线双工对讲机的思路可以用如下所示的系统图表示。主要由弱声音采集、前置运算放大器、消侧音电路、减小信号衰减电路、低通滤波器电路、功率放大电路、扬声器等模块组成。基本原理驻极体话筒 话筒的基本结构由一片单面涂有金属的驻极体薄膜与一个上面有若干小孔的金属电极（背称为背电极）构成。驻极体面与背电极相对，中间有一个极小的空气隙，形成一个以空气隙和驻极体作绝缘介质，以背电极和驻极体上的金属层作为两个电极构成一个平板电容器。电容的两极之间有输出电极。由于驻极体薄膜上分布有自由电荷，当声波引起驻极体薄膜振动而产生位移时；改变了电容两极板之间的距离，从而引起电容的容量发生变化，由于驻极体上的电荷数始终保持恒定，根据公式：Q =CU 所以当C变化时必然引起电容器两端电压U的变化，从而输出电信号，实现声电的变换。不管是源极输出或漏极输出，驻极体话筒必须提供直流电压才能工作，因为它内部装有场效应管。反向比例放大 相比例放大电路的原理如上图所示，输出信号电压增益为R2与R1之比，相位反相变化180°（后面会再次反相）。在本实验中，我们将两个电阻的比值调整在10~100之间，即放大比例为10~100。消侧音在模拟音频收发信号共用一个信道的对讲系统中，为减小侧音对通话效果的影响，防止侧音的干扰，所有对讲设备均需增加消侧音电路。一方面让音频发送信号按一定比例出现在传输线上，另一方面让本方音频接收电路获得的信号足够小，不至于说话者从己方喇叭听到自己的声音，提高通话的质量。 在上图所示的串联分压电路中，R1、R2为纯电阻，v1、v2为输入电压，vo为输出电压，据叠加定理：v_{o}=\frac{v_{1}R_{2}+v_{2}R_{1}}{R_{1}+R_{2}}令vo=0，则v1R2+v2R1=0，即：\frac{v_{1}}{v_{2}}=-\frac{R_{1}}{R_{2}}特别地，当R1=R2时，v1=-v2。由上式可见，欲使vo=0，v1，v2须满足2个条件：每个频率分量的相位相反。每个频率分量幅度呈一定比例且比例相同。下面是该方法的一种实现方式的电路图： 根据文首所列的参考资料，我们找到了另一种原理相同且成本更低的实现方式，如下图所示。三极管发射极和集电极的信号反相且幅度相同，可以相叠加后将信号消去，一个三极管的作用相当于上述方法中的U1和U2。图中三级管的偏置电路没有画出，C1和C2将直流分量同传输线隔离开。需要特别提出的是，如果可调电阻P1足够大，从而对三极管的偏置影响足够小，可将C2去掉，可调电阻P1直接和三极管的c，e极并联。 需要注意的是，在这里传送到对方的信号的相位再次反相，与原始信号相一致。实验中，三极管采用9013，电位器采用104。为了使三极管正常工作，在三极管B端由R1和R3分压，使三极管的静态工作点VCQ≥4.5V，则令VEQ=VCC-VCQ，需要VR2=VR4，因此选R2=R4=1kΩ，则由IEQ=ICQ得VR2=VR4。由于有VBE=0.7V，则VBQ=VEQ+0.7，VBQ+VR1=VCC=12V。此时若ICQ=4mA，经计算，则R1与R3之比约为1.5。我们最终选取R1=6.7kΩ，R3=4.7kΩ来分压。减小信号衰减由于我们的目标是实现长导线远程传输信号，因此导线上的电阻是不可忽略的，这就导致了信号衰减的问题。在弱电的情况下，解决导线上信号衰减的方法有选取更优质的导线、改用电流信号输出、增大接收端的输入电阻等。利用电压跟随器输入电阻高的特性，我们决定在信号接收的两端分别添加一个电压跟随器（如下图所示）来抑制信号传输的衰减。 低通滤波人耳可以听到20HZ到20kHZ的音频信号，而人正常对话所发出的声音频率约为300HZ—3000HZ，频率较低，因此我们设计一个低通语音滤波器来滤除杂音，提高声音清晰度。我们的目的是设计一个低频增益A0=2，Q≈1（品质因数，越小则通带或阻带越平坦，电路的稳定性越好）, fH=3kHz，图如下所示是一个二阶压控电压源低通滤波器。 根据该电路低频增益A0=K=1+R27/R28=2，可知R27=R28,因此我们选R27=R28=10kΩ。根据fC=ωC/2π，当R25=R26=R，C10=C12=C时，有ωC=1/(RC)。因此fC=1/2πRC。由所需上限截止频率fH为3kHz，我们选择C=0.01μF，算出R=1/(2πfC) ≈5.3kΩ。这里我们选用4.7kΩ的电阻，可实现近似的功能。使用TDA1013B功率放大在最后的输出之前，我们需要对信号进行功率的放大。在查阅相关资料之后，我们发现TDA1013B比较符合我们的功能需求。TDA1013B是一个音频功率放大器集成电路，内部具有按对数曲线变化的直流音量控制电路，控制范围可达80dB，它具有很宽的电源电压范围（10V~40V），输出功率位4W~10W，是理想的音频功率放大器。根据文首列出的数据手册，TDA1013B的伴音电路连接方式如下。 其中各个引脚的功能分别是：1脚：电源地。2脚：放大器输出，这里作伴音输出。3脚：电源。4脚：电源。5脚：功放输入。6脚：控制单元输出。7脚：控制电压，这里可用于音量控制。8脚：控制单元输入，这里输入音频。9脚：信号地。通过分析和查阅资料（见本文参考），我们确定了芯片的连接方式如下图所示。 设计实现如下是我们的仿真总电路图（还缺少最后的两级功率放大电路）。我们使用的是multisim14.0，由于软件的库中没有TDA1013B芯片，且声音信号难以在仿真软件中模拟，因此我们在仿真模拟阶段选择分模块检验功能的实现效果。我们在第一级放大电路前后使用示波器检测放大效果，我们输入100mV峰峰值、1000Hz频率的交流信号，得到输出如下，其中通道A为放大之后的的信号，通道B显示的是放大之前的信号。我们还检测了声音低通滤波器的功能实现情况，我们将对应的模块分离出来，利用波特测试仪画出该电路的波特图，结果如下所示。分别使用了对数和线性的横坐标轴（频率），且分别设扫描上限为100kHz和20kHz，由图易知，在3kHz左右处，增益开始下降，基本符合我们的设计要求。]]></content>
      <categories>
        <category>程序与设计</category>
      </categories>
      <tags>
        <tag>模拟电路</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[verilog笔记：频率计实现与verilog中timescale的解释]]></title>
    <url>%2Fverilog20200107215404%2F</url>
    <content type="text"><![CDATA[之前每次创建完一个新的文件，文件最上方总会显示一行timescale。当初觉得没什么作用，删了之后依旧没有任何问题便没把它当回事，直到后来做频率计数器的时候我才决定一探究竟。那么这篇文章也就一并谈一下这个问题。References：电子文献：https://blog.csdn.net/m0_37652453/article/details/90301902https://blog.csdn.net/ciscomonkey/article/details/83661395https://blog.csdn.net/qq_16923717/article/details/81099833基本原理一个简易的频率计数器主要由分频器和计数器构成，其基本原理就是记录由分频器得到的一段时间内被测信号上升沿的个数，从而求得被测信号的频率。控制信号转换模块123456789101112131415161718`timescale 1ns / 1ps //unit 1ns, precision 1psmodule control( output reg Cnt_EN, //enable the counter count, so that the counting period can be controled output wire Cnt_CR, //clear the counter every time when the measure begins output wire Latch_Sig, //at its posedge, the value of the counter will be stored/latched input nRST, //system reset signal input CP //1Hz standard clock signal ); always @ (posedge CP or negedge nRST) begin if(~nRST) //generate enable counting signal Cnt_EN = 1'b0; //don't count else Cnt_EN = ~Cnt_EN; //two frequency divider for the clock signal end assign Latch_Sig = ~Cnt_EN; //generate latch signal assign Cnt_CR = nRST &amp; (~CP &amp; Latch_Sig); //generate the clear signal for the counterendmodule计数模块1234567891011121314151617`timescale 1ns / 1ps //unit 1ns, precision 1psmodule counter( output reg [3:0] Q, input CR, EN, CP ); always @ (posedge CP or posedge CR) begin if(CR) Q &lt;= 4'b0000; //reset to zero else if(~EN) Q &lt;= Q; //stop counting else if(Q == 4'b1001) Q &lt;= 4'b0000; else Q &lt;= Q + 1'b1; //counting, plus one endendmodule寄存模块123456789101112`timescale 1ns / 1ps //unit 1ns, precision 1psmodule Latch( output reg [15:0] Qout, input [15:0] Din, input Load, CR ); always @ (posedge Load or posedge CR) if(CR) Qout &lt;= 16'h0000; //reset to zero first else Qout &lt;= Din; endmodule顶层文件12345678910111213141516171819202122`timescale 1ns / 1ps //unit 1ns, precision 1psmodule Fre_Measure( output wire [15:0] BCD, //transfer to display part input _1HzIN, SigIN, nRST_Key, output wire Cnt_EN, Cnt_CR, //control signals of the counter output wire Latch_Sig, //latch singal output wire [15:0] Cnt //8421BCDcode output ); //call control block control U0(Cnt_EN, Cnt_CR, Latch_Sig, nRST_Key, _1HzIN); //measure counter counter U1(Cnt[3:0], Cnt_CR, Cnt_EN, SigIN); counter U2(Cnt[7:4], Cnt_CR, Cnt_EN, ~(Cnt[3:0] == 4'h9)); counter U3(Cnt[11:8], Cnt_CR, Cnt_EN, ~(Cnt[7:0] == 8'h99)); counter U4(Cnt[15:12], Cnt_CR, Cnt_EN, ~(Cnt[11:0] == 12'h999)); //call latch block Latch U5(BCD, Cnt, Latch_Sig, ~nRST_Key); endmodule仿真文件1234567891011121314151617181920212223242526272829303132333435363738`timescale 1ns / 1ps //unit 1ns, precision 1psmodule simulateFile(); wire [15:0] BCD; //transfer to display part wire [15:0] Cnt; //8421BCDcode output reg CLK, RST, Signal; parameter T1 = 0.1, //100Hz T2 = 0.01, //1000Hz T3 = 0.002; //5000Hz wire Cnt_EN, Cnt_CR; //control signals of the counter wire Latch_Sig; //latch singal Fre_Measure Watch(BCD, CLK, Signal, RST, Cnt_EN, Cnt_CR, Latch_Sig, Cnt); initial begin RST = 0; CLK = 0; Signal = 0; end always forever #10 CLK = ~CLK; //generate clock signal always #T1 Signal = ~Signal; //T1 or T2 or T3 always begin #10 RST = 1; #200 RST = 0; #10 RST = 1; #200 RST = 0; #10 RST = 1; #200 RST = 0; #10 RST = 1; end endmodule仿真结果以100Hz（T1）为例，输出的仿真结果如下。其中CLK是固定的1Hz基准时钟信号；RST是系统需求的复位按键，当按下复位即RST为下降沿时，可以看到计数器Cnt被清零同时第一排译码输出的BCD码也被清零；Signal为输入的信号，这里我使用的是100Hz的，由我自己设定，由于频率较快，可以看到波形图非常密集；Cnt_EN是计数使能信号，可见在它为高电平时，Cnt随着输入信号一样快速计数；Cnt_CR是清零信号，在每次计数使能的上升沿或者复位的下降沿到来时Cnt_CR置零，也就是对Cnt清零操作；此外，当时钟信号到来时，假如系统不在计数（Cnt_EN=0），那么Latch_Sig将置1，也就是把记录数值存入锁存器。实际上，这种设计方案会存在±1的计数误差，应为输入信号不一定与分频器同周期，即有可能每次测量的起始位置出于输入信号一个周期内的不同状态。timescaletimescale是Verilog中的预编译指令，指定位于它后边的module的时间单位和时间精度，直到遇到新的timescale指令或者resetall指令。它的语法如下：1`timescale time_unit / time_precision假如我们延时x个time_unit，那延时的总时间time = x * time_unit，但最后真正延时的时间是根据time_precision对time进行四舍五入后的结果。注意：time_unit和time_precision只能是1、10和100这三种整数，单位有s、ms、us、ns、ps和fs。time_precision必须小于等于time_unit。timescale的时间精度设置是会影响仿真时间的，1ps精度可能是1ns精度仿真时间的一倍还多，并且占用更多的内存，所以如果没有必要，应尽量将时间精度设置得更大一些。仿真时间之前进行仿真时，我往往是让它自动运行至系统默认时间然后停止，这就会造成出现好几次重复循环的情况。在Vivado中，窗口上方有三个类似播放器中的按钮，从左往右依次是：复位、不停运行、按指定时长（在后面的栏中设定）运行。此外，如果计算不准时间，可以直接在仿真文件末尾或者想要结束的地方使用$stop或者$finifsh来终止仿真。]]></content>
      <categories>
        <category>程序与设计</category>
      </categories>
      <tags>
        <tag>数字电路</tag>
        <tag>代码实现</tag>
        <tag>verilog</tag>
        <tag>Vivado</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[verilog笔记：运动码表的硬件描述语言实现]]></title>
    <url>%2Fverilog20200107211139%2F</url>
    <content type="text"><![CDATA[继上一篇logisim笔记：基本使用及运动码表的实现，我还使用了硬件描述语言对同样需求的运动码表进行了实现，那么就在这里一并也总结一下吧。References：电子文献：https://blog.csdn.net/leon_zeng0/article/details/78441871https://www.cnblogs.com/douzi2/p/5147151.htmlhttps://blog.csdn.net/FPGADesigner/article/details/82425612整体设计由于需求与使用Logisim实现时一致，因此我的设计思路也基本沿用上一篇博文中提到的方案。但是要注意的是，这里的000状态并不在作为按键抬起之后的中间态，而是进入系统时的一个默认初始状态。Vivado中一些高亮的含义在具体的代码之前，我还想先归纳一下本次实践过程中遇到的和发现的Vivado中一些高亮提醒的含义。土黄色高亮土黄色高亮出现的原因主要可能是下面三种情况：定义重复。定义放在了调用处的后面（identifier used before its declaration）。声明残缺（empty statement）。蓝色高亮含有undeclared symbol。和上面土黄色高亮相搭配出现，有定义重复时指明重复定义的位置。16位数值比较器该模块用于比较两个16位二进制数的大小，以确定是否需要存入记录的数据。代码如下：1234567891011121314module _16bit_Comp( input [15:0] A, input [15:0] B, output reg Y ); always @ (A or B) begin if(A &lt; B) Y &lt;= 1; else Y &lt;= 0; endendmodule16位寄存器TMRecord表示码表暂停时的读数，regRecord表示寄存器中已经存储的记录，初始值为9999。控制信号有使能信号和reset信号。当收到reset信号时，直接将记录改为9999。当有使能信号时，将TMRecord记录下来。代码如下：1234567891011121314151617181920module _16bit_Reg( input enable, input [15:0] TMRecord, input [15:0] regRecord, input [15:0] maxDefault, input reset, input clock, output reg [15:0] next_record ); always @ (reset or enable) begin if(reset &amp; enable) next_record &lt;= maxDefault; else if(enable) next_record &lt;= TMRecord; else next_record &lt;= regRecord; endendmodule数码管显示驱动将BCD码转化为7位二进制数，即对应7段数码管，用于显示。12345678910111213141516171819202122module watchDrive( input [3:0] BCD, output reg [6:0] light ); always @ (BCD) begin case(BCD) 0: light = 7'B0111111; 1: light = 7'B0001001; 2: light = 7'B1011110; 3: light = 7'B1011011; 4: light = 7'B1101001; 5: light = 7'B1110011; 6: light = 7'B1110111; 7: light = 7'B0011001; 8: light = 7'B1111111; 9: light = 7'B1111011; default: light = 7'B0111111; endcase endendmodule顶层文件该部分主要包括对各个变量的定义和初始化；状态转换，即共设计了5种状态，对应不同的功能，当按下不同按键时，选择对应的状态并作为次态；数码管的显示与进位，即对数码管4个位置依次改动，从低位开始计算，当进位时产生进位信号到下一位。当有重置信号（这里使用的是reset和start的上升沿）时清零。123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212`timescale 1ns / 1ps//top modulemodule runningwatch( input start, input stop, input store, input reset, input CLK, output reg TM_EN, //enable the timer output reg SD_EN, //enable register output reg DP_SEL, //the screen show which data output reg [15:0] regRecord, //data the register storing output reg [15:0] TMRecord, //timer data output [6:0] tenmsLight, hunmsLight, onesLight, tensLight ); reg [3:0] tenMS, hunMS, oneS, tenS; //four 4bit digits from small to high and each from 0 to 9 reg tenmsup, hunmsup, onesup; //the signals if the bigger digit than itself should add //allocate state parameter S0 = 3'B000; //initial state parameter S1 = 3'B001; //TIMING:timer set as 00.00,record does not change,show timer,timer counts,TM_EN = 1, SD_EN = 0, DP_SEL = 0 parameter S2 = 3'B010; //PAUSE:timer does not change,record does not change,show timer,timer does not count,TM_EN = 0, SD_EN = 0, DP_SEL = 0 parameter S3 = 3'B011; //UPDATE:timer does not change,record set as timer,show register,timer does not count,TM_EN = 0, SD_EN = 1, DP_SEL = 1 parameter S4 = 3'B100; //KEEP:timer does not change,record does not change,show register,timer does not count,TM_EN = 0, SD_EN = 0, DP_SEL = 1 parameter S5 = 3'B101; //RESET:timer set as 00.00,record set as 99.99,show timer,timer does not count,TM_EN = 0, SD_EN = 1, DP_SEL = 0 reg [2:0] state; reg [2:0] next_state; //reg CLK; initial //only run once begin regRecord = 16'B0010011100001111; TMRecord = 16'B0000000000000000; state = S0; tenMS = 0; hunMS = 0; oneS = 0; tenS = 0; end //to judge if store the timer's data wire new; reg newRecord; _16bit_Comp comparator(TMRecord, regRecord, new); //compare always @ (new) newRecord = new; reg [15:0] MAX = 16'B0010011100001111; //sequential logic part always @ (posedge CLK) //update the state at each posedge begin state &lt;= next_state; end //combinatory logic part //state transform always @ (state or start or stop or store or reset or newRecord) begin next_state = S0; //if not press the key, back to the initial state case(state) S0: //initial state begin TM_EN = 0; SD_EN = 0; DP_SEL = 0; if(start) begin next_state = S1; TM_EN = 1; SD_EN = 0; DP_SEL = 0; end else if(stop) begin next_state = S2; TM_EN = 0; SD_EN = 0; DP_SEL = 0; end else if(store &amp; newRecord) begin next_state = S3; TM_EN = 0; SD_EN = 1; DP_SEL = 1; end else if(store &amp; ~newRecord) begin next_state = S4; TM_EN = 0; SD_EN = 0; DP_SEL = 1; end else if(reset) begin next_state = S5; TM_EN = 0; SD_EN = 1; DP_SEL = 0; end else begin next_state = S0; TM_EN = 0; SD_EN = 0; DP_SEL = 0; end end S1: //TIMING begin TM_EN = 1; SD_EN = 0; DP_SEL = 0; if(start) begin next_state = S1; TM_EN = 1; SD_EN = 0; DP_SEL = 0; end else if(stop) begin next_state = S2; TM_EN = 0; SD_EN = 0; DP_SEL = 0; end else if(store &amp; newRecord) begin next_state = S3; TM_EN = 0; SD_EN = 1; DP_SEL = 1; end else if(store &amp; ~newRecord) begin next_state = S4; TM_EN = 0; SD_EN = 0; DP_SEL = 1; end else if(reset) begin next_state = S5; TM_EN = 0; SD_EN = 1; DP_SEL = 0; end else begin next_state = S0; TM_EN = 0; SD_EN = 0; DP_SEL = 0; end end S2: //PAUSE begin TM_EN = 0; SD_EN = 0; DP_SEL = 0; if(start) begin next_state = S1; TM_EN = 1; SD_EN = 0; DP_SEL = 0; end else if(stop) begin next_state = S2; TM_EN = 0; SD_EN = 0; DP_SEL = 0; end else if(store &amp; newRecord) begin next_state = S3; TM_EN = 0; SD_EN = 1; DP_SEL = 1; end else if(store &amp; ~newRecord) begin next_state = S4; TM_EN = 0; SD_EN = 0; DP_SEL = 1; end else if(reset) begin next_state = S5; TM_EN = 0; SD_EN = 1; DP_SEL = 0; end else begin next_state = S0; TM_EN = 0; SD_EN = 0; DP_SEL = 0; end end S3: //UPDATE begin TM_EN = 0; SD_EN = 1; DP_SEL = 1; if(start) begin next_state = S1; TM_EN = 1; SD_EN = 0; DP_SEL = 0; end else if(stop) begin next_state = S2; TM_EN = 0; SD_EN = 0; DP_SEL = 0; end else if(store &amp; newRecord) begin next_state = S3; TM_EN = 0; SD_EN = 1; DP_SEL = 1; end else if(store &amp; ~newRecord) begin next_state = S4; TM_EN = 0; SD_EN = 0; DP_SEL = 1; end else if(reset) begin next_state = S5; TM_EN = 0; SD_EN = 1; DP_SEL = 0; end else begin next_state = S0; TM_EN = 0; SD_EN = 0; DP_SEL = 0; end end S4: //KEEP begin TM_EN = 0; SD_EN = 0; DP_SEL = 1; if(start) begin next_state = S1; TM_EN = 1; SD_EN = 0; DP_SEL = 0; end else if(stop) begin next_state = S2; TM_EN = 0; SD_EN = 0; DP_SEL = 0; end else if(store &amp; newRecord) begin next_state = S3; TM_EN = 0; SD_EN = 1; DP_SEL = 1; end else if(store &amp; ~newRecord) begin next_state = S4; TM_EN = 0; SD_EN = 0; DP_SEL = 1; end else if(reset) begin next_state = S5; TM_EN = 0; SD_EN = 1; DP_SEL = 0; end else begin next_state = S0; TM_EN = 0; SD_EN = 0; DP_SEL = 0; end end S5: //RESET begin TM_EN = 0; SD_EN = 1; DP_SEL = 0; if(start) begin next_state = S1; TM_EN = 1; SD_EN = 0; DP_SEL = 0; end else if(stop) begin next_state = S2; TM_EN = 0; SD_EN = 0; DP_SEL = 0; end else if(store &amp; newRecord) begin next_state = S3; TM_EN = 0; SD_EN = 1; DP_SEL = 1; end else if(store &amp; ~newRecord) begin next_state = S4; TM_EN = 0; SD_EN = 0; DP_SEL = 1; end else if(reset) begin next_state = S5; TM_EN = 0; SD_EN = 1; DP_SEL = 0; end else begin next_state = S0; TM_EN = 0; SD_EN = 0; DP_SEL = 0; end end default: begin next_state = S0; TM_EN = 0; SD_EN = 0; DP_SEL = 0; end //default initial state endcase end //reset to zero always @ (posedge reset or posedge start) begin tenMS &lt;= 0; hunMS &lt;= 0; oneS &lt;= 0; tenS &lt;= 0; TMRecord &lt;= 0; end //the followings are which have stated before: //reg [3:0] tenMS, hunMS, oneS, tenS are four 4bit digits from small to high and each from 0 to 9 //reg tenmsup, hunmsup, onesup are the signals if the bigger digit than itself should add //timer, divide into four digits //10ms always @ (posedge CLK) begin if(TM_EN) begin TMRecord = TMRecord + 1; if(tenMS &lt; 9) begin tenMS &lt;= tenMS + 1; tenmsup &lt;= 0; end else begin tenMS &lt;= 0; tenmsup &lt;= 1; end end end //100ms always @ (posedge tenmsup) begin if(TM_EN) begin if(hunMS &lt; 9) begin hunMS &lt;= hunMS + 1; hunmsup &lt;= 0; end else begin hunMS &lt;= 0; hunmsup &lt;= 1; end end end //1s always @ (posedge hunmsup) begin if(TM_EN) begin if(oneS &lt; 9) begin oneS &lt;= oneS + 1; onesup &lt;= 0; end else begin oneS &lt;= 0; onesup &lt;= 1; end end end //10s always @ (posedge onesup) begin if(TM_EN) begin if(tenS &lt; 9) tenS &lt;= oneS + 1; else oneS &lt;= 0; end end //save to the register wire [15:0] newReg; _16bit_Reg register(SD_EN, TMRecord, regRecord, MAX, reset, CLK, newReg); always @ (newReg) regRecord = newReg; //change BCD to tube lights watchDrive TENms(tenMS, tenmsLight); watchDrive HUNms(hunMS, hunmsLight); watchDrive ONEs(oneS, onesLight); watchDrive TENs(tenS, tensLight); endmodule仿真文件最后我们还需要自行编写一个仿真文件。123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354module simulateFile(); reg start, stop, store, reset; wire TM_EN, SD_EN, DP_SEL; wire [15:0] regRecord; wire [15:0] TMRecord; wire [6:0] tenmsLight; wire [6:0] hunmsLight; wire [6:0] onesLight; wire [6:0] tensLight; reg CLK; runningwatch watch(start, stop, store, reset, CLK, TM_EN, SD_EN, DP_SEL, regRecord, TMRecord, tenmsLight, hunmsLight, onesLight, tensLight); initial begin CLK = 0; start = 0; stop = 0; store = 0; reset = 0; end begin always //generate clock signal forever #10 CLK = ~CLK; //always #100 //&#123;start, stop, store, reset&#125; = 4'B0000; //begin start = 0; stop = 0; store = 0; reset = 0; end always begin #50 &#123;start, stop, store, reset&#125; = 4'B0001; #50 &#123;start, stop, store, reset&#125; = 4'B1000; #500 &#123;start, stop, store, reset&#125; = 4'B0100; #50 &#123;start, stop, store, reset&#125; = 4'B0010; #50 &#123;start, stop, store, reset&#125; = 4'B1000; #50 &#123;start, stop, store, reset&#125; = 4'B0100; #100 &#123;start, stop, store, reset&#125; = 4'B0010; #50 &#123;start, stop, store, reset&#125; = 4'B0001; #50 $stop; //stop simulation end endendmodule仿真结果Run simulation，得到如下输出波形图。遇到的问题报错：[Synth 8-462] no clock signal specified in event control我原本直接在顶层文件中定义时钟并使用forever生成连续的时钟信号，结果出现了如上报错。在仿佛调整后，最后发现解决的办法是将时钟信号放在仿真文件里生成然后作为input输入到顶层文件。使用模块的输出赋值时遇到问题这个问题的主要原因还是因为我对于verilog赋值规则以及变量性质的不熟悉，这里做一个小归纳：给wire赋值必须用assign。给reg赋值用always。使用非阻塞赋值时，reg不能给wire赋值，反之则可以。使用阻塞赋值时，reg可以给wire赋值，反之则不行。]]></content>
      <categories>
        <category>程序与设计</category>
      </categories>
      <tags>
        <tag>踩坑血泪</tag>
        <tag>数字电路</tag>
        <tag>代码实现</tag>
        <tag>verilog</tag>
        <tag>Vivado</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[logisim笔记：基本使用及运动码表的实现]]></title>
    <url>%2Flogisim20200107121101%2F</url>
    <content type="text"><![CDATA[似乎好久没有写新的博文了。由于今年的春节较往年要早，近一个月来，各门考试接踵而至让我忙得不可开交。虽然非常赶，终于还是考完了。当初想都不敢想的13天考10门的考试周也就这样熬过去了。事实证明，即便是很大的困难，逼一下自己还是能挺过来的。不过最后还是和往年的冬天一样发了个一年一度的高烧，真的难受，以后还得更加注重自己的身体。废话不多说，这几天就打算先把这学期期末阶段一些有意义、有价值的的知识与经历整理记录一下。首先是一个运动码表的大作业，本文主要是使用Logisim对运动码表进行实现的方法与过程。需求分析该项目要求我们设计一个简单的运动码表，包括四个按键组成的输入模块和四个7段数码管组成的输出模块。四个按键分别是：Start，按下时计时器归零并重新开始计时；Stop，按下时停止计时并显示计时数据；Store，按下时尝试更新系统记录，要求记录当前码表值，若已有记录，则和当前待记录的值作对比，如果计时数据比记录数据要小即用时要短，则更新系统记录并显示；Reset，按下时进行复位，将计时数据置为00.00，系统记录置为99.99。Logisim使用在具体说明实现的方法之前，我想先把之前整理的一些Logisim的基本使用做一个简短的总结。Logisim的使用其实不难，可以参考网上整理的Logisim文档翻译就可以快速入门。实际上，在具备一定数字电路知识的情况下，到处点点也能够学会Logisim的基本使用方法。Logisim为数字电路的设计提供了很大的帮助，我们可以通过填写真值表或者输入逻辑函数快速生成原本手动连线根本无法完成的复杂电路。如果想要获得Logisim最新的优化版本，可以加入华科谭志虎教授创建的计算机硬件系统设计交流群（群号：957283876），此群汇集了多个高校的学生，是一个比较活跃的技术交流群，谭教授秒回且超赞的解答真的忍不住让人点赞！下面是一些我当初刚使用时记录在note上的一些Logisim的使用与常见处理，分享在此。引脚在Logisim中，引脚即Pin，可以使用键盘上、下、左、右光标键来改变引脚的朝向。注意：这里该表朝向不是按照旋转方向来的，而是按哪个方向就是指向哪个方向。此外，根据形状的不同，引脚分为输入引脚（较方）和输出（较圆）引脚，可以使用UI左上角最左侧的手型的戳工具点击对应的引脚来修改引脚的值（高电平、低电平、未知x态）。当我们选中引脚时，按下alt+数字组合，就可以改变到对应的位宽。与门选中与门，按下数字键，就修改输入引脚个数。线颜色的含义亮绿色：高电平。深绿色：低电平。蓝色：未知状态。灰色：飞线。红色：信号冲突。黑色：多位总线。橙色：位宽不匹配。时钟ctrl+K：驱动与关闭时钟连续运行。ctrl+T：驱动时钟单步运行。其它快捷键上面列出的都是一些并比较典型的特例，别的地方使用基本类似，可以多多尝试。下面再列出两个比较常用的快捷键：ctrl+D：创建副本。ctrl+Z：撤销。整体设计为了实现运动码表的功能，我们将整个项目拆分成如下几个模块：计时模块（包括计时使能模块）。码表驱动与显示模块。系统记录数据寄存模块。码表状态功能控制模块。数值比较模块。数值选择模块。此外，根据需求分析，我们设计了如下6个状态，并构建状态机如下：000 中间态：每次按键弹起后，回到该状态。101 复位：计时变成00.00，记录变成99.99，显示计时数值，时钟不计时。001 计时：计时变成00.00，记录不变，显示计时数值，时钟计时。010 停止：计时不变，记录不变，显示计时数值，时钟不计时。011 更新（小于系统记录NewRecord=1）：计时不变，记录变成计时，显示记录数值，时钟不计时。100 不更新（大于等于系统记录NewRecord=0）：计时不变，记录不变，显示记录数值，时钟不计时。可得状态转换关系的真值表如下所示：为了实现上述状态转换与控制信号输出，我们设计了如下码表控制电路。其中SD-EN控制寄存器使能，DP-SEL控制码表显示数值的选择，TM-Reset控制是否复位。设计实现这是设计完成后最终circ文件中所包含的内容，下面我简述一下各个模块中的内容及思路。数码管驱动：即将4位2进制数转化成7个二进制信号，驱动7段数码管的亮灭。4位BCD计数器：基于下面的BCD计数器状态转换（即在0-9之间递增循环）和BCD计数器输出函数（即在达到9时输出进位信号）。码表计数器：由四个4位BCD计数器组成。码表显示驱动：即将四个4位二进制组成的数字通过四个7段数码管显示出来，内部基于上面的显示驱动转换电路即数码管驱动的封装。码表控制器：上文的设计中已经提及，基于码表控制器状态转换（输入信号+现态-&gt;次态）和码表控制器输出函数（现态-&gt;控制信号）。计时使能：这里我比较巧妙地运用了D触发器的置位清零两个端，将在后文提及。最后就是运动码表的总电路图：遇到的问题复位后自动开始计时这是最让我头疼的一个问题，由于上述状态机的设计方法和Logisim中按键在鼠标松开后自动弹起的功能，导致在我按下Reset清零后系统会自动重新开始计时（因为这时的现态已经不是复位状态），这显然是不符合需求的。为了使计时使能在下一次按键到来之前能保持现态，我将复位控制信号从状态转换电路中单独取出，并使用一个D触发器的置位清零两个端子实现了这个保持功能，效果不错。计时器遇到9时跳跃进位当最初的电路实现完成后，我兴奋地开始计时，结果计时器的花式进位方式顿时让我傻了眼。仔细研究后我发现，我起初设计的电路在进位时只考虑了低一位计数器传来的进位信号，而事实上，高位的进位条件往往是由后几位共同决定的，为此，修改电路如下： 问题解决。码表在更新数据的前一个时钟内会显示较大的记录数值这个问题其实不是非常重要，但同班的一位同学还是注意到了这点。也就是说，当计时数据比系统记录要小的时候，系统应该更新记录并显示最好的成绩，然而当按下Store进行存储更新时，在前一个时钟周期内，码表会短暂地先显示原本较大即较差地系统记录，这是不希望出现的。解决的办法可以在寄存器和显示选择器之间添加一个三态门，具体可以看上文中的总电路图。]]></content>
      <categories>
        <category>程序与设计</category>
      </categories>
      <tags>
        <tag>踩坑血泪</tag>
        <tag>Logisim</tag>
        <tag>数字电路</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[matlab笔记：一个非线性方程问题的多种求解方法]]></title>
    <url>%2Fmatlab20191116003545%2F</url>
    <content type="text"><![CDATA[上周科学计算引论结课了，借着写上机报告的机会，我把书本上所有的求解非线性方程的方法（标量型）都用matlab实现了一下，并对一个实际工程问题进行求解。关于实现过程中遇到的问题以及注意事项，我已写在matlab笔记：久未使用之后踩的一堆坑内。实际问题在电机学中，凸极同步发电机的功角特性可表示为：P_{em}=\frac{E_{0}V}{x_{d}}sin\theta +V^{2}\left ( \frac{1}{x_{q}}-\frac{1}{x_{d}} \right )sin\theta \cdot cos\theta式中，$P_{em}$表示发电机的电磁功率；$E_{0}$表示发电机电势；$V$表示发电机端电压；$x_{q}$表示横轴同步电抗；$x_{d}$表示纵轴同步电抗；$\theta$表示功率角，$\theta \in \left ( 0,\frac{\pi }{2} \right )$。如令$\frac{E_{0}V}{x_{d}}=P_{j}$，$V^{2}\left ( \frac{1}{x_{q}}-\frac{1}{x_{d}} \right )=P_{2e}$，则上式可以简化为：P_{em}=P_{j}sin\theta +P_{2e}sin\theta \cdot cos\theta在电力系统稳定计算中，我们往往要由上式求出功率角$\theta$。我们可以使用几何方法求解，也可以利用迭代法求解该非线性方程。我们将上式变为：\theta =arcsin\frac{P_{em}}{P_{j}+P_{2e}cos\theta }以许实章编《电机学习题集》第367页26-1为例，将$P_{em}=1$，$P_{j}=1.878$，$P_{2e}=0.75$代入，得到方程：\theta =arcsin\frac{1}{1.878+0.75cos\theta }问题求解在《计算方法》第二章，我们学习了一些非线性方程的数值解法，这里我们分别使用几何法和迭代法求解上述问题并进行比较。设方程求解的预定精度为$0.001^{\circ}$即$1.7\times 10^{-3}rad$，由闭区间上连续函数的性质和初步估计，可确定方程的解位于区间$[0,\frac{\pi }{6}]$。几何方法由几何法的求解方法，我们定义函数：f(\theta )=arcsin\frac{1}{1.878+0.75cos\theta }-\theta求解$\theta$即求解$f(\theta )$在$[0,\frac{\pi }{6}]$上的零点。在MATLAB中，将该函数实现如下：123456function [y] = func(x)%几何法函数方程%统一使用弧度制format long %为提高精度，保留更多位数y = asin(1 / (1.878 + 0.75 * cos(x))) - x;end由于MATLAB计算过程中默认保留$4$位小数，为了提高精度，我使用了format long保留更多有效数字。值得注意的是，计算时统一使用弧度制，待求出解之后再使用rad2deg函数转化为角度。二分法根据二分法的格式，编写二分法函数如下：123456789101112131415161718192021222324252627function [errors, ans, time] = bisection(low, high, max, stopError) %二分法%func是待求零点的函数%low，high分别是解区间的上下限%max是最多循环步数，防止死循环%stopError是预定精度，作为终止条件%errors记录每次循环的误差%ans记录最终求解结果，表示为角度%time是总的循环次数format long %为提高精度，保留更多位数fl = func(low);fh = func(high);error = high - low;for i = 1 : max mid = (low + high ) / 2; fm = func(mid); if fm * fl &gt; 0 low = mid; else high = mid; enderror = high - low;errors(i) = error; if error &lt; stopError, break, endendtime = i;ans = rad2deg(mid); %转换成角度end输入命令[errorsB, ans, time] = bisection(0, pi / 6, 50, 1.7e-5)，求得结果如下：1234567891011121314151617181920212223errorsB = 列 1 至 5 0.261799387799149 0.130899693899575 0.065449846949787 0.032724923474894 0.016362461737447 列 6 至 10 0.008181230868723 0.004090615434362 0.002045307717181 0.001022653858590 0.000511326929295 列 11 至 15 0.000255663464648 0.000127831732324 0.000063915866162 0.000031957933081 0.000015978966540ans = 22.909240722656250time = 15使用二分法共迭代$15$次，求得结果为$22.909240722656250^{\circ}$。此外，迭代误差为$0.000015978966540$，符合预设精度要求。然而，此种方法计算次数较多，因此我又尝试了下面的方法。弦截法根据弦截法的计算方法，编写弦截法函数如下：1234567891011121314151617181920212223242526function [errors, ans, time] = linecut(a, b, max, stopError)%弦截法%func是待求零点的函数%a，b是在解的领域取的两点，这里取解区间的两个端点%max是最多循环步数，防止死循环%stopError是预定精度，作为终止条件%errors记录每次循环的误差%ans记录最终求解结果，表示为角度%time是总的循环次数format long %为提高精度，保留更多位数fa = func(a);fb = func(b);error = abs(a - b); for i = 1 : max x = b - (b - a) * fb / (fb - fa); a = b; b = x; fa = func(a); fb = func(b); error = abs(a - b); errors(i) = error; if error &lt; stopError, break, endendtime = i;ans = rad2deg(b); %转换成角度end输入命令[errorsL, ans, time] = linecut(0, pi / 6, 50, 1.7e-5)，求得结果如下：12345678910111213errorsL = 0.120609794441825 0.003164447033051 0.000026217912123 0.000000005427336ans = 22.909760215805360time = 4与上面的二分法相比，弦截法只需计算$4$次，效率大大提高。计算所得结果为$22.909760215805360^{\circ}$，迭代误差为$0.000000005427336$，符合预设精度要求，而且比二分法的最终迭代误差更小，显然可以发现弦截法的收敛速度要快于二分法。下面我再用弦截法的改造方法Steffensen方法进行试验。Steffensen方法根据Steffensen方法的计算方法，编写函数如下：12345678910111213141516171819202122function [errors, ans, time] = steffensen(x, max, stopError)%Steffensen方法%func是待求零点的函数%x是初始点%max是最多循环步数，防止死循环%stopError是预定精度，作为终止条件%errors记录每次循环的误差%ans记录最终求解结果，表示为角度%time是总的循环次数format long %为提高精度，保留更多位数f = func(x);for i = 1 : max o = x - f ^ 2 / (f - func(x - f)); error = abs(x - o); x = o; f = func(o); errors(i) = error; if error &lt; stopError, break, endendtime = i;ans = rad2deg(o); %转换成角度end选取初始点为$0$，输入命令[errorsS1, ans, time] = steffensen(0, 50, 1.7e-5)，求得结果如下：12345678910111213errorsS1 = 0.381516143583955 0.018291709371805 0.000042893415627 0.000000000236814ans = 22.909760215804820time = 4发现计算次数没有比弦截法少，因此修改初值为$\frac{\pi }{6}$，再次计算，得结果：12345678910111213errorsS2 = 0.125855705283236 0.002107105082521 0.000000571210576ans = 22.909760215802425time = 3一般而言，Steffensen方法的收敛速度要快于弦截法，但这与初始点的选取有关。对于这个问题，当设初始点为$0$时，Steffensen方法的迭代次数与弦截法持平；当设初始点为$\frac{\pi }{6}$时，迭代次数才小于弦截法。因此，想让Steffensen方法更快地收敛，需选取合适的初始点。在我看来，Steffensen方法的一大优势就是它是一种单步迭代方法，相比二步迭代方法的弦截法，Steffensen方法只需要一个初值就可以开始迭代。Picard迭代法上述的方法都是基于几何图形的求解方法，而下面的Picard迭代法则是基于不动点原理给出的。首先，我们编写迭代函数：123456function [y] = interation(x)%迭代函数%统一使用弧度制format long %为提高精度，保留更多位数y = asin(1 / (1.878 + 0.75 * cos(x)));end我们使用如下命令查看函数图像：123&gt;&gt; x = 0 : pi / 36 : pi / 6;&gt;&gt; y = asin(1 * (1.878 + 0.75 * cos(x)) .^ (-1));&gt;&gt; plot(x, y)得到：易验证，该函数在$[0,\frac{\pi }{6}]$上满足Picard迭代条件。Picard迭代法根据Picard迭代法原理，定义Picard迭代函数：1234567891011121314151617181920function [errors, ans, time] = picard(x, max, stopError)%Picard迭代法%interation是迭代函数%x是初始点%max是最多循环步数，防止死循环%stopError是预定精度，作为终止条件%errors记录每次循环的误差%ans记录最终求解结果，表示为角度%time是总的循环次数format long %为提高精度，保留更多位数for i = 1 : max o = interation(x); error = abs(x - o); x = o; errors(i) = error; if error &lt; stopError, break, endendtime = i;ans = rad2deg(o); %转换成角度end输入命令[errorsP, ans, time] = picard(0, 50, 1.7e-5)求解：12345678910111213errorsP = 0.390355832559508 0.009044503640668 0.000428788831489 0.000020583068808 0.000000988625736ans = 22.909757357777192time = 5Picard迭代结果为$22.909757357777192^{\circ}$，且精度符合要求。下面使用Picard迭代法的改进Aitken加速迭代法进行试验。Aitken加速迭代法编写Aitken加速迭代法函数代码如下：12345678910111213141516171819202122function [errors, ans, time] = aitken(x, max, stopError)%Aitken迭代法%interation是迭代函数%x是初始点%max是最多循环步数，防止死循环%stopError是预定精度，作为终止条件%errors记录每次循环的误差%ans记录最终求解结果，表示为角度%time是总的循环次数format long %为提高精度，保留更多位数for i = 1 : max x1 = interation(x); x2 = interation(x1); x3 = x2 - (x2 - x1) ^ 2 / (x2 - 2 * x1 + x); error = abs(x3 - x); x = x3; errors(i) = error; if error &lt; stopError, break, endendtime = i;ans = rad2deg(x); %转换成角度end输入命令[errorsA, ans, time] = aitken(0, 50, 1.7e-5)求解得到：12345678910111213errorsA = 0.399614867056990 0.000235879375045 0.000000000176165ans = 22.909760215804827time = 3可以看到，Aitken加速迭代法仅需3次迭代就得到了符合条件的解，且它的迭代误差只用$0.000000000176165$，小于上述所有的方法，由此该方法的优势得以体现。Newton迭代法Newton迭代法也是一种求解非线性方程的高效算法，因此我也对其进行实现。这里要用到func的导数，经计算，编写func的导函数为程序df：123456function [y] = df(x)%求导数%统一使用弧度制format longy = (0.75 * sin(x) / (1 - (1 / (1.878 + 0.75 * cos(x))) ^ 2) ^ 0.5) / (1.878 + 0.75 * cos(x)) ^ 2 - 1;endNewton迭代法编写Newton迭代法的计算程序如下：1234567891011121314151617181920function [errors, ans, time] = newton(x, max, stopError)%Newton迭代法%func是待求零点的函数%x是初始点%max是最多循环步数，防止死循环%stopError是预定精度，作为终止条件%errors记录每次循环的误差%ans记录最终求解结果，表示为角度%time是总的循环次数format long %为提高精度，保留更多位数for i = 1 : max o = x - func(x) / df(x); error = abs(o - x); x = o; errors(i) = error; if error &lt; stopError, break, endendtime = i;ans = rad2deg(x); %转换成角度end输入命令[errorsN, ans, time] = newton(0, 50, 1.7e-5)进行计算，得解：12345678910111213errorsN = 0.390355832559508 0.009488988787132 0.000005925259246ans = 22.909760215672179time = 3Newton下山法为尽可能避免因初值选取不当导致计算过程缓慢收敛或者发散（经上面计算，此问题不存在这种情况），引入下山因子$\lambda \in (0,1]$，得到改进后的Newton下山法，其计算程序如下：12345678910111213141516171819function [errors, ans, time] = hill(x, max, stopError)%Newton下山法% 此处显示详细说明format long %为提高精度，保留更多位数l = 1;for i = 1 : max o = x - l * func(x) / df(x); while abs(func(o)) &gt; abs(func(x)) %不满足下山条件 l = l / 2; o = x - l * func(x) / df(x); end error = abs(o - x); x = o; errors(i) = error; if error &lt; stopError, break, endendtime = i;ans = rad2deg(x); %转换成角度end计算得到结果如下：12345678910111213errorsH = 0.390355832559508 0.009488988787132 0.000005925259246ans = 22.909760215672179time = 3由于此问题初值选取得当，即初值取$0$时使用一般Newton迭代法不存在缓慢收敛或者发散的问题，因此Newton下山法在这里并没有发挥作用。可以看到，Newton迭代法仅$3$步就完成了求解的任务，非常高效。问题的解以上方法都得到了一致的答案，根据精度要求，我们得出此条件下功率角$\theta$为$22.910^{\circ}$，与许实章编《电机学习题集》中的例题答案$22.9^{\circ}$相吻合。方法比较将上述各种方法的误差记录绘制成图表，由于Newton下山法在该问题中没有发挥作用，因此仅作出Newton迭代法的迭代误差图像。根据图片，我们可以观察到以上各个方法均收敛。其中，表现较为优越的有Aitken加速迭代法、Newton迭代法和Steffensen迭代法，均用了$3$次迭代就达到了精度要求。然而这里Steffensen法的收敛速度更依赖于初值的选取，当初值选为$0$时，它的收敛速度就类似于弦截法在该问题上的收敛速度了。因此，总的来说，对于这个问题，最高效的算法是Aitken加速迭代法和Newton迭代法。另外，二分法最简单却也最低效，迭代了$15$次才达到预设的精度要求。可见，各种算法的效率大致上与它们的复杂度和高级程度成正比关系。]]></content>
      <categories>
        <category>程序与设计</category>
      </categories>
      <tags>
        <tag>matlab</tag>
        <tag>代码实现</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[verilog笔记：数值比较器实现与vivado使用]]></title>
    <url>%2Fverilog20191115233116%2F</url>
    <content type="text"><![CDATA[本周数字电路课程老师布置了一个利用verilog语言进行数值比较器波形仿真的作业。可以利用Modelsim或者Vivado实现。由于Vivado默认安装大小就有将近30个GB（2018版好像是27GB左右，2014版是12.68GB，这版本的容量增速跟maltab有得一拼啊），因此之前装了之后不太会使用便又卸了。最近刚好趁着双十一降价给自己的laptop加了一个SSD，因此正好赶快学习一下如何使用。有关如何给笔记本加装SSD的问题，这里有两个视频可以解决，安装准备与步骤、安装后点亮磁盘。References：电子文献：https://blog.csdn.net/qq_41154156/article/details/80989125https://wenku.baidu.com/view/0294cbb3bb4cf7ec4bfed01a.html关于vivado相比于Modelsim，Vivado的UI还是要舒服许多的，有点像Multisim之于Pspice。关于Vivado的使用，上面参考的文章中的步骤比较详细，照做一遍之后基本就会了。1位数值比较器1位数值比较器的逻辑图如下：使用verilog代码实现：1234567891011121314module _1bit_Comp( input A, input B, output AGB, output AEB, output ALB ); wire Anot, Bnot; not n0(Anot, A), n1(Bnot, B); and n2(AGB, A, Bnot), n3(ALB, Anot, B); nor n4(AEB, AGB, ALB);endmodule为了输出仿真波形，新建一个仿真文件：123456789module simulateFile(); reg A, B; wire AGB, AEB, ALB; _1bit_Comp u1(A, B, AGB, AEB, ALB); initial begin A = 0; B = 0; end always #50 &#123;A, B&#125; = &#123;A, B&#125; + 1;endmodule其中，过程赋值语句always只能给寄存器类型变量赋值，因此，在这里A、B要定义为reg类型。这里“#50”表示延时，使用{A, B}使AB变成二进制数，方便生成所有不同的输入，在这里即00、01、10、11。Run Simulation，输出波形：2位数值比较器2位数值比较器的逻辑图如下：使用verilog代码，调用1位数值比较器，实现2位数值比较器如下：12345678910111213141516171819module _2bit_Comp( input A1, input A0, input B1, input B0, output FAGB, output FAEB, output FALB ); wire AGB1, AEB1, ALB1, AGB0, AEB0, ALB0; //signal inside wire G1O, G2O; //the output of and gate G1, G2 _1bit_Comp C1(A1, B1, AGB1, AEB1, ALB1); //Instantiate 1-bit Comparator _1bit_Comp C0(A0, B0, AGB0, AEB0, ALB0); and G1(G1O, AEB1, AGB0), G2(G2O, AEB1, ALB0), G3(FAEB, AEB1, AEB0); or G4(FAGB, AGB1, G1O); or G5(FALB, ALB1, G2O);endmodule可以使用RTL ANALYSIS来仿真出2位数值比较器的RTL schematic电子原理图。类似的，编写仿真文件：123456789module simulateAgain(); reg A1, A0, B1, B0; wire FAGB, FAEB, FALB; _2bit_Comp u2(A1, A0, B1, B0, FAGB, FAEB, FALB); initial begin A1 = 0; A0 = 0; B1 = 0; B0 = 0; end always #30 &#123;A1, A0, B1, B0&#125; = &#123;A1, A0, B1, B0&#125; + 1;endmoduleRun Simulation，输出波形：出现的问题报错：[Synth 8-439] module’xxx’not found当初遇到这个问题后，我的第一反应是上网搜索原因。得到的解释有模块未添加、IP未正确设置等。对照网上的解决方案之后，我发现除了我无法理解的，网上所述的问题我都不存在。于是我只好独立进行思考。果然，我还犯不了像网上那样“高级”的错误。错误的原代码如下：12not n0(Anot, A); n1(Bnot, B);报错：[Synth 8-439] module’n1’not found。当我调用门的时候，由于内部变量换行导致我将逗号误用成了分号，因此导致分号之后的变量not found，修改后错误即可解决。其实，这个问题仔细观察即可发觉，相比于n1，同样格式的n0就没有报错，那么很有可能错误就在两者之间。ERROR: [Common 17-39] ‘xxx’ failed due to earlier errors这是我在执行仿真文件时遇到的error。仔细检查后，发现错误也与上一个问题相同。由于我在设计完电路后没有Run Synthesis综合并生成网表文件来进行检验，也没有进行其它的仿真操作，因此之前并没能发现这个问题。于是最后当调用该电路的仿真文件开始运行时就会报告这样的错误。要注意的点和matlab中函数文件的要求类似，verilog定义模块时，需要新建的模块文件名称与模块的文件名称一致。例如，我上面的1位数值比较器module名为_1bit_Comp，那么对应的文件名就应该是_1bit_Comp.v。此外，每个模块应使用一个文件来表示，且一个文件最多能表示一个模块（可以在其中调用其它模块，这点和matlab很像），两者呈一一对应关系。新建project时，如要从RTL代码开始综合，就选择RTL project（默认的这个）。要注意的是，下面的“Do not specify sources at this time”（此时不定义源文件）可以勾上。否则，下一步会进入添加source file。如果在一个project中已经建立了一个仿真文件，那么当你新建一个仿真文件时，需要建立在create的new file内，这样在后面对不同的仿真文件进行仿真时可以将对应的文件夹依次分别激活。在source窗口中，一般情况下，Vivado会自动加粗识别出来的top module，同时对应module名称前面也会有一个二叉树状的图标表示这是顶层模块。有时候，软件也会识别错误或者与实际需求不符，这时候我们可以右键想要置顶的module，在弹出的菜单中点击Set As Top将其设为顶层。当在同一个project中创建了多个仿真文件时，如要在进行完一次仿真之后对另一个仿真文件，需要对对应的文件夹进行激活。方法是右键仿真文件，然后在弹出的菜单中点击Make Active即可。加装固态盘前文提到给笔记本加装SSD，给了两个示范视频，这里我还是想再稍稍补充一下关于加装固态盘的一些事情。首先必须确保自己的本有空位。我使用的是小电池版本，因此有一整个2.5英寸7mm的硬盘位，这个请在决定购买新的硬盘前和卖家自己核对确认。如果还是不放心，那么最好亲自拆开查看，眼见为实嘛。要注意的是，必须使用完全对应规格的螺丝刀（比如我使用的是梅花T5螺丝刀），否则很容易发生滑丝，即螺牙连接处由于受力过大或其它原因导致螺牙磨损而使螺牙无法咬合，这会为今后的拆机带来不必要的麻烦。上面是我买的固态盘和数据线，相同或者类似机型的可以参考一下。在到货之后我发现，我所买的闪迪SSD较7mm稍薄些，因此平时拿动时（一般较大幅度翻动laptop时）会感到里面有东西松动的响声，不过使用至今没遇到任何问题。另外，为了适配各类硬盘，数据线的长度也有可能不能完全匹配，其实也没有关系，稍稍用力将数据线对应接头按入SATA3接口并用架子将固态盘固定即可。相比移动硬盘的USB接口，内置固态的SATA3读取速度还是相当不错的。]]></content>
      <categories>
        <category>程序与设计</category>
      </categories>
      <tags>
        <tag>踩坑血泪</tag>
        <tag>数字电路</tag>
        <tag>代码实现</tag>
        <tag>verilog</tag>
        <tag>Vivado</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[markdown笔记：写报告时的一些应用]]></title>
    <url>%2Fmarkdown20191110194256%2F</url>
    <content type="text"><![CDATA[在上一篇matlab笔记：久未使用之后踩的一堆坑中，我用matlab完成了实验，接下来就是写报告了。然而写博客用惯了markdown，现在极其嫌弃word文档。考虑到这次报告中大量的公式及代码，word文档的观感可想而知，因此果断决定使用markdown来书写实验报告。那么这篇文章就及时跟进一下我在写报告的时候发现的一些之前没注意的新应用吧。References：电子文献：https://blog.csdn.net/m0_37925202/article/details/80461714https://www.w3school.com.cn/tags/att_p_align.asp居中文字之前写markdown的时候一直没有考虑到要把文字居中，而这回报告的标题就有这个要求了。实现的方法有如下两种。1&lt;center&gt;这样就居中了&lt;/center&gt;1&lt;p align="center"&gt;这样就居中了&lt;/p&gt;此外html中的&lt;p&gt;标签的align属性还有其它的用法：1234&lt;p align="left"&gt; &lt;!--左对齐--&gt;&lt;p align="right"&gt; &lt;!--右对齐--&gt;&lt;p align="center"&gt; &lt;!--居中--&gt;&lt;p align="justify"&gt; &lt;!--对行进行伸展，使每行都可以有相等的长度（就像在报纸和杂志中）--&gt;转pdf文件考虑到markdown文件不能作为最终的报告提交，我就想办法将markdown转化为pdf。我首先找到了VScode里面的markdown PDF扩展，然而安装之后有一个东西一直安装失败。这里我想起了之前我在markdown笔记：markdown的基本使用中强烈推荐过的typora（我的报告最后就是用它书写的），打开之后，果然没有让我失望。直接点击“文件”“导出”选择PDF，瞬间就完成了pdf文件的转换。此外，typora提供的导出格式实在是丰富，虽然导出的word与原markdown文件的颜值有少许降低，但还是不得不赞叹typora的实用！行间公式刚开始使用typora时，我遇到了一个疑惑：似乎typora不能插入行内公式块。在查找相关资料之后，我终于找到了解决的方案。点击“文件”“偏好设置”，把markdown扩展语法中的内联公式项打上勾。补充：当使用$夹着文字而非LaTex格式的公式时，字体会发生变化，如$我会变$我没变，呈现的效果是：$我会变$我没变。插入图片typora插入图片的方法与hexo中类似，也是可以创建一个文件夹存放图片，然后在偏好设置里进行设置。当指定路径之后，typora存放图片的文件夹名可以与markdown文件的名字不一致，而hexo中则需要一致才能够直接用图片名调用。此外，还是在偏好设置中，我们可以调整字体，我一般使用的是16px。]]></content>
      <categories>
        <category>操作和使用</category>
      </categories>
      <tags>
        <tag>markdown</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[matlab笔记：久未使用之后踩的一堆坑]]></title>
    <url>%2Fmatlab20191110193640%2F</url>
    <content type="text"><![CDATA[本周科学计算引论结课了，就花了一整天时间把要求的实验报告写了。根据考核说明，算法可以使用各种工具、语言来实现，但由于这门课程的上机实验统一使用的是matlab，再加上我上一次使用matlab大量实践是在劳动节的数学建模华中赛了，还是很有必要重拾起来再熟悉一下。于是乎，一大波坑就等着我去填了。References：电子文献：https://blog.csdn.net/zhanshen112/article/details/79728887报错：未找到具有匹配签名的构造函数我编写了一个二分法的函数求解非线性方程，然而当我调用的时候，却遇到报错：“未找到具有匹配签名的构造函数”，这是怎么回事呢？我们来冷静分析一下。识破！原来是我定义的函数名为half，而half也是matlab自带的函数之一，可以使用help functionname查看函数具体的使用方法与功能。调用时默认优先使用自带的函数，因此修改函数名为matlab自带函数之外的即可。报错：矩阵维度必须一致这是我在画图时遇到的一个问题，首先先补充一下matlab中作函数图像的方法，如下图所示。当我尝试使用上述方法作简单的函数图像时，并没有报错，而当我想要作出我实验所需函数（如下所示）的图像时，却出现了“矩阵维度必须一致”的错误信息。1y = asin(1 / (1.878 + 0.75 * cos(x)))查阅相关资料，我才发现是乘除的时候出现的问题，为此我将其中的除/修改为乘*，并用.^代替^作乘方计算，即将原函数式修改为：1y = asin(1 * (1.878 + 0.75 * cos(x)) .^ (-1))这样，问题就解决了。这里我想强调一下在matlab中^与.^的区别：.^是点乘，而^是乘法。直接用^进行乘法的话，在这里即矩阵乘法，也就是说，必须满足前一个矩阵的列数等于后一个矩阵的行数。而使用.^点乘操作，是使每一个元素相乘，也就是向量或者矩阵中对应元素相乘，也很好记忆，加个点就是点乘。函数输出只有一个这是一个很愚蠢的问题，显然我好久没用了，因为matlab不必使用return返回结果，在函数声明的第一句就确定了返回值的数量和顺序。因此在调用函数的时候，必须也提供对应的变量去接收返回值，否则只能得到第一个返回的元素。使用diff求导不是导数值用惯了pytorch，总想着能够自动求导，一查matlab还真有这么一个函数，即diff函数。然而，事实证明它不是我想要的。我们可以在命令行中使用这一个函数：声明变量x：syms x。它代表着声明符号变量x，只有声明了符号变量才可以进行符号运算，包括求导。定义一个需要求导的函数：f(x) = sin(x) + x ^ 2。使用diff函数求导：diff(f(x))。也可以对已经定义好的m文件中的函数直接求导。这里，我们会得到ans为2*x + cos(x)。如果想pretty一些，可以使用pretty函数将结果转化成书面格式：pretty(ans)。然而，当我代入不同的具体数值想得到函数的导数值的时候，发现输出的结果却是0。使用help查阅diff函数的用法，得到的说明是：1234此MATLAB函数计算沿大小不等于1的第一个数组维度的X相邻元素之间的差分： Y = diff(X) Y = diff(X,n) Y = diff(X,n,dim)原来这个函数的主要用法是对向量或者矩阵中的元素进行差分计算，当用它来求导时，得到的只是一个表达式，且函数一但复杂，得到的就是一个参数众多的逼近格式。唉，总而言之，还是老老实实地拿起笔自己算出导函数吧。都用矩阵实验室（matlab）了，手动求个导还是得会的呀。角度制弧度制互换无论使用计算器还是编程计算，这都是一个需要注意的点，matlab默认使用的是弧度制，在计算出结果之后，可以使用rad2deg函数进行转换。同样的，我推测角度制转弧度制的函数名为deg2rad，一试，果不其然。这个函数还是挺好记的，英文里有许多同音词与字符的妙用，比如这里的to和2的two，还有at和@，感觉既方便又高级。保留更多位数matlab默认是保留4位有效数字，为了提升计算精度，可以使用format long来增加计算过程中保留的位数。常用操作难得开一篇写matlab使用的博文，那就在这补上几个我记忆中的较为常用的命令或操作。Ctrl+C终止操作。这跟许多地方都一样，在matlab中，Ctrl+C平时可以用来粘贴剪切板上的内容，而在程序运行时，可以使用它来终止运行，这在死循环的时候非常有用。clc清空命令行。bench测试性能。这其实不是一个很常用的命令，可以跟朋友输这个命令看看自己电脑的性能，下图是我的结果。需要注意的是，笔记本电脑电池使用模式的不同对这个排名影响还是挺大的，如果想让排名高一些的话，请确保电池开在最佳性能的模式。另外，不同的电脑的比较对象可能会不一样，比如学校的台式机和我的笔记本在这里比较的对象就不一样。 常用的还有许多，以后在使用中不断增加，先在这占个位。]]></content>
      <categories>
        <category>程序与设计</category>
      </categories>
      <tags>
        <tag>踩坑血泪</tag>
        <tag>matlab</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[front end笔记：使用div实现居中显示]]></title>
    <url>%2Ffront-end20191110165901%2F</url>
    <content type="text"><![CDATA[最近心血来潮花了好久给自己的博客添加了一个粒子时钟，最后想要使它在sidebar中居中显示废了我好大功夫，为了以后不在这上面浪费时间，我决定浪费现在的时间把这个问题记下来。References：电子文献：https://www.jianshu.com/p/f0bffc42c1cehttps://blog.csdn.net/chwshuang/article/details/52350559swig由于我要将我的时钟显示在侧边栏，需要插入到header.swig文件中。hexo博客的源码中有大量这个格式的文件，然而具体的使用方法我也不是很清楚。在查阅了一些文章之后，我对swig有了一个初步的认知。swig是一个JS前端模板引擎，它有如下特点：支持大多数主流浏览器。表达式兼容性好。面向对象的模板继承。将过滤器和转换应用到模板中的输出。可根据路劲渲染页面。支持页面复用。支持动态页面。可扩展、可定制。可以通过VSchart将swig与其它前端模板框架进行对比，这个网站由维基支持，可以在里面进行各种对比，非常有意思，在这里推荐一下。关于swig的基本用法，可以在我文首的第一个参考链接中找到，个人认为不搞前端的话大概率是用不到的。原本的方法当我添加完时钟本地测验时，我发现添加的时钟在侧边栏的位置没有居中。根据之前学习html的记忆，我尝试了使用&lt;p align=center&gt;&lt;/p&gt;标签包裹我的插入语句，然而并没有达到想要的效果。使用div实现居中为了达到上述的目的，我使用&lt;div&gt;标签来分割出块，并使用div的属性来实现居中显示的效果。123&lt;div style="Text-align:center;width:100%;"&gt; &#123;% include '../_custom/clock.swig' %&#125;&lt;/div&gt;使用nav实现隐藏本以为大功告成，结果在移动端查看时，发现竖屏显示效果非常煞风景。其实在电脑端浏览器也可以预览移动端的效果，方法很简单，就是直接将浏览器窗口小化，减小两边间距，网页就会自动变成竖屏显示的状态（除非没有）。此外，当我们F12检查元素或者查看源时，网页也会被挤到一侧从而变成竖屏显示的状态。注：这里补充一下检查元素和查看源之间的区别，一般的浏览器右键都会有这两个功能，表面上看起来似乎也差不多，但是它们还是有区别的。检查元素看的是渲染过的最终代码，可以做到定位网页元素、实时监控网页元素属性变化的功能，可以及时调试、修改、定位、追踪检查、查看嵌套，修改样式和查看js动态输出信息。这让我想起了自己当初就是这样直接修改四级成绩，然后骗朋友的，不知道的人还真的想不出这原因，就以为的确是真的啦哈哈。另一方面，查看源只是把网页输出的源代码，即就是别人服务器发送到浏览器的原封不动的代码直接打开，既不能动态变化，也不能修改。为了解决这个问题，追求美观，我就想到可以把时钟和标签、归档、分类等菜单中的索引一起，在竖屏状态下不点击时就不显示。在分析了header.swig中菜单部分的源码之后，我注意到一个标签&lt;nav&gt;，它是是HTML5的新标签，可以标注一个导航链接的区域。于是，我将插入时钟的语句移入nav所包裹的块中，就完美达到了我的需求。意外的问题在我写这篇博文的时候，出现了一个奇怪的问题。每当我想要本地预览（部署应该也会出现这个问题），都会报错：“Nunjucks Error: [Line 17, Column 239] unexpected token: }”，这就让我非常的苦恼。根据错误信息，我开始一个一个寻找我文中的花括号。在反复删减和搜索相关问题之后，我发现是我插入在行间的一个include的swig语句惹的祸（我要是写出来又报错，插入在段间就没问题，可以到上文找）。这类异常一般是文章中使用了大括号{}这个特殊字符，且没有转义导致编译不通过，解决的办法是使用&amp;#123; &amp;#125;对大的花括号进行转换。补充：小的圆括号可用&amp;#40; &amp;#41;进行转换。没有这类问题当然再好不过啦，如果出现了，可以试试上面的方法。这类涉及转义的符号还是得熟悉其规则，避免老是出错。]]></content>
      <categories>
        <category>程序与设计</category>
      </categories>
      <tags>
        <tag>前端</tag>
        <tag>踩坑血泪</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[kaggle笔记：手写数字识别——使用KNN和CNN尝试MNIST数据集]]></title>
    <url>%2Fkaggle20191102112435%2F</url>
    <content type="text"><![CDATA[kaggle是一个著名的数据科学竞赛平台，暑假里我也抽空自己独立完成了三四个getting started级别的比赛。对于MNIST数据集，想必入门计算机视觉的人应该也不会陌生。kaggle上getting started的第一个比赛就是Digit Recognizer：Learn computer vision fundamentals with the famous MNIST data。当时作为入门小白的我，使用了入门级的方法KNN完成了我的第一次机器学习（自认为KNN是最最基础的算法，对它的介绍可见我的另一篇博文machine-learning笔记：机器学习的几个常见算法及其优缺点，真的非常简单，但也极其笨拙）。而最近我又使用CNN再一次尝试了这个数据集，踩了不少坑，因此想把两次经历统统记录在这，可能会有一些不足之处，留作以后整理优化。References：电子文献：https://blog.csdn.net/gybinlero/article/details/79294649https://blog.csdn.net/qq_43497702/article/details/95005248https://blog.csdn.net/a19990412/article/details/90349429KNN首先导入必要的包，这里基本用不到太多：1234567import numpy as npimport csvimport operatorimport matplotlibfrom matplotlib import pyplot as plt%matplotlib inline导入训练数据：12345678910trainSet = []with open('train.csv','r') as trainFile: lines=csv.reader(trainFile) for line in lines: trainSet.append(line) trainSet.remove(trainSet[0])trainSet = np.array(trainSet)rawTrainLabel = trainSet[:, 0] #分割出训练集标签rawTrainData = trainSet[:, 1:] #分割出训练集数据我当时用了一种比较笨拙的办法转换数据类型：12345678910111213rawTrainData = np.mat(rawTrainData) #转化成矩阵，或许不需要m, n = np.shape(rawTrainData)trainData = np.zeros((m, n)) #创建初值为0的ndarrayfor i in range(m): for j in range(n): trainData[i, j] = int(rawTrainData[i, j]) #转化并赋值rawTrainLabel = np.mat(rawTrainLabel) #或许不需要m, n = np.shape(rawTrainLabel)trainLabel = np.zeros((m, n))for i in range(m): for j in range(n): trainLabel[i, j] = int(rawTrainLabel[i, j])这里我们可以查看以下数据的维度，确保没有出错。为了方便起见，我们把所有pixel不为0的点都设置为1。12345m, n = np.shape(trainData)for i in range(m): for j in range(n): if trainData[i, j] != 0: trainData[i, j] = 1仿照训练集的步骤，导入测试集并做相同处理：12345678910111213141516171819202122testSet = []with open('test.csv','r') as testFile: lines=csv.reader(testFile) for line in lines: testSet.append(line) testSet.remove(testSet[0])testSet = np.array(testSet)rawTestData = testSetrawTestData = np.mat(rawTestData)m, n = np.shape(rawTestData)testData = np.zeros((m, n))for i in range(m): for j in range(n): testData[i, j] = int(rawTestData[i, j])m, n = np.shape(testData)for i in range(m): for j in range(n): if testData[i, j] != 0: testData[i, j] = 1同样的，可使用testData.shape查看测试集的维度，保证它是28000*784，由此可知操作无误。接下来，我们定义KNN的分类函数。12345678910111213141516def classify(inX, dataSet, labels, k): inX = np.mat(inX) dataSet = np.mat(dataSet) labels = np.mat(labels) dataSetSize = dataSet.shape[0] diffMat = np.tile(inX, (dataSetSize, 1)) - dataSet sqDiffMat = np.array(diffMat) ** 2 sqDistances = sqDiffMat.sum(axis = 1) distances = sqDistances ** 0.5 sortedDistIndicies = distances.argsort() classCount=&#123;&#125; for i in range(k): voteIlabel = labels[0, sortedDistIndicies[i]] classCount[voteIlabel] = classCount.get(voteIlabel, 0) + 1 sortedClassCount = sorted(classCount.iteritems(), key = operator.itemgetter(1), reverse = True) return sortedClassCount[0][0]为了更好地分类，这里我们需要选择合适的k值，我选取了4000个样本作为验证机进行尝试，找到误差最小的k值并作为最终的k值输入。1234567891011121314151617181920212223242526272829303132333435trainingTestSize = 4000#分割出验证集m, n = np.shape(trainLabel)trainingTrainLabel = np.zeros((m, n - trainingTestSize))for i in range(m): for j in range(n - trainingTestSize): trainingTrainLabel[i, j] = trainLabel[i, j] trainingTestLabel = np.zeros((m, trainingTestSize))for i in range(m): for j in range(trainingTestSize): trainingTestLabel[i, j] = trainLabel[i, n - trainingTestSize + j] m, n = np.shape(trainData)trainingTrainData = np.zeros((m - trainingTestSize, n))for i in range(m - trainingTestSize): for j in range(n): trainingTrainData[i, j] = trainData[i, j] trainingTestData = np.zeros((trainingTestSize, n))for i in range(trainingTestSize): for j in range(n): trainingTestData[i, j] = trainData[m - trainingTestSize + i, j]#使k值为3到9依次尝试training = []for x in range(3, 10): error = 0 for y in range(trainingTestSize): answer = (classify(trainingTestData[y], trainingTrainData, trainingTrainLabel, x)) print 'the classifier came back with: %d, %.2f%% has done, the k now is %d' % (answer, (y + (x - 3) * trainingTestSize) / float(trainingTestSize * 7) * 100, x) #方便知道进度 if answer != trainingTestLabel[0, y]: error += 1 training.append(error)这个过程比较长，结果会得到training的结果是[156, 173, 159, 164, 152, 155, 156]。可以使用plt.plot(training)更直观地查看误差，呈现如下：注意：这里的下标应该加上3才是对应的k值。可以看图手动选择k值，但由于事先无法把握训练结束的时间，可以编写函数自动选择并使程序继续进行。123456theK = 3hasError = training[0]for i in range(7): if training[i] &lt; hasError: theK = i + 3 hasError = training[i]在确定k值后，接下来就是代入测试集进行结果的计算了。由于KNN算法相对而言比较低级，因此就别指望效率了，跑CPU的话整个过程大概需要半天左右。123456m, n = np.shape(testData)result = []for i in range(m): answer = (classify(testData[i], trainData, trainLabel, theK)) result.append(answer) print 'the classifier came back with: %d, %.2f%% has done' % (answer, i / float(m) * 100)最后，定义一个保存结果的函数，然后saveResult(result)之后，再对csv文件进行处理（后文会提到），然后就可以submit了。1234567def saveResult(result): with open('result.csv', 'w') as myFile: myWriter = csv.writer(myFile) for i in result: tmp = [] tmp.append(i) myWriter.writerow(tmp)最终此方法在kaggle上获得的score为0.96314，准确率还是挺高的，主要是因为问题相对简单，放到leaderboard上，这结果的排名就要到两千左右了。CNN在学习了卷积神经网络和pytorch框架之后，我决定使用CNN对这个比赛再进行一次尝试。首先还是导入相关的包。1234567891011121314151617import torchimport torch.nn as nnimport torch.nn.functional as Fimport torch.optim as optimimport pandas as pdimport numpy as npfrom math import *%matplotlib inlineimport matplotlib.pyplot as pltimport matplotlib.cm as cmimport torch.utils.data as Datafrom torch.autograd import Variableimport csv导入训练数据，可以使用train.head()查看导入的结果，便于后续的处理。1train = pd.read_csv("train.csv")对数据进行处理，由于要使用的是CNN，我们必须要把数据整理成能输入的形式，即从数组变成高维张量。12345train_labels = torch.from_numpy(np.array(train.label[:]))image_size = train.iloc[:, 1:].shape[1]image_width = image_height = np.ceil(np.sqrt(image_size)).astype(np.uint8)train_data = torch.FloatTensor(np.array(train.iloc[:, 1:]).reshape((-1, 1, image_width, image_height))) / 255 #灰度压缩，进行归一化注：reshape中的-1表示自适应，这样我们能让我们更好的变化数据的形式。我们可以使用matplotlib查看数据处理的结果。123plt.imshow(train_data[1].numpy().squeeze(), cmap = 'gray')plt.title('%i' % train_labels[1])plt.show()可以看到如下图片，可以与plt.title进行核对。注：可以用squeeze()函数来降维，例如：从[[1]]—&gt;[1]。与之相反的是便是unsqueeze(dim = 1)，该函数可以使[1]—&gt;[[1]]。以同样的方式导入并处理测试集。12test= pd.read_csv("test.csv")test_data = torch.FloatTensor(np.array(test).reshape((-1, 1, image_width, image_height))) / 255接下来我们定义几个超参数，这里将要使用的是小批梯度下降的优化算法，因此定义如下：1234#超参数EPOCH = 1 #整个数据集循环训练的轮数BATCH_SIZE = 10 #每批的样本个数LR = 0.01 #学习率定义好超参数之后，我们使用Data对数据进行最后的处理。1234567trainData = Data.TensorDataset(train_data, train_labels) #用后会变成元组类型train_loader = Data.DataLoader( dataset = trainData, batch_size = BATCH_SIZE, shuffle = True)上面的Data.TensorDataset可以把数据进行打包，以方便我们更好的使用；而Data.DataLoade可以将我们的数据打乱并且分批。要注意的是，这里不要对测试集进行操作，否则最终输出的结果就难以再与原来的顺序匹配了。接下来，我们定义卷积神经网络。1234567891011121314151617181920212223242526272829303132333435#build CNNclass CNN(nn.Module): def __init__(self): super(CNN, self).__init__() #一个卷积层 self.conv1 = nn.Sequential( nn.Conv2d( #输入(1, 28, 28) in_channels = 1, #1个通道 out_channels = 16, #输出层数 kernel_size = 5, #过滤器的大小 stride = 1, #步长 padding = 2 #填白 ), #输出(16, 28, 28) nn.ReLU(), nn.MaxPool2d(kernel_size = 2), #输出(16, 14, 14) ) self.conv2 = nn.Sequential( #输入(16, 14, 14) nn.Conv2d(16, 32, 5, 1, 2), #这里用了两个过滤器，将16层变成了32层 nn.ReLU(), nn.MaxPool2d(kernel_size = 2) #输出(32, 7, 7) ) self.out = nn.Linear(32 * 7 * 7, 10) #全连接层，将三维的数据展为2维的数据并输出 def forward(self, x): #父类已定义，不能修改名字 x = self.conv1(x) x = self.conv2(x) x = x.view(x.size(0), -1) output = F.softmax(self.out(x)) return outputcnn = CNN()optimzer = torch.optim.Adam(cnn.parameters(), lr = LR) #define optimezerloss_func = nn.CrossEntropyLoss() #loss function使用交叉嫡误差print(cnn) # 查看net architecture完成以上的操作之后，就可以开始训练了，整个训练时间在CPU上只需要几分钟，这比KNN算法要优越许多。123456789101112for epoch in range(EPOCH): for step, (x, y) in enumerate(train_loader): b_x = Variable(x) b_y = Variable(y) output = cnn(b_x) loss = loss_func(output, b_y) #cross entropy loss #update W optimzer.zero_grad() loss.backward() optimzer.step() print('epoch%d' % (epoch + 1), '-', 'batch%d' % step, '-', 'loss%f' % loss) #查看训练过程 print('No.%depoch is over' % (epoch + 1))代入测试集求解：12345output = cnn(test_data[:])#print(output)result = torch.max(output, 1)[1].squeeze()#print(result)仿照KNN中的结果转存函数，定义saveResult函数。1234567def saveResult(result): with open('result.csv', 'w') as myFile: myWriter = csv.writer(myFile) for i in result: tmp = [] tmp.append(i) myWriter.writerow(tmp)最后使用saveResult(result.numpy())把结果存入csv文件。改进然而，若使用上述的CNN，得出的结果在leaderboard上会达到两千三百多名，这已经进入所有参赛者的倒数两百名之内了。为什么这个CNN的表现甚至不如我前面的KNN算法呢？我觉得主要有下面三个原因。首先，由于CNN的参数较多，仅经过1轮epoch应该是不足够把所有参数训练到最优或者接近最优的位置的。个人认为，靠前的数据在参数相对远离最优值时参与训练而在之后不起作用，很有可能导致最后顾此失彼，因此有必要增加epoch使之前的数据多次参与参数的校正。同时，也要增大batch size使每次优化参数使用的样本更多，从而在测试集上表现更好。训练结束后，我发现我的C盘会被占用几个G，不知道是不是出错了，也有可能是参数占用的空间，必须停止kernel才能得到释放（我关闭了VScode后刷新，空间就回来了）。关于内存，这里似乎存在着一个问题，我将在后文阐述。注：由于VScode前段时间也开始支持ipynb，喜欢高端暗黑科技风又懒得自己修改jupyter notebook的小伙伴可以试一试。学习率过大。尽管我这里的学习率设置为0.01，但对于最后的收敛来说或许还是偏大，这就导致了最后会在最优解附近来回抖动而难以接近的问题。关于这个问题，可以到deep-learning笔记：学习率衰减与批归一化中看看我较为详细的分析与解决方法。由于训练时间和epoch轮数相对较小，我推测模型可能会存在过拟合的问题。尤其是最后的全连接层，它的结构很容易造成过拟合。关于这个问题，也可以到machine-learning笔记：过拟合与欠拟合和machine-learning笔记：机器学习中正则化的理解中看看我较为详细的分析与解决方法。针对上述原因，我对我的CNN模型做了如下调整：首先，增加训练量，调整超参数如下。1234#超参数EOPCH = 3BATCH_SIZE = 50LR = 1e-4引入dropout随机失活，加强全连接层的鲁棒性，修改网络结构如下。1234567891011121314151617181920212223242526272829303132333435#build CNNclass CNN(nn.Module): def __init__(self): super(CNN, self).__init__() #一个卷积层 self.conv1 = nn.Sequential( nn.Conv2d( #输入(1, 28, 28) in_channels = 1, #1个通道 out_channels = 16, #输出层数 kernel_size = 5, #过滤器的大小 stride = 1, #步长 padding = 2 #填白 ), #输出(16, 28, 28) nn.ReLU(), nn.MaxPool2d(kernel_size = 2), #输出(16, 14, 14) ) self.conv2 = nn.Sequential( #输入(16, 14, 14) nn.Conv2d(16, 32, 5, 1, 2), #这里用了两个过滤器，将16层变成了32层 nn.ReLU(), nn.MaxPool2d(kernel_size = 2) #输出(32, 7, 7) ) self.dropout = nn.Dropout(p = 0.5) #每次减少50%神经元之间的连接 self.fc = nn.Linear(32 * 7 * 7, 1024) self.out = nn.Linear(1024, 10) #全连接层，将三维的数据展为2维的数据并输出 def forward(self, x): x = self.conv1(x) x = self.conv2(x) x = x.view(x.size(0), -1) x = self.fc(x) x = self.dropout(x) output = F.softmax(self.out(x)) return output本想直接使用torch.nn.functional中的dropout函数轻松实现随机失活正则化，但在网上看到这个函数好像有点坑，因此就不以身试坑了，还是在网络初始化中先定义dropout。注：训练完新定义的网络之后我一直在思考dropout添加的方式与位置。在看了一些资料之后，我认为或许去掉全连接层、保持原来的层数并在softmax之前dropout可能能达到更好的效果。考虑到知乎上有知友提到做研究试验不宜在MNIST这些玩具级别的数据集上进行，因此暂时不再做没有太大意义的调整，今后有空在做改进试验。经过上面的改进后，我再次训练网络并提交结果，在kaggle上的评分提高至0.97328，大约处在1600名左右，可以继续调整超参数（可以分割验证集寻找）和加深网络结构以取得更高的分数，但我的目的已经达到了。与之前的KNN相比，无论从时间效率还是准确率，CNN都有很大的进步，这也体现了深度学习相对于一些经典机器学习算法的优势。出现的问题由于这个最后的网络是我重复构建之后完成的，因此下列部分问题可能不存在于我上面的代码中，但我还是想汇总在这，以防之后继续踩相同的坑。报错element 0 of tensors does not require grad and does not have a grad_fnpytorch具有自动求导机制，这就省去了我们编写反向传播的代码。每个Variable变量都有两个标志：requires_grad和volatile。出现上述问题的原因是requires_grad = False，修改或者增加（因为默认是false）成True即可。RuntimeError: Dimension out of range (expected to be in range of [-1, 0], but got 1)这个好像是我在计算交叉熵时遇到的，原因是因为torch的交叉熵的输入第一个位置的输入应该是在每个label下的概率，而不是对应的label，详细分析与举例可参考文首给出的第三个链接。AttributeError: ‘tuple’ object has no attribute ‘numpy’为了查看数据处理效果，我在数据预处理过程中使用matplotlib绘制出处理后的图像，但是却出现了如上报错，当时的代码如下：123plt.imshow(trainData[1].numpy().squeeze(), cmap = 'gray')plt.title('%i' % train_labels[1])plt.show()查找相关资料之后，我才知道torch.utils.data会把打包的数据变成元组类型，因此我们绘图还是要使用原来train_data中的数据。转存结果时提醒DefaultCPUAllocator: not enough memory由于当初在实现KNN算法转存结果时使用的函数存入csv文件后还要对文件进行空值删除处理，比较麻烦（后文会写具体如何处理），因此我想借用文章顶部给出的第二个链接中提供的方法：123out = pd.DataFrame(np.array(result), index = range(1, 1 + len(result)), columns = ['ImageId', 'Label'])#torch和pandas的类型不能直接的转换，所以需要借助numpy中间的步骤，将torch的数据转给pandasout.to_csv('result.csv', header = None)结果出现如下错误：我好歹也是八千多买的DELL旗舰本，8G内存，它居然说我不够让我换块新的RAM？什么情况…尝试许久，我怀疑是训练得到的参数占用了我的内存，那只好先把训练出的result保存下来，再导入到csv文件。最后我还是选择自己手动处理csv文件中的空值，应该有其它的转存csv文件的方法或者上述问题的解决措施，留待以后实践过程中发现解决，也欢迎大家不吝赐教。excel/csv快速删除空白行如果你使用的是我的saveResult函数或者类似，你就很有可能发现更新后的csv文件中数据之间双数行都是留空的，即一列数据之间都有空白行相隔，那么可以使用如下方法快速删除空白行。选中对应列或者区域。在“开始”工具栏中找到“查找与选择”功能并点击。在下拉菜单中，点击“定位条件”选项。在打开的定位条件窗口中，选择“空值”并确定。待电脑为你选中所有空值后，任意右键一个被选中的空白行，在弹出的菜单中点击“删除”。如果数据量比较大，这时候会有一个处理时间可能会比较长的提醒弹出，确认即可。等待处理完毕。]]></content>
      <categories>
        <category>人工智能</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>深度学习</tag>
        <tag>踩坑血泪</tag>
        <tag>代码实现</tag>
        <tag>pytorch</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[hexo笔记：SEO优化]]></title>
    <url>%2Fhexo20191101212014%2F</url>
    <content type="text"><![CDATA[在toefl笔记：首考考托福——记一次裸考经历文章末尾我曾提到在被百度收录之后要好好做SEO，这段时间我也的确有所尝试与改进，因此在本文中将一些我认为比较有效的或者依旧存疑的SEO优化方法写下来，供日后参考深究。References：电子文献：https://blog.csdn.net/lzy98/article/details/81140704https://www.jianshu.com/p/86557c34b671https://baijiahao.baidu.com/s?id=1616368344109675728&amp;wfr=spider&amp;for=pchttps://www.jianshu.com/p/7e1166eb412ahttps://baijiahao.baidu.com/s?id=1597172076743185609&amp;wfr=spider&amp;for=pcSEO之前在知乎上碰巧看到一篇别人是如何推广自己的博客的文章，里面就提到了SEO这个概念。我当时也很好奇，百度之后才发现它完全不同于CEO、CTO等概念。SEO（Search Engine Optimization），汉译为搜索引擎优化。它是一种方式，即利用搜索引擎的规则提高网站在有关搜索引擎内的自然排名。通俗的讲就是post的内容更容易被搜索引擎搜索到或者收录，且在搜索结果列表中显示靠前。看了一圈，SEO的办法真的是多种多样，下面我就简单记录一部分我试过的方法。优化url同样在站点配置文件下面，可以找到站点的url设置。如果你尚未更改过，你会发现默认的url是http://yoursite.com，我在这里吃了不少亏，之前苹果上add to favorites、RSS订阅后点开的链接以及copyright的链接都会直接跳转到yoursite而非我的博文链接。SEO搜索引擎优化认为，网站的最佳结构是用户从首页点击三次就可以到达任何一个页面，但是我们使用hexo编译的站点默认打开文章的url是：sitename/year/mounth/day/title四层的结构，这样的url结构很不利于SEO，爬虫就会经常爬不到我们的文章，于是，我们可以将url直接改成sitename/title的形式，并且title最好是用英文（中文的url会出现好多乱码，我这方面还有待改进）。基于以上原因，我在根目录的站点配置文件下修改url设置如下（注释中是默认的）：如此，再次添加RSS订阅，就可以跟yoursite这个鬼地方say goodbye啦。对permalink的修改将会是你的站点的一次巨大的变动，会造成大量的死链。死链会造成搜索引擎对网站的评分降低并有可能会降权。我们可以直接百度搜索“site:url”（url即你的站点网址）查看已经被搜索引擎收录的网址。如下图所示，目前我已被收录了四个，其中前两个经此番调整已成为死链。这时我们可以在百度站长平台中提交死链，由于死链文件制作稍较复杂，我们可以选择规则提交的方式提交死链（处理死链过程较长，我提交的死链目前还在处理中）。很重要的是，我们需要在自己的所有博文中修改链接，我使用了VScode的搜索关键字符功能对所有markdown文件进行了修改，效率相对较高。此外，如果使用了leancloud等第三方服务，那么也需要修改对应的url与新的相匹配，否则会造成原来数据的丢弃，还是挺可惜的。压缩文件关于压缩的方法，网上有很多，可以选择简易的应用。我选择的是用hexo-neat，安装插件后在站点配置文件添加如下设置，效果不错。12345678910111213141516171819202122# hexo-neat# 博文压缩neat_enable: true# 压缩htmlneat_html: enable: true exclude:# 压缩css neat_css: enable: true exclude: - &apos;**/*.min.css&apos;# 压缩jsneat_js: enable: true mangle: true output: compress: exclude: - &apos;**/*.min.js&apos; - &apos;**/jquery.fancybox.pack.js&apos; - &apos;**/index.js&apos;添加完成之后，每次generate你就会在git bash终端看到neat压缩的反馈信息。另外也有和很多网友使用的是gulp压缩，设置也很简便且有效。压缩网站文件不仅可以提高访问加载的速度，同时减少了大量空白符，对SEO也是有不小的帮助的，推荐尝试。主动推送首先在根目录下安装插件npm install hexo-baidu-url-submit --save。在根目录站点配置文件中新增如下字段：12345baidu_url_submit: count: 100 # 提交最新的一个链接 host: gsy00517.github.io # 在百度站长平台中注册的域名 token: lY..........Fk # 请注意这是您的秘钥，所以请不要把博客源代码发布在公众仓库里! path: baidu_urls.txt # 文本文档的地址，新链接会保存在此文本文档里域名和秘钥可以在站长工具平台的连接提交中的接口调用地址中找到，即对应host与token后面的字段。再把主题配置文件中的deploy修改如下：123456789# Deployment## Docs: https://hexo.io/docs/deployment.htmldeploy:- type: git repo: github: git@github.com:Gsy00517/Gsy00517.github.io.git coding: git@git.dev.tencent.com:gsy00517/gsy00517.git branch: master- type: baidu_url_submitter注意：必须严格按照上述格式，否则无法deploy。这样以后每次执行hexo d，新的链接就会主动推送给百度，然后百度就会更快地派爬虫来发现你站点中的新链接，可以在第一时间收录新建的链接。自动推送自动推送是百度搜索资源平台为提高站点新增网页发现速度推出的工具，安装自动推送JS代码的网页，在页面被访问时，页面url将立即被推送给百度。详情可以查看百度的站长工具平台使用帮助。事实上，如果已经实行了主动推送，那么自动推送其实不是那么必要，因为主动推送是在生成url的时候第一时间进行推送，之后访问页面时进行的自动推送就显得晚了一步。不同推送方式的效果大概是：主动推送&gt;自动推送&gt;sitemap。下面还是写一下自动推送的实现方法。在blog\themes\next\source\js\src目录下，创建名为bai.js的文件，并根据百度提供的自动推送功能方法添加以下代码：1234567891011121314&lt;script&gt;(function()&#123; var bp = document.createElement(&apos;script&apos;); var curProtocol = window.location.protocol.split(&apos;:&apos;)[0]; if (curProtocol === &apos;https&apos;) &#123; bp.src = &apos;https://zz.bdstatic.com/linksubmit/push.js&apos;; &#125; else &#123; bp.src = &apos;http://push.zhanzhang.baidu.com/push.js&apos;; &#125; var s = document.getElementsByTagName(&quot;script&quot;)[0]; s.parentNode.insertBefore(bp, s);&#125;)();&lt;/script&gt;此外，还可以blog\scaffolds目录下的模板文件post.md的分隔线之后添加这么一行：1&lt;script type=&quot;text/javascript&quot; src=&quot;/js/src/bai.js&quot;&gt;&lt;/script&gt;这样以后每次创建新的文章就会自动在文末添加这行代码，即在生成的模板中包含这行代码。如此，只要访问你的这个页面，它就会自动向百度推送你的这个网页。sitemap首先需要安装sitemap站点地图自动生成插件。windows下打开git bash，输入安装命令：12npm install hexo-generator-sitemap --savenpm install hexo-generator-baidu-sitemap --save然后在站点配置文件_config.yml中找到如下对应的位置（一般默认有，没有的话可以添加），修改如下：12345# 自动生成sitemapsitemap: path: sitemap.xmlbaidusitemap: path: baidusitemap.xml特别要注意的是，上面的path一定要缩进，否则在hexo generate时会无法编译导致报错。（似乎有一些版本的hexo不存在这样的问题，关于版本可以使用hexo version命令查看）这样以后每次generate后都会在public目录下面生成sitemap.xml和baidusitemap.xml两个文件，即你的站点地图。也可以deploy后直接在域名后面加上这两个文件名查看你的站点地图。在百度站长平台中，有sitemap提交的选项，由于我当初提交的网站协议前缀是http，因此xml文件所在的https前缀的链接不属于我提交的网站，而我的github page和coding page都设置了强制https访问。这个问题以后有机会再做解决，不存在这个问题的可以试试提交sitemap。疑惑在优化的过程中，我发现我的post模板也被改变了（原因目前未知），从原本的：12345title: &#123;&#123; title &#125;&#125;date: &#123;&#123; date &#125;&#125;tags:copyright: truetop:变成了：1234567891011---noteId: &quot;9bfafbb............dd5a3&quot;tags: []title: [object Object]: nulldate: [object Object]: nullcopyright: truetop: null---更奇怪的是，我无法删除noteId并恢复到原来的样式，每次更改保存之后又会自动给我换回来，为了方便使用，我将其修改为：1234567891011---noteId: &quot;55c6d..............d6fcf&quot;title: &#123;&#123; title &#125;&#125;date: &#123;&#123; date &#125;&#125;copyright: truetop:categories:tags:---这样就可以直接提取文章标题和创建时间了。对于noteId的作用，网上也找不到相关信息，可能是类似于网站的ID标识的一个代号吧，我对它之后的改进以及用法可见后文。使用noteId改进url今天看了几个url中含有noteId的网站，立马想到其实noteId其实可以用来替代url的中文等符号从而消除乱码，这更方便了爬虫的抓取。于是，我把站点配置文件下的url设置修改如下：同时我把模板文件post.md修改为：12345678910111213---noteId: &apos;prefix+time remember to change!!!&apos;title: &#123;&#123; title &#125;&#125;date: &#123;&#123; date &#125;&#125;copyright: truetop:categories:tags:---&lt;script type=&quot;text/javascript&quot; src=&quot;/js/src/bai.js&quot;&gt;&lt;/script&gt;这就把我的博文网址修改成了“关键词+创建时间”的形式，当然要手动更改。同样的，以上的操作也带来了巨大的麻烦。我需要给之前没有生成noteId的博文一一加上noteId，同时也免不了对外部辅助平台和网站内链的大幅度修改。对于检查网站死链，我推荐一个挺实用的轻量工具Xenu，下载安装之后，选择file，然后check URL，输入网站地址，即可检查站内所有的连接中是否存在死链。下面是我仅修改了url设置而未更改内链时检测的情况，其中红色的就是死链。修改url之后大约是在我修改了url格式的两天后，当我再用“site:url”查询收录情况时，我发现被收录的死链已经减少了一个（似乎不是提交死链的原因，因为规则提交的死链还在处理中），然而我之前被收录、修改后原url依然可用的主页和分类页面却也消失了，这就使我非常得纳闷。这几日也查找了许多资料寻找原因，总结如下。首先，网站url的变动产生大量死链，很有可能会导致网站排名消失，原来积累的权重大大减少甚至清除。还好目前我只是一个新站，倘若已运行并被收录了一段时间，应该要慎重考虑是否是因为网址必须得精简等原因从而放弃网站的排名。要注意的是，如果网站url链接过深，会影响搜索引擎蜘蛛抓取，时间久了，蜘蛛来的次数就会减少，最后导致网站不收录。一般建议扁平化结构，url在三层以内方便蜘蛛爬行，这在上文也提到过。此外，如果是新站的话，收录之后消失也是正常的。事实上，上线6个月之内的网站都可以被称为新站。因为搜索引擎蜘蛛对新站有一个好奇心，发现新鲜的事物喜欢去抓取一下，这就是收录，但是收录之后会有一个审核期，包括这个收录之后又消失的问题，审核期过后如果在数据库找不到相同的信息就会认为这是一篇原创，这个时候再去看收录就又会恢复了。值得注意的是，新站上线短期内，只新增更新内容就行了，不要去改动以前的内容（特别是标题、url等，搜索引擎对这些内容很敏感）以免延长新站考核时间，当网站索引趋于稳定状态后可以适当改动。总而言之，目前没什么好担心的，担心也没有用，还是认认真真好好地写笔记好啦！添加robots文件Robots协议（也称为爬虫协议、机器人协议等）的全称是“网络爬虫排除标准”（Robots Exclusion Protocol），网站可以通过Robots协议告诉搜索引擎哪些页面可以抓取，哪些页面不能抓取。如果一个网站使用大量的js、flash、ifrmae等内容，或者如果一个网站结构混乱，那么整个网站就会是乱七八糟、毫无章法，不仅用户体验极差，更重要的是蜘蛛也不会喜欢，也没有心思去抓取网站的内容了。robots.txt是搜索引擎蜘蛛访问网站时要查看的第一个文件，并且会根据robots.txt文件的内容来爬行网站。在某种意义上说，它的一个任务就是指导蜘蛛爬行，减少搜索引擎蜘蛛的工作量。当搜索引擎蜘蛛访问网站时，它会首先检查该站点根目录下是否存在robots.txt文件，如果该文件存在，搜索引擎蜘蛛就会按照该文件中的内容来确定爬行的范围；如果该文件不存在，则所有的搜索引擎蜘蛛将能够访问网站上所有没有被口令保护的页面。通常搜索引擎对网站派出的蜘蛛是有配额的，多大规模的网站放出多少蜘蛛。如果我们不配置robots文件，那么蜘蛛来到网站以后会无目的地爬行，造成的一个结果就是，需要它爬行的目录，没有爬行到，不需要爬行的，也就是我们不想被收录的内容却被爬行并放出快照。所以robots文件对于SEO具有重要的意义。如果网站中没有robots.txt文件，则网站中的程序脚本、样式表等一些和网站内容无关的文件或目录即使被搜索引擎蜘蛛爬行，也不会增加网站的收录率和权重，只会浪费服务器资源。此外，搜索引擎派出的蜘蛛资源也是有限的，我们要做的应该是尽量让蜘蛛爬行网站重点文件、目录，最大限度的节约蜘蛛资源。在站点根目录的source文件下添加robots.txt文件，加入如下内容：1234567891011121314User-agent: * Allow: /Allow: /archives/Disallow: /categories/Disallow: /tags/Disallow: /vendors/Disallow: /js/Disallow: /css/Disallow: /fonts/Disallow: /vendors/Disallow: /fancybox/Sitemap: https://gsy00517.github.io/sitemap.xmlSitemap: https://gsy00517.github.io/baidusitemap.xml注意sitemap中要修改成自己的url。另外，可以在站长工具平台检测robots文件。]]></content>
      <categories>
        <category>操作和使用</category>
      </categories>
      <tags>
        <tag>hexo</tag>
        <tag>配置优化</tag>
        <tag>SEO</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[machine learning笔记：机器学习的几个常见算法及其优缺点]]></title>
    <url>%2Fmachine-learning20191101192042%2F</url>
    <content type="text"><![CDATA[接触机器学习也有一段较长的时间了，不敢说自己全部掌握甚至精通，但是期间也了解或者尝试了许多机器学习的算法。这次就结合参考资料和我自己的感受小结一下几种机器学习的常见算法及其优点和缺点。决策树算法学过数据结构中的树应该对这个算法不会感到困惑，下面就简单介绍一下其优缺点。优点易于理解和解释，可以可视化分析，容易提取出规则。可以同时处理标称型和数值型数据。测试数据集时，运行速度比较快。决策树可以很好的扩展到大型数据库中，同时它的大小独立于数据库大小。缺点对缺失数据处理比较困难。容易出现过拟合问题，容易受到例外的干扰，对测试集非常不友好。忽略数据集中属性的相互关联。ID3算法计算信息增益时结果偏向数值比较多的特征。改进措施对决策树进行剪枝。可以采用交叉验证法和加入正则化的方法。较为理想的决策树是叶子节点数少且深度较小。使用基于决策树的combination算法，如bagging算法，randomforest算法，可以解决过拟合的问题。常见算法C4.5算法ID3算法是以信息论为基础，以信息熵和信息增益度为衡量标准，从而实现对数据的归纳分类。ID3算法计算每个属性的信息增益，并选取具有最高增益的属性作为给定的测试属性。C4.5算法核心思想是ID3算法，是ID3算法的改进，改进方面有：用信息增益率来选择属性，克服了用信息增益选择属性时偏向选择取值多的属性的不足。在树构造过程中进行剪枝。能处理非离散的数据。能处理不完整的数据。优点产生的分类规则易于理解，准确率较高。缺点在构造树的过程中，需要对数据集进行多次的顺序扫描和排序，因而导致算法的低效。C4.5只适合于能够驻留于内存的数据集，当训练集大得无法在内存容纳时程序无法运行。CART分类与回归树这是一种决策树分类方法，采用基于最小距离的基尼指数估计函数，用来决定由该子数据集生成的决策树的拓展形。如果目标变量是标称的，称为分类树；如果目标变量是连续的，称为回归树。分类树是使用树结构算法将数据分成离散类的方法。优点非常灵活，可以允许有部分错分成本，还可指定先验概率分布，可使用自动的成本复杂性剪枝来得到归纳性更强的树。在面对诸如存在缺失值、变量数多等问题时CART显得非常稳健。下面对决策树的各种算法做一个小结：算法支持模型树结构特征选择ID3分类多叉树信息增益C4.5分类多叉树信息增益比CART分类、回归二叉树基尼系数、均方差补充：信息熵：表示随机变量的不确定性，熵越大，不确定性越大。这与物理中的熵性质类似。信息增益：即不确定性减小的幅度。信息增益=信息熵（前）-信息熵（后）。在构造决策树的时候往往选择信息增益大的特征优先作为节点分类标准。信息增益比：由于仅根据信息增益构建决策树，那么三叉树以及多叉树比二叉树的效果一般来说分类效果要好，然而这很有可能会导致过拟合的问题。因此定义信息增益比=惩罚参数*信息增益。当特征个数较多时，惩罚参数较小；当特征个数较少时，惩罚参数较大，从而使信息增益比较大，进而克服信息增益偏向于选取取值较多的特征的问题。总的来说，信息增益比相对于信息增益更客观。基尼系数：表示集合的不确定性，基尼系数越大，则表示不平等程度越高。分类算法KNN算法优点KNN是一种在线技术，新数据可以直接加入数据集而不必进行重新训练。KNN理论简单，容易实现。实际上，KNN没有训练过程，或者说，它的训练过程就是导入数据集。缺点KNN对于样本容量大的数据集计算量比较大，极易引发维度灾难。样本不平衡时，预测偏差比较大。如：某一类的样本比较少，而其它类样本比较多。KNN每一次分类都会重新进行一次全局运算，耗时久。这在实践中会非常有体会，可以参考kaggle笔记：手写数字识别——使用KNN和CNN尝试MNIST数据集。在CV领域，KNN已经被完全弃用。这是因为它不适合用来表示图像之间的视觉感知差异，如下图所示，这是CS231n中提到的一个例子，后三张图片经过不同的变换，结果与第一张原图的L2距离居然是一样的，而显然对我们而言这三张图是有很大区别的，在实际应用中往往应该区分开。应用领域文本分类。模式识别。聚类分析。多分类领域。支持向量机（SVM）支持向量机是一种基于分类边界的方法。其基本原理是（以二维数据为例）：如果训练数据分布在二维平面上的点，它们按照其分类聚集在不同的区域。基于分类边界的分类算法的目标是：通过训练，找到这些分类之间的边界（直线的称为线性划分，曲线的称为非线性划分）。对于多维数据（如N维），可以将它们视为N维空间中的点，而分类边界就是N维空间中的面，称为超面（超面比N维空间少一维）。线性分类器使用超平面类型的边界，非线性分类器使用超曲面。支持向量机的原理是将低维空间的点映射到高维空间，使它们成为线性可分，再使用线性划分的原理来判断分类边界。在高维空间中是一种线性划分，而在原有的数据空间中，是一种非线性划分。在我的博文machine-learning笔记：一个支持向量机的问题中，我提及了SVM的简介与一个问题，感兴趣的话可以了解一下。优点解决小样本下机器学习问题，相对于其他训练分类算法不需要过多样本。解决非线性问题。擅长应付线性不可分，主要用松弛变量（惩罚变量）和核函数来实现。无局部极小值问题。（相对于神经网络等算法）引入了核函数，可以很好的处理高维数据集。泛化能力比较强。结构风险最小，指分类器对问题真实模型的逼近与真实解之间的累计误差。缺点对于核函数的高维映射解释力不强，尤其是径向基函数。对缺失数据敏感。应用领域：文本分类。图像识别。主要二分类领域。朴素贝叶斯算法朴素贝叶斯，即naive bayes。说白了就是要“sometimes naive”。优点对大数量训练和查询时具有较高的速度。即使使用超大规模的训练集，针对每个项目通常也只会有相对较少的特征数，并且对项目的训练和分类也仅仅是特征概率的数学运算而已。支持增量式运算。即可以实时的对新增的样本进行训练。朴素贝叶斯对结果解释容易理解。缺点由于使用了样本属性独立性的假设，所以如果样本属性有关联时其效果不好。应用领域文本分类。欺诈检测。Logistic回归算法优点计算代价不高，易于理解和实现。缺点容易产生欠拟合。分类精度不高。应用领域用于二分类领域，可以得出概率值，适用于根据分类概率排名的领域，如搜索排名等。Logistic回归的扩展softmax可以应用于多分类领域，如手写字识别等。聚类算法K-means算法K-means算法，即K均值算法，是一个简单的聚类算法，把n个对象根据它们的属性分为k个分割，k小于n。算法的核心就是要优化失真函数J，使其收敛到局部最小值但不是全局最小值。它比较适合凸数据集，即任意两个数据点之间的连线都在数据集内部。算法流程随机选择k个随机的点（称为聚类中心）。对数据集中的每个数据点，按照距离k个中心的距离，将其与最近的中心点关联起来，与同一中心点关联的点聚成一类。计算每一组的均值，将该组所关联的中心点移到平均值的位置。重复第2、3两步，直到中心点不再变化。优点算法速度很快。缺点分组的数目k是一个输入超参数，不合适的k可能返回较差的结果。EM最大期望算法EM算法是基于模型的聚类方法，是在概率模型中寻找参数最大似然估计的算法，其中概率模型依赖于无法观测的隐藏变量。E步估计隐含变量，M步估计其他参数，交替将极值推向最大。EM算法比K-means算法计算复杂，收敛也较慢，不适于大规模数据集和高维数据，但比K-means算法计算结果稳定、准确。EM经常用在机器学习和计算机视觉的数据集聚（Data Clustering）领域。集成算法（AdaBoost）俗话说的好“三个臭皮匠，顶个诸葛亮”，集成算法就是将多个弱分类器集成在一起，构建一个强分类器。事实上，它可能不属于算法，而更像一种优化手段。优点很好的利用了弱分类器进行级联。可以将不同的分类算法作为弱分类器。AdaBoost具有很高的精度。相对于bagging算法和randomforest算法，AdaBoost充分考虑的每个分类器的权重。缺点AdaBoost迭代次数也就是弱分类器数目不太好设定，可以使用交叉验证来进行确定。数据不平衡导致分类精度下降。训练比较耗时，每次重新选择当前分类器最好切分点。应用领域模式识别。计算机视觉领域。二分类和多分类场景。神经网络算法优点分类准确度高，学习能力极强。对噪声数据鲁棒性和容错性较强。有联想能力，能逼近任意非线性关系。缺点神经网络参数较多，权值和阈值。我在训练一个只有四层的CNN时，C盘就被占用了几个G，详细情况可见kaggle笔记：手写数字识别——使用KNN和CNN尝试MNIST数据集。黑盒过程，不能观察中间的结果，甚至无法完全理解其是怎么达到效果的。学习过程比较长，有可能陷入局部极小值。应用领域计算机视觉。自然语言处理。语音识别等。]]></content>
      <categories>
        <category>人工智能</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[toefl笔记：首考考托福——记一次裸考经历]]></title>
    <url>%2Ftoefl20191019150438%2F</url>
    <content type="text"><![CDATA[今天上午终于把自己开学以来耿耿于怀的托福考完了。目前来看是铁定要二战了，因此下午抽空把这次考试的经历总结一下，方便再战的时候可以吸取经验和教训。前期大一下学期的时候去听了一个学姐的出国分享会，当然是新东方支持的，讲座已结束就被新东方的老师搞了一波传销。当时也不知道托福的有效期是两年，头脑一热就报了班和十月份的托福。不过后来想想可能暑研和暑校也用得上，或许不亏。于是我抽了个周末去新东方校区做了一个入班小测，很幸运的是我的分能进入强化班，毕竟高中的底子还可以。然而很坑的是，我从同学那得知去年同期的同一个班报名费比我少了将近一千（我报的强化班4400rmb），简直坐地起价啊！可能时间比较早校方不是很担心没人报。然后到了暑假，我开始零零散散地准备。在家做了一套听力一套阅读，心态就崩了，怎么这么难！感觉就是跟四六级不在一个档次上。于是三分钟热度就被浇没了，之后就只是背单词了，打算等八月上了课听了老师的解题方法再强化练习。到了八月中旬，开始上课了，听了几节课明白题型之后，感觉托福或许也没有想象得那么可怕，熟悉就好。那段时间回去后断断续续刷了几套TPO的阅读和听力部分。然后开学一周，很快就到了国庆。根据我的原计划，身为拖延症重度患者的我打算在国庆力挽狂澜，为此我早早地准备好了新东方的《7天搞定托福高频核心词》，当时觉得时间充裕，计划合理，未来充满希望。然而…等到国庆结束，我也明白了一个道理：不能被事物的表面现象所迷惑。不过，乐观的我依然觉得剩下的两周足以完成复习。However，国庆上来劈头盖脸砸过来的课程和任务让我分身乏术。周三晚上的实验课，我做到十一点才回寝室，这更是对我的致命一击。因为回寝太晚，没时间更换被子，导致挨冻一晚上（武汉的天气太怪了）。最后，又是喉咙痛，又是犯鼻炎，那时就感觉托福基本要凉。考前第二天，我又刷了一套新托福的阅读和听力，成绩不是特别理想。由于新东方的TPO加载速度感人，也可能是学校网络的问题，总之直到考试之前，我只在小站刷过三套TPO，在新东方刷过两套老TPO和一套新TPO。甚至直到写这篇文章的标题之前，我都不知道托福的英文拼写是“TOEFL”（之前一直觉得是“TOFEL”）。考试前一天，病情加重，我也就不刷题了，把上课的笔记和题型又好好熟悉了一下就早点休息了。考前考前问了新东方替我报名的老师，她说要打印确认信。不知道为什么我无法下载确认信成pdf格式的，最后屏幕截图打算打印，早上去考场的时候却忘记了。不过还好最后发现根本不需要确认信。早上提早一个小时出寝室，结果发现找不到租八戒了，不知道为什么周六大家起这么早。最后租了辆摩拜拖着病体艰难地骑到了考场。到了考试的楼下，有一个小姐姐热心地给我指路，不过我马上就发现她别有目的。她让我填一个貌似是培训机构的表格，善良易上当的我稀里糊涂地填了，本来想写个假的电话，结果感冒头很晕也没多想就如实写了，反正我平时也不怎么接陌生电话。坐电梯到了考试的楼层，碰到我们学院一个经常见到的学长在做志愿者。他总是活跃在各种场合，好像是英语协会的，总之看到他也是开心了一小下。之后就是看序号，签到，领钥匙，去存背包。在储物室的时候，一边的考试人员一直重复说“A考场的人可以把水拿出来”，我没听太懂。由于之前问过她我鼻炎犯了可不可以带餐巾纸（她说考场会提供），就不想再问第二次了。之前看别人的考试经历，说中途休息时可以出来喝水吃零食，还可以看写作模板，我以为是可以回到储物室的，后来才发现不能。放完东西，我本来想再看会英语进入一下状态，结果过安检之后就只能一直在里边等了。我们来到一个签承诺书的房间，大家都一排排坐在一种比较矮的长凳上，我拿了一张承诺书和一支笔就往后坐了。其实还可以拿个写字的时候用来垫的板子，我没注意，不过好多人和我一样都是在腿上或者趴凳子上写的。写承诺书的时候我没仔细看黑板上的要求，写错了一次，只好挺无奈地找工作人员换了一张。不一会人就好多了，我发现这次考托福的女生比较多，大约是男生的两倍，这在我们学校很不常见啦。考试的也有大人，在我观察是不是还有培训机构的老师的时候，我的隔壁也是实验班的一个朋友也来考试了。他在C考场，那个考场更大。我的A考场人最少，相对来说环境要理想一些。不过不同考场的同学还是在同一个房间等待的。我继续观察，发现还是有几个大二面孔的，和几个人说了几句，发现还是有不少首考的人的。入场过了一会有一个男老师进来说一些有关考试的注意事项，说完没多久大家就到隔壁的考试教室刷脸入场了。考场的教室和等候的教室一样，也是黄色的日光灯，看着也挺舒适。入场顺序是按照姓氏的首字母顺序的，我进去的比较早。尽管A考场大概也就二三十个人，但整个入场过程还是挺久的。考官把我领到座位上，虽然是随机抽的但好像我的考位还是我的序号。为我把身份证插在旁边的卡槽里之后，考官又为我输了激活码进入考试界面，然后没说什么就走了。考试的隔间挺好，靠桌子往里坐一点就完全看不到旁边了。首先是确认姓名的界面，然而考官走了我也没处问，担心确认了就直接开始考试了因此久久没敢点。由于别人还在入场，因此我不敢太早开始考试。我回头看了一眼，发现是我进候考室以来就注意到的那个男生。虽然没问过他，但看上去他这次绝对不是一战了。过了一会，我听到有人点鼠标的声音，于是我也鼓足勇气开始点。前面大概有七八的页面都是只需continue的direction界面，而且这个界面是不会自动跳转的，我在这里停留了很久。终于，大概第十个人入场的时候，我听到有人开始试音了。意外的是，第一个开始试音的人居然真的在介绍他生活的城市。哈哈哈看来也是首考的，不知道待会整个考场一齐开始诠释人类的本质的时候，他有什么感想。这里我暗暗庆幸自己报了班。当大家都在诠释人类的本质时，我心里觉得还是挺可乐的。不过就在这时，我听到前面提到的那位久经沙场的老哥也开始复读了，于是我又点了一个continue。每个continue我的拖好久才点，不过入场真的是挺久的。大概有十个人完成试音之后，我才看到了describe the city you live in，心想这个时间还是可以的。阅读直到考试，我才知道review的用法，点击之后，会出现一个表格，可以看到哪些题已经answered了。阅读第一篇是关于研究者根据化石推断远古的自然环境，第二篇是什么记不太清了，好像也是历史相关的，第三篇是北美西海岸土著人的一些文化，还有配图。没有遇到加试。总的来说，考场里效率还是比寝室要好一些的。我大概还有40分钟时做完了第一篇，到最后一篇时时间还有20多分钟，相对来说还是比较宽裕的。听力接下来就是听力了，我的听力是加试，有3个section。第一个section的对话我考虑太久，导致最后答lecture三道题要在一分钟之内答完。当时也只能以这个section只有50%的概率计入成绩来安慰自己。第二个section做得还行，一些笔记还是没记到要点上，还是得多练。遗憾的是，我听力有好几篇都没听懂听力开头“you will hear part of a lecture in a ……gy class”中学科具体是什么，如果能听懂的话肯定是有一定帮助的，词汇量还不够啊！到了第三个section，我之前在考试教室门外抽的餐巾纸用完了，我只能忍着做题，结果第三个section的lecture的conversation中的男老师似乎有异国口音，说得很不清楚，在lecture部分我也走了一会神。同样的，我发现我不是很善于掌握1个conversation+1个lecture情况下的时间，lecture的时间又分配得不多。当时也只能又以这个section只有50%的概率计入成绩来安慰自己，好吧其实两个section都凉了。考试前两天对自己的listening还是最自信的，现在看来还是得花真功夫才行。休息终于休息了，我出门的时候拿到张纸条，上面提醒我11：04返场。我本来想喝口热水缓解一下我喉咙的疼痛，却被告知不能回储物室了。我这时候看到别的同学放在楼梯口的一个大桌子上的水和零食，心里真的拔凉拔凉的。我5块钱买的士力架啊！我的口语写作模板啊！不过好像大多数人都没怎么吃东西，要是我不生病的话应该也没什么问题。考场外面的钟不是很准，我一直担心里面开始口语了我还没进去，后来发现开始第二部分的考试也是需要考官输入验证码的，因此完全不必担心。什么都没带，我那十分钟也就上了个厕所并且补充了餐巾纸。口语之前开始的晚，因此我休息的时间也差不多在大家的中间。口语部分一开始的continue就比较少了，又试了一次音。这回就没有人真正介绍自己的城市了，大家又当了一回复读机。可能由于感冒造成鼻音太重的问题，我试音的音量偏低，得说得用点劲才行。我在这里也停顿得有点久，因为待会等大家都开始说了，我就可以偷偷混投入其中以掩盖我拙劣的口语哈哈。事实上，和大家一起说真的能说得更开更自信，当大部分人说完之后，我们考场里有一个女生还在说，我明显地听到她顿了一下，然后声音顿时小了很多。之前超牛的老哥老早就进去了（他很早就完成了听力），我进去之后本想偷听他在说什么，因为口语题都一样，结果…天呐竟然跟不上他的语速！还好这时候有一个水平不高但的确可以帮到我的吞吞吐吐的小哥开始讲了，我听到他在说work什么的，自己在脑子里构思了一下便也开始答题了。然而，我的提前构思反倒先入为主了。当题目放出来时，我花了好久才读清题目，因为跟我想的太不一样了。以后还是不能太期望于听到别人的答案。最后，第一部分比较凉。其实整个口语都比较凉，因为我感冒鼻音简直太重了，就像蒙着几个口罩一样，特别是其中有个录音我还咳嗽了几声。写作最后两部分考试感觉时间飞快，既然口语凉了，也听说写作给分还可以，我就放飞自我开始写了。在我听听力的时候，就听到劈里啪啦的打字声了。一直对自己打字速度很有自信的我还是小惊讶了一下。两篇作文都不是很难，我第二篇大概只写了三百词出头一些，细节还是写的有点少，都是论述的，这点下回要改进。两篇作文都是到点自动保存提交的，没有整体检查拼写，打字速度还是得练，盲打还是得加强。考场的机械键盘相对来说比较扁平，跟笔记本手感还是比较相近的。考完最后还有一个report成绩还是cancel的选项，考官明确说过这个不能提问，于是我看的很仔细。还好我的水平还是无压力看懂了，砸了两千块当然要report啦！出考场后还是有点不放心特地查了一下，发现还是有网友选cancel的，不过好像可以付额外的费用解决。出考场才知道已经十二点半了，由于中途没补充零食，肚子也是饿得咕咕叫（写大作文的时候开始明显感到饿）。从储物柜拿手机的时候，不小心带了出来，掉在地上了，心里一惊，还好只在钢化膜上留下一条线，当时也觉得无所谓了。考完也挺平静的，感觉托福考试也就这样，只可惜这次时运不济，命途多舛。以后考托福一定要在学期初考，并且好好准备，关键是要注意身体的健康！早上起来的时候百度了一下自己的博客，发现已经被李彦宏爸爸的百度收录了，可以直接百度到我的博客和文章，也是今天比较开心的一件事吧，以后会好好做SEO的。结果出成绩了，更新一下。10月19号上午考完的试，31号凌晨3：51终于刷新的成绩。查询页面显示的是“2019年10月23日的MyBest Scores”，不知道是不是23号就批好了成绩。考完后一直关注贴吧和公众号，似乎我那周是最后一次最多两周出成绩的考试，以后托福的成绩好像都会考后10天就出结果。雅思更狠，马上跟着改成了6天出成绩，它们是不是也在竞争呢…查分的时候还是很忐忑的，没想到这次首考的成绩能到90+，虽然完全不够，但还是比我想象得要好一些的。口语果然离20还是差了一点，或许有生病的影响，但的确能体现我的水平，还是得加强练习！别人口中的提分项——写作，我也没有取得高分，看来还是不能大意，平时需要熟能生巧。但愿二战能够取得一定的进步吧！今天去听了我们学校的海外交流项目介绍的讲座，大体上的语言成绩要求是CET4&gt;550，CET6&gt;500，TOEFL&gt;80，IELTS&gt;6.0，否则要电话面试，但这些基本都是相对来说比较中规中矩的科研项目或者学分项目，还是得努力提高英语水平啊！]]></content>
      <categories>
        <category>英语</category>
      </categories>
      <tags>
        <tag>个人经历</tag>
        <tag>托福</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[artificial intelligence笔记：吴恩达——阅读论文的建议]]></title>
    <url>%2Fartificial-intelligence20191007232512%2F</url>
    <content type="text"><![CDATA[接触科研，读paper是一件很头疼的事情。本文就来写一下吴恩达对于阅读ML、DL相关方面论文的建议，方便参考。建议首先要说明的是，这里的建议不是我想出来的，仅仅是对吴恩达提供的建议做搬运及整理。如果你读到这里，应该也知道这个领域的先驱+巨佬Andrew Ng的大名。吴恩达（Andrew Ng），著名的美籍华裔计算机科学家，曾担任百度首席科学家，任教于Stanford，大家刚入门的时候想必都了解过或者看过由吴恩达老师讲授的斯坦福的经典课程CS229机器学习、CS230深度学习，此外，Andrew Ng还特地在网易云上为中国学生提供了中文字幕的课程（Andrew Ng英语说得比中文溜好多了哈哈）。另外，他还是著名教育平台Coursera的创始人，那里的课程更新鲜更优质，而且还可以锻炼英语能力，旁听即可。呃放错了，不是上面那张，是这张。对于如何阅读论文，Andrew Ng的建议是：不要从头读到尾。相反，需要多次遍历论文。具体有如下几个注意点：阅读文章标题、摘要和图通过阅读文章标题、摘要、关键网络架构图，或许还有实验部分，你将能够对论文的概念有一个大致的了解。在深度学习中，有很多研究论文都是将整篇论文总结成一两个图形，而不需要费力地通读全文。尤其是在描述网络架构的时候，作者一般会采用比较通用的格式，读多了就会熟悉起来，比如下面DenseNet的结构：读介绍、结论、图，略过其他介绍、结论和摘要是作者试图仔细总结自己工作的地方，以便向审稿人阐明为什么他们的论文应该被接受发表。此外，略过相关的工作部分（如果可能的话），这部分的目的是突出其他人所做的工作，这些工作在某种程度上与作者的工作有关。因此，阅读它可能是有用的，但如果不熟悉这个主题，有时会很难理解。通读全文，但跳过数学部分这里我说一下我对于数学部分的处理：一般我会把重要的公式等略读一遍，然后参照着CSDN博客等网站上其他网友的解释与详解进行理解。通读全文，但略过没有意义的部分Andrew Ng还解释说，当你阅读论文时（即使是最有影响力的论文），你可能也会发现有些部分没什么用，或者没什么意义。因此，如果你读了一篇论文，其中一些内容没有意义（这并不罕见），那么你可以先略读。除非你想要掌握它，那就花更多的时间。确实，当我在阅读ILSVRC、COCO等顶级比赛许多获奖模型的论文时，其中都有对比赛情况的详细结果介绍，我觉得这些部分一定程度上是可以扫读和跳读的。分享关于论文，我之前也做过一些分享，详情可以看看我之前的文章。在deep-learning笔记：开启深度学习热潮——AlexNet一文中，我提到了刚开始阅读英文论文的比较有效的方法。在deep-learning笔记：使网络能够更深——ResNet简介与pytorch实现一文中，我也提供了许多经典模型论文的英文版、中文版、中英对照的链接。最后要说明的是，本篇文章中Andrew Ng的建议有部分摘自公众号Datawhale的推送文章。我关注了不少这方面的公众号，筛选了几个比较优质的，在今后也会一一放到博客中推荐。]]></content>
      <categories>
        <category>人工智能</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>深度学习</tag>
        <tag>论文分享</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[欢迎到访：写在前面]]></title>
    <url>%2Fpreface20191007202443%2F</url>
    <content type="text"><![CDATA[不知不觉，建站已有不少日子了，无论是内容还是界面，都逐渐丰富了起来。觉得有必要补充一篇类似于导言的文字，今天抽出点时间写一下，日后继续完善。关于我我来自浙江，就读于华科，目前是一名电子信息工程专业的大二本科生。从大学开始真正比较全面地接触信息技术，一年下来，在课余接触并尝试过的方面主要有编程语言python与R、linux操作系统、前端设计、机器学习与深度学习。我的博客也主要围绕这几个方面展开，具体也可以看看标签页，我也还在不断地探索与学习中。目前我主要学习的是计算机视觉方面的相关知识。入门没多久，许多理解也还比较浅薄，博客内容主要是一些干货的搬运分享并结合自己积累的一些理解与经验，会有不足与疏漏，如果大家能给予指导，我将非常感激！今后我会尽量陆续加入更多深层次的内容。对于这个博客网站，可以把它看作一个技术博客，而我更多的把它看成一个自己的空间，因此偶尔也会加入一些学习生活的元素，请别见怪！此外，我会尽我所能提升文章的质量，在发布后也会不断地查漏补缺，小幅修正与大幅补充结合，使每篇文章尽可能更规范易懂、更丰富充实。或许在这个移动端主导的时代，搭网站写博客的价值有所降低。但当我写了一段时间之后，发现驱使我写博客的动力不减反增。最大的收获就是我每写一篇文章就会不断地想把相关的知识彻底搞懂，这就不断促使我去深究（哪怕钻牛角尖），因此总能收获好多写之前根本没想到的内容。关于博客为了提高访问速度，我对我的博客进行了github+coding双线部署，url如下：github page：https://gsy00517.github.io/coding page：http://gsy00517.coding.me/大家可以择优访问，事实上我并没有感到github的速度比coding慢。此外，由于我是同一个源文件双线部署，因此选择了把所有功能优先应用在github page上（比如文章打分、文章链接），但其实coding page也没有太大的区别。由于我的文章主要是按时间顺序由新到老排列的，多了之后不方便查找和浏览，因此我新增了标签和分类，或许可以帮助你更快地查看想看的内容。此外，也推荐使用我页面上的搜索功能利用关键词查找，非常便捷。注：PC体验更佳~关于订阅首先，我想说的是，如果你觉得我写的内容，或者方向对你有一点用处的话，非常欢迎收藏或订阅我的博客！如果你也在写博客的话，我们可以互相关注！在侧栏，你可能会发现这样一个图标。点击之后，你会进入一个看不懂的atom.xml文件。其实，看不懂是正常的，因为这个是给电脑看的。一个便捷的办法就是使用chrome的扩展程序添加feed，然后打开网页时，就可以直接点击浏览器右上角的图标（会显示加号）进行订阅。这样以后每次更新了新的博文，你就可以收到提醒。此外还可以像收藏其他网站一样进行收藏，这里不详述了。欢迎大家常来踩踩，也欢迎大家留下评论。评论很简单，无需登录任何账号直接评论即可~我目前也还在学习的过程中，欢迎大家和我交流，也欢迎各种批评与建议，我会努力改进！Good News好消息！好消息！本站已和百度、谷歌等世界知名搜索引擎达成战略合作关系！如果我写的文章有不足或疏漏之处、看完后有费解有困惑，都可以直接问度娘和谷哥就好啦~哈哈哈皮这一下很开心。]]></content>
  </entry>
  <entry>
    <title><![CDATA[calculus笔记：分部积分表格法]]></title>
    <url>%2Fcalculus20191007184856%2F</url>
    <content type="text"><![CDATA[假期里想着不能让b站收藏夹里的学习资源一直吃灰，于是又刷了一遍b站的收藏夹。碰巧就看到了自己之前收藏的一种积分方法，那么这篇文章就来搬运一下这种方法的计算流程。表格法事实上，这种方法说白了还是分部积分法，但使用起来却要方便好多。我们直接看例子：求解$ \int \left ( x^{2}+x \right )e^{x}dx $。画一个两行的表格。把多项式部分写在第一行，然后把剩余的部分写在第二行。$ x^{2}+x\ $$ e^{x}\ $接下来，我们对第一行求导，直到导数为零为止。对第二行积分，直到与第一行的0对齐为止。$ x^{2}+x\ $$ 2x+1\ $20$ e^{x}\ $$ e^{x}\ $$ e^{x}\ $$ e^{x}\ $第三步就是交叉相乘，在本题即为第一行第一列与第二行第二列相乘，第一行第二列与第二行第三列相乘，第一行第三列与第二行第四列相乘。要注意的是，这里的交叉相乘还需要带符号，依次为正负正负正…以此类推。最后，将相乘结果相加，整理即可得到最终的解。+\left ( \left ( x^{2}+x \right )*e^{x} \right )-\left ( \left ( 2x+1 \right )*e^{x} \right )+\left ( 2*e^{x} \right )=\left ( x^{2}-x+1 \right )*e^{x}+C\注意：别忘了加上常数C。下面再来看一个例子熟悉一下：求解$ \int xsinxdx $。画表格：$ x\ $$ 1\ $$ 0\ $$ sinx\ $$ -cosx\ $$ -sinx\ $求解：+\left ( x*\left ( -cosx \right ) \right )-\left ( 1*\left ( -sinx \right ) \right )=-xcosx+sinx+C\其实b站上还是有挺多这样的干货的，此生无悔入b站！其它运算终止情况看完上面的部分，细心的你肯定会想到以上的方法并不普适，仅仅适用于导数能求导至零及含有多项式因式的情况。因此，为了能更灵活地运用分部积分表格法，下面补充其它两种运算可以终止的情况。第一行出现零元素这就是上面所说的含多项式的情况，也一并列写在这里，方便总览归纳。某列函数的乘积（或它的常数倍）等于第一列按照分部积分的一般做法，当出现之后的某一项恰好是原来积分或者是原来积分的常数倍时，计算进入循环。这时就可以把两者移到等式的同一侧，计算出结果，这在表格法的分部积分中也是类似的。来看看例子：求解$ \int e^{3x}sin2xdx $。$ e^{3x}\ $$ 3e^{3x}\ $$ 9e^{3x}\ $$ sin2x\ $$ -\frac{cos2x}{2}\ $$ -\frac{sin2x}{4}\ $可见，第三列的乘积和第一列的乘积相差一个常数（这里是$ -\frac{9}{4} $），因此仿照之前的方法交叉相乘列出积分：\int e^{3x}sin2xdx=e^{3x}(-\frac{cos2x}{2})-3e^{3x}(-\frac{sin2x}{4})+9(-\frac{1}{4})\int e^{3x}sin2xdx\移项化简可得：\int e^{3x}sin2xdx=\frac{1}{13}e^{3x}(3sin2x-2cos2x)+C\即为所求。看完这种情况，你一定会敏锐地发现，其实分部积分表格法本质上和一般的分部积分法一模一样，不过的确在使用上还是有一定的优势的。某列的两个函数乘积（记为$ f(x) $）是一个容易计算的积分这种情况下，先把之前的项用之前的方法类似列出，再在结果后加上不定积分$ (-1)^{k-1}\int f(x)dx $。来看例子：求解$ \int x^{2}arctanxdx $。$ arctanx\ $$ \frac{1}{1+x^{2}}\ $$ x^{2}\ $$ \frac{1}{3}x^{3}\ $可得解：\int x^{2}arctanxdx=\frac{1}{3}x^{3}arctanx-\frac{1}{3}\int \frac{x^{3}}{1+x^{2}}dx=\frac{1}{3}\left \{ x^{3}arctanx-\frac{1}{2}[x^{2}-ln(1+x^{2})] \right \}+C\另外，当表中的第一行的某列出现多项之和，而再求导无法改变该函数或者该函数中某一项的属性，则终止表格，后再重新组合，另建表格求解。这种情况一般不会出现在题目中，如遇到再做补充。]]></content>
      <categories>
        <category>知识点与小技巧</category>
      </categories>
      <tags>
        <tag>微积分</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[game笔记：海贼王燃烧之血键盘操作]]></title>
    <url>%2Fgame20191007172952%2F</url>
    <content type="text"><![CDATA[这是一篇关于海贼王也关于游戏的文章，出于对海贼王狂热的喜爱，我决定还是在博客里添一篇这样的文章，感兴趣可以看看。画展国庆期间，我留在了武汉。幸运的是，海贼王官方在大陆的首次巡展“路飞来了”正好此时也在武汉开展。作为一名海贼铁粉，我当然是毫不犹豫地买了票。其实根据我博客网站的icon以及我目前的个人头像，应该很容易看出我对海贼王的热爱哈哈。去的时候快接近饭点了，人还是不少，都是真爱啊~不过像我这样我单身一人的占比不大，但也有。让我惊讶的是当我看着日文的原稿时竟能直接反应出中文，果然那么多年来全套漫画没白买。期间跟一位貌似是艺术生的小哥聊得挺开的，只可惜最后没留联系方式，有缘再见吧。整个展看下来还是挺震撼的，尤其是刚进去的时候，激动地鸡皮疙瘩都起来了。不过跟我在东京塔下面的海贼王主题乐园激动得哭出来还是有一定差距的。看完展之后我买了几张原稿的复刻版，花了不少钱，但觉得挺值，珍藏了。我是一个漫画党，除了剧场版或特别篇之外我看的都是漫画（不过是通过动画入坑的，星空卫视司法岛，一代人的记忆哈哈）。说实话，尾田构思之精巧，漫画史上无人能及，感兴趣可以看看知乎上关于尾田构思的讨论，漫画真的埋了很多神一般的线索，这是动画里办不到的，细细看很有意思。燃烧之血看完展，我心中对于海贼王的热血再一次得到激发，回学校就打开燃烧之血，回到海贼世界过把瘾。海贼王燃烧之血（One Piece：Burning Blood）是16年发行的一款海贼王题材的格斗游戏，个人觉得其中的自然系元素化以及霸气设定真的太棒了！另外各种招式都还原得很全很细致，简直就是一边玩一边享受精彩的画面。文末提供了一些图，可以欣赏一下，真的很赞。由于steam版价格原因（加上全部DLC需两三百rmb）以及原本这游戏好像是在游戏机上的（PC版是移植的），导致PC键盘操作方式的教程不是很全，因此本篇文章主要就是对该款游戏的按键操作做一个补充。按键操作十几个小时玩下来，基本的按键摸得比较熟了，其实键盘操作也有键盘的优势，熟练就好。首先是很普适的移动方式：前进 W后退 S左行 A右行 D下面是一些基本的战斗操作：攻击 K重击 O跳跃 L防御 ；（分号键）往后换人 E往前换人 I突破极限状态 右ctrl必杀技（突破极限状态下） 右ctrl如果要使用招式，那么按下Q，在战斗界面的左侧就会出现招式列表，即三个招式的名称及按键操作，按住Q不松，再配合对应按键，就可以使出对应的招式。招式一 （招式列表情况下） K招式二 （招式列表情况下） O招式三 （招式列表情况下） ；一般情况下，长按对应键不松可以延迟招式的释放时间（比如在对手倒地时可以尝试）。此外，一些招式延迟附带蓄力效果，可以打出更强的攻击（附带破防效果）。接下来是一些组合按键的操作：破防 K+L重击破防 O+；侧步闪躲 W、S、A、D+；范围攻击 S+K范围重击 S+O跳跃攻击 L+K跳跃重击 L+O有些角色还拥有特殊的衍生技能，需通过一定的按键组合释放，这里举两个例子，别的可以参考收藏图鉴：艾斯 神火•不知火 L+O白胡子 垂直跳跃攻击 L+O接下来就是非常有特色的能力啦，按住P键就可以开启自然系能力者的元素化，可以轻松躲掉普攻并适时反击。如果有霸气的话，按住P也可开启霸气，在期间进行攻击就可以造成更大伤害，也不用惧怕自然系了。此外，一些角色开启能力时还可以实现特定的能力，作为海贼迷真的感动到哭，下面举几个例子：女帝 快速后闪 P+L黄猿 瞬移 P+L+方向大熊 瞬移 P+L白胡子 双震 P+招式一白胡子的双震是我最喜欢的技能，真的非常有打击感和冲击力，上图截自b站up主的操作教程，我的许多操作都是从那学来的，他在b站和爱奇艺上都有很详细的连招教程，同时配音也很逗，感兴趣的话可以去观摩一下。角色特点这里补充一些目前我发现的角色的特点，也是只有海贼迷才懂的，可以说这游戏做得真的赞，后续我发现更多的话会继续补充。1.众所周知，路飞无法被女帝石化。2.山治对抗女性角色时，只能对女性示爱，因此只有挨打的份。画面欣赏静态无声的画面比起动态有声的还是差多了，但依旧不影响其魅力，看着就觉得很兴奋啦~当然，游戏仅是起娱乐作用，劳逸结合是关键。如果你也热爱海贼王的话，欢迎和我交流！]]></content>
      <categories>
        <category>游戏</category>
      </categories>
      <tags>
        <tag>海贼王</tag>
        <tag>个人经历</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[hexo笔记：ssh与https以及双线部署的一些注意点]]></title>
    <url>%2Fhexo20191006232704%2F</url>
    <content type="text"><![CDATA[不久前，我对本博客网站做了一些优化，其中包括将网站同时部署到github和coding上。关于双线部署如何具体操作，网上有许多较为详尽的教程可以参考，如果有问题的话可以参考多篇不同的教程找出原因解决。在这篇文章中，我主要想讲讲我这期间遇到的一些小事项。github与coding考虑到每次打开博客的加载速度问题，我前几天尝试了把博客部署到coding上，实现了coding+github双线部署。coding现已经被腾讯云收购，可以直接用微信登录。部署完成后，为了看一看效果，我使用了站长工具分别对coding和github上的网站速度进行了测试。测试结果如下：可见，部署在coding上确实能提高一点速度。不过事实上，在实际使用中，并没有感到coding更快，搜索之后发现似乎是coding的服务器也不在内地而在香港的原因。不过，coding page的确能被百度更快地爬取，更新的文章能够很快地被收录。这里附上我的两个链接，可以看看效果，择优访问：github page：https://gsy00517.github.io/coding page：https://gsy00517.coding.me/ssh与https在网上的一个教程中，作者提到使用ssh比https更加稳定，尝试后暂时没有发现明显的区别，但是另一个直观的改变就是在push代码时，使用ssh url就不需要输入账号和密码。下面是我在hexo配置文件中的设置，也就是位于站点根目录下的_config.yml文件，其中后面注释中的https://github.com/Gsy00517/Gsy00517.github.io.git是原本的https url。上面对应的ssh url一般可以从平台上直接复制获取，也可以参照我的格式进行设置。这里简要说一说ssh与https的区别。一般默认情况下使用的是https，除了需要在fetch和push时使用密码之外，使用https的配置比较方便。然而，使用ssh url却需要先配置和添加好ssh key，并且你必须是这个项目的拥有或者管理者，而https就没有这些要求。其实，配置ssh key也并没有那么繁琐，而且这是一劳永逸的，所以推荐还是使用ssh。要注意的是，ssh key保存的默认位置或许会不同于网上的教程，不过可以自行更改。我的默认地址是在用户文件夹下的AppData\Roaming\SPB_16.6的ssh文件夹中。AppData文件夹默认是隐藏的，可以通过查看隐藏的项目打开。此外，如果需要经常清理temp文件的话，不妨取消这个文件夹的隐藏，这在释放windows空间中还是挺有效的，可以参见windows笔记：释放空间。key所在的文件是上图所示的第二个publisher文件，然而似乎无法直接用office打开，选择打开方式为记事本即可。当然，如果实在找不到key所在的文件，也可以直接使用文件资源管理器的搜索功能查找名为.ssh的文件夹即可。注：http与https的区别在于，http是明文传输的，而https是使用ssl加密的，更加安全。若要将连接提交百度站点验证，就需要使用https协议，这个在github和coding都有强制https访问的选项。双线部署注意事项LeanCloud这里主要针对hexo博客双线部署后可能会出现的几个问题说明一下注意点。首先，如果之前使用的是LeanCloud来接受记录评论和统计阅读量的，那么为了共享数据，必须在LeanCloud控制台设置的安全中心中，添加新增的web安全域名，保存后即可解决问题。Widget如果使用的是基于Widget的评分系统，那么必须更改Widget设置中的domain。我是免费使用Widget，只能同时添加一个domain。我继续使用github page的域名，因此只能在我的github page中看到评分系统。文内链接因为双线部署用的依旧还是同一份本地源码文件，因此在博文中提供的链接依旧是一致的。这里我也将继续使用github page的链接，也就是文内推荐的我本人的博文链接依旧还是指向github page的。事实上，这并无任何影响。]]></content>
      <categories>
        <category>操作和使用</category>
      </categories>
      <tags>
        <tag>hexo</tag>
        <tag>配置优化</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[deep learning笔记：使网络能够更深——ResNet简介与pytorch实现]]></title>
    <url>%2Fdeep-learning20191001184216%2F</url>
    <content type="text"><![CDATA[之前我用pytorch把ResNet18实现了一下，但由于上周准备国家奖学金答辩没有时间来写我实现的过程与总结。今天是祖国70周年华诞，借着这股喜庆劲，把这篇文章补上。References：电子文献：https://blog.csdn.net/weixin_43624538/article/details/85049699https://blog.csdn.net/u013289254/article/details/98785869参考文献：[1]Deep Residual Learning for Image Recognition简介ResNet残差网络是由何凯明等四名微软研究院的华人提出的，当初看到论文标题下面的中国名字还是挺高兴的。文章引入部分，作者就探讨了深度神经网络的优化是否就只是叠加层数、增加深度那么简单。显然这是不可能的，增加深度带来的首要问题就是梯度爆炸、消散的问题，这是由于随着层数的增多，在网络中反向传播的梯度会随着连乘变得不稳定，从而变得特别大或者特别小。其中以梯度消散更为常见。值得注意的是，论文中还提到深度更深的网络反而出现准确率下降并不是由过拟合所引起的。为了解决这个问题，研究者们做出了很多思考与尝试，其中的代表有relu激活函数的使用，Batch Normalization的使用等。关于这两种方法，可以参考网上的资料以及我的博文deep-learning笔记：开启深度学习热潮——AlexNet和deep-learning笔记：学习率衰减与批归一化。对于上面这个问题，ResNet作出的贡献是引入skip/identity connection。如下所示就是两个基本的残差模块。上面这个block可表示为：$ F(X)=H(X)-x $。在这里，X为浅层输出，H(x)为深层的输出。当浅层的X代表的特征已经足够成熟，即当任何对于特征X的改变都会让loss变大时，F(X)会自动趋向于学习成为0，X则从恒等映射的路径继续传递。这样，我们就可以在不增加计算成本的情况下使得在前向传递过程中，如果浅层的输出已经足够成熟（optimal），那么就让深层网络后面的层仅实现恒等映射的作用。当X与F（X）通道数目不同时，作者尝试了两种identity mapping的方式。一种即对X缺失的通道直接补零从而使其能够对齐，这种方式比较简单直接，无需额外的参数；另一种则是通过使用1x1的conv来映射从而使通道也能达成一致。论文老规矩，这里还是先呈上我用黄色荧光高亮出我认为比较重要的要点的论文原文，这里我只有英文版。如果需要没有被我标注过的原文，可以直接搜索，这里我仅提供一次，可以点击这里下载。不过，虽然没有pdf中文版，但其实深度学习CV方向一些比较经典的论文的英文、中文、中英对照都可以到Deep Learning Papers Translation上看到，非常方便。自己实现在论文中，作者提到了如下几个ResNet的版本的结构。这里我实现的是ResNet18。由于这不是我第一次使用pytorch进行实现，一些基本的使用操作我就不加注释了，想看注释来理解的话可以参考我之前VGG的实现。由于残差的引入，导致ResNet的结构比较复杂，而论文中并没有非常详细的阐述，在研究官方源码之后，我对它的结构才有了完整的了解，这里我画出来以便参考。注：此模块在2016年何大神的论文中给出了新的改进，可以参考我的博文deep-learning笔记：记首次ResNet实战。ResNet18的每一layer包括了两个这样的basic block，其中1x1的卷积核仅在X与F（X）通道数目不一致时进行操作，在我的代码中，我定义shortcut函数来对应一切通道一致、无需处理的情况。123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105import torchimport torch.nn as nnimport torch.nn.functional as Fclass ResNet(nn.Module): def __init__(self): super(ResNet, self).__init__() self.conv1 = nn.Conv2d(in_channels = 3, out_channels = 64, kernel_size = 7, stride = 2, padding = 3, bias = False) self.max = nn.MaxPool2d(kernel_size = 3, stride = 2, padding = 1) self.bn1 = nn.BatchNorm2d(64) self.bn2 = nn.BatchNorm2d(64) self.bn3 = nn.BatchNorm2d(128) self.bn4 = nn.BatchNorm2d(256) self.bn5 = nn.BatchNorm2d(512) self.shortcut = nn.Sequential() self.shortcut3 = nn.Sequential(nn.Conv2d(64, 128, kernel_size = 1, stride = 2, bias = False), nn.BatchNorm2d(128)) self.shortcut4 = nn.Sequential(nn.Conv2d(128, 256, kernel_size = 1, stride = 2, bias = False), nn.BatchNorm2d(256)) self.shortcut5 = nn.Sequential(nn.Conv2d(256, 512, kernel_size = 1, stride = 2, bias = False), nn.BatchNorm2d(512)) self.conv2 = nn.Conv2d(in_channels = 64, out_channels = 64, kernel_size = 3, stride = 1, padding = 1, bias = False) self.conv3_1 = nn.Conv2d(in_channels = 64, out_channels = 128, kernel_size = 3, stride = 2, padding = 1, bias = False) self.conv3_2 = nn.Conv2d(in_channels = 128, out_channels = 128, kernel_size = 3, stride = 1, padding = 1, bias = False) self.conv4_1 = nn.Conv2d(in_channels = 128, out_channels = 256, kernel_size = 3, stride = 2, padding = 1, bias = False) self.conv4_2 = nn.Conv2d(in_channels = 256, out_channels = 256, kernel_size = 3, stride = 1, padding = 1, bias = False) self.conv5_1 = nn.Conv2d(in_channels = 256, out_channels = 512, kernel_size = 3, stride = 2, padding = 1, bias = False) self.conv5_2 = nn.Conv2d(in_channels = 512, out_channels = 512, kernel_size = 3, stride = 1, padding = 1, bias = False) self.avg = nn.AdaptiveAvgPool2d((1, 1)) #adaptive自适应，只给定输入和输出大小，让机器自行调整选择核尺寸和步长大小 self.fc = nn.Linear(512, 1000) def forward(self, x): x = F.relu(self.bn1(self.conv1(x))) x1 = self.max(x) #layer1 x = F.relu(self.bn2(self.conv2(x1))) x = self.bn2(self.conv2(x)) x += self.shortcut(x1) #pytorch0.4.0之后这里要改为x = x + self.shortcut(x1) x2 = F.relu(x) x = F.relu(self.bn2(self.conv2(x2))) x = self.bn2(self.conv2(x)) x += self.shortcut(x2) x3 = F.relu(x) #layer2 x = F.relu(self.bn3(self.conv3_1(x3))) x = self.bn3(self.conv3_2(x)) x += self.shortcut3(x3) x4 = F.relu(x) x = F.relu(self.bn3(self.conv3_2(x4))) x = self.bn3(self.conv3_2(x)) x += self.shortcut(x4) x5 = F.relu(x) #layer3 x = F.relu(self.bn4(self.conv4_1(x5))) x = self.bn4(self.conv4_2(x)) x += self.shortcut4(x5) x6 = F.relu(x) x = F.relu(self.bn4(self.conv4_2(x6))) x = self.bn4(self.conv4_2(x)) x += self.shortcut(x6) x7 = F.relu(x) #layer4 x = F.relu(self.bn5(self.conv5_1(x7))) x = self.bn5(self.conv5_2(x)) x += self.shortcut5(x7) x8 = F.relu(x) x = F.relu(self.bn5(self.conv5_2(x8))) x = self.bn5(self.conv5_2(x)) x += self.shortcut(x8) x = F.relu(x) #ending x = self.avg(x) #变换维度，可以设其中一个尺寸为-1，表示机器内部自己计算，但同时只能有一个为-1 x = x.view(-1, self.num_flat_features(x)) x = self.fc(x) x = F.softmax(x, dim = 1) return x def num_flat_features(self, x): size = x.size()[1:] num_features = 1 for s in size: num_features *= s return num_features net = ResNet()同样的，我们可以随机生成一个张量来进行验证：123input = torch.randn(1, 3, 48, 48)out = net(input)print(out)如果可以顺利地输出，那么模型基本上是没有问题的。出现的问题在这里我还是想把自己踩的一些简单的坑记下来，引以为戒。softmax输出全为1当我使用F.softmax之后，出现了这样的一个问题：查找资料后发现，我错误的把对每一行softmax当作了对每一列softmax。因为这个softmax语句是我从之前的自己做的一道kaggle题目写的代码中ctrl+C+V过来的，复制过来的是x = F.softmax(x, dim = 0)，在这里，dim = 0意味着我对张量的每一列进行softmax，这是因为我之前的场景中需要处理的张量是一维的，也就是tensor（）里面只有一对“[]”，此时它默认只有一列，我对列进行softmax自然就没有问题。而放到这里，我再对列进行softmax时，每列上就只有一个元素。那么结果就都是1即100%了。解决的方法就是把dim设为1。下面我在用一组代码直观地展示一下softmax的用法与区别。1234567import torchimport torch.nn.functional as Fx1= torch.Tensor( [ [1, 2, 3, 4], [1, 3, 4, 5], [3, 4, 5, 6]])y11= F.softmax(x1, dim = 0) #对每一列进行softmaxy12 = F.softmax(x1, dim = 1) #对每一行进行softmaxx2 = torch.Tensor([1, 2, 3, 4])y2 = F.softmax(x2, dim = 0)我们输出每个结果，可以看到：bias或许你可以发现，在我的代码中，每个卷积层中都设置了bias = False，这是我在参考官方源码之后补上的。那么，这个bias是什么，又有什么用呢？我们在学深度学习的时候，最早接触到的神经网络应该是感知器，它的结构如下图所示。 要想激活这个感知器，就必须使x1 * w1 + x2 * w2 + ... + xn * wn &gt; T（T为一个阈值），而T越大，想激活这个感知器的难度越大。考虑样本较多的情况，我不可能手动选择一个阈值，使得模型整体表现最佳，因此我们不如使得T变成可学习的，这样一来，T会自动学习到一个数，使得模型的整体表现最佳。当把T移动到左边，它就成了bias偏置，x1 * w1 + x2 * w2 + ... + xn * wn - T &gt; 0。显然，偏置的大小控制着激活这个感知器的难易程度。在比感知器高级的神经网络中，也是如此。但倘若我们要在卷积后面加上归一化操作，那么bias的作用就无法体现了。我们以ResNet卷积层后的BN层为例。可参考我的上一篇博文，BN处理过程中有这样一步： 对于分子而言，无论有没有bias，对结果都没有影响；而对于下面分母而言，因为是方差操作，所以也没有影响。因此，在ResNet中，因为每次卷积之后都要进行BN操作，那就不需要启用bias，否则非但不起作用，还会消耗一定的显卡内存。官方源码如果你此时对ResNet的结构已经有了比较清晰的理解，那么可以尝试着来理解一下官方源码的思路。其实我觉得先看像我这样直观的代码实现再看官方源码更有助理解且更高效。123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256257258259260261262263264265266267268269270271272273274275276277278279280281282283284285286287288289290291292293294295296297298299300301302303304305306307308309310311312313314315316317318319320321322323324325326327328329330331332333334335336337338339340341342343344import torchimport torch.nn as nnfrom .utils import load_state_dict_from_url__all__ = ['ResNet', 'resnet18', 'resnet34', 'resnet50', 'resnet101', 'resnet152', 'resnext50_32x4d', 'resnext101_32x8d', 'wide_resnet50_2', 'wide_resnet101_2']model_urls = &#123; 'resnet18': 'https://download.pytorch.org/models/resnet18-5c106cde.pth', 'resnet34': 'https://download.pytorch.org/models/resnet34-333f7ec4.pth', 'resnet50': 'https://download.pytorch.org/models/resnet50-19c8e357.pth', 'resnet101': 'https://download.pytorch.org/models/resnet101-5d3b4d8f.pth', 'resnet152': 'https://download.pytorch.org/models/resnet152-b121ed2d.pth', 'resnext50_32x4d': 'https://download.pytorch.org/models/resnext50_32x4d-7cdf4587.pth', 'resnext101_32x8d': 'https://download.pytorch.org/models/resnext101_32x8d-8ba56ff5.pth', 'wide_resnet50_2': 'https://download.pytorch.org/models/wide_resnet50_2-95faca4d.pth', 'wide_resnet101_2': 'https://download.pytorch.org/models/wide_resnet101_2-32ee1156.pth',&#125;def conv3x3(in_planes, out_planes, stride=1, groups=1, dilation=1): """3x3 convolution with padding""" return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride, padding=dilation, groups=groups, bias=False, dilation=dilation)def conv1x1(in_planes, out_planes, stride=1): """1x1 convolution""" return nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=stride, bias=False)class BasicBlock(nn.Module): expansion = 1 __constants__ = ['downsample'] def __init__(self, inplanes, planes, stride=1, downsample=None, groups=1, base_width=64, dilation=1, norm_layer=None): super(BasicBlock, self).__init__() if norm_layer is None: norm_layer = nn.BatchNorm2d if groups != 1 or base_width != 64: raise ValueError('BasicBlock only supports groups=1 and base_width=64') if dilation &gt; 1: raise NotImplementedError("Dilation &gt; 1 not supported in BasicBlock") # Both self.conv1 and self.downsample layers downsample the input when stride != 1 self.conv1 = conv3x3(inplanes, planes, stride) self.bn1 = norm_layer(planes) self.relu = nn.ReLU(inplace=True) self.conv2 = conv3x3(planes, planes) self.bn2 = norm_layer(planes) self.downsample = downsample self.stride = stride def forward(self, x): identity = x out = self.conv1(x) out = self.bn1(out) out = self.relu(out) out = self.conv2(out) out = self.bn2(out) if self.downsample is not None: identity = self.downsample(x) out += identity out = self.relu(out) return outclass Bottleneck(nn.Module): expansion = 4 def __init__(self, inplanes, planes, stride=1, downsample=None, groups=1, base_width=64, dilation=1, norm_layer=None): super(Bottleneck, self).__init__() if norm_layer is None: norm_layer = nn.BatchNorm2d width = int(planes * (base_width / 64.)) * groups # Both self.conv2 and self.downsample layers downsample the input when stride != 1 self.conv1 = conv1x1(inplanes, width) self.bn1 = norm_layer(width) self.conv2 = conv3x3(width, width, stride, groups, dilation) self.bn2 = norm_layer(width) self.conv3 = conv1x1(width, planes * self.expansion) self.bn3 = norm_layer(planes * self.expansion) self.relu = nn.ReLU(inplace=True) self.downsample = downsample self.stride = stride def forward(self, x): identity = x out = self.conv1(x) out = self.bn1(out) out = self.relu(out) out = self.conv2(out) out = self.bn2(out) out = self.relu(out) out = self.conv3(out) out = self.bn3(out) if self.downsample is not None: identity = self.downsample(x) out += identity out = self.relu(out) return outclass ResNet(nn.Module): def __init__(self, block, layers, num_classes=1000, zero_init_residual=False, groups=1, width_per_group=64, replace_stride_with_dilation=None, norm_layer=None): super(ResNet, self).__init__() if norm_layer is None: norm_layer = nn.BatchNorm2d self._norm_layer = norm_layer self.inplanes = 64 self.dilation = 1 if replace_stride_with_dilation is None: # each element in the tuple indicates if we should replace # the 2x2 stride with a dilated convolution instead replace_stride_with_dilation = [False, False, False] if len(replace_stride_with_dilation) != 3: raise ValueError("replace_stride_with_dilation should be None " "or a 3-element tuple, got &#123;&#125;".format(replace_stride_with_dilation)) self.groups = groups self.base_width = width_per_group self.conv1 = nn.Conv2d(3, self.inplanes, kernel_size=7, stride=2, padding=3, bias=False) self.bn1 = norm_layer(self.inplanes) self.relu = nn.ReLU(inplace=True) self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1) self.layer1 = self._make_layer(block, 64, layers[0]) self.layer2 = self._make_layer(block, 128, layers[1], stride=2, dilate=replace_stride_with_dilation[0]) self.layer3 = self._make_layer(block, 256, layers[2], stride=2, dilate=replace_stride_with_dilation[1]) self.layer4 = self._make_layer(block, 512, layers[3], stride=2, dilate=replace_stride_with_dilation[2]) self.avgpool = nn.AdaptiveAvgPool2d((1, 1)) self.fc = nn.Linear(512 * block.expansion, num_classes) for m in self.modules(): if isinstance(m, nn.Conv2d): nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu') elif isinstance(m, (nn.BatchNorm2d, nn.GroupNorm)): nn.init.constant_(m.weight, 1) nn.init.constant_(m.bias, 0) # Zero-initialize the last BN in each residual branch, # so that the residual branch starts with zeros, and each residual block behaves like an identity. # This improves the model by 0.2~0.3% according to https://arxiv.org/abs/1706.02677 if zero_init_residual: for m in self.modules(): if isinstance(m, Bottleneck): nn.init.constant_(m.bn3.weight, 0) elif isinstance(m, BasicBlock): nn.init.constant_(m.bn2.weight, 0) def _make_layer(self, block, planes, blocks, stride=1, dilate=False): norm_layer = self._norm_layer downsample = None previous_dilation = self.dilation if dilate: self.dilation *= stride stride = 1 if stride != 1 or self.inplanes != planes * block.expansion: downsample = nn.Sequential( conv1x1(self.inplanes, planes * block.expansion, stride), norm_layer(planes * block.expansion), ) layers = [] layers.append(block(self.inplanes, planes, stride, downsample, self.groups, self.base_width, previous_dilation, norm_layer)) self.inplanes = planes * block.expansion for _ in range(1, blocks): layers.append(block(self.inplanes, planes, groups=self.groups, base_width=self.base_width, dilation=self.dilation, norm_layer=norm_layer)) return nn.Sequential(*layers) def forward(self, x): x = self.conv1(x) x = self.bn1(x) x = self.relu(x) x = self.maxpool(x) x = self.layer1(x) x = self.layer2(x) x = self.layer3(x) x = self.layer4(x) x = self.avgpool(x) x = torch.flatten(x, 1) x = self.fc(x) return xdef _resnet(arch, block, layers, pretrained, progress, **kwargs): model = ResNet(block, layers, **kwargs) if pretrained: state_dict = load_state_dict_from_url(model_urls[arch], progress=progress) model.load_state_dict(state_dict) return modeldef resnet18(pretrained=False, progress=True, **kwargs): r"""ResNet-18 model from `"Deep Residual Learning for Image Recognition" &lt;https://arxiv.org/pdf/1512.03385.pdf&gt;`_ Args: pretrained (bool): If True, returns a model pre-trained on ImageNet progress (bool): If True, displays a progress bar of the download to stderr """ return _resnet('resnet18', BasicBlock, [2, 2, 2, 2], pretrained, progress, **kwargs)def resnet34(pretrained=False, progress=True, **kwargs): r"""ResNet-34 model from `"Deep Residual Learning for Image Recognition" &lt;https://arxiv.org/pdf/1512.03385.pdf&gt;`_ Args: pretrained (bool): If True, returns a model pre-trained on ImageNet progress (bool): If True, displays a progress bar of the download to stderr """ return _resnet('resnet34', BasicBlock, [3, 4, 6, 3], pretrained, progress, **kwargs)def resnet50(pretrained=False, progress=True, **kwargs): r"""ResNet-50 model from `"Deep Residual Learning for Image Recognition" &lt;https://arxiv.org/pdf/1512.03385.pdf&gt;`_ Args: pretrained (bool): If True, returns a model pre-trained on ImageNet progress (bool): If True, displays a progress bar of the download to stderr """ return _resnet('resnet50', Bottleneck, [3, 4, 6, 3], pretrained, progress, **kwargs)def resnet101(pretrained=False, progress=True, **kwargs): r"""ResNet-101 model from `"Deep Residual Learning for Image Recognition" &lt;https://arxiv.org/pdf/1512.03385.pdf&gt;`_ Args: pretrained (bool): If True, returns a model pre-trained on ImageNet progress (bool): If True, displays a progress bar of the download to stderr """ return _resnet('resnet101', Bottleneck, [3, 4, 23, 3], pretrained, progress, **kwargs)def resnet152(pretrained=False, progress=True, **kwargs): r"""ResNet-152 model from `"Deep Residual Learning for Image Recognition" &lt;https://arxiv.org/pdf/1512.03385.pdf&gt;`_ Args: pretrained (bool): If True, returns a model pre-trained on ImageNet progress (bool): If True, displays a progress bar of the download to stderr """ return _resnet('resnet152', Bottleneck, [3, 8, 36, 3], pretrained, progress, **kwargs)def resnext50_32x4d(pretrained=False, progress=True, **kwargs): r"""ResNeXt-50 32x4d model from `"Aggregated Residual Transformation for Deep Neural Networks" &lt;https://arxiv.org/pdf/1611.05431.pdf&gt;`_ Args: pretrained (bool): If True, returns a model pre-trained on ImageNet progress (bool): If True, displays a progress bar of the download to stderr """ kwargs['groups'] = 32 kwargs['width_per_group'] = 4 return _resnet('resnext50_32x4d', Bottleneck, [3, 4, 6, 3], pretrained, progress, **kwargs)def resnext101_32x8d(pretrained=False, progress=True, **kwargs): r"""ResNeXt-101 32x8d model from `"Aggregated Residual Transformation for Deep Neural Networks" &lt;https://arxiv.org/pdf/1611.05431.pdf&gt;`_ Args: pretrained (bool): If True, returns a model pre-trained on ImageNet progress (bool): If True, displays a progress bar of the download to stderr """ kwargs['groups'] = 32 kwargs['width_per_group'] = 8 return _resnet('resnext101_32x8d', Bottleneck, [3, 4, 23, 3], pretrained, progress, **kwargs)def wide_resnet50_2(pretrained=False, progress=True, **kwargs): r"""Wide ResNet-50-2 model from `"Wide Residual Networks" &lt;https://arxiv.org/pdf/1605.07146.pdf&gt;`_ The model is the same as ResNet except for the bottleneck number of channels which is twice larger in every block. The number of channels in outer 1x1 convolutions is the same, e.g. last block in ResNet-50 has 2048-512-2048 channels, and in Wide ResNet-50-2 has 2048-1024-2048. Args: pretrained (bool): If True, returns a model pre-trained on ImageNet progress (bool): If True, displays a progress bar of the download to stderr """ kwargs['width_per_group'] = 64 * 2 return _resnet('wide_resnet50_2', Bottleneck, [3, 4, 6, 3], pretrained, progress, **kwargs)def wide_resnet101_2(pretrained=False, progress=True, **kwargs): r"""Wide ResNet-101-2 model from `"Wide Residual Networks" &lt;https://arxiv.org/pdf/1605.07146.pdf&gt;`_ The model is the same as ResNet except for the bottleneck number of channels which is twice larger in every block. The number of channels in outer 1x1 convolutions is the same, e.g. last block in ResNet-50 has 2048-512-2048 channels, and in Wide ResNet-50-2 has 2048-1024-2048. Args: pretrained (bool): If True, returns a model pre-trained on ImageNet progress (bool): If True, displays a progress bar of the download to stderr """ kwargs['width_per_group'] = 64 * 2 return _resnet('wide_resnet101_2', Bottleneck, [3, 4, 23, 3], pretrained, progress, **kwargs)pth文件在阅读官方源码时，我们会注意到官方提供了一系列版本的model_urls，其中，每一个url都是以.pth结尾的。当我下载了对应的文件之后，并不知道如何处理，于是我通过搜索，简单的了解了pth文件的概念与使用方法。简单来说，pth文件就是一个表示Python的模块搜索路径（module search path）的文本文件，在xxx.pth文件里面，会书写一些路径，一行一个。如果我们将xxx.pth文件放在特定位置，则可以让python在加载模块时，读取xxx.pth中指定的路径。下面我使用pytorch对pth文件进行加载操作。首先，我把ResNet18对应的pth文件下载到桌面。1234567891011import torchimport torchvision.models as models# pretrained = True就可以使用预训练的模型net = models.resnet18(pretrained = False)#注意，根据model的不同，这里models.xxx的内容也是不同的，比如models.squeezenet1_1pthfile = r'C:\Users\sheny\Desktop\resnet18-5c106cde.pth'#pth文件所在路径net.load_state_dict(torch.load(pthfile))print(net)输出结果如下：123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384ResNet( (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False) (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False) (layer1): Sequential( (0): BasicBlock( (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) (1): BasicBlock( (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) ) (layer2): Sequential( (0): BasicBlock( (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (downsample): Sequential( (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False) (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) ) (1): BasicBlock( (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) ) (layer3): Sequential( (0): BasicBlock( (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (downsample): Sequential( (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False) (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) ) (1): BasicBlock( (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) ) (layer4): Sequential( (0): BasicBlock( (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (downsample): Sequential( (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False) (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) ) (1): BasicBlock( (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) ) (avgpool): AdaptiveAvgPool2d(output_size=(1, 1)) (fc): Linear(in_features=512, out_features=1000, bias=True))这样你就可以看到很详尽的参数设置了。我们还可以加载所有的参数。1234567import torchpthfile = r'C:\Users\sheny\Desktop\resnet18-5c106cde.pth'net = torch.load(pthfile)print(net)输出如下：12345678OrderedDict([(&apos;conv1.weight&apos;, Parameter containing:tensor([[[[-1.0419e-02, -6.1356e-03, -1.8098e-03, ..., 5.6615e-02, 1.7083e-02, -1.2694e-02], [ 1.1083e-02, 9.5276e-03, -1.0993e-01, ..., -2.7124e-01, -1.2907e-01, 3.7424e-03], [-6.9434e-03, 5.9089e-02, 2.9548e-01, ..., 5.1972e-01, 2.5632e-01, 6.3573e-02], ...,]]></content>
      <categories>
        <category>人工智能</category>
      </categories>
      <tags>
        <tag>深度学习</tag>
        <tag>论文分享</tag>
        <tag>踩坑血泪</tag>
        <tag>代码实现</tag>
        <tag>pytorch</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[deep learning笔记：学习率衰减与批归一化]]></title>
    <url>%2Fdeep-learning20191001151454%2F</url>
    <content type="text"><![CDATA[一段时间之前，在一个深度学习交流群里看到一个群友发问：为什么他的训练误差最后疯狂上下抖动而不是一直降低。作为一个入门小白，我当时也很疑惑。但后来我结合所学，仔细思考之后，发现这是一个挺容易犯的错误。References：电子文献：https://blog.csdn.net/bestrivern/article/details/86301619https://www.jianshu.com/p/9643cba47655https://www.cnblogs.com/eilearn/p/9780696.htmlhttps://blog.csdn.net/donkey_1993/article/details/81871132https://www.pytorchtutorial.com/how-to-use-batchnorm/问题事实上，这是一个在机器学习中就有可能遇到的问题，当学习速率α设置得过大时，往往在模型训练的后期难以达到最优解，而是在最优解附近来回抖动。还有可能反而使损失函数越来越大，甚至达到无穷，如下图所示。而在深度学习中，假设我们使用mini-batch梯度下降法，由于mini-batch的数量不大，大概64或者128个样本，在迭代过程中会有噪声。这个时候使用固定的学习率导致的结果就是虽然下降朝向最小值，但不会精确地收敛，只会在附近不断地波动（蓝色线）。但如果慢慢减少学习率，在初期，学习还是相对较快地，但随着学习率的变小，步伐也会变慢变小，所以最后当开始收敛时，你的曲线（绿色线）会在最小值附近的一个较小区域之内摆动，而不是在训练过程中，大幅度地在最小值附近摆动。对于这个问题，我目前收集了有下面这些解决办法。直接修改学习率在吴恩达的机器学习课程中，他介绍了一种人为选择学习率的规则：每三倍选择一个学习率。比如：我们首先选择了0.1为学习率，那么当这个学习率过大时，我们修改成0.3。倘若还是偏大，我们继续改为0.01、0.003、0.001…以此类推，当学习率偏小是也是以三倍增加并尝试检验，最终选出比较合适的学习率。但这种方法只适用于模型数量小的情况，且这种方法终究还是固定的学习率，依旧无法很好地权衡从而达到前期快速下降与后期稳定收敛的目的。学习率动态衰减学习率衰减的本质在于，在学习初期，你能承受并且需要较大的步伐，但当开始收敛的时候，小一些的学习率能让你步伐小一些，从而更稳定地达到精确的最优解。为此，我们另外增添衰减率超参数，构建函数使学习率能够在训练的过程中动态衰减。\alpha = \frac{1}{1+decayrate*epochnum}*\alpha _{0}\其中decay rate称为衰减率，epoch num是代数，$ \alpha _{0} $是初始学习率。此外还有下面这些构造方法：指数衰减：$ \alpha =0.95^{epochnum}*\alpha _{0} $其他常用方法：\alpha =\frac{k}{\sqrt{epochnum}}*\alpha _{0}\\alpha =\frac{k}{\sqrt{t}}\alpha _{0}\其中k为mini-batch的数字。几种衰减方法的实现在pytorch中，学习率调整主要有两种方式：1.直接修改optimizer中的lr参数。2.利用lr_scheduler()提供的几种衰减函数。即使用torch.optim.lr_scheduler，基于循环的次数提供了一些方法来调节学习率。3.利用torch.optim.lr_scheduler.ReduceLROnPlateau，基于验证测量结果来设置不同的学习率.下面提供几种实现方法：准备（对下列通用）：1234567891011import torchfrom torch.optim import * #包含Adam，lr_scheduler等import torch.nn as nn#生成一个简单全连接神经网络class net(nn.Module): def __init__(self): super(net, self).__init__() self.fc = nn.Linear(1, 10) def forward(self, x): return self.fc(x)手动阶梯式衰减1234567model = net()LR = 0.01optimizer = Adam(model.parameters(), lr = LR)for epoch in range(100): if epoch % 5 == 0: for p in optimizer.param_groups: p['lr'] *= 0.9 #学习率超参的位置：optimizer.state_dict()['param_groups'][0]['lr']这里是每过5个epoch就进行一次衰减。lambda自定义衰减12345678import numpy as np model = net()LR = 0.01optimizer = Adam(model.parameters(), lr = LR)lambda1 = lambda epoch: np.sin(epoch) / epochscheduler = lr_scheduler.LambdaLR(optimizer, lr_lambda = lambda1)for epoch in range(100): scheduler.step()lr_lambda会接收到一个int参数：epoch，然后根据epoch计算出对应的lr。如果设置多个lambda函数的话，会分别作用于optimizer中的不同的params_group。StepLR阶梯式衰减123456model = net()LR = 0.01optimizer = Adam(model.parameters(), lr = LR)scheduler = lr_scheduler.StepLR(optimizer, step_size = 5, gamma = 0.8)for epoch in range(100): scheduler.step()每个epoch，lr会自动乘以gamma。三段式衰减123456model = net()LR = 0.01optimizer = Adam(model.parameters(), lr = LR)scheduler = lr_scheduler.MultiStepLR(optimizer, milestones = [20,80], gamma = 0.9)for epoch in range(100): scheduler.step()这种方法就是，当epoch进入milestones范围内即乘以gamma，离开milestones范围之后再乘以gamma。这种衰减方式也是在学术论文中最常见的方式，一般手动调整也会采用这种方法。连续衰减123456model = net()LR = 0.01optimizer = Adam(model.parameters(), lr = LR)scheduler = lr_scheduler.ExponentialLR(optimizer, gamma = 0.9)for epoch in range(100): scheduler.step()这种方法就是在每个epoch中lr都乘以gamma，从而达到连续衰减的效果。余弦式调整123456model = net()LR = 0.01optimizer = Adam(model.parameters(), lr = LR)scheduler = lr_scheduler.CosineAnnealingLR(optimizer, T_max = 20)for epoch in range(100): scheduler.step()这里的T_max对应1/2个cos周期所对应的epoch数值。基于loss和accuracy123456model = net()LR = 0.01optimizer = Adam(model.parameters(), lr = LR)scheduler = lr_scheduler.ReduceLROnPlateau(optimizer, mode = 'min', factor = 0.1, patience = 10, verbose = False, threshold = 0.0001, threshold_mode = 'rel', cooldown = 0, min_lr = 0, eps = 1e-08)for epoch in range(100): scheduler.step()当发现loss不再降低或者accuracy不再提高之后，就降低学习率。注：上面代码中各参数意义如下：mode：’min’模式检测metric是否不再减小，’max’模式检测metric是否不再增大；factor：触发条件后lr*=factor；patience：不再减小（或增大）的累计次数；verbose：触发条件后print；threshold：只关注超过阈值的显著变化；threshold_mode：有rel和abs两种阈值计算模式，rel规则：max模式下如果超过best(1+threshold)为显著，min模式下如果低于best(1-threshold)为显著；abs规则：max模式下如果超过best+threshold为显著，min模式下如果低于best-threshold为显著；cooldown：触发一次条件后，等待一定epoch再进行检测，避免lr下降过速；min_lr：最小的允许lr；eps：如果新旧lr之间的差异小与1e-8，则忽略此次更新。这里非常感谢facebook的员工给我们提供了如此多的选择与便利！对于上述方法如有任何疑惑，还请查阅torch.optim文档。批归一化（Batch Normalization）除了对学习率进行调整之外，Batch Normalization也可以有效地解决之前的问题。我是在学习ResNet的时候第一次遇到批归一化这个概念的。随着深度神经网络深度的加深，训练越来越困难，收敛越来越慢。为此，很多论文都尝试解决这个问题，比如ReLU激活函数，再比如Residual Network，而BN本质上也是解释并从某个不同的角度来解决这个问题的。通过使用Batch Normalization，我们可以加快网络的收敛速度，这样我们就可以使用较大的学习率来训练网络了。此外，BN还提高了网络的泛化能力。BN的基本思想其实相当直观：首先，因为深层神经网络在做非线性变换前的激活输入值（就是x=WU+B，U是输入）随着网络深度加深或者在训练过程中，其分布逐渐发生偏移或者变动，之所以训练收敛慢，一般是整体分布逐渐往非线性函数的取值区间的上下限两端靠近（对于Sigmoid函数来说，意味着激活输入值WU+B是大的负值或正值），这就导致了反向传播时低层神经网络的梯度消失，这是训练深层神经网络收敛越来越慢的本质原因。事实上，神经网络学习过程本质上是为了学习数据的分布，而BN就是通过一定的规范化手段，把每层神经网络任意神经元这个输入值的分布强行拉回到均值为0、方差为1的标准正态分布，其实就是把越来越偏的分布强制拉回比较标准的分布，这样使得激活输入值落在非线性函数对输入比较敏感的区域，这样输入的小变化就会导致损失函数较大的变化，从而让梯度变大，避免梯度消失问题产生，而且梯度变大意味着学习收敛速度快，因此通过BN能大大加快训练速度。下面来看看BN的具体操作过程：即以下四个步骤：1.计算样本均值。2.计算样本方差。3.对样本数据进行标准化处理。4.进行平移和缩放处理。这里引入了γ和β两个参数。通过训练可学习重构的γ和β这两个参数，让我们的网络可以学习恢复出原始网络所要学习的特征分布。下面是BN层的训练流程：这里的详细过程如下：输入：待进入激活函数的变量。输出：1.这里的K，在卷积网络中可以看作是卷积核个数，如网络中第n层有64个卷积核，就需要计算64次。注意：在正向传播时，会使用γ与β使得BN层输出与输入一样。2.在反向传播时利用γ与β求得梯度从而改变训练权值（变量）。3.通过不断迭代直到训练结束，求得关于不同层的γ与β。4.不断遍历训练集中的图片，取出每个batch_size中的γ与β，最后统计每层BN的γ与β各自的和除以图片数量得到平均值，并对其做无偏估计直作为每一层的E[x]与Var[x]。5.在预测的正向传播时，对测试数据求取γ与β，并使用该层的E[x]与Var[x]，通过图中11：所表示的公式计算BN层输出。注意：在预测时，BN层的输出已经被改变，因此BN层在预测中的作用体现在此处。上面输入的是待进入激活函数的变量，在残差网络ResNet中，的确也是先经过BN层再用relu函数做非线性处理的。那么，为什么BN层一般用在线性层和卷积层的后面，而不是放在非线性单元即激活函数之后呢？因为非线性单元的输出分布形状会在训练过程中变化，归一化无法消除他的方差偏移。相反的，全连接和卷积层的输出一般是一个对称、非稀疏的一个分布，更加类似高斯分布，对他们进行归一化会产生更加稳定的分布。比如，我们对一个高斯分布的数据relu激活，那么小于0的直接就被抑制了，这样得到的结果很难是高斯分布了，这时候再添加一个BN层就很难达到所需的效果。很多实验证明，BatchNorm只要用了就有效果，所以在一般情况下没有理由不用。但也有相反的情况，比如当每个batch里所有的sample都非常相似的时候，相似到mean和variance都基本为0时，最好不要用BatchNorm。此外如果batch size为1，从原理上来讲，此时用BatchNorm是没有任何意义的。注意：通常我们在进行Transfer Learning的时候，会冻结之前的网络权重，注意这时候往往也会冻结BatchNorm中训练好的moving averages值。这些moving averages值只适用于以前的旧的数据，对新数据不一定适用。所以最好的方法是在Transfer Learning的时候不要冻结BatchNorm层，让moving averages值重新从新的数据中学习。批归一化实现这里还是使用pytorch进行实现。准备（对下列通用）：12import torchimport torch.nn as nn2d或3d输入123456# 添加了可学习的仿射变换参数m = nn.BatchNorm1d(100)# 未添加可学习的仿射变换参数m = nn.BatchNorm1d(100, affine = False)input = torch.autograd.Variable(torch.randn(20, 100))output = m(input)我们查看m，可以看到有如下形式：1BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)这里解释一下涉及到的参数：num_features：来自期望输入的特征数，该期望输入的大小为：batch_size * num_features(* width)eps：为保证数值稳定性（分母不能趋近或取0），给分母加上的值，默认为1e-5。momentum：计算动态均值和动态方差并进行移动平均所使用的动量，默认为0.1。affine：一个布尔值，当设为true时，就给该层添加可学习的仿射变换参数。仿射变换将在后文做简单介绍。BatchNorm1d可以有两种输入输出：1.输入（N，C），输出（N，C）。2.输入（N，C，L），输出（N，C，L）。3d或4d输入12345m = nn.BatchNorm2d(100)#或者m = nn.BatchNorm2d(100, affine = False)input = torch.autograd.Variable(torch.randn(20, 100, 35, 45))output = m(input)BatchNorm2d也可以有两种输入输出：1.输入（N，C，L），输出（N，C，L）。2.输入（N，C，H，W），输出（N，C，H，W）。4d或5d输入123m = nn.BatchNorm3d(100)#或者m = nn.BatchNorm3d(100, affine=False)BatchNorm3d同样支持两种输入输出：1.输入（N，C，H，W），输出（N，C，H，W）。2.输入（N，C，D，H，W），输出（N，C，D，H，W）。仿射变换这里我简单介绍一下仿射变换的概念，仿射变换（Affine Transformation或Affine Map）是一种二维坐标（x, y）到二维坐标（u, v）的变换，它是另外两种简单变换的叠加，一是线性变换，二是平移变换。同时，仿射变换保持了二维图形的“平直性”、“平行性”和“共线比例不变性”，非共线的三对对应点确定一个唯一的仿射变换。补充：共线性：若几个点变换前在一条线上，则仿射变换后仍然在一条线上。平行性：若两条线变换前平行，则变换后仍然平行。共线比例不变性：变换前一条线上两条线段的比例，在变换后比例不变。在二维图像变换中，它的一般表达如下：可以视为线性变换R和平移变换T的叠加。另外，仿射变换可以通过一系列的原子变换的复合来实现，包括平移，缩放，翻转，旋转和剪切。因此我们可以将几种简单的变换矩阵相乘来实现仿射变换。]]></content>
      <categories>
        <category>人工智能</category>
      </categories>
      <tags>
        <tag>深度学习</tag>
        <tag>代码实现</tag>
        <tag>pytorch</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[windows笔记：一些快捷的操作]]></title>
    <url>%2Fwindows20191001130531%2F</url>
    <content type="text"><![CDATA[之前在网上看到一个windows系统下的上帝模式，很好奇，尝试之后感觉不错，这里介绍一下创建的方法。除此之外附上一些类似的快捷操作。上帝模式上帝模式，即”God Mode”，或称为“完全控制面板”。它是windows系统中隐藏的一个简单的文件夹窗口，包含了几乎所有windows系统的设置，如控制面板的功能、界面个性化、辅助功能选项等控制设置，用户只需通过这一个窗口就能实现所有的操控，而不必再去为调整一个小小的系统设置细想半天究竟该在什么地方去打开设置。打开上帝模式后你将会看到如下界面：好吧我承认和想象中的上帝模式不太一样，不过下面我还是介绍一下这个略显简陋的上帝模式是怎么设置的。方式一：添加桌面快捷方式首先在桌面新建一个文件夹。将新建的文件夹命名为：GodMode.{ED7BA470-8E54-465E-825C-99712043E01C}。重命名完成后，你将看到一个类似于控制面板但没有名称的图标，双击打开，就可以看到之前所展示的上帝模式的界面了。方式二：添加到快捷菜单win+R运行，输入regedit打开注册表编辑器，允许更改。依次展开路径至HKEY_CLASSES_ROOT\DesktopBackground\Shell。点击shell后在右侧窗口鼠标右击，选择新建项。把新建的项重命名为“上帝模式”。点击上帝模式后，双击右侧窗口中的默认，在数值数据处输入上帝模式，点击确定。右击上帝模式，选择新建项。把新建的项重命名为“command”。点击command后，双击右侧窗口中的默认，在数值数据处输入：explorer shell:::{ED7BA470-8E54-465E-825C-99712043E01C}，确定。这时候在桌面空白处右键打开快捷菜单，就可以看到上帝模式已成功添加。类似的操作在上面我的快捷菜单中，可以看到还有关机、重启、锁屏等选项。其实它们添加的操作和添加上帝模式的步骤是一样的，只需把命名为“上帝模式”的地方修改成“关机”等文字，并且在上文中的第8步中，用对应的数值数据即可。这里提供四种功能对应的数值数据，其实这些和上面上帝模式的commmand命令都是可以直接在cmd中执行的：关机Shutdown -s -f -t 00注销Shutdown -l重启Shutdown -r -f -t 00锁屏Rundll32 User32.dll,LockWorkStation事实上，锁屏功能可以直接使用win+L快捷键达到目的。除此之外，win还可以搭配其他的一些按键完成一些快捷操作，比如win+D可以快速最小化一切窗口回到桌面，想知道win有哪些搭配可以右键左下角的win图标查看。]]></content>
      <categories>
        <category>操作和使用</category>
      </categories>
      <tags>
        <tag>命令操作</tag>
        <tag>配置优化</tag>
        <tag>windows</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[machine learning笔记：过拟合与欠拟合]]></title>
    <url>%2Fmachine-learning20191001104538%2F</url>
    <content type="text"><![CDATA[本文介绍在模型评估可能会出现的过拟合与欠拟合两种现象，并对解决方法做一个总结。解释我们先通过图片来直观地解释这两种现象：在上图中，右边是过拟合的情况，它指的是模型对于训练数据拟合过度，反映到评估指标上，就是模型在训练集上的表现很好，但在测试集和新数据上的表现较差。这是因为在这种条件下，模型过于复杂，导致把噪声数据的特征也学习到了模型中，导致模型的泛化能力下降，从而在后期的应用过程中很容易输出错误的预测结果。左边是欠拟合的情况，它指的是在训练和预测时的表现都不好，这样的模型没有很好地捕捉到数据地特征，从而不能够很好地拟合数据。相比而言，中间是拟合适当的情况，这种模型在应用中就具有很好的鲁棒性。解决方法针对过拟合获取更多数据更多的样本可以让模型学到更多有效的特征，从而减小噪声的影响。当然，一般情况下直接增加数据是很困难的，因此我们需要通过一定的规则来扩充训练数据。比如，在图像分类问题上，我们可以使用数据增强的方法，通过对图像的平移、旋转、缩放等方式来扩充数据；更进一步地，可以使用生成式对抗网络来合成大量新的训练数据。降低模型复杂度模型复杂度过高是数据量较小时过拟合的主要原因。适当降低模型的复杂度可以避免模型拟合过多的噪声。比如，在神经网络模型中减少网络层数、神经元个数等；在决策树模型中降低树的深度、进行剪枝等。注意：网络深度增加引起的准确率退化不一定是过拟合引起的，这是因为深度造成的梯度消失、梯度爆炸等问题，这在ResNet的论文中有讨论，详细可以看我的博文deep-learning笔记：使网络能够更深——ResNet简介与pytorch实现。正则化方法这里的方法主要是权重正则化法，具体说明可以参考machine-learning笔记：机器学习中正则化的理解。交叉验证交叉验证包括简单交叉验证（数据丰富时）、S折交叉验证（最常用）和留一交叉验证（数据匮乏时）。集成学习即把多个模型集成在一起，从而降低单一模型的过拟合风险。主要有Bagging（bootstrap aggregating）和Boosting（adaptive boosting）这两种集成学习方法。针对欠拟合解决欠拟合问题也可以参照解决过拟合问题的思路；添加新特征当特征不足或者现有特征与样本标签的相关性不强时，模型容易出现欠拟合。因此，通过挖掘“上下文特征”、“组合特征”等新的特征，往往能够取得更好的效果。在深度学习中，也有很多模型可以帮助完成特征工程，比如因此分解机、梯度提升决策树、Deep-crossing等都可以成为丰富特征的方法。增加模型复杂度当模型过于简单时，增加模型复杂度可以使模型拥有更强的拟合能力。比如，在线性模型中添加高次项，在神经网络模型中增加网络层数、神经元个数等。对于模型的选择，我在文末补充了两种模型选择的准则供参考。减小正则化系数正则化是用来防止过拟合的，但当模型出现欠拟合现象时，我们就应该有针对性地减小正则化系数。模型选择准则模型选择的信息准则有很多，我这里介绍我知道的两个比较常用的模型选择准则：AIC准则赤池信息准则（Akaike Information Criterion，AIC）公式定义如下：AIC=2k-2ln(L)\其中k表示模型参数个数（复杂度），L表示经验误差（似然函数）。当需要从一组可供选择的模型中选择最佳模型时，通常选择AIC最小的模型。BIC准则贝叶斯信息准则（Bayesian Information Criterion，BIC）是对AIC准则的改进，定义如下：BIC=kln(n)-2ln(L)\与AIC不同，这里k的系数不再是常数。其中n代表的是样本量（数据量），这样，BIC准则就与样本量相关了。当样本量足够时，过拟合的风险变小，我们就可以允许模型复杂一些。这里再次附上这张直观的图片，方便理解与体会。简析可参考machine-learning笔记：机器学习中正则化的理解。]]></content>
      <categories>
        <category>人工智能</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>模型评估</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[artificial intelligence笔记：人工智能前沿发展情况分享]]></title>
    <url>%2Fartificial-intelligence20191001101334%2F</url>
    <content type="text"><![CDATA[这是我在一个相关的群里看到的一个论文，这篇论文比较新，看完之后觉得对目前AI发展状况的了解有一定价值，就放了上来。论文这里直接提供图片形式的原文：其他难得有一篇以AI开篇的文章，由于在我不到一年前真正接触AI相关知识时，一直疑惑人工智能、机器学习与深度学习之间的关系。直到看了台大教授李宏毅的课才知道三者之间的包含关系，这里就把课件中的一张图片放上来，一目了然：最后，再补张和deep-learning笔记：一篇非常经典的论文——NatureDeepReview文末对应的一张我觉得挺真实的图哈哈。不得不说，目前丰富的库和各种深度学习框架的确极大地方便了AI的学习与研究，许多轮子都已造好。学会运用这些工具还是很有帮助的！]]></content>
      <categories>
        <category>人工智能</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>深度学习</tag>
        <tag>论文分享</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[machine learning笔记：一个支持向量机的问题]]></title>
    <url>%2Fmachine-learning20191001093428%2F</url>
    <content type="text"><![CDATA[在学习机器学习理论的过程中，支持向量机（SVM）应该是我们会遇到的第一个对数学要求比较高的概念。理解它的原理要花费了我不少时间，写这篇博文是因为我之前看到的一个有关SVM的问题，其解答需用到SVM的相关数学原理，可以促使我思考。支持向量机的具体原理以及推导网上有大量资源，我也会在文中提供一些供参考。简介支持向量机是一种有监督的学习方法，主要思想是建立一个最优决策超平面，使得该平面两侧距离该平面最近的两类样本之间的距离最大化，从而对分类问题提供良好的泛化能力。这里有个小故事，也是我第一次看SVM课程时老师提到的，可以通过这个小故事大致理解一下SVM在做什么。它的优点主要有如下四点：（1）相对于其他训练分类算法，SVM不需要过多的样本。（2）SVM引入了核函数，可以处理高维的样本。（3）结构风险最小。也就是说，分类器对问题真实模型的逼近与真实解之间的累计误差最小。（4）由于SVM的非线性，它擅长应付线性不可分的问题。这主要是用松弛变量（惩罚变量）和核函数来实现的。这里我附上我所知的三个SVM的常用软件工具包：SVMLight、LibSVM、Liblinear。问题下面就是我在文章开头提到的问题，直接搬运：解析中提到的拉格朗日乘子法和KKT条件，也是我在看到这个问题后才尝试去理解的。能力有限，不能自己很好的解释，这里附上瑞典皇家理工学院（KTH）“统计学习基础”课程的KKT课件，个人觉得讲的很直观且详细了。]]></content>
      <categories>
        <category>人工智能</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[database笔记：范式的理解]]></title>
    <url>%2Fdatabase20190921195840%2F</url>
    <content type="text"><![CDATA[今天终于完成了计算机三级数据库的考试，这也是本学期的第一门考试。听说计算机三级中要属计算机网络最简单，然而出于学到更多有用的知识的目的，我报了数据库。然而事实证明也没学到多少，毕竟这个计算机等级考试是给非计算机专业的人设置的，现在只求能过。不过两三天书看下来，还是有些收获，现在考完了有时间就在这里记一下，方便自己和别人今后有需要看。References：电子文献：https://blog.csdn.net/he626shidizai/article/details/90707037https://blog.csdn.net/u013011841/article/details/39023859范式注意：本文中的范式指的是数据库范式。在设计数据库时，为了设计一个良好的逻辑关系，必须要使关系受一定条件的约束，这种约束逐渐成为一种规范，就是我们所说的范式。目前关系数据库有六种范式：第一范式（1NF）、第二范式（2NF）、第三范式（3NF）、巴斯-科德范式（BCNF）、第四范式(4NF）和第五范式（5NF）。要求最低的是1NF，往后依次变得严格。其中最后的5NF又称完美范式。数据库一般只需满足3NF，下面我就介绍一下前三种范式。第一范式数据库考试官方教程并没有对每个范式的定义进行讲解，另外因为文字定义比较晦涩难懂，我这里通过多方参考，用图片的形式来展示各个约束条件。首先，1NF是所有关系型数据库最基本的要求，它的定义为：符合1NF的关系中的每个属性都不可再分。下图就是一个违反1NF的例子：修改如下：上面的情况就符合1NF了。我们还可以把第一范式分成两点来理解：每个字段都只能存放单一值还是上反例： 上图中，第一行的课程有两个值，这就不符合第一范式了。因此要修改成这样：每笔记录都要能用一个唯一的主键识别 这里出现了重复组，同样也不满足1NF，因为缺乏唯一的标识码。因此修改如下：第二范式第二范式是建立在第一范式的基础上的，它的改进在于：消除了非主属性对于码的部分函数依赖。第二范式消除了非主属性对于码的部分函数依赖，也就是说，第二范式中所有非主属性完全依赖于主键，即不能依赖于主键的一部分属性。为了解释明白，还是通过实例的说明：上表中，学号和课程号组合在一起是主键，但是姓名只由学号决定，这就违反了第二范式。同样的，课程名只由课程号决定，这也违反了第二范式。此外，只需要知道学号和课程号就能知道成绩。为了满足第二范式，我们就需要对上表做如下拆分：第三范式同样的，第三范式建立在第二范式的基础上。不同之处在于，在第二范式的基础之上，第三范式中非主属性都不传递依赖于主键。这是什么意思？还是看图说话：上表中，主键是学号，且已满足第二范式。然而，学校的地址也可以根据学校名称来确定，第三范式就是在这里再做一个分解：]]></content>
      <categories>
        <category>知识点与小技巧</category>
      </categories>
      <tags>
        <tag>数据库</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[hexo笔记：在linux（ubuntu）下安装使用]]></title>
    <url>%2Fhexo20190917085649%2F</url>
    <content type="text"><![CDATA[之前一直在win10下使用hexo搭建部署博客，方法参见：hexo笔记：开始创建个人博客——方法及原因。那么，如果想在linux环境下使用hexo，该如何操作呢？References：电子文献：https://blog.csdn.net/y5492853/article/details/79529410原因由于在更改主题配置文件_config.yml时，长达八百多行的配置文件总是让我找得头晕目眩。由于VScode（好像）没有提供字符的快速查找匹配功能，我之前一直采用一种笨拙的办法，即在文件的空处输入想要查找的字符然后左键选中，这个时候文件中同样的字符也会被选中，这样快速拉动滚动条时就可以比较明显地发现想找的目标字符了。然而对于这种办法，我觉得主要有两大问题：忘记删除在空白处添加的文字我就犯过这样低级的错误，找到并更改之后没有删除自己添加的字符就直接快乐地ctrl+S了，于是就造成了网站能打开但是一片空白的bug。所以大家没事还是不要随意在主题配置文件中添加文字。不是长久之计虽然这个八百多行的文件已经让我够呛了，然后或许今后还会遇到更长的文件，那么这种方法就会变得极其低效（而且伤眼睛）。基于这些因素，我脑子里的第一个反映就是vim编辑器中的对文件字符的查找定位的功能（关于vim的使用，等我多多尝试并熟练之后再做小结）。好了，接下来就开始操作吧。更正最近突然发现VScode自带了搜索功能，可以直接在整个文件夹中搜索关键词。这里所给的快捷键是ctrl+shift+F，但win10用户可能会发现按了之后没有任何反应。事实上，反应还是有的，当你再次打字时，就会发现简体变成了繁体，再次按ctrl+shift+F即可恢复。直接点击搜索图标即可便捷地进行搜索，为我之前眼瞎没有发现表示无奈，但下面还是写一下怎么安装。安装首先安装node.js。这里就没windows下直接双击exe安装包那么easy啦，打开终端，老老实实输命令：123sudo apt-get install nodejssudo apt install nodejs-legacysudo apt install npm其实熟练之后觉得apt是真的好用。由于ubuntu源中的node.js是旧版本，下面会出现问题，我在后文解释。由于npm服务器在国外可能会影响下载速度，和windows下的步骤一样，我们换成淘宝镜像：1sudo npm config set registry https://registry.npm.taobao.org这时候如果我们直接安装hexo，会出现如下错误：因此，我们安装node升级工具n：1sudo npm install n -g并且使用sudo n stable升级版本，若看到如下输出，说明升级成功：注意：fetch可能需要花费一点时间，这时候终端不会有任何输出，不要以为出错了，耐心等待即可，不要ctrl+C中止。最后，我们安装hexo：1sudo npm install -g hexo注：-g表示安装到全局环境。接下来的初始化操作跟windows下基本一样，可以参照我之前的博文。我继续对原来的博客进行编辑，所以无需初始化一个新的，直接把windows的对应文件夹整个copy过来就行了。]]></content>
      <categories>
        <category>操作和使用</category>
      </categories>
      <tags>
        <tag>hexo</tag>
        <tag>ubuntu</tag>
        <tag>安装教程</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[machine learning笔记：机器学习中正则化的理解]]></title>
    <url>%2Fmachine-learning20190915150339%2F</url>
    <content type="text"><![CDATA[在接触了一些ml的知识后，大家一定会对正则化这个词不陌生，但是我感觉根据这个词的字面意思不能够直接地理解它的概念。因此我打算写一篇文章做个记录，方便以后回忆。References：参考文献：[1]统计学习方法（第2版）线性代数中的正则化如果直接搜索正则化这个名词，首先得到的一般是代数几何中的一个概念。百度词条对它的解释是：给平面不可约代数曲线以某种形式的全纯参数表示。怎么样？是不是觉得一头雾水。这里我推荐使用谷歌或者维基百科来查询这些专业名词。对于不能科学上网的朋友，没关系，我这里提供了谷歌镜像和wikiwand，大家可以在上面得到一样的搜索结果。我们直接到维基百科搜索regularization：里面第一段是这样解释的：In mathematics, statistics, and computer science, particularly in machine learning and inverse problems, regularization is the process of adding information in order to solve an ill-posed problem or to prevent overfitting.这就和我们在机器学习应用中的目的比较相近了。机器学习中的正则化在机器学习中，正则化是一种为了减小测试误差的行为（有时候会增加训练误差）。我们在构造机器学习模型时，最终目的是让模型在面对新数据的时候，可以有很好的表现。当你用比较复杂的模型比如神经网络去拟合数据时，很容易出现过拟合现象（训练集表现很好，测试集表现较差），这会导致模型的泛化能力下降，这时候，我们就需要使用正则化，来降低模型的复杂度。为了加深印象，我下面简单介绍几种常用的机器学习正则化方法：早停法（Early Stopping）早停法，就是当训练集的误差变好，但是验证集的误差变坏（即泛化效果变差）的时候停止训练。这种方法可以一定程度上有效地防止过拟合，同时这也说明了验证集在机器学习中的重要性。权重正则化法因为噪声相比于正常信号而言，通常会在某些点出现较大的峰值。所以，只要我们保证权重系数在绝对值意义上足够小，就能够保证噪声不会被过度响应，这也是奥卡姆剃刀原理的表现，即模型不应过度复杂，尤其是当数据量不大的时候。上面是在一个网课上看到的、我觉得可以较好地呈现模型的复杂度与数据量对模型预测表现的影响的一张图片，其中向左的横轴表示数据量大小，向右的横轴表示模型复杂度，竖轴是预测表现。通过这张图，可以很明显地观察到：模型的复杂度提升需要大量的数据作为依托。权重正则化主要有两种：L1正则：$ J=J_{0}+\lambda \left | w \right |_{1} $，其中J代表损失函数（也称代价函数），$ \left | w \right |_{1} $代表参数向量w的L1范数。L2正则（weight decay）：$ J=J_{0}+\lambda \left | w \right |_{2} $，其中$ \left | w \right |_{2} $代表参数向量w的L2范数。 这里就产生了Lasso回归与岭回归两大机器学习经典算法。其中Lasso回归是一种压缩估计，可以通过构造惩罚函数得到一个较为精炼的模型，使得它可以压缩一些系数，同时设定一些系数为0，从而达到特征选择的目的。基于Lasso回归这种可以选择特征并降维的特性，它主要有这些适用情况：样本量比较小，但指标量非常多的时候（易于过拟合）。进行高维统计时。需要对特征进行选择时。对于这些回归的详细解释，大家可以到网上搜集相关信息。补充：L0范数：向量中非零元素的个数。L1范数：向量中每个元素绝对值的和。L2范数：向量元素绝对值的平方和再开方。下面我再附上一组图，希望能帮助更好地理解权重正则化：首先我们可视化一个损失函数。下面我们看一看正则化项的图示，这里使用L1范数作为正则化项。接着，我们将上面两者线性组合：我们来看看结果：可见，正则化项的引入排除了大量原本属于最优解的点，上图的情况中剩下一个唯一的局部最优解。正则化项的引入，除了符合奥卡姆剃刀原理之外。同时从贝叶斯估计的角度看，正则化项对应于模型的先验概率，即相当于假设复杂的模型具有较小的先验概率，而简单的模型具有较大的先验概率。数据增强数据增强可以丰富图像数据集，有效防止过拟合。这种方法在AlexNet中有很好的应用，大家可以看看我的博文deep-learning笔记：开启深度学习热潮——AlexNet。随机失活（dropout）dropout即随机砍掉一部分神经元之间的连接，每次只更新一部分，这可以有效地增加它的鲁棒性，提高泛化能力。这个方法在AlexNet中也有详细的解释，推荐大家去看一下。 以上就是比较常规且流行的正则化方式，今后或许会有补充，也欢迎大家提供意见~奥卡姆剃刀原理上文在正则化一节提到了奥卡姆剃刀原理，这里就简单做个说明。奥卡姆剃刀原理应用于模型选择时可以简单表述为如下思想：在所有可能选择的模型中，能够很好地解释已知数据并且十分简单的模型才是最好的模型，也就是我们应该选择的模型。]]></content>
      <categories>
        <category>人工智能</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[deep learning笔记：开启深度学习热潮——AlexNet]]></title>
    <url>%2Fdeep-learning20190915113859%2F</url>
    <content type="text"><![CDATA[继之前那篇deep-learning笔记：着眼于深度——VGG简介与pytorch实现，我觉得还是有必要提一下VGG的前辈——具有历史意义的AlexNet，于是就写了这篇文章简要介绍一下。References：电子文献：https://blog.csdn.net/zym19941119/article/details/78982441参考文献：[1]ImageNet Classiﬁcation with Deep Convolutional Neural Networks简介ALexNet是第一个运用大型深度卷积神经网络的模型，在ILSVRC中一下子比前一年把错误率降低了10%，这是非常惊人的，也很快引起了注意。于是，自2012年开始，深度学习热潮由此引发。根据我之前听网课的笔记以及网上的其他文章，我把AlexNet主要的进步归纳如下：使用大型深度卷积神经网络。分组卷积（groupconvolution）来充分利用GPU。随机失活dropout：一种有效的正则化方法。关于正则化，可以看我的博文machine-learning笔记：机器学习中正则化的理解。数据增强data augumentation：增大数据集以减小过拟合问题。relu激活函数：即max（0，x），至今还被广泛应用。个人思考这段时间也看了不少东西，对于如何提升神经网络的性能这个问题，我觉得主要有如下三个方面：从网络本身入手增加深度。增加宽度。减少参数量。防止过拟合。解决梯度消失的问题。从数据集入手尽可能使用多的数据。从硬件入手提升GPU性能。充分利用现有的GPU性能。当你阅读完AlexNet的论文，你会发现它在这几个方面都有思考且做出了非常优秀的改进。论文在放论文之前，我还是先贴一张流程图，方便在阅读论文的时候进行对照与理解。下面奉上宝贵的论文：论文原版论文中文版从introduction第一句开始，作者就开始了一段长长的吐槽：Current approaches to object recognition make essential use of machine learning methods…吐槽Yann LeCun大佬的论文被顶会拒收了仅仅因为Yann LeCun使用了神经网络。其实，那段时间之前，由于SVM等机器学习方法的兴起，神经网络是一种被许多ml大佬们看不起的算法模型。在introduction的最后，作者留下了这样一句经典的话：All of our experiments suggest that our results can be improved simply by waiting for faster GPUs and bigger datasets to become available.有没有感觉到一种新世界大门被打开的感觉呢？有关论文别的内容，我暂不多说了，大家可以自己看论文学习与体会。附上推荐重点阅读的章节：3.1 ReLU Nonlinearity；3.5 Overall Architecture；4 Reducing Overfitting。说明同我写VGG的那篇文章中一样，我在英文原版中用黄颜色高亮了我觉得重要的内容给自己和大家今后参考。另外，我在这里推荐大家还是先尝试阅读英文原版。一方面由于一些公式、符号以及名词的原因，英文原版叙述更精准，中文翻译有缺漏、偏颇之处；另一方面更重要的，接触这些方面的知识仅参考中文是远远不够的。在这里我推荐一个chrome英文pdf阅读插件，大家可以自己到chrome里面搜索安装：有了这个插件，遇到不认识的单词，只需双击单词，就可以看到中文释义，一定程度上可以保证阅读的流畅性。但是如果想从根本上解决问题，只有好好背单词吧（我也在朝这个方向努力…）。另外，iPad的上也有好多强大的app，在这里不一一推荐了。补充：最近又发现一款特别好用且美观的查词插件，功能非常强大，推荐一下：沙拉查词。]]></content>
      <categories>
        <category>人工智能</category>
      </categories>
      <tags>
        <tag>深度学习</tag>
        <tag>论文分享</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[markdown笔记：公式插入和代码高亮]]></title>
    <url>%2Fmarkdown20190915095628%2F</url>
    <content type="text"><![CDATA[在上一篇文章deep-learning笔记：着眼于深度——VGG简介与pytorch实现中，我用到了markdown其他的一些使用方法，因此我想在此对之前的一篇文章markdown笔记：markdown的基本使用做一些补充。References：电子文献：https://www.jianshu.com/p/25f0139637b7https://www.jianshu.com/p/fd97e1f8f699https://www.jianshu.com/p/68e6f82d88b7https://www.jianshu.com/p/7c02c112d532公式插入无论是学习ml还是dl，我们总是离不开数学的，于是利用markdown插入数学公式就成了一个的需求。那么怎么在markdown中插入公式呢？markdown中的公式分为两类，即行内公式与行间公式。它们对应的代码如下：12$ \Gamma(z) = \int_0^\infty t^&#123;z-1&#125;e^&#123;-t&#125;dt\,. $$$\Gamma(z) = \int_0^\infty t^&#123;z-1&#125;e^&#123;-t&#125;dt\,.$$让我们来看一下效果：行内公式：$ \Gamma(z) = \int_0^\infty t^{z-1}e^{-t}dt\,. $行间公式：\Gamma(z) = \int_0^\infty t^{z-1}e^{-t}dt\,.如果你是使用hexo编写博客，那么默认的设置是无法转义markdown公式的，解决这个问题的配置方法可以参考本文顶部给出的第三个链接。另外要注意，在使用公式时，对应文件需开启mathjax选项。补充更新：在查看next主题配置文件时，我注意到next好像自带mathjax支持，设置如下，这样就无需在每个文件中添加开启mathjax的选项。markdown公式的具体语法可以参照本文的第一个链接，你可以在typora中根据它的官方文档进行尝试。注意：在typora中，只需输入$或者$$就可直接进入公式编辑，无需输入一对。有机会我再对上面提到的语法进行搬运。下面介绍一种更简单省力的方法（也是我在用的方法）：打开在线LaTex公式编辑器。在上方的框框中输入你想要的公式： 你可以在下方的GIF图中随时观察你的输入时候符合预期，如在书写word文档等类似文本时需要插入公式，也可以直接复制图片。拷贝下方黄颜色方框中的代码到markdown文件。 你可以选择去掉两边的“\”和方括号，否则你的公式两侧将会套有方括号，另外你还需要使用上文提到的$来确定公式显示方式。这里我们这样输入：$ x+y=z $。得到：$ x+y=z $。以上就是使用LaTex给markdown添加公式的方法。你也可以使用黄颜色框中的URL选项来添加代码，格式是![](URL)。例如，输入：![](https://latex.codecogs.com/gif.latex?x&amp;plus;y=z)可以看到：这种方法就不需要文章顶部链接三中的配置了，也是一种推荐的方法。注：若在在线LaTex公式编辑器中找不到需要的元素或者符号的话，可以看一看LaTex常用公式整理。代码高亮markdown中使代码高亮的格式如下：三个反引号+语言名代码…三个反引号例如，输入：可以看到：1print("hello world!")同样的，在typora中，你也不必输入成对的三个反引号。这里我要提醒一个我以前用Rmarkdown时踩过的坑：注意！他俩是不一样的！真正的“`”在这里：]]></content>
      <categories>
        <category>操作和使用</category>
      </categories>
      <tags>
        <tag>markdown</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[deep learning笔记：着眼于深度——VGG简介与pytorch实现]]></title>
    <url>%2Fdeep-learning20190915073809%2F</url>
    <content type="text"><![CDATA[VGG是我第一个自己编程实践的卷积神经网络，也是挺高兴的，下面我就对VGG在这篇文章中做一个分享。References：电子文献：https://blog.csdn.net/xiaohuihui1994/article/details/89207534https://blog.csdn.net/sinat_33487968/article/details/83584289https://blog.csdn.net/qq_32172681/article/details/95971492参考文献：[1]Very Deep Convolutional Networks For Large-scale Image Recognition简介VGG模型在2014年取得了ILSVRC竞赛的第二名，第一名是GoogLeNet。但是VGG在多个迁移学习任务中的表现要优于googLeNet。相比之前的神经网路，VGG主要有两大进步：其一是它增加了深度，其二是它使用了小的3x3的卷积核，这可以使它在增加深度的时候一定程度上防止了参数的增长。缺点是它的参数量比较庞大，但这并不意味着它不值得我们仔细研究。下图展示的是VGG的结构。为了通过对比来对VGG的一些改进进行解释，VGG的作者在论文中提供了多个版本。论文要详细分析VGG，我可能不能像网上写的那样好，更不可能像论文一样明白。那么我在这里就先附上论文。论文原版论文中文版我在英文原版中用黄颜色高亮了我觉得比较重要的内容，大家可以参考一下。大家也可以自己到网上进行搜索，这类经典的网络网上有许多介绍与分析。通过论文或者网上的资源对这个网络有一定理解之后，你可以看看我下面的代码实现。结论此篇论文的得出了一些结论，总结如下：在一定范围内，通过增加网络深度能有效地提升网络性能。这在ResNet里更是得到了显著地体现，上面的ILSVRC历年winner表现统计图就是一个很好的证明，可参见deep-learning笔记：使网络能够更深——ResNet简介与pytorch实现。与AlexNet对比可知，多个小卷积核比单个大卷积核性能要好。AlexNet中用到的LRN层（局部响应归一化层）并没有带来性能的提升，因此可以排除。尺度抖动（scale jittering）即多尺度训练、多尺度测试有利于网络性能的提升。最佳模型为VGG16，其从头到尾只用了3x3的卷积和2x2的池化。特点VGG的特点（创新点）主要有如下四个：小卷积核VGG使用多个小卷积核来代替大的，这样一方面可以减少参数，另一方面相当于进行了更多的非线性映射，可以增加网络的拟合、表达能力。小池化核相比AlexNet的3x3池化核，VGG一律采用了2x2的池化核。层数更深若仅计算conv、fc层的话，VGG中常用的网络层数达到了16、19层（VGG16效果最好），这相较于前几年的研究是一个深度的提升。conv替代fc在基本的CNN中，全连接层的作用是将经过多个卷积层和池化层的图像特征图中的特征进行整合，获取图像特征具有的高层含义，用于图像分类。如果我们把全连接层的输出不再看成n个节点的集合，而是视作一个1x1xn的输出层，那么我们就可以用卷积层来替换全连接层了。并且从数学角度看，它和全连接层是一样的。因为卷积层没有全连接层对输入的限制，因此使用卷积层代替全连接层可以接收任意宽或高的输入。此外，相对于全连接层而言，使用卷积层不会破坏图像的空间结构。这也是一些的网络使用1x1的全卷积层代替全连接层的重要原因。感受野这里会涉及一个名为感受野（Receptive Field）的概念，它指的是卷积神经网络每一层输出的特征图（feature map）上的像素点在输入图片上映射的区域大小。简而言之，就是特征图上的一点跟原有图上有关系的点的区域。一般取一个pixel为单位，而输入的感受野就是1即只对应其自身的那个像素。画图易知，两层3x3的卷积层所得到的感受野与一层5x5的卷积层的感受野相同，这也是VGG使用3x3小卷积核来代替的原理之一。感受野是CNN中一个比较重要的概念，一些目标检测的流行算法如SSD、Faster Rcnn等中的prior box和anchor box的设计都是以感受野为依据的。可以看一下computer-vision笔记：anchor-box。1x1卷积核虽然VGG所用的是2x2的卷积核，但是在上文提到了一些网络使用1x1的全卷积层代替全连接层，那么顺便就对1x1卷积核的作用做一个总结。如上文所述，使用卷积层就没有全连接层对输入尺寸的限制，这也方便了许多。全连接层会改变网络的空间结构，卷积层不会破坏图像的空间结构。可以用于为决策增加非线性因素。一些模型用1x1的全连接层来调整网络维度。比如MobileNet使用1x1的卷积核来扩维，GoogleNet、ResNet使用1x1的卷积核来降维，这里的降维类似于压缩处理，并不会影响训练结果，而1x1的卷积核可以使网络变薄，可以成倍地减少计算量。如下图所示，如果我们使用naive的inception，那么最后concatenate出来的特征图厚度会很大，而添加上1x1的卷积核之后就可以调整厚度了。 自己实现这里我使用pytorch框架来实现VGG。pytorch是一个相对较新的框架，但热度上升很快。根据网上的介绍，pytorch是一个非常适合于学习与科研的深度学习框架。我尝试了之后，也发现上手很快。在pytorch中，神经网络可以通过torch.nn包来构建。这里我不一一介绍了，大家可以参考pytorch官方中文教程来学习，照着文档自己动手敲一遍之后，基本上就知道了pytorch如何使用了。为了方便直观的理解，我先提供一个VGG16版本的流程图。下面是我实现VGG19版本的代码：首先，我们import所需的包。12import torchimport torch.nn as nn接下来，我们定义神经网络。1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980class VGG(nn.Module): def __init__(self, num_classes = 1000): #imagenet图像库总共1000个类 super(VGG, self).__init__() #先运行父类nn.Module初始化函数 self.conv1_1 = nn.Conv2d(in_channels = 3, out_channels = 64, kernel_size = 3, padding = 1) #定义图像卷积函数：输入为图像（3个频道，即RGB图），输出为64张特征图，卷积核为3x3正方形，为保留原空间分辨率，卷积层的空间填充为1即padding等于1，也就是防止每次卷积尺寸缩小过快导致无法使用更多的卷积层 self.conv1_2 = nn.Conv2d(in_channels = 64, out_channels = 64, kernel_size = 3, padding = 1) self.conv2_1 = nn.Conv2d(in_channels = 64, out_channels = 128, kernel_size = 3, padding = 1) self.conv2_2 = nn.Conv2d(in_channels = 128, out_channels = 128, kernel_size = 3, padding = 1) self.conv3_1 = nn.Conv2d(in_channels = 128, out_channels = 256, kernel_size = 3, padding = 1) self.conv3_2 = nn.Conv2d(in_channels = 256, out_channels = 256, kernel_size = 3, padding = 1) self.conv3_3 = nn.Conv2d(in_channels = 256, out_channels = 256, kernel_size = 3, padding = 1) self.conv3_4 = nn.Conv2d(in_channels = 256, out_channels = 256, kernel_size = 3, padding = 1) self.conv4_1 = nn.Conv2d(in_channels = 256, out_channels = 512, kernel_size = 3, padding = 1) self.conv4_2 = nn.Conv2d(in_channels = 512, out_channels = 512, kernel_size = 3, padding = 1) self.conv4_3 = nn.Conv2d(in_channels = 512, out_channels = 512, kernel_size = 3, padding = 1) self.conv4_4 = nn.Conv2d(in_channels = 512, out_channels = 512, kernel_size = 3, padding = 1) self.conv5_1 = nn.Conv2d(in_channels = 512, out_channels = 512, kernel_size = 3, padding = 1) self.conv5_2 = nn.Conv2d(in_channels = 512, out_channels = 512, kernel_size = 3, padding = 1) self.conv5_3 = nn.Conv2d(in_channels = 512, out_channels = 512, kernel_size = 3, padding = 1) self.conv5_4 = nn.Conv2d(in_channels = 512, out_channels = 512, kernel_size = 3, padding = 1) self.relu = nn.ReLU(inplace = True) #inplace=TRUE表示原地操作 self.max = nn.MaxPool2d(kernel_size = 2, stride = 2) self.fc1 = nn.Linear(512 * 7 * 7, 4096) #定义全连接函数1为线性函数:y = Wx + b，并将512*7*7个节点连接到4096个节点上 self.fc2 = nn.Linear(4096, 4096) self.fc3 = nn.Linear(4096, num_classes) #定义全连接函数3为线性函数:y = Wx + b，并将4096个节点连接到num_classes个节点上，然后可用softmax进行处理 #定义该神经网络的向前传播函数，该函数必须定义，一旦定义成功，向后传播函数也会自动生成（autograd） def forward(self, x): x = self.relu(self.conv1_1(x)) x = self.relu(self.conv1_2(x)) x = self.max(x) #输入x经过卷积之后，经过激活函数ReLU，循环两次，最后使用2x2的窗口进行最大池化Max pooling，然后更新到x x = self.relu(self.conv2_1(x)) x = self.relu(self.conv2_2(x)) x = self.max(x) x = self.relu(self.conv3_1(x)) x = self.relu(self.conv3_2(x)) x = self.relu(self.conv3_3(x)) x = self.relu(self.conv3_4(x)) x = self.max(x) x = self.relu(self.conv4_1(x)) x = self.relu(self.conv4_2(x)) x = self.relu(self.conv4_3(x)) x = self.relu(self.conv4_4(x)) x = self.max(x) x = self.relu(self.conv5_1(x)) x = self.relu(self.conv5_2(x)) x = self.relu(self.conv5_3(x)) x = self.relu(self.conv5_4(x)) x = self.max(x) x = x.view(-1, self.num_flat_features(x)) #view函数将张量x变形成一维的向量形式,总特征数并不改变,为接下来的全连接作准备 x = self.fc1(x) #输入x经过全连接1，然后更新x x = self.fc2(x) x = self.fc3(x) return x def num_flat_features(self, x): size = x.size()[1:] #all dimensions except the batch dimension num_features = 1 for s in size: num_features *= s return num_featuresvgg = VGG()print(vgg)我们print网络，可以看到输出如下：1234567891011121314151617181920212223VGG( (conv1_1): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (conv1_2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (conv2_1): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (conv2_2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (conv3_1): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (conv3_2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (conv3_3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (conv3_4): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (conv4_1): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (conv4_2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (conv4_3): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (conv4_4): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (conv5_1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (conv5_2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (conv5_3): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (conv5_4): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (relu): ReLU(inplace=True) (max): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False) (fc1): Linear(in_features=25088, out_features=4096, bias=True) (fc2): Linear(in_features=4096, out_features=4096, bias=True) (fc3): Linear(in_features=4096, out_features=1000, bias=True))最后我们随机生成一个张量来进行验证。123input = torch.randn(1, 3, 224, 224)out = vgg(input)print(out)其中(1, 3, 224, 224)表示1个3x224x224的矩阵，这是因为VGG输入的是固定尺寸的224x224的RGB（三通道）图像。如果没有报错，那么就说明你的神经网路可以运行通过了。我们也可以使用torch.nn.functional来实现激活函数与池化层，这样的话，你需要还需要多引入一个包：123import torchimport torch.nn as nnimport torch.nn.functional as F #新增同时，你不需要在init中实例化激活函数与最大池化层，相应的，你需要对forward前馈函数进行更改：1234567891011121314151617181920212223242526272829303132333435def forward(self, x): x = F.relu(self.conv1_1(x)) x = F.relu(self.conv1_2(x)) x = F.max_pool2d(x, kernel_size = 2, stride = 2) #输入x经过卷积之后，经过激活函数ReLU，循环两次，最后使用2x2的窗口进行最大池化Max pooling，然后更新到x x = F.relu(self.conv2_1(x)) x = F.relu(self.conv2_2(x)) x = F.max_pool2d(x, kernel_size = 2, stride = 2) x = F.relu(self.conv3_1(x)) x = F.relu(self.conv3_2(x)) x = F.relu(self.conv3_3(x)) x = F.relu(self.conv3_4(x)) x = F.max_pool2d(x, kernel_size = 2, stride = 2) x = F.relu(self.conv4_1(x)) x = F.relu(self.conv4_2(x)) x = F.relu(self.conv4_3(x)) x = F.relu(self.conv4_4(x)) x = F.max_pool2d(x, kernel_size = 2, stride = 2) x = F.relu(self.conv5_1(x)) x = F.relu(self.conv5_2(x)) x = F.relu(self.conv5_3(x)) x = F.relu(self.conv5_4(x)) x = F.max_pool2d(x, kernel_size = 2, stride = 2) x = x.view(-1, self.num_flat_features(x)) #view函数将张量x变形成一维的向量形式,总特征数并不改变,为接下来的全连接作准备 x = self.fc1(x) #输入x经过全连接1，然后更新x x = self.fc2(x) x = self.fc3(x) return x如果你之前运行通过的话，那么这里也是没有问题的。这里我想说明一下torch.nn与torch.nn.functional的区别。这两个包中有许多类似的激活函数与损失函数，但是它们又有如下不同：首先，在定义函数层（继承nn.Module）时，init函数中应该用torch.nn，例如torch.nn.ReLU，torch.nn.Dropout2d，而forward中应该用torch.nn.functionl，例如torch.nn.functional.relu，不过请注意，init里面定义的是标准的网络层。只有torch.nn定义的才会进行训练。torch.nn.functional定义的需要自己手动设置参数。所以通常，激活函数或者卷积之类的都用torch.nn定义。另外，torch.nn是类，必须要先在init中实例化，然后在forward中使用，而torch.nn.functional可以直接在forward中使用。大家还可以通过官方文档torch.nn.functional与torch.nn来进一步了解两者的区别。大家或许发现，我的代码中有大量的重复性工作。是的，你将在文章后面的官方实现中看到优化的代码，但是相对来说，我的代码更加直观些，完全是按照网络的结构顺序从上到下编写的，可以方便初学者（including myself）的理解。出现的问题虽然我的代码比较简单直白，但是过程中并不是一帆风顺的，出现了两次报错：输入输出不匹配当我第一遍运行时，出现了一个RuntimeError： 这是一个超级低级的错误，经学长提醒后我才发现，我两个卷积层之间输出输入的channel数并不匹配： 唉又是ctrl+C+V惹的祸，改正后的网络可以参见上文。在这里，我想提醒我自己和大家注意一下卷积层输入输出的维度公式：假设输入的宽、高记为W、H。超参数中，卷积核的维度是F，stride步长是S，padding是P。那么输出的宽X与高Y可用如下公式表示：X=\frac{W+2P-F}{S}+1\Y=\frac{H+2P-F}{S}+1\然而，当我在计算ResNet的维度的时候，发现套用这个公式是除不尽的。于是我搜索到了如下规则：1.对卷积层操作，除不尽时，向下取整。2.对池化层操作，除不尽时，向上取整。没有把张量转化成一维向量上面的问题解决了，结果还有错误： 根据报错，可以发现3584x7是等于25088的，结合pytorch官方文档，我意识到我在把张量输入全连接层时，没有把它拍扁成一维。因此，我按照官方文档添加了如下代码：12345678x = x.view(-1, self.num_flat_features(x))def num_flat_features(self, x): size = x.size()[1:] #all dimensions except the batch dimension num_features = 1 for s in size: num_features *= s return num_features再运行，问题解决。另外，我原本写的代码中，在卷积层之间的对应位置都加上了relu激活函数与池化层。后来我才意识到，由于它们不具有任何需要学习的参数，我可以直接把它们拿出来单独定义：12self.relu = nn.ReLU(inplace = True)self.max = nn.MaxPool2d(kernel_size = 2, stride = 2)虽然是一些很低级的坑，但我还是想写下来供我自己和大家今后参考。官方源码由于VGG的结构设计非常有规律，因此官方源码给出了更简洁的版本：123456789101112131415161718192021222324252627282930313233343536373839import torch.nn as nnimport mathclass VGG(nn.Module): def __init__(self, features, num_classes=1000, init_weights=True): super(VGG, self).__init__() self.features = features self.classifier = nn.Sequential( nn.Linear(512 * 7 * 7, 4096), nn.ReLU(True), nn.Dropout(), nn.Linear(4096, 4096), nn.ReLU(True), nn.Dropout(), nn.Linear(4096, num_classes), ) if init_weights: self._initialize_weights() def forward(self, x): x = self.features(x) x = x.view(x.size(0), -1) x = self.classifier(x) return x def _initialize_weights(self): for m in self.modules(): if isinstance(m, nn.Conv2d): n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels m.weight.data.normal_(0, math.sqrt(2. / n)) if m.bias is not None: m.bias.data.zero_() elif isinstance(m, nn.BatchNorm2d): m.weight.data.fill_(1) m.bias.data.zero_() elif isinstance(m, nn.Linear): m.weight.data.normal_(0, 0.01) m.bias.data.zero_()因为VGG中卷积层的重复性比较高，所以官方使用一个函数来循环产生卷积层：1234567891011121314def make_layers(cfg, batch_norm=False): layers = [] in_channels = 3 for v in cfg: if v == 'M': layers += [nn.MaxPool2d(kernel_size=2, stride=2)] else: conv2d = nn.Conv2d(in_channels, v, kernel_size=3, padding=1) if batch_norm: layers += [conv2d, nn.BatchNorm2d(v), nn.ReLU(inplace=True)] else: layers += [conv2d, nn.ReLU(inplace=True)] in_channels = v return nn.Sequential(*layers)接下来定义各个版本的卷积层（可参考上文中对论文的截图），这里的“M”表示的是最大池化层。1234567891011121314151617181920212223242526272829303132333435363738394041424344cfg = &#123; 'A': [64, 'M', 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512, 'M'], 'B': [64, 64, 'M', 128, 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512, 'M'], 'D': [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 'M', 512, 512, 512, 'M', 512, 512, 512, 'M'], 'E': [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 256, 'M', 512, 512, 512, 512, 'M', 512, 512, 512, 512, 'M'],&#125;def vgg11(**kwargs): model = VGG(make_layers(cfg['A']), **kwargs) return modeldef vgg11_bn(**kwargs): model = VGG(make_layers(cfg['A'], batch_norm=True), **kwargs) return modeldef vgg13(**kwargs): model = VGG(make_layers(cfg['B']), **kwargs) return modeldef vgg13_bn(**kwargs): model = VGG(make_layers(cfg['B'], batch_norm=True), **kwargs) return modeldef vgg16(**kwargs): model = VGG(make_layers(cfg['D']), **kwargs) return modeldef vgg16_bn(**kwargs): model = VGG(make_layers(cfg['D'], batch_norm=True), **kwargs) return modeldef vgg19(**kwargs): model = VGG(make_layers(cfg['E']), **kwargs) return modeldef vgg19_bn(**kwargs): model = VGG(make_layers(cfg['E'], batch_norm=True), **kwargs) return modelif __name__ == '__main__': # 'VGG', 'vgg11', 'vgg11_bn', 'vgg13', 'vgg13_bn', 'vgg16', 'vgg16_bn', 'vgg19_bn', 'vgg19' # Example net11 = vgg11() print(net11)附上pytorch官方源码链接。可以在vision/torchvision/models/下找到一系列用pytorch实现的经典神经网路模型。好了，以上就是VGG的介绍与实现，如有不足之处欢迎大家补充！]]></content>
      <categories>
        <category>人工智能</category>
      </categories>
      <tags>
        <tag>深度学习</tag>
        <tag>论文分享</tag>
        <tag>踩坑血泪</tag>
        <tag>代码实现</tag>
        <tag>pytorch</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[deep learning笔记：一篇非常经典的论文——NatureDeepReview]]></title>
    <url>%2Fdeep-learning20190914142553%2F</url>
    <content type="text"><![CDATA[这是一篇非常经典的有关深度学习的论文，最近在看一个网课的时候又被提到了，因此特地找了pdf文档放在这里和大家分享。简述这篇文章首先介绍了深度学习的基本前期储备知识、发展背景，并对机器学习范畴内一个重要方向——监督学习进行完整介绍，然后介绍了反向传播算法和微积分链式法则等深度学习基础内容。文章的接下来重点介绍了卷积神经网络CNN的实现过程、几个非常重要的经典卷积神经网络以及深度卷积神经网络对于视觉任务理解的应用。文章最后探讨了分布表示和语言模型，循环神经网络RNN原理以及对未来的展望和现实的实现。总而言之，我觉得这是一篇值得逐字逐句反复阅读咀嚼的文章，读完这篇文章，大概就相当于打开了深度学习的大门了吧。这篇文章的个人理解与感悟或许我以后会补上，在接触还不深的情况下我不说废话啦，先附上原文，其中黄色高亮部分是一些比较重要的内容，大家有时间的话可以认真看一下。下面附上原文链接。最后贴一张我觉得挺搞笑的图。这张图片还有张兄弟图，可以看看我的另一篇论文分享artificial-intelligence笔记：人工智能前沿发展情况分享。]]></content>
      <categories>
        <category>人工智能</category>
      </categories>
      <tags>
        <tag>深度学习</tag>
        <tag>论文分享</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ubuntu笔记：重装ubuntu——记一段辛酸血泪史]]></title>
    <url>%2Fubuntu20190914100050%2F</url>
    <content type="text"><![CDATA[这是不久前我踩过的一个巨坑，在这里我想先强调一下：不要升级linux发行版！！！重要的事情说三遍！！！不要升级linux发行版！！！重要的事情说三遍！！！不要升级linux发行版！！！重要的事情说三遍！！！为什么？不要问我为什么。我按系统提示升级ubuntu到18.04LTS后，就再也进不去系统了。不信你可以尝试一下，你将会看到如下界面：注：图片来自网络，我就不再为了截图而踩一次坑了。不仅是图形界面，命令行界面也进不去了（据说可以在重启时选择recovery mode并且狂按回车强行进入界面，但我失败了）。不过如果你真的尝试了并且掉坑里了的话，没关系，你获得了一个很好的重装系统的锻炼机会。下面我们就按步骤锻炼一下。截止本文最后一次更新前，我已用此方法安装过3遍系统（2遍16.04LTS，1遍18.04LTS），可以放心食用。由于我目前使用的是18.04LTS版本，而ubuntu18.04的社区也日渐活跃，因此本文将会涉及一些18.04版的安装步骤，基本上是一致的。注：LTS表示的是长期支持版本，一般.04都是LTS的，每两年发布一次，支持期好像是5年。本文同时适用于ubuntu16.04LTS和ubuntu18.04LTS。References：电子文献：https://blog.csdn.net/Spacegene/article/details/86659349准备U盘准备一个2G以上的无用的U盘，或者备份好里面的文件。然后将其格式化。镜像下载ubuntu16.04LTS镜像或者ubuntu18.04LTS镜像到本地。也可使用清华源镜像或者阿里云镜像更快下载。删除分区你可以通过控制面板中的创建并格式化硬盘分区来看到你的windows与ubuntu分区的情况。（以下操作都是针对重装，不再重新分区，需要的可以自行上网查找教程）在重新安装ubuntu16.04之前我们需要删除原先Ubuntu的EFI分区及启动引导项，这里推荐直接使用windows下的diskpart来删除。使用win+R输入diskpart打开diskpart.exe，允许其对设备进行更改。接下来使用list dick，我的笔记本当时只有一块SSD，两个系统都装在上面，故select disk 0进入disk 0。然后就可以输入list partition来查看具体的分区信息。其中类型未知的便是分给ubuntu的分区，我这里有一块8G的swap分区和60G的/分区。接下来执行如下命令：1234select partition 7delete partition override #删除该分区select partition 8delete partition override #删除该分区注意：以上命令是针对我的情况，具体请按照对应ubuntu分区的序号删除。现在你可以在控制面板中的创建并格式化硬盘分区中看到你删除的分区已经合并成一块未分配的空间，这也意味着你与你原来ubuntu上的数据彻底说再见了。删除ubuntu启动引导项首先下载EasyUEFI，使用免费试用版EasyUEFI Trial即可。如果试用期过了的话可以到网上找破解版来下。下载完成后安装，打开EasyUEFI如图： 选择管理EFI启动选项Manage EFI Boot Option，然后选择ubuntu启动引导项，点击中间的删除按钮来删除该引导项。 现在重新启动，你会发现已经没有让你选择系统的引导界面，而是直接进入windows系统。制作启动U盘首先我们下载一个免费的U盘制作工具rufus。此时插入已经格式化的U盘，打开rufus，一般情况下它会自动选择U盘，你也可以在device选项下手动选择或确认。点击select，选择之前下载好的镜像文件。其他设置保留默认即可，不放心的话可以比对下图： 然后start开始制作。如果此时rufus提示需要下载一些其它文件，选择Yes继续即可。没有问题的话制作完的U盘会如图所示： 在下面的步骤中，请一直插着U盘不要拔。安装现在重新启动电脑，开机的过程中不停地快按F12进入bios界面（我的是戴尔的电脑，不同电脑按键或有不同，自行百度；如果快按不行的话再次重启试一试长按，因为网上有些教程说的是长按，而我是长按不行而快按可以）。随后选择U盘启动（不同电脑这个界面也可能不一样，具体可以百度，我选择的是UEFI BOOT中UEFI：U盘名那项）。接下来就进入了紫红色的GNU GRUB界面，选择install ubuntu。如果在这里迟疑了一下，会自动进入trying without install，这也没关系，你也可以进入后在图形界面中双击安装，安装之后可以继续试用，直到重启。随后就是些比较简单的安装过程，基本上可以按默认进行，因为是重装，好像不需要联网安装且有汉化包。注意：若是安装18.04LTS，这里会出现一个“正常安装”还是“最小安装”的选择，一般无脑选择正常安装即可，但请事先对安装时间做好心里准备（我大概花了一个多小时）且安装完后有些软件还是比较多余的，可以手动卸载。接下来是比较重要的部分：进入安装类型installation type界面后，选择其他选项something else，这样我们就可以自己配置ubuntu分区了，点击继续。接下来会进入一个磁盘分区的界面，选中之前清出来的未分配分区（名为“空闲”，也可以通过大小来判断），点击下方+号，新建一个swap交换分区，大小为8G左右（一般和电脑的内存相当即可，不分这个区会有警告，也可不分之后再加）。再次双击空闲分区，挂载点下拉，选择/（相当于windows的C盘）。在安装启动引导器的设备选项中，选择Windows Boot Manager。结果可以参考下图：确认无误后点击现在安装，然后就一路默认直到安装完成。这里会有一个设置用户名和计算机名的界面，建议设置得短一些比较好，否则在终端中每条键入的命令前都会有很长的一串“用户名@计算机名”。安装完成后可以进行试用，此时一切操作都不会被保留。如果无需试用，就重新启动系统，此时会提醒可以拔出installation medium即启动U盘。后期别忘了把U盘格式化回来，可以继续使用，留着做纪念也行，说不定哪天又要重装。下面我展示一下我目前的一部分美化效果，亲测发现这只会牺牲一点点儿CPU，所以并不用担心，大胆地美化就是，可能这也是使用linux发行版不多的几种乐趣之一吧。以上就是重装ubuntu的全部内容，欢迎补充！我也会在新问题出现时及时更新。]]></content>
      <categories>
        <category>操作和使用</category>
      </categories>
      <tags>
        <tag>踩坑血泪</tag>
        <tag>个人经历</tag>
        <tag>ubuntu</tag>
        <tag>安装教程</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ubuntu笔记：释放空间]]></title>
    <url>%2Fubuntu20190914094853%2F</url>
    <content type="text"><![CDATA[前一篇讲了如何清理windows下的空间，然而虽然ubuntu中垃圾文件没win10那么多，可是我给ubuntu分配的空间比win10少得多了，于是我又找了些清理ubuntu的方法。References：电子文献：https://www.jb51.net/article/164589.htmhttps://blog.csdn.net/m0_37407756/article/details/79903837查看我们可以在终端中使用df命令来查看磁盘的利用情况。另外，可以加一个-h即“human reading”使显示的磁盘利用状况列表更加适合我们阅读（主要是转化了单位和列名）。方法清理apt缓存文件ubuntu在/var/cache/apt/archives目录中会保留deb软件包的缓冲文件。随着时间的推移，这些缓存可能会占有很多空间。我们可以使用sudo du -sh /var/cache/apt来查看当前apt缓存文件的占用的大小。我们可以直接在终端执行如下命令以清理过时的软件包：1sudo apt-get autoclean我们可以在终端中执行以下命令来移除所有apt缓存中的软件包：1sudo apt-get clean实践证明，这两条命令其实清理得不是非常干净（会剩下kb级的缓存），不过如果很久没清理的话，还是非常强力的。删除其他软件依赖的但现在已不用的软件包下面这条命令可以移除系统不再需要的依赖库和软件包。这些软件包是自动安装的，是当初为了使得某个安装的软件包满足依赖关系，而此时已不再需要。1sudo apt-get autoremove除了移除不再被系统需要的孤立软件包，这条命令也会移除安装在系统中的linux旧内核（有更精确的操作方法，有点专业，这里就不说了）。注意，这条命令执行后，软件的配置文件还是会保留的。可以使用purge选项来同时清除软件包和软件的配置文件。1sudo apt-get autoremove --purge补充：这里扯点题外话，最近看到一个挺好用的命令apt-get install -f，其作用是修复依赖关系（depends），即假如系统上有某个package不满足依赖条件，这个命令就会自动安装那个package所依赖的package。清除缩略图缓存可以使用du -sh ~/.cache/thumbnails/查看缩略图缓存占用的空间。其实如果不是摄影爱好者或者类似的使用者的话，这个缓存不会特别大，不过对缓存强迫症患者还是可以清一下的。1rm -rf ~/.cache/thumbnails/*清除残余配置文件可以使用dpkg --list | grep &quot;^rc&quot;查看残余的配置文件，如果没有的话，可以跳过后文。这里的rc表示软件包已经删除（Remove），但配置文件（Config-file）还在的文件。这里具体的介绍可以看一下我新写的文章ubuntu笔记：安装与卸载deb软件包。若有，咱们来删除：1dpkg -l |grep ^rc|awk &apos;&#123;print $2&#125;&apos; |sudo xargs dpkg -P或者1dpkg --list | grep &quot;^rc&quot; | cut -d &quot; &quot; -f 3 | xargs sudo dpkg --purge这时候如果出现如下错误，那无需担心，因为已经不存在残余的配置文件了。可以把上面的命令按顺序执行一遍，就完成了对ubuntu系统的空间释放。]]></content>
      <categories>
        <category>操作和使用</category>
      </categories>
      <tags>
        <tag>命令操作</tag>
        <tag>ubuntu</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[windows笔记：释放空间]]></title>
    <url>%2Fwindows20190914091023%2F</url>
    <content type="text"><![CDATA[暑假里想跑CVPR中的代码，发现作者提供的环境配置都是基于linux终端的，这样windows的git bash就满足不了我了。二话不说我花了两天时间装了个ubuntu+windows双系统，好不容易装好了，却发现我的硬盘空间已经岌岌可危（理论上要留内存的三倍左右可以保证系统顺畅运行，我的内存是8G，也就是说我C盘应空出20G左右为宜）。于是我就找了些释放空间的办法，分享在这里。References：电子文献：http://www.udaxia.com/wtjd/9147.html利用磁盘属性进行清理这是最稳的方法，但释放的空间也相对较少，不过还是有效的。选择“此电脑”，右键C盘，属性，然后就可以在常规下面看到磁盘清理。一般按默认选择的进行清理，当然全点上勾也无所谓。注意：千万不能选择压缩此驱动器以节约磁盘空间！另外在工具下面你可以看到一个优化的选项，一般系统会定期自动执行优化，如果你是强迫症，时时刻刻都容不得一点冗余的话，可以手动优化。据我们数据结构的老师说，由于数据在存储时大多是稀疏矩阵，存在许多的空间浪费，而磁盘碎片整理优化的就是这个。清理系统文件在前面的磁盘清理界面中我们还可以看到清理系统文件这一选项，可以选择它进行进一步清理，这里面有一项是以前的windows安装文件，如果不打算回退的话清理无妨。有可能你还会注意到一个名为“系统错误内存转储文件”的选项，这个也可以大胆清除，对一般使用者（基本不需排查系统问题）这个文件完全没有任何作用。对于防止系统错误内存转储文件占用空间，还有一劳永逸直接禁止生成的办法，首先打开高级系统设置，来到如图所示选项卡。打开“启动和故障修复”设置窗口，在写入调试信息的下拉列表中选择“无”并确定即可。当然，如果你觉得你或许用得上系统错误内存转储文件，那么也可以选择这里的小内存转储或者核心内存转储，这样同样也能节省空间。清理系统文件是给windows10瘦身最有效的办法之一，事实上，若无特殊需求，扫描结果中的文件均可以勾选清除。删除临时文件这里有两个临时文件中的全部文件可以删除，一个是C:\Users\用户名\AppData\Local\Temp目录下的文件，这里是临时文件最多的地方，可以上到几个G；另一个是C:\Windows\Temp，这里文件大小相对较小，可以忽略不计。另外我也找到了一些其他的临时文件，但似乎它们的体积都是0，可能是系统自动清理了。注意：千万不要误删上一级目录！如果担心删除出错，可先放到回收站，重启之后看有无异常再做决定。亲测上述两个文件夹中的所有文件均可删除。删除冗余更新众所周知，windows系统会自动更新，这也是许多人弃windows的一大原因，然而windows还是要用的，于是我找到了一种可以清理多余的windows更新文件的方法。首先右键左下角开始菜单，选择“Windows PowerShell（管理员）” 弹出是否允许进行更改选择“是”。输入命令dism.exe /Online /Cleanup-Image /AnalyzeComponentStore并执行。这时候会显示“推荐使用组件存储清理：是or否”，因为我前不久清过，所以这里显示为“否”，那么就别清理了。如果显示为“是”，那么进行第四步。输入命令dism.exe /online /Cleanup-Image /StartComponentCleanup并执行。这样电脑就会开始清理啦。这个过程会比较长，不要着急和担心，在这期间你可以做些别的事情，比如看看我其他的几篇博客。重装系统俗话说得好“大力出奇迹”，重装系统无疑是最有效清理空间的一种方法。只要备份好数据，重装其实没有想象的那么困难。我本人这一年多来就重装过3次系统(两次是被迫的…)，其实装了几次就熟练了，我曾看到某linux大牛（忘了是谁）总共重装了19次linux。你可以在我的另一篇博文ubuntu笔记：重装ubuntu——记一次辛酸血泪史中看到如何在双系统情况下重装ubuntu的过程。]]></content>
      <categories>
        <category>操作和使用</category>
      </categories>
      <tags>
        <tag>命令操作</tag>
        <tag>windows</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[front end笔记：制作web时的一些小技巧与小问题]]></title>
    <url>%2Ffront-end20190914080224%2F</url>
    <content type="text"><![CDATA[大一有一段时间，我沉迷于web前端制作网页，比较熟练地掌握了html的语法，还根据需要接触了一些CSS以及js的内容。说白了，html只是一种标记语言（不属于编程语言），但是它简单易学，且很容易获得可视化的效果，对于培养兴趣而言我感觉是很有帮助的。油管up主，现哈佛在读学霸John Fish（请科学上网）当初就是从html进入计算机世界的。下面贴一个我自己做的网页，是综合web三大语言编写的，大一的时候把自己需要的网站都放上面了，也有一种归属感吧。主页上那个是python之禅，也是我很喜欢的一段文字，在python环境下import this就可以看到。由于上面的网页是我学web的时候边看书边编的，各种元素都尝试了一下，最后也没有美化一直到现在，所以大佬们勿喷哈。小技巧我这里强烈推荐使用VScode写前端，它有很多强大的插件，我这里推荐其中一个吧。如介绍所写，使用alt+B快捷键可以直接在默认浏览器下查看你写的网页，而shift+alt+B可以选浏览器查看，因为有些时候microsoft自带的edge浏览器无法实现你编写的效果（巨坑），推荐使用chrome打开浏览。使用这个插件能让你更快捷地预览你编写的效果并进行修改，大大提高了效率。其他的插件网上有很多推荐，也等待着你自己去发现，这里就不一一列出了。还有一个快捷的操作就是快速生成代码块，在VScode中是这样操作的（其他编辑器也应该类似）：输入一个！：按tab键或者回车： 这样就可以节省很多时间，非常方便。小问题在我想使用web来打开我本地的txt文件时，我遇到过这样一个问题：打开的中文文档在浏览器中显示为乱码。在尝试其他浏览器后，我发现这不是浏览器的问题。最后我大致找到了两种解决办法：一种是找到head下面的meta charset，修改代码如下：另一种是另存为文件时修改一下格式，这里我们修改成“UTF-8”。另外我室友在使用python导入文件的时候也因为格式导致报错，修改成ANSI后即可。 希望通过上面两种方法的尝试能让你解决乱码问题，几种编码格式的区别在这里暂不说明，以后有空补上。]]></content>
      <categories>
        <category>环境配置</category>
      </categories>
      <tags>
        <tag>前端</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[anaconda笔记：conda的各种命令行操作]]></title>
    <url>%2Fanaconda20190913231748%2F</url>
    <content type="text"><![CDATA[anaconda是一个开源的包、环境管理器，可以比较有效地配置多个虚拟环境，当python入门到一定程度时，安装anaconda是很必要的。前段时间室友学习python的时候问到过我一些相关的问题，我就在这里简单写一些我知道的以及我搜集到的知识。环境变量安装anaconda过程中一个很重要的步骤就是配置环境变量，网上有很多手动添加环境变量的教程，其实很简单，只需添加三个路径，当然更简单的是直接在安装的时候添加到path（可以无视warning）。我想在这里写的是环境变量的概念问题，其实直到不久前帮同学安装我才明白。环境变量是指在操作系统中用来指定操作系统运行环境的一些参数。当要求系统运行一个程序而没有告诉它程序所在的完整路径时，系统除了在当前目录下面寻找此程序外，还会到path中指定的路径去找。这就是为什么不添加C:\Users\用户名\Anaconda3\Scripts到path就无法执行conda命令，因为此时conda.exe无法被找到。conda与pip利用conda install与pip install命令来安装各种包的过程中，想必你也对两者之间的区别很疑惑，下面我就总结一下我搜集到的相关解答。简而言之，pip是python包的通用管理器，而conda是一个与语言无关的跨平台环境管理器。对我们而言，最显着的区别可能是这样的：pip在任何环境中安装python包，conda安装在conda环境中装任何包。因此往往conda list的数量会大于pip list。要注意的是，如果使用conda install多个环境时，对于同一个包只需要安装一次，有conda集中进行管理。但是如果使用pip，因为每个环境安装使用的pip在不同的路径下，故会重复安装，而包会从缓存中取。总的来说，我推荐尽早安装anaconda并且使用conda来管理python的各种包。升级我们可以在命令行中或者anaconda prompt中执行命令进行操作。123conda update conda #升级condaconda update anaconda #升级anaconda前要先升级condaconda update --all #升级所有包在升级完成之后，我们可以使用命令来清理一些无用的包以释放一些空间：12conda clean -p #删除没有用的包conda clean -t #删除保存下来的压缩文件（.tar）虚拟环境conda list命令用于查看conda下的包，而conda env list命令可以用来查看conda创建的所有虚拟环境。下面就简述一下如何创建这些虚拟环境。使用如下命令，可以创建一个新的环境：conda create -n Python27 python=2.7其中Python27是自定义的一个名称，而python=2.7是一个格式，可以变动等号右边的数字来改变python环境的kernel版本，这里我们安装的是python2.7版本（将于2020年停止维护）。在anaconda prompt中，我们可以看到我们处在的是base环境下，也就是我安装的python3环境下，我们可以使用下面两个命令来切换环境：在创建环境的过程中，难免会不小心取了个难听的环境名，别担心，我们有方法来删除环境。conda remove -n 难听的名字 --all有时候一个环境已经配置好了，但我们想要重命名，这怎么办呢？可以这样办：12conda create -n 新名字 --clone 老名字conda remove -n 老名字 --all把环境添加到jupyter notebook首先通过activate进入想要添加的环境中，然后安装ipykernel，接下来就可以进行添加了。12pip install ipykernelpython -m ipykernel install --name Python27 #Python27可以取与环境名不一样的名字，但方便起见建议统一我们可以使用如下命令来查看已添加到jupyter notebook的kernel：jupyter kernelspec list显示如下：我们也可以在jupyter notebook中的new或者kernel下查看新环境是否成功添加。若想删除某个指定的kernel，可以使用命令jupyter kernelspec remove kernel_name来完成。在这里我想说明一下为什么要分开python的环境。由于python是不向后兼容的，分开环境可以避免语法版本不一引起的错误，同时这也可以避免工具包安装与调用的混乱。]]></content>
      <categories>
        <category>环境配置</category>
      </categories>
      <tags>
        <tag>命令操作</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[jupyter notebook笔记：一些细小的操作]]></title>
    <url>%2Fjupyter-notebook20190913225022%2F</url>
    <content type="text"><![CDATA[首先强烈安利jupyter notebook，它是一种交互式笔记本，安装anaconda的时候会一并安装，下载VS的时候也可以选择安装。我是在初学机器学习的时候接触jupyter notebook的，立刻就被它便捷的交互与结果呈现方式所吸引，现在python编程基本不使用其他的软件。其实完全可以在初学python的时候使用jupyter notebook，可以立即得到反馈以及分析错误，可以进步很快！打开操作当初安装好之后还不了解，每次打开jupyter notebook都会先弹出一个黑框框，这时候千万别关掉，等一会就能来到网页。另外打开之后也别关掉，使用的时候是一直需要的，因为只有开着才能访问本机web服务器发布的内容。另外你也可以不通过快捷方式，直接在命令行中直接输入jupyter notebook来打开它。有些时候，当你插入硬盘或者需要直接在特定的目录下打开jupyter notebook（它的默认打开是在“usr/用户名/”路径下），那你可以在输入命令的后面加上你想要的路径。123jupyter notebook D:\ #打开D盘，于我是我的移动硬盘jupyter notebook E:\ #我的U盘jupyter notebook C:\Users\用户名\Desktop #在桌面打开快捷键操作在jupyter notebook中可以通过选中cell然后按h的方式查询快捷键。其他在jupyter notebook中可以直接使用markdown，这对学习可以起到很大的辅助作用，markdown的基本操作可以看我的另一篇博文markdown笔记：markdown的基本使用此外，在我的博文anaconda笔记：conda的各种命令行操作中，也介绍了如何将python的虚拟环境添加到jupyter notebook中，欢迎阅读。VScode前段时间也开始支持ipynb，喜欢高端暗黑科技风又懒得自己修改jupyter notebook的小伙伴可以试一试，不过我在kernel配置方面似乎还有一些小问题有待解决。]]></content>
      <categories>
        <category>环境配置</category>
      </categories>
      <tags>
        <tag>命令操作</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[markdown笔记：markdown的基本使用]]></title>
    <url>%2Fmarkdown20190913211144%2F</url>
    <content type="text"><![CDATA[既然要写技术博客，那么markdown肯定是必备的了，这篇文章就来介绍一下markdown的基本使用操作。References：电子文献：https://www.jianshu.com/p/191d1e21f7ed介绍Markdown是一种可以使用普通文本编辑器编写的标记语言，其功能比纯文本更强，因此许多程序员用它来写blog。在这里我先推荐一款markdown编辑器——typora，大家可以免费下载使用。注意在我刚开始使用markdown的时候总是跳进这个坑，在这里提上来提醒一下，在使用markdown标记后要添加文字时，需要在相应标记后空一格，否则标记也会被当作文本来处理，例如我输入“#####错误”时：错误正确的做法是输入“##### 正确”：正确一种简单的判别方法就是使用IDE，这样对应的标记就会有语法高亮。使用标题话不多说，直接示范：123456# 这是一级标题## 这是二级标题### 这是三级标题#### 这是四级标题##### 这是五级标题###### 这是六级标题效果如下：这是一级标题这是二级标题这是三级标题这是四级标题这是五级标题这是六级标题字体还是直接示范：1234**这是加粗的文字***这是倾斜的文字*`***这是斜体加粗的文字***~~这是加删除线的文字~~这是加粗的文字这是倾斜的文字这是斜体加粗的文字这是加删除线的文字引用1234&gt;我引用&gt;&gt;我还引用&gt;&gt;&gt;我再引用&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;扶我起来，我还能继续引用！我引用我还引用我再引用扶我起来，我还能继续引用！引用是可以嵌套的，可以加很多层，我一般使用一个&gt;来表示额外的需要注意的内容。另外，如果想让下一段文字不被引用，需要空一行。分割线分割线使用三个及以上的-或*就可以。有时候用---会造成别的文字的格式变化，因此我在使用VScode编辑时，如果看到---被高亮（分割线正常其作用时应该不高亮），就会改用***。12---***效果如下：图片markdown中添加图片的语法是这样的：1![显示在图片下方的文字](图片地址 &quot;图片title&quot;)其中title可加可不加，它就是鼠标移动到图片上时显示的文字。然而我在使用hexo搭建我的个人博客的过程中，遇到了使用上述语法图片却无法显示的情况，因此我改用了下列标签插件：1&#123;% asset_img xxxxx.xxx 图片下方的名字 %&#125;其中xxxxx.xxx只需直接输入图片名称以及格式即可，因为我使用了hexo-asset-image插件，它可以在_posts文件中创建与博文名称相同的对应的文件夹，只需把图片移入即可。注意，这里的图片名中间不能有空格，否则会加载失败（它会以为图片名称到第一个空格为止）。其安装命令：npm install hexo-asset-image --save也可用cnpm更快地安装：cnpm install hexo-asset-image --save补充：后来发现在关于本人中无法使用上述asset_img标签插件来对图片进行插入，故又尝试了![显示在图片下方的文字](图片地址 &quot;图片title&quot;)的方法，发现可行！原因可能是之前误用了中文括号导致的。可以参考一下Hexo文章中插入图片的方法。在插入图片的后面，会留有一小段空白区，看着不舒服的话可以不要回车，即直接在插入图片的语句后面跟进下一段的文字或者图片等，这样行间隙就会小很多。其实在hexo中可以直接使用img标签，它会自行处理，并且这样还更方便调整高度和宽度。1&lt;img src=&quot;&quot; width=&quot;50%&quot; height=&quot;50%&quot;&gt;超链接由于我希望在新的页面打开链接，而似乎markdown本身的语法不支持在新标签页打开链接，因此我推荐直接使用html语言来代替。1&lt;a href=&quot;超链接地址&quot; target=&quot;_blank&quot;&gt;超链接名&lt;/a&gt;列表无序列表123- 列表内容+ 列表内容* 列表内容列表内容列表内容列表内容有序列表1231. 列表内容2. 列表内容3. 列表内容列表内容列表内容列表内容可以看到，上面显示的列表是有嵌套的，方法就是敲三个空格缩进。表格1234表头|表头|表头---|:--:|---:内容|内容|内容内容|内容|内容其中第二行的作用分割表头和内容，-有一个就行，为了对齐可多加几个。此外文字默认居左，有两种改变方法：两边加：表示文字居中。右边加：表示文字居右。然而我在hexo使用表格时，出现了无法正常转换的问题，因此我改用了如下HTML的表格形式。12345678910&lt;table border=&quot;1&quot;&gt;&lt;tr&gt;&lt;td&gt;第一行第一列&lt;/td&gt;&lt;td&gt;第一行第二列&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;第二行第一列&lt;/td&gt;&lt;td&gt;第三行第二列&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;效果如下：第一行第一列第一行第二列第二行第一列第二行第二列代码最后的最后，是我最喜欢ctrl+C+V的代码了。单行或句中代码输入方式：1`来复制我呀`显示：来复制我呀其中`在键盘的左上角，我当初找了好久。多行代码块的写法就是用上下两对```围住。好了于是你现在就可以自由的复制粘贴啦。]]></content>
      <categories>
        <category>操作和使用</category>
      </categories>
      <tags>
        <tag>markdown</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[hexo笔记：开始创建个人博客——方法及原因]]></title>
    <url>%2Fhexo20190913153310%2F</url>
    <content type="text"><![CDATA[大家好，这是我的第一篇博文，这也是我的第一个自己搭建的网站，既然搭了，那第一篇就讲讲我搭建的过程吧。安装步骤安装node.js进入官网。选择对应系统（我这里用win10），选择LTS（长期支持版本）安装，安装步骤中一直选择next即可。安装完后就可以把安装包删除了。安装git进入官网。选择对应系统的版本下载，同样也是按默认安装。安装成功后，你会在开始菜单中看到git文件夹。 其中Git Bash与linux和mac中的终端类似，它是git自带的程序，提供了linux风格的shell，我们可以在这里面执行相应的命令。注意：bash中的复制粘贴操作与linux中类似，ctrl+C用于终止进程，可以用鼠标中键进行粘贴操作。不嫌麻烦的话可以使用ctrl+shift+C和ctrl+shift+V进行复制粘贴操作。安装hexohexo是一个快速、简洁且高效的博客框架，在这里我们使用hexo来搭建博客。首先，新建一个名为“blog”的空文件夹，以后我们的操作都在这个文件夹里进行，可以在bash中使用pwd命令查看当前所处位置。创建这个文件夹的目的是万一因为创建的博客出现问题或者不满意想重来等原因可以直接简单地把文件夹删除，也方便了对整个网站本地内容的移动。打开新建的文件夹，右键空白处，选择Git Bash Here。 接下来我们输入两行命令来验证node.js是否安装成功。 如出现如图所示结果，则表明安装成功。为了提高以后的下载速度，我们需要安装cnpm。cnpm是淘宝的国内镜像，因为npm的服务器位于国外有时可能会影响安装。继续在bash中输入如下命令安装cnpm：npm install -g cnpm --registry=https://registry.npm.taobao.org检验安装是否成功，输入cnpm： 接下来我们安装hexo，输入命令：cnpm install -g hexo-cli和上面一样，我们可以用hexo -v来验证是否成功安装hexo，这里就不贴图了。接下来我们输入如下命令来建立整个项目：hexo init你会发现你的文件夹中多了许多文件，你也可以用ls -l命令来看到新增的文件。 完成本地环境的搭建至此，我们已经完成了本地环境的搭建，在这里，我想先介绍hexo中常用的命令。hexo n &quot;文章标题&quot;用于创建新的博文（欲删除文章，直接删除md文件并用下面的命令更新即可）。hexo shexo会监视文件变动并自动更新，通过所给的localhost:4000/就可以直接在本地预览更新后的网站了。部署到远端服务器三步曲：123hexo clean #清除缓存，网页正常情况下可以忽略此条命令，执行该指令后，会删掉站点根目录下的public文件夹。hexo g #generate静态网页（静态网页这里指没有前端后端的网页而不是静止），该命令把md编译为html并存到public文件目录下。hexo d #将本地的更改部署到远端服务器（需要一点时间，请过一会再刷新网页）。此外，上面最后两步也可以使用hexo g -d直接代替。如果出现ERROR Deployer not found: git报错，可以使用npm install --save hexo-deployer-git命令解决。注意：由于部署到远端输入密码时密码不可见，有时候会导致部署失败，只有出现INFO Deploy done: git的结果才表明部署成功，否则再次部署重输密码即可。现在我们在bash中运行hexo s，打开浏览器，输入localhost:4000/，就可以看到hexo默认创建的页面了。部署到远端服务器为了让别人能访问到你搭建的网站，我们需要部署到远端服务器。这里有两种选择，一种是部署到github上，新建一个repository，然后创建一个xxxxx.github.io域名（这里xxxxx必须为你的github用户名）。另一种选择是部署到国内的coding，这是考虑到访问速度的问题，不过我选择的是前者，亲测并没感觉有速度的困扰。个人比较推荐用github pages创建个人博客。部署这块网上有许多教程，这里不详细解释了，以后有机会补上。在部署的时候涉及到对主题配置文件的操作，linux和mac用户可以使用vim进行编辑，不过也可以使用VScode、sublime等代码编辑器进行操作。注：为了国内的访问速度，我最后添加了coding/github双线部署，两者的操作方式大同小异。值得注意的是，如果使用的是leancloud的第三方阅读量与评论统计系统，那么还得在leancloud的安全中心中添加coding的web域名。创建原因首先说明，我只是一个刚起步的入门级小白，懂得不多，别喷我哈~步入大二，虽然我是大学才算真正接触编程，但一年多下来我也接触并且学习了不少技术知识。接触的多了、遇到的问题也复杂了起来，导致每次百度到的答案不一定能够解决我遇到的问题。此外，之前在学习编程语言、操作系统、ml、dl等知识的时候，为方便起见利用文本记了些笔记。然而笔记分散在四处，不方便管理与查看，因此就萌生了写博客的想法。由于个人比较喜欢自由DIY，所以没有使用CSDN、博客园等知名技术博客网站。最后还是非常感谢我们华科的校友程序羊在b站和其他站点上分享的各种经验，我就是通过他的视频来搭建起自己的第一个博客网站的。他的其他视频也给了我很多启迪。最后，最关键的原因，还是因为今天中秋节有空闲的时间哈哈，祝大家节日快乐！]]></content>
      <categories>
        <category>操作和使用</category>
      </categories>
      <tags>
        <tag>hexo</tag>
        <tag>安装教程</tag>
      </tags>
  </entry>
</search>
