<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>高深远的博客</title>
  
  
  <link href="/atom.xml" rel="self"/>
  
  <link href="https://gsy00517.github.io/"/>
  <updated>2020-01-28T07:09:55.493Z</updated>
  <id>https://gsy00517.github.io/</id>
  
  <author>
    <name>高深远</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>deep learning笔记：迁移学习</title>
    <link href="https://gsy00517.github.io/deep-learning20200128143652/"/>
    <id>https://gsy00517.github.io/deep-learning20200128143652/</id>
    <published>2020-01-28T06:36:52.000Z</published>
    <updated>2020-01-28T07:09:55.493Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Tue Jan 28 2020 15:40:41 GMT+0800 (GMT+08:00) --><p>迁移学习（Transfer Learning），又称预训练。即利用社区内开源的权重参数更快更好地训练自己的网络。然而，我一直纳闷的是，别人训练好的参数是怎么直接用到自己的网络上来的，倘若网络结构内部有一点不同那岂不是完全不一样了吗？Andrew Ng的课程给了我很大的启发，结合上自己的一些想法，写下来。</p><hr><h1 id="如何使用迁移学习"><a href="#如何使用迁移学习" class="headerlink" title="如何使用迁移学习"></a>如何使用迁移学习</h1><ol><li><h2 id="训练数据较少"><a href="#训练数据较少" class="headerlink" title="训练数据较少"></a>训练数据较少</h2>首先，我们把开源的代码（即网络结构）和对应的权重都下载下来。当手头的训练集较小时，我们可以冻结所有层的参数，去掉网络中的softmax层或者其它与最后输出相联系的层，并且创建自己的softmax单元。这主要是考虑到下载的模型所对应的输出类别或者其他的需求与我们的不一致。在训练过程中，我们保持之前所有层冻结（用于特征提取等），只训练和我们自己设计的softmax层有关的参数。<br>其实这就相当于用预训练的网络构成一个映射关系，对每一个输入都能产生一个特征向量。然后用自己设计的一个很浅的softmax网络对这些特征向量做预测。<blockquote><p>注：这些特征向量可以存到硬盘中，以节约每次都要遍历训练集重新计算这个激活值的时间。这在用Siamese网络进行人脸识别时是一个较为常用的操作。</p></blockquote></li><li><h2 id="训练数据中等"><a href="#训练数据中等" class="headerlink" title="训练数据中等"></a>训练数据中等</h2>如果数据较多，我们可以冻结较少的层。一般冻结前面的层。对于后面的层，我们有两种方法。<br>（1）可以加载权重作为初始化，然后用同样的结构和自己的数据集继续训练。<br>（2）也可以直接去掉这几层，换成我们自己的隐藏单元和自己的输出层。<br>其实我觉得这里的基本思想还是相当于把冻结的那几层看成一个关于特征的映射。</li><li><h2 id="训练数据较多"><a href="#训练数据较多" class="headerlink" title="训练数据较多"></a>训练数据较多</h2>若有足够多的数据用来作训练集，我们这时就可以把所有的参数都仅用来初始化。<br>其实规律就是：拥有越多的数据，我们需要冻结的层数（参数）越少，我们能够训练的层数（参数）就越多。</li></ol><hr><h1 id="为什么要迁移学习"><a href="#为什么要迁移学习" class="headerlink" title="为什么要迁移学习"></a>为什么要迁移学习</h1><p>关于使用迁移学习的原因，Andrew Ng没有提及，不多简单想来主要的原因主要有如下几点：</p><ol><li>迁移学习可以弥补训练数据的不足。</li><li>迁移学习可以大大减少训练时间。</li><li>迁移学习（特别是同一领域相关的参数）可以有效地防止梯度下降卡在局部最优解处。</li></ol><script type="text/javascript" src="/js/src/bai.js"></script><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      
      
        &lt;!-- build time:Tue Jan 28 2020 15:40:41 GMT+0800 (GMT+08:00) --&gt;&lt;p&gt;迁移学习（Transfer Learning），又称预训练。即利用社区内开源的权重参数更快更好地训练自己的网络。然而，我一直纳闷的是，别人训练好
      
    
    </summary>
    
    
      <category term="人工智能" scheme="https://gsy00517.github.io/categories/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/"/>
    
    
      <category term="深度学习" scheme="https://gsy00517.github.io/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>ubuntu笔记：权限管理</title>
    <link href="https://gsy00517.github.io/ubuntu20200127185146/"/>
    <id>https://gsy00517.github.io/ubuntu20200127185146/</id>
    <published>2020-01-27T10:51:46.000Z</published>
    <updated>2020-01-28T07:02:12.793Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Tue Jan 28 2020 15:40:41 GMT+0800 (GMT+08:00) --><p>之前在windows下面从来没想过权限的事情，而在ubuntu中这点就很受重视，可谓时时都会遇到权限管理的问题。即使如此，搞崩ubuntu的次数还是比windows要多的，不过好像心也没那么痛，可能ubuntu就是拿来玩的。虽然不是每次搞崩都是因为权限，但还是有必要理一理。</p><p><strong>References</strong>：</p><p>参考文献：<br>[1]完美应用ubuntu（第3版）</p><hr><h1 id="权限"><a href="#权限" class="headerlink" title="权限"></a>权限</h1><p>在linux系统中，我们可以在终端使用<code>ls -l</code>查看目录下所有子目录和文件的权限属性。其输出结果中每一列的含义如下：</p><ul><li>第一列：文件类型和权限。</li><li>第二列：i节点，即硬链接数。</li><li>第三列：文件的属主，即文件的所有者。</li><li>第四列：文件的属组。</li><li>第五列：文件的大小。</li><li>第六列：mtime，即最后一次修改时间。</li><li>第七列：文件或者目录名。</li></ul><p>其实不做服务器的话没必要搞那么懂，我就讲一下我认为最重要的第一列。<br>首先，第一个字母表示的是文件类型，主要有下面几种：</p><ul><li><strong>-</strong>：表示普通文件。</li><li><strong>d</strong>：表示目录。</li><li><strong>l</strong>：表示链接文件。</li><li><strong>b</strong>：表示块设备文件，比如硬盘的存储设备等。</li><li><strong>c</strong>：表示字符设备文件，比如键盘。</li><li><strong>s</strong>：表示套接字文件，主要跟网络程序有关。</li><li><strong>p</strong>：表示管道文件。</li></ul><p>其次，之后的九个字母三个为一组，分别表示的是文件所有者（u）的权限、同组用户（g）的权限和其他用户（o）的权限。这里属主一般就是<code>sudo</code>赋权进入的那个用户，一般在个人系统中就是特权用户root。另外，可以用“a”表示all users。<br>在每个三个字母组成的一组中，依次分别为读（r）、写（w）和执行（x）权限。若是字母，则表示可；若是“-”，则表示不可。例如“rw-”表示的是“可读可写不可执行”。<br>为了方便，还可以用数字代表权限：用4代表读权限，用2代表写权限，用1代表执行权限。可以发现，这样的三个数字之和（0-7）可以表示任何一种权限组合。<br>较为常用权限组合的有：</p><ul><li>7（可读可写可执行——rwx——4+2+1=7）</li><li>6（可读可写不可执行——rw-——4+2+0=6）</li><li>4（可读不可写不可执行——r———4+0+0=4）</li></ul><hr><h1 id="chmod"><a href="#chmod" class="headerlink" title="chmod"></a>chmod</h1><p>一般通过<code>chmod</code>命令来修改权限，主要有两种方法。</p><ol><li><h2 id="数字法"><a href="#数字法" class="headerlink" title="数字法"></a>数字法</h2><p>这种方法最简洁，其基本格式是<code>chmod (-R) 模式 文件名</code>。这里的<code>-R</code>可以用来进行多级目录的权限设定，也就是将指定文件夹内的所有文件都修改权限。<br>以两个较为常用的使用为例。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">sudo chmod 666 文件名</span><br><span class="line">#赋予所有用户读和写的权限，一般没有权限时我都会使用这个命令</span><br><span class="line"></span><br><span class="line">sudo chmod 600 文件名</span><br><span class="line">#赋予文件所有者读和写的权限，给group和other只读权限</span><br></pre></td></tr></table></figure></li><li><h2 id="参数法"><a href="#参数法" class="headerlink" title="参数法"></a>参数法</h2><p>这种方法适用于只需要改变单个用户的权限而又不想考虑或者计算别的用户的权限情况，其基本格式是<code>chmod [u/g/o/a] [+/-/=] (rwxst) 文件名</code>。<br>这里先解释一下几个重要的参数和符号。<br><strong>u</strong>：所属用户。<br><strong>g</strong>：同组用户。<br><strong>o</strong>：其他用户。<br><strong>a</strong>：所有用户，相当于ugo。<br><strong>+</strong>：原权限基础上增加权限。<br><strong>-</strong>：原权限基础上减少权限。<br><strong>=</strong>：无论原权限是什么，最后的权限都修改为这里指定的权限。<br><strong>r</strong>：不解释，不懂的话没好好看前文。<br><strong>w</strong>：不解释，不懂的话没好好看前文。<br><strong>x</strong>：不解释，不懂的话没好好看前文。<br><strong>s</strong>：运行时可置UID。<br><strong>t</strong>：运行时可置GID。<br>来看例子：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">sudo chmod u+rw 文件名</span><br><span class="line">#给用户增加读写权限</span><br><span class="line"></span><br><span class="line">sudo chmod o-rwx 文件名</span><br><span class="line">#不允许其他用户读写执行</span><br><span class="line"></span><br><span class="line">sudo chmod g=rx 文件名</span><br><span class="line">#使同组用户只能读和执行</span><br></pre></td></tr></table></figure></li></ol><script type="text/javascript" src="/js/src/bai.js"></script><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      
      
        &lt;!-- build time:Tue Jan 28 2020 15:40:41 GMT+0800 (GMT+08:00) --&gt;&lt;p&gt;之前在windows下面从来没想过权限的事情，而在ubuntu中这点就很受重视，可谓时时都会遇到权限管理的问题。即使如此，搞崩ubuntu的次数
      
    
    </summary>
    
    
      <category term="操作和使用" scheme="https://gsy00517.github.io/categories/%E6%93%8D%E4%BD%9C%E5%92%8C%E4%BD%BF%E7%94%A8/"/>
    
    
      <category term="命令操作" scheme="https://gsy00517.github.io/tags/%E5%91%BD%E4%BB%A4%E6%93%8D%E4%BD%9C/"/>
    
      <category term="ubuntu" scheme="https://gsy00517.github.io/tags/ubuntu/"/>
    
  </entry>
  
  <entry>
    <title>ubuntu笔记：移动文件的妙用</title>
    <link href="https://gsy00517.github.io/ubuntu20200127184352/"/>
    <id>https://gsy00517.github.io/ubuntu20200127184352/</id>
    <published>2020-01-27T10:43:52.000Z</published>
    <updated>2020-01-27T10:48:35.416Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Tue Jan 28 2020 15:40:41 GMT+0800 (GMT+08:00) --><p>有时候会遇到文件无法重命名的问题，这里介绍一种很神奇的方法，亲测有效。</p><hr><h1 id="方法"><a href="#方法" class="headerlink" title="方法"></a>方法</h1><ol><li>首先，在终端把目录切到要重命名的文件目录下，或者直接在对应目录中打开终端。</li><li>接下来就是神奇的地方了，为了防止权限不够加个<code>sudo</code>赋个权。<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo mv 原文件名 新文件名</span><br></pre></td></tr></table></figure></li></ol><script type="text/javascript" src="/js/src/bai.js"></script><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      
      
        &lt;!-- build time:Tue Jan 28 2020 15:40:41 GMT+0800 (GMT+08:00) --&gt;&lt;p&gt;有时候会遇到文件无法重命名的问题，这里介绍一种很神奇的方法，亲测有效。&lt;/p&gt;&lt;hr&gt;&lt;h1 id=&quot;方法&quot;&gt;&lt;a href=&quot;#方法&quot; cla
      
    
    </summary>
    
    
      <category term="知识点与小技巧" scheme="https://gsy00517.github.io/categories/%E7%9F%A5%E8%AF%86%E7%82%B9%E4%B8%8E%E5%B0%8F%E6%8A%80%E5%B7%A7/"/>
    
    
      <category term="ubuntu" scheme="https://gsy00517.github.io/tags/ubuntu/"/>
    
  </entry>
  
  <entry>
    <title>ubuntu笔记：snap软件包管理及问题</title>
    <link href="https://gsy00517.github.io/ubuntu20200127123832/"/>
    <id>https://gsy00517.github.io/ubuntu20200127123832/</id>
    <published>2020-01-27T04:38:32.000Z</published>
    <updated>2020-01-27T10:34:14.733Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Tue Jan 28 2020 15:40:41 GMT+0800 (GMT+08:00) --><p>在我安装ubuntu18.04LTS的时候，由于下载语言包是真的久，我就翻了一下ubuntu安装界面的介绍，其中一开始就是对snap store的介绍。<br><img src="/ubuntu20200127123832/snap商店.JPG" title="snap商店"><br>这篇文章就说说snap和我之前遇到的问题。</p><p><strong>References</strong>：</p><p>电子文献：<br><a href="https://www.jb51.net/article/128368.htm" target="_blank" rel="noopener">https://www.jb51.net/article/128368.htm</a><br><a href="https://blog.csdn.net/u011870280/article/details/80213866" target="_blank" rel="noopener">https://blog.csdn.net/u011870280/article/details/80213866</a></p><hr><h1 id="snap"><a href="#snap" class="headerlink" title="snap"></a>snap</h1><p>snap是ubuntu母公司Canonical于2016年4月发布ubuntu16.04时候引入的一种安全的、易于管理的、沙盒化的软件包格式，与传统的dpkg和apt有着很大的区别。在ubuntu软件中心下载安装的似乎都是snap管理的。这让一些商业闭源软件也能在linux上发布，说白了是ubuntu为了获得linux发行版霸权的一个重要举措，因此没少招黑。知乎上看到这么一句话，笑半天：“Fuck the political correct, make linux great again.(says 川·乌班图·普)”。</p><hr><h1 id="常用命令"><a href="#常用命令" class="headerlink" title="常用命令"></a>常用命令</h1><ol><li><h2 id="列出已经安装的snap包"><a href="#列出已经安装的snap包" class="headerlink" title="列出已经安装的snap包"></a>列出已经安装的snap包</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo snap list</span><br></pre></td></tr></table></figure></li><li><h2 id="搜索要安装的snap包"><a href="#搜索要安装的snap包" class="headerlink" title="搜索要安装的snap包"></a>搜索要安装的snap包</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo snap find &lt;text to search&gt;</span><br></pre></td></tr></table></figure></li><li><h2 id="安装一个snap包"><a href="#安装一个snap包" class="headerlink" title="安装一个snap包"></a>安装一个snap包</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo snap install &lt;snap name&gt;</span><br></pre></td></tr></table></figure></li><li><h2 id="更新一个snap包"><a href="#更新一个snap包" class="headerlink" title="更新一个snap包"></a>更新一个snap包</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo snap refresh &lt;snap name&gt;</span><br></pre></td></tr></table></figure><blockquote><p>注：如果后面不加包的名字就更新所有的snap包。</p></blockquote></li><li><h2 id="把一个包还原到以前安装的版本"><a href="#把一个包还原到以前安装的版本" class="headerlink" title="把一个包还原到以前安装的版本"></a>把一个包还原到以前安装的版本</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo snap revert &lt;snap name&gt;</span><br></pre></td></tr></table></figure></li><li><h2 id="删除一个snap包"><a href="#删除一个snap包" class="headerlink" title="删除一个snap包"></a>删除一个snap包</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo snap remove &lt;snap name&gt;</span><br></pre></td></tr></table></figure></li><li><h2 id="查看最近的更改"><a href="#查看最近的更改" class="headerlink" title="查看最近的更改"></a>查看最近的更改</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">snap changes</span><br></pre></td></tr></table></figure></li><li><h2 id="终止snap进程"><a href="#终止snap进程" class="headerlink" title="终止snap进程"></a>终止snap进程</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo snap abort &lt;进程序号&gt;</span><br></pre></td></tr></table></figure></li></ol><p>后面两个命令将在下面的问题中发挥作用。</p><hr><h1 id="问题"><a href="#问题" class="headerlink" title="问题"></a>问题</h1><p>当我在snap store也就是ubuntu软件中心下载pycharm和VScode时，遇到了如下报错：<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">snapd returned status code 409: Conflict</span><br></pre></td></tr></table></figure><p></p><p>上网查找之后，才知道这个错误码409表示的是：由于和被请求的资源的当前状态之间存在冲突，请求无法完成。即并发执行时返回的错误码。<br>由于之前ubuntu软件中心无响应被我强制退出了，因此的确很有可能与之前进行到一半的安装冲突。于是使用<code>snap changes</code>查看最近的snap更改。<br><img src="/ubuntu20200127123832/终止进程.png" title="终止进程"><br>果然看见之前的snap进程依旧在“Doing”，因此根据对应的序号使用<code>sudo snap abort</code>终止进程。<br>这时再回到软件中心安装，就没有之前的报错了。<br>然而…</p><hr><h1 id="关于国内使用snap"><a href="#关于国内使用snap" class="headerlink" title="关于国内使用snap"></a>关于国内使用snap</h1><p>因为网络原因，而且也没有可用的镜像，导致snapcraft在中国大陆地区访问速度非常非常慢，下载软件需要很长的时间并且很容易中途出错。<br>此外，由于snap软件会把主分区分成好多个loop，看起来真的不想说什么了。图源自贴吧，可以看到这挂载的snap软件包可以说是相当壮（别）观（扭）了。<br><img src="/ubuntu20200127123832/果断放弃.png" title="果断放弃"><br>还有一个杀死强迫症（比如我）的问题就是，snap会在家目录（即18.04的主文件夹）中创建一个snap文件夹，里面各种快捷方式、循环嵌套的文件夹，害…无法用语言描述，看了就知道，总之就是非常不爽。主要是一些资料、文件一般也会放在家目录下面，看到了snap在那边亮眼睛真的难受。<br>实在不知道为什么社区里有不少人推崇snap（不过国外没速度限制，snap对他们来说挺方便的）。<br>总而言之，综合速度（硬伤）和美观舒适度考虑，还是尽量避免使用snap命令安装软件，也不要下载ubuntu软件商店中的snap格式的软件包（基本都是）。甚至有些“安装ubuntu之后必做的…件事”等诸如此类的ubuntu配置或者美化的教程内直接把卸载snap列作其中一项哈哈。<br>总之管理软件还是apt优先，详见我的博文<a href="https://gsy00517.github.io/ubuntu20200117094401/" target="_blank">ubuntu笔记：apt包管理以及如何更新软件列表</a>。</p><script type="text/javascript" src="/js/src/bai.js"></script><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      
      
        &lt;!-- build time:Tue Jan 28 2020 15:40:41 GMT+0800 (GMT+08:00) --&gt;&lt;p&gt;在我安装ubuntu18.04LTS的时候，由于下载语言包是真的久，我就翻了一下ubuntu安装界面的介绍，其中一开始就是对snap store
      
    
    </summary>
    
    
      <category term="操作和使用" scheme="https://gsy00517.github.io/categories/%E6%93%8D%E4%BD%9C%E5%92%8C%E4%BD%BF%E7%94%A8/"/>
    
    
      <category term="命令操作" scheme="https://gsy00517.github.io/tags/%E5%91%BD%E4%BB%A4%E6%93%8D%E4%BD%9C/"/>
    
      <category term="ubuntu" scheme="https://gsy00517.github.io/tags/ubuntu/"/>
    
  </entry>
  
  <entry>
    <title>ubuntu笔记：安装与卸载deb软件包</title>
    <link href="https://gsy00517.github.io/ubuntu20200126083448/"/>
    <id>https://gsy00517.github.io/ubuntu20200126083448/</id>
    <published>2020-01-26T00:34:48.000Z</published>
    <updated>2020-01-26T02:12:38.151Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Tue Jan 28 2020 15:40:41 GMT+0800 (GMT+08:00) --><p>似乎是为了支持由武汉深之度科技开发的国产linux系统Deepin，近年来许多常用软件都提供了linux客户端，比如QQ for linux，baidunetdisk for linux。然而我安装百度网盘后发现打不开，一打开就报错，后来才知道百度网盘仅支持ubuntu18之后的版本。于是就又涉及到deb包的卸载问题了。</p><p><strong>References</strong>：</p><p>电子文献：<br><a href="https://askubuntu.com/questions/18804/what-do-the-various-dpkg-flags-like-ii-rc-mean" target="_blank" rel="noopener">https://askubuntu.com/questions/18804/what-do-the-various-dpkg-flags-like-ii-rc-mean</a><br><a href="https://blog.csdn.net/sun2333/article/details/82707362" target="_blank" rel="noopener">https://blog.csdn.net/sun2333/article/details/82707362</a></p><hr><h1 id="dpkg-flag"><a href="#dpkg-flag" class="headerlink" title="dpkg flag"></a>dpkg flag</h1><p>我们可以使用<code>dpkg -l | grep &#39;软件名&#39;</code>来查看相应软件的安装状态，这时一般会出现有两个字母组成的一个flag。具体可以看后文中的截图。这里我想先整理一下这两个字母的含义。</p><ol><li><h2 id="第一个字母：所需的状态desired-package-state（”selection-state”）"><a href="#第一个字母：所需的状态desired-package-state（”selection-state”）" class="headerlink" title="第一个字母：所需的状态desired package state（”selection state”）"></a>第一个字母：所需的状态desired package state（”selection state”）</h2><ul><li><strong>u</strong>——未知unknown</li><li><strong>i</strong>——安装install</li><li><strong>r</strong>——删除/卸载remove/deinstall</li><li><strong>p</strong>——清除（除包含配置文件）purge（remove including config files）</li><li><strong>h</strong>——保持hold</li></ul></li><li><h2 id="第二个字母：当前包状态current-package-state"><a href="#第二个字母：当前包状态current-package-state" class="headerlink" title="第二个字母：当前包状态current package state"></a>第二个字母：当前包状态current package state</h2><ul><li><strong>n</strong>——未安装not-installed</li><li><strong>i</strong>——已安装installed</li><li><strong>c</strong>——仅安装配置文件config-files（only the config files are installed）</li><li><strong>U</strong>——解包unpacked</li><li><strong>F</strong>——由于某种原因配置失败half-configured（configuration failed for some reason）</li><li><strong>h</strong>——由于某种原因安装失败half-installed（installation failed for some reason）</li><li><strong>W</strong>——包正在等待来自另一个包的触发器triggers-awaited（package is waiting for a trigger from another package）</li><li><strong>t</strong>——包已被触发triggers-pending（package has been triggered）</li></ul></li><li><h2 id="第三个字母：错误状态error-state"><a href="#第三个字母：错误状态error-state" class="headerlink" title="第三个字母：错误状态error state"></a>第三个字母：错误状态error state</h2>第三个字母通常情况下是一个空格，一般不会看到。<ul><li><strong>R</strong>——包破损，需要重新安装reinst-required（package broken, reinstallation required）</li></ul></li></ol><hr><h1 id="安装"><a href="#安装" class="headerlink" title="安装"></a>安装</h1><p>使用如下命令进行安装。<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo dpkg -i package-file-name</span><br></pre></td></tr></table></figure><p></p><p>这里的<code>-i</code>表示的是install。注意，这里的package-file-name包括后缀如“.deb”。</p><hr><h1 id="卸载"><a href="#卸载" class="headerlink" title="卸载"></a>卸载</h1><p>下面这张图就是我卸载的过程。<br><img src="/ubuntu20200126083448/卸载.png" title="卸载"><br>首先我使用了<code>dpkg -l | grep &#39;软件名&#39;</code>命令来查看我系统上百度网盘的安装状态。结果显示为“ii”，表示“installed ok installed”即它应该被安装并且已安装。<br>随后，利用<code>-r</code>参数，使用下面命令进行移除。<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo dpkg -r 软件名</span><br></pre></td></tr></table></figure><p></p><p>注意，这里的软件名不需要添加引号。<br>移除之后，我们可以再次使用<code>dpkg -l | grep &#39;软件名&#39;</code>来查看百度网盘的安装状态。结果显示为“rc”，表示“removed ok config-files”即它已经被移除/卸载，但它的配置文件仍然存在。<br>这时我们也是使用如下命令来彻底卸载软件包（包括配置文件）。<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo dpkg -P 软件名</span><br></pre></td></tr></table></figure><p></p><p>在<a href="https://gsy00517.github.io/ubuntu20190914094853/" target="_blank">ubuntu笔记：释放空间</a>一文中，有一次性清理所有残留配置文件的方法，可以看一下。</p><script type="text/javascript" src="/js/src/bai.js"></script><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      
      
        &lt;!-- build time:Tue Jan 28 2020 15:40:41 GMT+0800 (GMT+08:00) --&gt;&lt;p&gt;似乎是为了支持由武汉深之度科技开发的国产linux系统Deepin，近年来许多常用软件都提供了linux客户端，比如QQ for linux，b
      
    
    </summary>
    
    
      <category term="操作和使用" scheme="https://gsy00517.github.io/categories/%E6%93%8D%E4%BD%9C%E5%92%8C%E4%BD%BF%E7%94%A8/"/>
    
    
      <category term="命令操作" scheme="https://gsy00517.github.io/tags/%E5%91%BD%E4%BB%A4%E6%93%8D%E4%BD%9C/"/>
    
      <category term="ubuntu" scheme="https://gsy00517.github.io/tags/ubuntu/"/>
    
  </entry>
  
  <entry>
    <title>ubuntu笔记：安装typora</title>
    <link href="https://gsy00517.github.io/ubuntu20200123103954/"/>
    <id>https://gsy00517.github.io/ubuntu20200123103954/</id>
    <published>2020-01-23T02:39:54.000Z</published>
    <updated>2020-01-28T02:00:33.599Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Tue Jan 28 2020 15:40:41 GMT+0800 (GMT+08:00) --><p>今天ubuntu系统又双叒叕被我搞崩了，折腾一个大半天之后还是无解，没办法只好根据之前博文<a href="https://gsy00517.github.io/ubuntu20190914100050/" target="_blank">ubuntu笔记：重装ubuntu——记一段辛酸血泪史</a>中的方法重装系统。心里还是非常庆幸还好当初留心写了一下。<br>痛定思痛，由于之前没有系统地学习linux操作系统，鸟哥的书也就看了一部分，因此觉得自己以后应该更加谨慎小心一些，每一步命令都要看明白再执行，不然再翻车的话真的要心态爆炸的。<br>之前在<a href="https://gsy00517.github.io/markdown20190913211144/" target="_blank">markdown笔记：markdown的基本使用</a>中介绍过typora，这里主要是以它为例，仔细地分析一下安装软件时每一步命令的作用。</p><p><strong>References</strong>：</p><p>电子文献：<br><a href="https://support.typora.io/Typora-on-Linux/" target="_blank" rel="noopener">https://support.typora.io/Typora-on-Linux/</a></p><hr><h1 id="安装过程"><a href="#安装过程" class="headerlink" title="安装过程"></a>安装过程</h1><ol><li><h2 id="信任软件包密匙"><a href="#信任软件包密匙" class="headerlink" title="信任软件包密匙"></a>信任软件包密匙</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">sudo apt-key adv --keyserver keyserver.ubuntu.com --recv-keys BA300B7755AFCFAE</span><br><span class="line">#optional, but recommended</span><br></pre></td></tr></table></figure><p>这条命令应该就是添加新的密匙并信任，一般在配置<code>apt-get</code>源之前运行。<br>对<code>apt-key</code>的描述如下：“apt-key is used to manage the list of keys used by apt to authenticate packages. Packages which have been authenticated using these keys will be considered trusted.”由于每个发布的Debian软件包都是通过密钥认证的，而apt-key命令正是用来管理Debian软件包密钥的。</p></li><li><h2 id="添加软件库"><a href="#添加软件库" class="headerlink" title="添加软件库"></a>添加软件库</h2><p>由于默认的软件仓库里是没有typora的，所以要添加对应的软件仓库。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">sudo add-apt-repository &apos;deb https://typora.io/linux ./&apos;</span><br><span class="line">#add Typora&apos;s repository</span><br></pre></td></tr></table></figure></li><li><h2 id="更新软件列表"><a href="#更新软件列表" class="headerlink" title="更新软件列表"></a>更新软件列表</h2><p>在添加了新的软件仓库之后，我们需要更新软件列表使得后面的操作能找到对应的软件包。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo apt-get update</span><br></pre></td></tr></table></figure></li><li><h2 id="安装"><a href="#安装" class="headerlink" title="安装"></a>安装</h2><p>更新<code>apt-get</code>之后，就可以安装前面添加的库中的软件包了。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">sudo apt-get install typora</span><br><span class="line">#install typora</span><br></pre></td></tr></table></figure></li><li><h2 id="有软件包无法下载"><a href="#有软件包无法下载" class="headerlink" title="有软件包无法下载"></a>有软件包无法下载</h2><p>在install的过程中，提示我：“有几个软件包无法下载”。<br>于是我照着提示执行了下面的命令：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo apt-get update --fix-missing</span><br></pre></td></tr></table></figure><p>然后再<code>sudo apt-get install typora</code>，就可以了。<br>如果还是有问题的话，可能需要更换软件源，换成国内的镜像比较好。</p></li><li><h2 id="更新"><a href="#更新" class="headerlink" title="更新"></a>更新</h2>安装后的typora由<code>apt-get</code>管理，因此可以用以下命令来更新软件包。<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo apt-get upgrade</span><br></pre></td></tr></table></figure></li></ol><hr><h1 id="软连接"><a href="#软连接" class="headerlink" title="软连接"></a>软连接</h1><p>痛定思痛，还是决定把这回翻车的地方写一下。<br>本来用命令行打开matlab挺好的，我自作自受想转个matlab-support想着用图标打开，结果报错：MATLAB is selecting SOFTWARE OPENGL rendering。到网上查资料后找到一个貌似可行的方法。<br>根据他所说，这是因为matlab的libstdc++库和系统库不匹配造成的，所以需要用如下命令建立一个连接。<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ln -s /usr/lib/x86_64-linux-gnu/libstdc++.so.6.0.21  /usr/local/MATLAB/R2015b/sys/os/glnxa64/libstdc++.so.6</span><br></pre></td></tr></table></figure><p></p><p>注意，这里是R2015b。由于我下载的是R2018b，显然这里的地址是不一样的，当时比较心急直接回车了。结果还是没有解决问题。<br>这句命令其实就是建立一个软连接，其基本格式是<code>ln –s 源文件目录 目标文件目录</code>。它只会在选定的位置上生成一个文件的镜像，不会占用磁盘空间，类似于windows中的快捷方式。若没加<code>-s</code>，就是硬链接，即会在选定的位置上生成一个和源文件大小相同的文件。不过，无论是软链接还是硬链接，文件都保持同步变化。<br><img src="/ubuntu20200123103954/软连接与硬链接.png" title="软连接与硬链接"><br>讲道理即使目录出错也是不会有问题的，然而当我再次开机尝试进入系统时，就出现了卡在recovering journal的情况。<br>卡住的位置仅有两行，第一行是recovering journal，第二行我在ubuntu社区里找到了一个比较类似的，如下图所示。<br><img src="/ubuntu20200123103954/同样的问题.jpg" title="同样的问题"><br>他后面解答的方法如下。<br><img src="/ubuntu20200123103954/解决的方法.png" title="解决的方法"><br>可以试一试，我也照着做下来了，但是没起作用。<br>我也在网上看了其它的一些办法，有先进入recovery mode然后选择resume normal boot就好了的（就是返回正常启动，很玄学），然而我没用；也有check all file systems的，我也尝试了but failed。<br><img src="/ubuntu20200123103954/recovery模式.JPG" title="recovery模式"></p><script type="text/javascript" src="/js/src/bai.js"></script><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      
      
        &lt;!-- build time:Tue Jan 28 2020 15:40:41 GMT+0800 (GMT+08:00) --&gt;&lt;p&gt;今天ubuntu系统又双叒叕被我搞崩了，折腾一个大半天之后还是无解，没办法只好根据之前博文&lt;a href=&quot;https://gsy00517.g
      
    
    </summary>
    
    
      <category term="操作和使用" scheme="https://gsy00517.github.io/categories/%E6%93%8D%E4%BD%9C%E5%92%8C%E4%BD%BF%E7%94%A8/"/>
    
    
      <category term="命令操作" scheme="https://gsy00517.github.io/tags/%E5%91%BD%E4%BB%A4%E6%93%8D%E4%BD%9C/"/>
    
      <category term="踩坑血泪" scheme="https://gsy00517.github.io/tags/%E8%B8%A9%E5%9D%91%E8%A1%80%E6%B3%AA/"/>
    
      <category term="ubuntu" scheme="https://gsy00517.github.io/tags/ubuntu/"/>
    
      <category term="安装教程" scheme="https://gsy00517.github.io/tags/%E5%AE%89%E8%A3%85%E6%95%99%E7%A8%8B/"/>
    
  </entry>
  
  <entry>
    <title>deep learning笔记：端到端学习</title>
    <link href="https://gsy00517.github.io/deep-learning20200122164503/"/>
    <id>https://gsy00517.github.io/deep-learning20200122164503/</id>
    <published>2020-01-22T08:45:03.000Z</published>
    <updated>2020-01-28T07:39:53.517Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Tue Jan 28 2020 15:40:41 GMT+0800 (GMT+08:00) --><p>以前觉得深度学习就是有很多层的神经网络，或者周志华提出的深度随机森林，总之只要是有很“深”的结构就是深度学习。直到不久前一位计科大佬告诉我深度学习是end-to-end（也表示成“e2e”）的，当时听的也是一知半解，回去查了一下后终于恍然大悟。本文主要基于Andrew Ng的课程中“What is end-to-end deep learning?”和“Whether to use end-to-end learning?”两节。推荐可以去看一看，讲得可以说是很浅显易懂了。</p><hr><h1 id="什么是end-to-end-learning"><a href="#什么是end-to-end-learning" class="headerlink" title="什么是end-to-end learning"></a>什么是end-to-end learning</h1><p>传统机器学习的流程往往由多个独立的模块组成，比如在一个典型的自然语言处理（Natural Language Processing）问题中，包括分词、词性标注、句法分析、语义分析等多个独立步骤，每个步骤是一个独立的任务，其结果的好坏会影响到下一步骤，从而影响整个训练的结果，这就是非端到端的。<br>而深度学习模型在训练过程中，从输入端（输入数据）到输出端得到一个预测结果，该结果与真实结果相比较会得到一个误差，这个误差将用于模型每一层的调整（比如反向传播），这种训练直到模型收敛或达到预期的效果才结束，这就是端到端（end-to-end）的。<br><img src="/deep-learning20200122164503/example.png" title="example"><br>相比传统方法每一个模块都有较为明确的输出，端到端的深度学习更像是一个神秘的整体。通俗的说，端到端的深度学习能够让“数据说话”。不过这种方法是很吃数据的，因此还不至于在每个领域都胜过甚至代替传统的机器学习方法。<br>由于以前被标注的数据没有那么丰富，因此经典机器学习方法始终占据主流。随着近年来一个又一个数据集的出现，这种状况发生了转变。一个重要的转折点就是AlexNet的横空出世，详见<a href="https://gsy00517.github.io/deep-learning20190915113859/" target="_blank">deep-learning笔记：开启深度学习热潮——AlexNet</a>。<br>在目标跟踪领域，继相关滤波大火之后，也出现了很多优秀的深度学习算法。其中，孪生网络充分借鉴了两者的优势，取得了不错的成绩。<br><img src="/deep-learning20200122164503/SiameseFC.jpg" title="SiameseFC"><br>上面是SiameseFC的主体架构，它借用了神经网络去提取特征，而不是利用一些较为经典的特征。实际上，也可以认为它是端到端的，在调整了相关滤波的形式之后，使相关滤波的操作过程可求导，从而实现了整个模型内部的前向传播和反向传播，实现端到端。<br>那么，这样做有什么意义呢？误差理论告诉，误差传播的途径本身会导致误差的累积，多个阶段大概率会导致误差累积，而端到端的训练就能减少误差传播的途径，实现联合优化。</p><hr><h1 id="何时该用end-to-end-learning"><a href="#何时该用end-to-end-learning" class="headerlink" title="何时该用end-to-end learning"></a>何时该用end-to-end learning</h1><p>相比之下，端到端学习省去了每一步中间的数据处理和每一步模型的设计（这往往会涉及相当多的专业知识），但是端到端学习也有两个重要的缺点。</p><ol><li><h2 id="缺点一：需要大量的数据"><a href="#缺点一：需要大量的数据" class="headerlink" title="缺点一：需要大量的数据"></a>缺点一：需要大量的数据</h2>Andrew Ng在视频课程中举了一个例子：百度的门禁系统可以识别靠近的人脸并放行。<br>如果直接使用端到端学习，那么需要训练的数据集就是一系列照片或者视频，其中人会随机出现在任何位置、任何距离等等，而这样标注好的数据集是很匮乏的。<br>但是，如果我们把这个任务拆解成两个子任务。首先，在照片或者视频中定位人脸，然后放大（使人脸居中等）；其次，对放大好的人脸再进行检验。这两种任务都有非常丰富的数据集或者方法可供使用。实际上，我觉得可以应用两个端到端的模型来解决这两个问题，但合起来就不是端到端的了。但在目前现有数据量的情况下，这依然能比直接端到端的方法表现得好。</li><li><h2 id="缺点二：可能排除有用的人工设计"><a href="#缺点二：可能排除有用的人工设计" class="headerlink" title="缺点二：可能排除有用的人工设计"></a>缺点二：可能排除有用的人工设计</h2>前面提到，人工设计的模块往往是基于知识的。而知识的注入有时候会大大简化模型（尤其是数据不足的时候）。这里Andrew Ng又举了一个例子：通过X光片来估计年龄。<br>传统的方法就是照一张图片，然后分割出每一块骨头并测量长度，然后通过这些长度结合理论和统计来估计年龄。<br>而若是使用端到端的模型，就是直接建立图片与年龄之间的联系，这显然是很难且很复杂的，训练结果的表现也可想而知。</li></ol><p>端到端学习确实在很多领域都能取得state-of-the-art的表现，但何时使用还是要具体问题具体分析。</p><hr><h1 id="神经网络学到了什么"><a href="#神经网络学到了什么" class="headerlink" title="神经网络学到了什么"></a>神经网络学到了什么</h1><p>我在前文中写了这样一句话：“相比传统方法每一个模块都有较为明确的输出，端到端的深度学习更像是一个神秘的整体”。但实际上一些研究者通过分离观察每一层，发现e2e的神经网络的确还是学到了一点东西的。<br>一般而言，神经网络前几层学到的内容包含的信息比较丰富具体。越到后面越抽象，即越到后面包含的语义信息越多。下面是对每一个卷积核（神经元）做可视化处理，左图为靠前的某层的可视化结果，而右图为靠后的某层的可视化结果。可以看到，相较于后面的层，前几层的卷积核似乎呈现出更明确的任务或者说功能。它们通常会找一些简单的特征，比如说边缘或者颜色阴影。<br><img src="/deep-learning20200122164503/学到了啥.png" title="学到了啥"><br>我们可以对第一层卷积层做特征可视化来看一下。<br><img src="/deep-learning20200122164503/特征可视化.png" title="特征可视化"><br>从特征可视化结果中看，出第一层卷积提取出了不同的特征，有些突出了斑马的形状，有些突出了背景，有些突出了斑马的斑纹等。<br>下面是Andrew Ng在课程中举得一个可视化例子，他所采用的方法是对每层中的隐藏单元用数据集去遍历，并且寻找出9个使得隐藏单元有较大的输出或是较大的激活的图片或者图像块。注意网络层数越深其感受野会越大。详见<a href="https://gsy00517.github.io/deep-learning20190915073809/" target="_blank">deep-learning笔记：着眼于深度——VGG简介与pytorch实现</a>。<br><img src="/deep-learning20200122164503/例子.jpg" title="例子"><br>实际上，实现可视化的方法有多种，上面的两个例子就是用了不同的方法（前者是针对一张图，而Andrew Ng用了一个数据集）。在Visualizing and Understanding Convolutional Networks一文中，作者也提出了一些更复杂的方式来可视化卷积神经网络的计算。<br>在NLP领域，也有类似的发现，比如一些训练过后的神经元对特定标点的响应特别强烈，而有一些训练过后的神经元对一些特定语气词的响应特别强烈。</p><script type="text/javascript" src="/js/src/bai.js"></script><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      
      
        &lt;!-- build time:Tue Jan 28 2020 15:40:41 GMT+0800 (GMT+08:00) --&gt;&lt;p&gt;以前觉得深度学习就是有很多层的神经网络，或者周志华提出的深度随机森林，总之只要是有很“深”的结构就是深度学习。直到不久前一位计科大佬告诉我深度学
      
    
    </summary>
    
    
      <category term="人工智能" scheme="https://gsy00517.github.io/categories/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/"/>
    
    
      <category term="深度学习" scheme="https://gsy00517.github.io/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>matlab笔记：MEX文件函数使用中的问题</title>
    <link href="https://gsy00517.github.io/matlab20200121194751/"/>
    <id>https://gsy00517.github.io/matlab20200121194751/</id>
    <published>2020-01-21T11:47:51.000Z</published>
    <updated>2020-01-28T01:56:53.849Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Tue Jan 28 2020 15:40:41 GMT+0800 (GMT+08:00) --><p>之前在<a href="https://gsy00517.github.io/matlab20200115222641/" target="_blank">matlab笔记：安装MinGW编译器</a>一文中已经介绍过，MEX文件函数是Matlab提供的一种混合编程方式。通过MEX，用户可以在matlab中调用C、C++（没有C#，但我想提一下其实C#的真正含义是C++++，因为#其实就是四个+）或者Fortran编写的计算程序，加速matlab内部的矩阵运算（尤其是加速matlab代码中的for循环）。mex本质上是一个动态链接库文件（dll），可以被matlab动态加载并执行。然而在使用的过程中，我又碰到了许多问题。</p><p><strong>References</strong>：</p><p>电子文献：<br><a href="https://ww2.mathworks.cn/help/matlab/call-mex-file-functions.html" target="_blank" rel="noopener">https://ww2.mathworks.cn/help/matlab/call-mex-file-functions.html</a><br><a href="https://blog.csdn.net/hijack00/article/details/52228253" target="_blank" rel="noopener">https://blog.csdn.net/hijack00/article/details/52228253</a><br><a href="https://jingyan.baidu.com/article/3a2f7c2ea00a9c66aed61163.html" target="_blank" rel="noopener">https://jingyan.baidu.com/article/3a2f7c2ea00a9c66aed61163.html</a></p><hr><h1 id="安装版本适配的MinGW编译器"><a href="#安装版本适配的MinGW编译器" class="headerlink" title="安装版本适配的MinGW编译器"></a>安装版本适配的MinGW编译器</h1><p>根据之前文章中写的配置方法，在编译MEX的时候，虽然没有问题，但是却出现了警告：使用的是不受支持的MinGW编译器版本。<br>于是我先查看了当前使用的编译器的版本。方法如下：</p><ol><li>在MinGW-w64编译器的安装目录中，找到gcc.exe可执行文件的存在位置。</li><li>打开命令行，切换到刚刚找到的gcc.exe文件所在的目录。</li><li>键入<code>gcc -v</code>即可查看当前编译器的版本。<img src="/matlab20200121194751/查看版本.png" title="查看版本"></li></ol><p>这时我使用的是5.1.0版本，于是我又到<a href="https://www.mathworks.com/matlabcentral/fileexchange/52848-matlab-support-for-mingw-w64-c-c-compiler" target="_blank">mathworks的网站</a>上看了一下各个matlab版本适配的编译器版本。<br><img src="/matlab20200121194751/各个版本配置.png" title="各个版本配置"><br>这里我把图片截过来了，就不用去找了。我使用的是matlab R2019a，因此适配的是MinGW GCC 6.3（似乎高一点或者低一点都不行）。<br>于是我根据它所提供的<a href="https://sourceforge.net/projects/mingw-w64/files/" target="_blank">SourceForge网址</a>去找新版本的安装包，下载解压之后是一个不含任何可执行文件（exe）的文件夹，而且是7.0.0版本的，无法自主地选择。<br>寻找良久之后，我终于发现了一个在线安装文件，也建议下载这个，因为后面需要选择特定版本来安装。<br><img src="/matlab20200121194751/在线安装可执行文件.png" title="在线安装可执行文件"><br>下载完成后直接双击安装，这里会有一个安装设置界面。这个要注意一下，别点过去了。版本号一定要设置成对应的，比如我是6.3.0；另外，由于安装在windows 64位系统上，所以选择x86_64以及win32；至于其它的选项可以任选，一般默认就好了。<br><img src="/matlab20200121194751/安装设置.png" title="安装设置"><br>之后就是一路“下一步”，记得记住安装路径。<br>之后就是用和<a href="https://gsy00517.github.io/matlab20200115222641/" target="_blank">matlab笔记：安装MinGW编译器</a>中所写的相同的方式添加环境变量。可以直接把之前已有的MW_MINGW64_LOC的值替换成刚刚记下的路径，最后别忘了在matlab中<code>setenv</code>。<br>这时也可以把之前的编译器删了，如果是TDM-GCC的话那很方便，直接在它的一个管理界面中uninstall就行了，另外还会剩下一个空文件夹，手动删除就行。</p><hr><h1 id="连接外部库"><a href="#连接外部库" class="headerlink" title="连接外部库"></a>连接外部库</h1><p>在我使用的过程中，我还遇到了如下ERROR: Unable to compile MEX function: “MEX 找不到使用 -l 选项指定的库 ‘ut’。<br><img src="/matlab20200121194751/报错.png" title="报错"><br>上网搜了一下后，我才知道MEX命令可以用<code>-L</code>选项指定第三方库的路径，用<code>-l</code>来连接第三方库文件。值得注意的是，这里使用的是该库文件的文件名，不包含其文件扩展名。其基本格式如下：<br></p><figure class="highlight matlab"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">mex -L&lt;library_path&gt; -l&lt;library&gt;</span><br></pre></td></tr></table></figure><p></p><p>可以发现，<code>-L</code>与<code>&lt;library_path&gt;</code>、<code>-l</code>与<code>&lt;library&gt;</code>之间是没有加空格的。<br>可是看了这些，我还是解决不了我的问题。<br>这里说一下我出现这个问题的背景，最近接触计算机视觉中的目标跟踪这一块，正在学习vot-toolkit的使用。<br>我既问了度娘又问了谷哥，可是没有看到任何这个问题及其解决方法。于是我缩小范围，看了看Github上vot-toolkit的issues和VOT Challenge technical support的Google groups，惊喜的是的确都找到了同样的问题。<br><img src="/matlab20200121194751/同样的问题1.png" title="同样的问题1"><br><img src="/matlab20200121194751/同样的问题2.png" title="同样的问题2"><br>然而都只有问题没有解答。无奈，还是自己想办法吧。<br>其实跟着报错的提示来修改并不难，关键是要找到该修改哪里。<br>由于报错提示的函数中根本没有MEX连接库文件的指令（我一行一行代码找的），于是我想能不能找到MEX的编译文件。最终我在vot-toolkit-master\utilities中找到了一个名为<code>compile_mex.m</code>的matlab文件，其中有这样的一串代码。<br></p><figure class="highlight matlab"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> is_octave()</span><br><span class="line">    arguments&#123;<span class="keyword">end</span>+<span class="number">1</span>&#125; = <span class="string">'-DOCTAVE'</span>;</span><br><span class="line"><span class="keyword">else</span></span><br><span class="line">    arguments&#123;<span class="keyword">end</span>+<span class="number">1</span>&#125; = <span class="string">'-lut'</span>;</span><br><span class="line"><span class="keyword">end</span></span><br></pre></td></tr></table></figure><p></p><p>我用的是matlab，不是octave（后者相当于轻量级的免费matlab，语法什么的基本一致），那么执行的应该是<code>else</code>后面的语句，而在这里可以看到调用ut的命令。<br>于是我就把这里的<code>-lut</code>改成了<code>-Lut</code>试了一下，果然成功了。<br>其实用有搜索功能的IDE的话或许能够更快地解决这个问题。</p><script type="text/javascript" src="/js/src/bai.js"></script><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      
      
        &lt;!-- build time:Tue Jan 28 2020 15:40:41 GMT+0800 (GMT+08:00) --&gt;&lt;p&gt;之前在&lt;a href=&quot;https://gsy00517.github.io/matlab20200115222641/&quot; target=&quot;_b
      
    
    </summary>
    
    
      <category term="环境配置" scheme="https://gsy00517.github.io/categories/%E7%8E%AF%E5%A2%83%E9%85%8D%E7%BD%AE/"/>
    
    
      <category term="安装教程" scheme="https://gsy00517.github.io/tags/%E5%AE%89%E8%A3%85%E6%95%99%E7%A8%8B/"/>
    
      <category term="matlab" scheme="https://gsy00517.github.io/tags/matlab/"/>
    
  </entry>
  
  <entry>
    <title>computer vision笔记：相关滤波与KCF</title>
    <link href="https://gsy00517.github.io/computer-vision20200120120823/"/>
    <id>https://gsy00517.github.io/computer-vision20200120120823/</id>
    <published>2020-01-20T04:08:23.000Z</published>
    <updated>2020-01-21T03:19:13.751Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Tue Jan 28 2020 15:40:41 GMT+0800 (GMT+08:00) --><p>相关滤波（cross-correlation）是在目标跟踪领域一种非常强大的方法，主打简洁和高速，各种基于相关滤波的算法层出不穷。其中，KCF（不是肯德基）是一个非常经典的算法，在目标跟踪领域虽说它不是最早运用相关滤波的算法（MOSSE要早于它），但是它对之后运用相关滤波进行目标跟踪的这一系列算法有重要的奠基作用。本文就最近对这些方面的了解，结合自己的思考，做一个简单的整理归纳，如有疏漏之处还请多多指教。</p><p><strong>References</strong>：</p><p>电子文献：<br><a href="https://blog.csdn.net/fhcfhc1112/article/details/83783588" target="_blank" rel="noopener">https://blog.csdn.net/fhcfhc1112/article/details/83783588</a><br><a href="https://blog.csdn.net/weixin_39467358/article/details/83304082" target="_blank" rel="noopener">https://blog.csdn.net/weixin_39467358/article/details/83304082</a><br><a href="https://www.cnblogs.com/jins-note/p/10215511.html" target="_blank" rel="noopener">https://www.cnblogs.com/jins-note/p/10215511.html</a></p><p>参考文献：<br>[1]High-Speed Tracking with Kernelized Correlation Filters<br>[2]Visual Object Tracking using Adaptive Correlation Filters</p><hr><h1 id="建议"><a href="#建议" class="headerlink" title="建议"></a>建议</h1><p>由于KCF这篇文章主要是从理论上面来论述相关滤波来做tracking，其中涉及数学、理论的东西还是挺繁琐的。因此在正文开始之前，推荐可以先看一下我之前总结的几篇有关的博文，后文涉及到的话就不详细写了。<br>循环矩阵：<a href="https://gsy00517.github.io/linear-algebra20200116095725/" target="_blank">linear-algebra笔记：循环矩阵</a>。<br>HOG特征：<a href="https://gsy00517.github.io/computer-vision20200119195116/" target="_blank">computer-vision笔记：HOG特征</a>。<br>正负样本：<a href="https://gsy00517.github.io/machine-learning20200118112156/" target="_blank">machine-learning笔记：数据采样之正样本和负样本</a>。<br>闭式解：<a href="https://gsy00517.github.io/machine-learning20200119145637/" target="_blank">machine-learning笔记：闭式解</a>。<br>此外，还可以先看看b站up主桥本环关于相关滤波两个算法的讲解视频<a href="https://www.bilibili.com/video/av74302620" target="_blank">目标跟踪：相关滤波算法MOSSE实现代码讲解</a>和<a href="https://www.bilibili.com/video/av76651535" target="_blank">目标跟踪：相关滤波算法KCF实现代码讲解</a>，个人觉得讲得挺不错的。</p><hr><h1 id="论文"><a href="#论文" class="headerlink" title="论文"></a>论文</h1><p>这里分别是<a href="MOSSE.pdf" target="_blank">MOSSE</a>和<a href="KCF.pdf" target="_blank">KCF</a>两个算法的论文，我在阅读时已用黄色高亮一部分重点。同时本篇文章也主要参考了这两篇paper，可以先看，也可以看完本文再看。</p><hr><h1 id="相关滤波"><a href="#相关滤波" class="headerlink" title="相关滤波"></a>相关滤波</h1><p>先来看一个公式：</p><script type="math/tex;mode=display">\left ( f\star g \right )\left ( \tau \right ):=\int_{-\infty }^{\infty }f^{\ast }\left ( t \right )g\left ( t+\tau \right )dt</script><p>这里$:=$表示“定义为”（等效于等号上加个delta或者def），$\ast$表示复数共轭（complex-conjugate），而这里的$\star$表示的就是相关滤波操作。<br>你可能会觉得这个式子非常熟悉，是的，它非常像卷积的式子。但是不同的是，一般的卷积操作为$\int_{-\infty }^{\infty }f\left ( t \right )g\left ( t-\tau \right )dt$，而在相关滤波这里是加号。这就是说，在卷积的时候，我们需要把模板先进行翻转，再进行卷积，而相关操作就不需要了。<br>其实相关操作就是用来衡量两个信号是否相关，当两个信号越相似、相关性越强的时候，他们做相关操作输出的响应就会越强。用到目标跟踪里面，当做相关操作的两个框中的目标越相似，我们就会获得越高的响应。<br>相关滤波的实际意义是把输入图像映射到一个理想响应图，将这个响应图当中的最高峰与目标中心点对应起来，也就是我们预测的目标接下来的位置。它一个最主要的优点就是能够借助于傅里叶变换，从而快速计算大量的候选样本的响应值。</p><hr><h1 id="循环矩阵"><a href="#循环矩阵" class="headerlink" title="循环矩阵"></a>循环矩阵</h1><p>在论文High-Speed Tracking with Kernelized Correlation Filters的introduction部分，有这样一句话：“we argue that undersampling negatives is the main factor inhibiting performance in tracking.”也就是说，负样本的欠采样是阻碍跟踪效果的主要因素。这在之前的文章中介绍过，这里就不在细述了。<br>这句话主要针对的问题是我们可以从一张图像中获得几乎无限的负样本。但由于跟踪的时间敏感性，我们的跟踪算法只能在尽可能多地采集样本和维持较低的计算需求之间取得一个平衡。之前通常的做法是从每一帧中随机选择几个样本。<br>KCF的一大贡献就是采用了一种更方便的方法迅速获取更多的负样本，以便于能够训练出一个更好的分类器。作者发现，在傅里叶域中，如果我们使用特定的模型进行转换，一些学习算法实际上变得更容易（in the Fourier domain, some learning algorithms actually become easier as we add more samples, if we use a speciﬁc model for translations）。<br>具体的做法如下，首先利用一个n维列向量来表示目标，记为$x$，然后利用$x$和一个循环位移矩阵$P$生成一个循环矩阵，其目的是使用一个base sample（正样本）和生成多个虚拟样本（负样本）来训练一个分类器。<br><img src="/computer-vision20200120120823/位移矩阵.png" title="位移矩阵"><br><img src="/computer-vision20200120120823/一维样本构成循环矩阵.png" title="一维样本构成循环矩阵"><br>根据循环特性可以推出下面两点：</p><ol><li>可以周期性的获得同样的信号。</li><li>同样的，我们可以把上面的变换等效成将base sample即生成向量正向移动一半长度和反向移动一半长度组合而成。</li></ol><p>这里是一个循环图片的示例，使用base sample，若我们向下移动15个像素，也就是从下面剪切15个像素拼到上面，就会变成左二图，若移动30个就可以生成左一图，右侧的图片是上移生成的。这就是在做tracking时循环采样的样本，一般会在目标周围取一个比目标更大的一个框，然后对大框框取的图像区域进行循环采样，那么就会生成这样一些新的样本来模拟我们的正样本并用于训练。<br><img src="/computer-vision20200120120823/示例.png" title="示例"><br>获得了这样一个循环矩阵之后，作者接下来说：“all circulant matrices are made diagonal by the Discrete Fourier Transform(DFT), regardless of the generating vector x.”就是说循环矩阵的生成向量是完全指定的，且循环矩阵有一个非常好的性质：对任意生成向量$\widehat{x}$，我们都可以通过离散傅立叶变换（具有线性性）对循环矩阵进行对角化。</p><script type="math/tex;mode=display">X=C(x)=F\cdot diag(\widehat{x})\cdot F^{H}</script><p>这里的$F$是一个与向量$\widehat{x}$无关的常数矩阵，如果这里看得不懂的话，可以参照<a href="https://gsy00517.github.io/linear-algebra20200116095725/" target="_blank">linear-algebra笔记：循环矩阵</a>。<br>如此，在傅里叶域内，用离散傅里叶变换来做之后的计算，对速度会有非常大的提升。<br>用到循环矩阵后，有两个常用的公式，可以参考我之前的文章。</p><hr><h1 id="训练"><a href="#训练" class="headerlink" title="训练"></a>训练</h1><p>KCF做训练时所用的是岭回归。在线性情况下，岭回归的优化目标方程如下所示：</p><script type="math/tex;mode=display">f\left ( z \right )=w^{T}z</script><script type="math/tex;mode=display">\underset{w}{min}\sum_{i}\left ( f\left ( x_{i}-y_{i} \right ) \right )^{2}+\lambda \left \| w \right \|^{2}</script><p>其闭式解为：</p><script type="math/tex;mode=display">w=\left ( X^{T}X+\lambda I \right )^{-1}X^{T}y</script><p>此时就可以利用循环矩阵在傅里叶域计算的性质来求解了。最后求出如下式子：</p><script type="math/tex;mode=display">F\left ( w \right )=\frac{\widehat{x}}{\widehat{x}\odot \widehat{x}^{\ast }+\lambda \delta }\odot F\left ( y \right )=\frac{\widehat{x}\odot \widehat{y}}{\widehat{x}\odot \widehat{x}^{\ast }+\lambda \delta }</script><p>从原来的矩阵相乘和求逆，转换到傅里叶域的点乘和点除（这里的除号是点除），一下子运算就简单了许多。<br>在非线形的情况下，也可以得到一个同样的情况，这里需要引入一个满足条件的核，例如高斯核、线性核等，最后可计算得出一个闭式解。<br><img src="/computer-vision20200120120823/满足条件的核.png" title="满足条件的核"><br>非线性情况下，引入核可得到一个类似的岭回归的优化目标方程：</p><script type="math/tex;mode=display">w=\sum_{i}\alpha _{i}\varphi \left ( x_{i} \right )</script><script type="math/tex;mode=display">f\left ( z \right )=w^{T}z=\sum_{i=1}^{n}\alpha _{i}\kappa \left ( z,x_{i} \right )</script><script type="math/tex;mode=display">\underset{w}{min}\sum_{i}\left ( f\left ( x_{i}-y_{i} \right ) \right )^{2}+\lambda \left \| w \right \|^{2}</script><p>这里我们定义核函数$\kappa$为基向量$\varphi \left ( x \right )$之间的点积，即$\varphi^{T} \left ( x \right )\varphi \left ( x{}’ \right )=\kappa \left ( x,x{}’ \right )$。<br>在岭回归/脊回归（Ridge Regression）中，闭式解的基本形式如下：</p><script type="math/tex;mode=display">\alpha =\left ( K+\lambda I \right )^{-1}y</script><p>这里$K$表示核空间的核矩阵，由核函数得到$K_{ij}=\kappa \left ( x_{i},x_{j} \right )=\varphi \left ( X \right )\varphi \left ( X \right )^{T}$。<br>最终可得：</p><script type="math/tex;mode=display">\widehat{\alpha }=\frac{\widehat{y}}{\widehat{k}^{xx}+\lambda }</script><p>这里的除号也是点除，此时求出来的$K$和$\alpha$就可以来做tracking了。同样的，我们还是利用循环矩阵的性质并且在傅里叶域内来做计算。</p><hr><h1 id="快速检测"><a href="#快速检测" class="headerlink" title="快速检测"></a>快速检测</h1><p>我们很少希望单独来评估一个图像块的回归函数$f\left ( z \right )$。为了检测感兴趣的目标对象，我们通常希望在几个图像位置上评估$f\left ( z \right )$，这几个候选块（candidate patches）可以通过循环位移来建模。<br>定义$K^{z}$表示所有训练样本和所有候选块之间的核矩阵$K^{z}=\varphi \left ( X \right )\varphi \left ( Z \right )^{T}$。由于样本和图像块都是分别通过基础样本$x$和基础图像块$z$的循环移位组成的，因此矩阵$K^{z}$的每个元素可以表示为：$K_{i,j}=k(P^{i-1}z,P^{j-1}x)$。这里的$P$表示的是位移矩阵。易验证，$K^{z}$也是循环矩阵。<br>可计算得到各测试样本的响应值：</p><script type="math/tex;mode=display">f\left ( z \right )=\left ( K^{z} \right )^{T}\alpha</script><script type="math/tex;mode=display">\widehat{f}\left ( z \right )=\widehat{k}^{xz}\odot \widehat{\alpha }</script><p>最后，我们可以求得一张feature map，也就是一张二维的响应图。</p><hr><h1 id="效果"><a href="#效果" class="headerlink" title="效果"></a>效果</h1><img src="/computer-vision20200120120823/效果对比.png" title="效果对比"><p>这是KCF在OTB2013上面做的一个实验，由于当时效果比较好的是struck（所以逃不了被针对的命运）。可以看到，KCF（使用HOG特征+高斯核函数）和DCF（也是同一个作者同一篇论文提出的，使用HOG特征+线性核函数，称为对偶相关滤波器）相比于struck来说，精度取得了显著的提升，从0.656提升到了0.732/0.728。<br><img src="/computer-vision20200120120823/效果统计.png" title="效果统计"><br>我们还可以根据这个统计图来看一下速度，即使使用了HOG特征和高斯核，KCF的速度还能达到172帧每秒。<br>此外，用了多通道扩展的DCF取得了更快的速度，但就精度而言较KCF稍差，但也是质的飞跃了。<br>相较而言，即使用了非常朴素的raw pixels，尽管效果比HOG特征差好多，但是速度并没有提高。这里也证明了HOG特征的强大。<br>另外可以看到KCF的祖宗MOSSE速度非常亮眼，但这是因为MOSSE它只用了简单的灰度特征，而不是HOG这样高维的特征，可想而知精确度总体效果还是要差一大截的。</p><hr><h1 id="其他"><a href="#其他" class="headerlink" title="其他"></a>其他</h1><p>头一回看这么“理论”的论文我真的头都大了，要全部搞懂的话估计要花整整一天还不够。真的不得不佩服科研工作者们的智慧，我还是老老实实打基础吧。<br><img src="/computer-vision20200120120823/随意感受一下.png" title="随意感受一下"><br>原文的理论性、数学性更强，本文把主要的几个核心公式整理了一下，有些许修改和添加，如有疏漏还请多多指教。</p><script type="text/javascript" src="/js/src/bai.js"></script><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      
      
        &lt;!-- build time:Tue Jan 28 2020 15:40:41 GMT+0800 (GMT+08:00) --&gt;&lt;p&gt;相关滤波（cross-correlation）是在目标跟踪领域一种非常强大的方法，主打简洁和高速，各种基于相关滤波的算法层出不穷。其中，KCF（
      
    
    </summary>
    
    
      <category term="人工智能" scheme="https://gsy00517.github.io/categories/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/"/>
    
    
      <category term="论文分享" scheme="https://gsy00517.github.io/tags/%E8%AE%BA%E6%96%87%E5%88%86%E4%BA%AB/"/>
    
      <category term="计算机视觉" scheme="https://gsy00517.github.io/tags/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/"/>
    
  </entry>
  
  <entry>
    <title>computer vision笔记：图像金字塔与高斯滤波器</title>
    <link href="https://gsy00517.github.io/computer-vision20200119231033/"/>
    <id>https://gsy00517.github.io/computer-vision20200119231033/</id>
    <published>2020-01-19T15:10:33.000Z</published>
    <updated>2020-01-28T01:47:30.677Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Tue Jan 28 2020 15:40:41 GMT+0800 (GMT+08:00) --><p>在<a href="https://gsy00517.github.io/computer-vision20200119195116/" target="_blank">computer-vision笔记：HOG特征</a>一文中，我曾提及了Image Pyramid。那么，这个图像金字塔究竟是一个什么名胜古迹呢？</p><p><strong>References</strong>：</p><p>电子文献：<br><a href="https://www.cnblogs.com/ronny/p/3886013.html" target="_blank" rel="noopener">https://www.cnblogs.com/ronny/p/3886013.html</a><br><a href="https://www.cnblogs.com/wynlfd/p/9704770.html" target="_blank" rel="noopener">https://www.cnblogs.com/wynlfd/p/9704770.html</a><br><a href="https://www.cnblogs.com/herenzhiming/articles/5276106.html" target="_blank" rel="noopener">https://www.cnblogs.com/herenzhiming/articles/5276106.html</a><br><a href="https://www.jianshu.com/p/73e6ccbd8f3f" target="_blank" rel="noopener">https://www.jianshu.com/p/73e6ccbd8f3f</a><br><a href="https://baike.baidu.com/item/%E9%AB%98%E6%96%AF%E6%BB%A4%E6%B3%A2/9032353?fr=aladdin" target="_blank" rel="noopener">https://baike.baidu.com/item/%E9%AB%98%E6%96%AF%E6%BB%A4%E6%B3%A2/9032353?fr=aladdin</a><br><a href="https://blog.csdn.net/lvquanye9483/article/details/81592574" target="_blank" rel="noopener">https://blog.csdn.net/lvquanye9483/article/details/81592574</a><br><a href="https://zhuanlan.zhihu.com/p/94014493" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/94014493</a></p><hr><h1 id="原因"><a href="#原因" class="headerlink" title="原因"></a>原因</h1><p>当用一个机器视觉系统分析未知场景时，计算机没有办法预先知识图像中物体尺度，因此，我们需要同时考虑图像在多尺度下的描述，获知感兴趣物体的最佳尺度。在很多时候，我们会将图像构建为一系列不同尺度的图像集，在不同的尺度中去检测我们感兴趣的特征。比如：在Haar特征检测人脸的时候，因为我们并不知道图像中人脸的尺寸，所以需要生成一个不同大小的图像组成的“金字塔”，扫描其中每一幅图像来寻找可能的人脸。<br><img src="/computer-vision20200119231033/不同尺度的图像.png" title="不同尺度的图像"></p><hr><h1 id="图像金字塔"><a href="#图像金字塔" class="headerlink" title="图像金字塔"></a>图像金字塔</h1><p>我们可以这样得到一个图像金字塔：首先，图像经过一个低通滤波器进行平滑处理（这个步骤会使图像变模糊，类似远处的物体没有近处的清晰），然后，对这个平滑处理后的图像进行抽样（一般抽样比例在水平和竖直方向上都为1/2），从而得到一系列缩小的图像。<br><img src="/computer-vision20200119231033/图像金字塔.jpg" title="图像金字塔"><br>假设高斯金字塔的第$l$层图像为$G_{l}$，则有：</p><script type="math/tex;mode=display">G_{l}\left ( i,j \right )=\sum_{m=-2}^{2}\sum_{n=-2}^{2}\omega \left ( m,n \right )G_{l-1}\left ( 2i+m,2j+n \right )</script><script type="math/tex;mode=display">\left ( 1\leq l\leq N,0\leq i\leq R_{l},0\leq j\leq C_{l} \right )</script><p>其中，$N$为高斯金字塔的层数，$R_{l}$和$C_{l}$分别为高斯金字塔第$l$层的行数和列数，$\omega \left ( m,n \right )$是一个二位可拆的5x5窗口函数，其表达式为：</p><script type="math/tex;mode=display">\omega =\frac{1}{256}\begin{bmatrix} 1 & 4 & 6 & 4 & 1\\ 4 & 16 & 24 & 16 & 4\\ 6 & 24 & 36 & 24 & 6\\ 4 & 16 & 24 & 16 & 4\\ 1 & 4 & 6 & 4 & 1 \end{bmatrix}=\frac{1}{16}\begin{bmatrix} 1 & 4 & 6 & 4 & 1 \end{bmatrix}\times \frac{1}{16}\begin{bmatrix} 1\\ 4\\ 6\\ 4\\ 1 \end{bmatrix}</script><p>上式说明，2维窗口的卷积算子，可以写成两个方向上的1维卷积核的乘积。上面卷积形式的公式实际上完成了两个功能：（1）高斯模糊；（2）降维。<br>按上述步骤生成的$G_{0}$，$G_{1}$，…，$G_{N}$就构成了图像的高斯金字塔，其中$G_{N}$为金字塔的底层（与原图像相同），$G_{N}$为金字塔的顶层。可见高斯金字塔的当前层图像是对其前一层图像进行高斯低通滤波、然后做隔行和隔列的降采样（去除偶数行与偶数列）生成的。其中每一层都是前一层图像大小的1/4。</p><hr><h1 id="高斯滤波器"><a href="#高斯滤波器" class="headerlink" title="高斯滤波器"></a>高斯滤波器</h1><p>本文讨论图像金字塔，怎么说着说着就变成高斯金字塔了呢。事实上，上面所提到的$\omega$其实是一个整数值的高斯核函数，进行了高斯滤波的平滑处理。</p><h2 id="高斯滤波"><a href="#高斯滤波" class="headerlink" title="高斯滤波"></a>高斯滤波</h2><p>这里先引入两个问题：</p><ol><li>为什么要对图像滤波？<br>主要有两个目的：（1）消除图像在数字化过程中产生或者混入的噪声；（2）提取图片对象的特征作为图像识别的特征模式。</li><li>如何理解滤波器？<br>这与电路中的滤波器类似但又不同，在图像处理中，滤波器可以想象成一个包含加权系数的窗口，当使用滤波器去处理图像时，输出就相当于通过这个窗口去看这个图像。</li></ol><p>滤波的方式有很多种，而高斯滤波是一种线性平滑滤波，适用于消除高斯噪声。<br>在图像处理中，高斯滤波一般有两种实现方式，一是用离散化窗口滑窗卷积，另一种通过傅里叶变换。最常见的就是第一种滑窗实现，只有当离散化的窗口非常大，用滑窗计算量非常大（尽管用了可分离滤波器依旧大）的情况下，可能会考虑基于傅里叶变化的实现方法。本文介绍的是滑窗卷积法。</p><h2 id="高斯噪声"><a href="#高斯噪声" class="headerlink" title="高斯噪声"></a>高斯噪声</h2><p>首先，噪声在图像中常表现为一引起较强视觉效果的孤立像素点或像素块。<br>而高斯噪声，就是噪声的概率密度函数服从正态分布。<br><img src="/computer-vision20200119231033/示例.png" title="示例"></p><h2 id="高斯函数"><a href="#高斯函数" class="headerlink" title="高斯函数"></a>高斯函数</h2><p>我们首先回顾一下概率论中的高斯函数。<br>一维高斯分布：$G\left ( x \right )=\frac{1}{\sqrt{2\pi }\sigma }e^{-\frac{x^{2}}{2\sigma ^{2}}}$。<br><img src="/computer-vision20200119231033/一维高斯函数.jpg" title="一维高斯函数"><br>二维高斯分布：$G\left ( x,y \right )=\frac{1}{\sqrt{2\pi }\sigma ^{2}}e^{-\frac{x^{2}+y^{2}}{2\sigma ^{2}}}$<br><img src="/computer-vision20200119231033/二维高斯函数.jpg" title="二维高斯函数"></p><blockquote><p>注：$\sigma$越大，高斯函数的高度越小，宽度越大。</p></blockquote><p>如上图所示，正态分布是一种钟形曲线，越接近中心，取值越大，越远离中心，取值越小。<br>我们只需要将“中心点”作为原点，其他点按照其在正态曲线上的位置，分配权重，就可以得到一个加权平均值。这也是高斯核函数的构造原理。<br>理论上，高斯分布在所有定义域上都有非负值，这就需要一个无限大的卷积核。实际上，仅需要取均值周围3倍标准差内的值（$3\sigma$准则），以外的部分可以直接去掉。<br><img src="/computer-vision20200119231033/一个高斯核权重矩阵.jpg" title="一个高斯核权重矩阵"><br>这里取$\sigma=1.5$，注意，由于要对这9个点计算加权平均，因此必须让它们的权重之和等于1，上图是最终所得的高斯模板。</p><h2 id="高斯模糊"><a href="#高斯模糊" class="headerlink" title="高斯模糊"></a>高斯模糊</h2><p>模糊处理的过程其实就是卷积的过程，使用上面的高斯模板，对图像的每一个像素进行卷积，就能使图像产生模糊平滑的效果。<br><img src="/computer-vision20200119231033/高斯滤波卷积过程.png" title="高斯滤波卷积过程"><br>上图最左边是原图9个像素点的灰度值，最右边是输出的结果。要注意的是，一次这样的操作实际上只得到了中心点的输出，即所求的加权平均结果为中心点的高斯滤波输出值。</p><h2 id="可分离滤波器"><a href="#可分离滤波器" class="headerlink" title="可分离滤波器"></a>可分离滤波器</h2><p>如上面图像金字塔一节所述，2维窗口的卷积算子，可以写成两个方向上的1维卷积核的乘积。由于高斯函数可以写成可分离的形式，因此可以采用可分离滤波器实现来加速。所谓的可分离滤波器，就是可以把多维的卷积化成多个一维卷积。具体到二维的高斯滤波，就是指先对行做一维卷积，再对列做一维卷积。这样就可以将计算时间复杂度从$O\left ( M\ast M\ast N\ast N \right )$降到$O\left ( 2\ast M\ast M\ast N \right )$，这里的$M$和$N$分别是图像和滤波器的窗口大小。</p><h2 id="性质"><a href="#性质" class="headerlink" title="性质"></a>性质</h2><ol><li><h3 id="旋转对称性"><a href="#旋转对称性" class="headerlink" title="旋转对称性"></a>旋转对称性</h3>二维高斯函数具有旋转对称性，即滤波器在各个方向上的平滑程度是相同的。一般一幅图像的边缘方向事先是不知道的，因此，在滤波前是无法确定一个方向上比另一方向上需要更多的平滑。而旋转对称性意味着高斯平滑滤波器在后续边缘检测中不会偏向任一方向。</li><li><h3 id="平滑程度易调"><a href="#平滑程度易调" class="headerlink" title="平滑程度易调"></a>平滑程度易调</h3>高斯滤波器宽度（决定着平滑程度）是由参数$\sigma$表征的，而且$\sigma$和平滑程度的关系是非常简单的。$\sigma$越大，高斯滤波器的频带就越宽，平滑程度就越好。通过调节平滑程度参数$\sigma$，可在图像特征过分模糊（过平滑）与平滑图像中由于噪声和细纹理所引起的过多的不希望突变量（欠平滑）之间取得balance。<br>此外，高斯函数是单值函数。这表明，高斯滤波器用像素邻域的加权均值来代替该点的像素值，而每一邻域像素点权值是随该点与中心点的距离单调增减的。这一性质很重要，因为边缘是一种图像局部特征，如果平滑运算对离算子中心很远的像素点仍然有很大作用，则平滑运算会使图像失真。</li><li><h3 id="平滑可以层叠"><a href="#平滑可以层叠" class="headerlink" title="平滑可以层叠"></a>平滑可以层叠</h3>由性质：两个高斯核的卷积等同于另外一个不同核参数的高斯核卷积。可以推得：不同的高斯核对图像的平滑是连续的。<script type="math/tex;mode=display">g\left ( \mu ,\sigma _{1} \right )\ast g\left ( \mu ,\sigma _{2} \right )=g\left ( \mu ,\sqrt{\sigma _{1}^{2}+\sigma _{2}^{2}} \right )</script></li><li><h3 id="可分离性"><a href="#可分离性" class="headerlink" title="可分离性"></a>可分离性</h3>根据上文分析，由于高斯函数的可分离性，大高斯滤波器可以高效地实现。二维高斯函数卷积可以分两步来进行，首先将图像与一维高斯核进行卷积，然后将卷积结果与方向垂直且函数形式相同一维高斯核再进行卷积。如此，二维高斯滤波的计算量随滤波模板宽度$N$成线性增长而不是成平方增长。</li></ol><p>值得一提的是，在Young对生理学的研究中发现，哺乳动物的视网膜和视觉皮层的感受区域可以很好地用4阶以内的高斯微分来建模。</p><hr><h1 id="拉普拉斯金字塔"><a href="#拉普拉斯金字塔" class="headerlink" title="拉普拉斯金字塔"></a>拉普拉斯金字塔</h1><p>由于高斯金字塔用于图片下采样（即减小图片的尺寸），是从金字塔的底层到上层自下而上的。而高斯滤波构造的图像金字塔具有局部极值递性，即图像的特征是在减少的。<br>那么我们自然而然会想到，需不需要一个自上而下的金字塔用于上采样，和高斯金字塔配合使用。<br>这里就引入了拉普拉斯金字塔，它可以认为是一个残差金字塔，用来存储下采样后图片与原始图片的差异。其每一层的图像为同一层高斯金字塔的图像减去上一层的图像进行上采样并高斯模糊的结果。<br><img src="/computer-vision20200119231033/高斯金字塔与拉普拉斯金字塔.png" title="高斯金字塔与拉普拉斯金字塔"><br>注意，高斯金字塔的下采样是不可逆的，可以这样理解：下采样过程丢失的信息不能通过上采样来完全恢复，即高斯金字塔中任意一张图$G_{i}$先进行下采样得到图$Down(G_{i})$，再进行上采样得到图$Up(Down(G_{i}))$，此时的$Up(Down(G_{i}))$与原本的$G_{i}$是存在差异的。而拉普拉斯金字塔的作用，就是记录高斯金字塔每一层下采样后再上采样得到的结果与下采样前的原图之间差异，其目的是为了能够完整的恢复出每一层的下采样前图像。可以用下面这个公式来简单表述：</p><script type="math/tex;mode=display">L_{i}=G_{i}-Up\left ( Down\left ( G_{i} \right ) \right )</script><p>若将第$i+1$层的高斯金字塔从顶层开始依次加上第$i$层拉普拉斯金子塔，那么就几乎可以复原原来图像（有点绕，建议结合上图体会）。<br>拉普拉斯的具体构造过程如下：</p><ol><li><h2 id="内插"><a href="#内插" class="headerlink" title="内插"></a>内插</h2>将$G_{l}$进行内插（这里是用与降维时相同的滤波核而不是双线性插值），得到放大的图像$G_{l}^{\ast }$，使$G_{l}^{\ast }$的尺寸与$G_{l-1}$的尺寸相同，表示为：<script type="math/tex;mode=display">G_{l}^{\ast }\left ( i,j \right )=4\sum_{m=-2}^{2}\sum_{n=-2}^{2}\omega \left ( m,n \right )G_{l}\left ( \frac{i+m}{2},\frac{j+n}{2} \right )</script><script type="math/tex;mode=display">\left ( 1\leq l\leq N,0\leq i\leq R_{l},0\leq j\leq C_{l} \right )</script><script type="math/tex;mode=display">G_{l}\left ( \frac{i+m}{2},\frac{j+n}{2} \right )=\left\{\begin{matrix} G_{l}\left ( \frac{i+m}{2},\frac{j+n}{2} \right ),when\,\frac{i+m}{2},\frac{j+n}{2}\,are\,integers\\ 0,others \end{matrix}\right.</script>这边的参数就不说明了，与上文相同。需要注意的是，这里的系数取$4$，是因为每次能参与加权的项的权值之和为4/256，这与$\omega$的选取有关。</li><li><h2 id="相减"><a href="#相减" class="headerlink" title="相减"></a>相减</h2>原理上文已经说了，接下来我们自上而下构造拉普拉斯金字塔。<script type="math/tex;mode=display">\left\{\begin{matrix} L_{l}=G_{l}-G_{l+1}^{\ast },when\,0\leq l< N\\ L_{N}=G_{N},when\,l=N \end{matrix}\right.</script></li></ol><p>如下图所示，此为文章前面小猫的图像金字塔所生成的拉普拉斯金字塔。不要以为这张图搞错了，细看的话就会发现，除了顶层的图片之外，下面的图片（大尺寸的）中仅有一些零散的点（不是屏幕上的灰尘）和淡淡的线，也就是残差。可以通过Gamma校正使这些残差特征更加清晰。<br><img src="/computer-vision20200119231033/拉普拉斯金字塔.png" title="拉普拉斯金字塔"><br>关于Gamma校正，可以看一看上一篇文章<a href="https://gsy00517.github.io/computer-vision20200119195116/" target="_blank">computer-vision笔记：HOG特征</a>，关于图像金字塔和高斯滤波器就说到这里了。</p><script type="text/javascript" src="/js/src/bai.js"></script><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      
      
        &lt;!-- build time:Tue Jan 28 2020 15:40:41 GMT+0800 (GMT+08:00) --&gt;&lt;p&gt;在&lt;a href=&quot;https://gsy00517.github.io/computer-vision20200119195116/&quot; tar
      
    
    </summary>
    
    
      <category term="人工智能" scheme="https://gsy00517.github.io/categories/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/"/>
    
    
      <category term="计算机视觉" scheme="https://gsy00517.github.io/tags/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/"/>
    
  </entry>
  
  <entry>
    <title>computer vision笔记：HOG特征</title>
    <link href="https://gsy00517.github.io/computer-vision20200119195116/"/>
    <id>https://gsy00517.github.io/computer-vision20200119195116/</id>
    <published>2020-01-19T11:51:16.000Z</published>
    <updated>2020-01-28T01:42:56.937Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Tue Jan 28 2020 15:40:41 GMT+0800 (GMT+08:00) --><p>这几天感觉知识的海洋真的是无边无际的，一旦查资料就意味着要查更多的资料，不懂的东西简直一个接着一个。希望某天能对这些知识有个大概的把握吧，废话不多说，开始积累！</p><p><strong>References</strong>：</p><p>电子文献：<br><a href="https://www.cnblogs.com/hrlnw/archive/2013/08/06/2826651.html" target="_blank" rel="noopener">https://www.cnblogs.com/hrlnw/archive/2013/08/06/2826651.html</a><br><a href="https://www.cnblogs.com/zyly/p/9651261.html" target="_blank" rel="noopener">https://www.cnblogs.com/zyly/p/9651261.html</a><br><a href="https://baike.baidu.com/item/HOG/9738560?fr=aladdin" target="_blank" rel="noopener">https://baike.baidu.com/item/HOG/9738560?fr=aladdin</a><br><a href="https://zhuanlan.zhihu.com/p/40960756" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/40960756</a><br><a href="https://www.jianshu.com/p/354acdcbae3f" target="_blank" rel="noopener">https://www.jianshu.com/p/354acdcbae3f</a><br><a href="https://blog.csdn.net/xjp_xujiping/article/details/89430002" target="_blank" rel="noopener">https://blog.csdn.net/xjp_xujiping/article/details/89430002</a></p><p>参考文献：<br>[1]Histograms of Oriented Gradients for Human Detection<br>[2]Understanding and Diagnosing Visual Tracking Systems</p><hr><h1 id="HOG特征"><a href="#HOG特征" class="headerlink" title="HOG特征"></a>HOG特征</h1><p>HOG（Histogram of Oriented Gradient），中文译为方向梯度直方图。它是一种在计算机视觉和图像处理中用来进行物体检测的特征描述子，通过计算像局部区域中不同方向上梯度的值，然后进行累积，得到代表这块区域特征的直方图，可将其输入到分类器里面进行目标检测。<br>下面是论文中所呈现的进行人物检测时所采用的特征提取与目标检测的流程，本文将主要分析特征提取的部分（论文中对运用SVM分类器做检测的部分分析甚少，因此不做重点）。<br><img src="/computer-vision20200119195116/流程图.png" title="流程图"></p><hr><h1 id="思想"><a href="#思想" class="headerlink" title="思想"></a>思想</h1><p>HOG特征的主要思想是：在图像中，局部目标的表象和形状能够被梯度或边缘的方向密度分布很好地描述。由于，梯度主要存在于边缘的地方，因此其密度分布能一定程度上描述目标物体的形状等特征。</p><hr><h1 id="思路"><a href="#思路" class="headerlink" title="思路"></a>思路</h1><p>在介绍HOG特征提取的基本思路之前，我先放一张图，我认为先了解block和cell有助于理解思路，否则有点抽象，可能会（像我当初一样）看得很迷。下图的分割尺度与后文的分析一致，可以先记下来。<br><img src="/computer-vision20200119195116/看清楚了.png" title="看清楚了"><br>HOG特征提取的基本思路是：</p><ol><li>为了减少光照因素的影响，首先需要将整个图像进行归一化，这种压缩处理能够有效地降低图像局部的阴影和光照变化。</li><li>将图像分成很多小的cell，采集cell中各像素点梯度的幅值和方向，然后在每个cell中统计出一个一维的梯度方向直方图。</li><li>为了对光照和阴影有更好的不变性，我们可以在更大的范围内，对block进行对比度归一化。</li><li>在归一化后，最终获得的即为HOG描述子。</li></ol><p>这里共做了两次normalization，不要搞混了。</p><hr><h1 id="流程"><a href="#流程" class="headerlink" title="流程"></a>流程</h1><ol><li><h2 id="Normalize-gamma-＆-colour"><a href="#Normalize-gamma-＆-colour" class="headerlink" title="Normalize gamma ＆ colour"></a>Normalize gamma ＆ colour</h2><p>由于在图像的纹理强度中，局部的表层曝光贡献的比重比较大，因此为了减少光照因素的影响，我们首先采用Gamma校正法对输入图像的颜色空间进行归一化。<br>Gamma校正可以提高图像中偏暗或者偏亮部分的对比效果，能有效降低图像局部的阴影和光照变化。其校正公式为：</p><script type="math/tex;mode=display">f\left ( I \right )=I^{\gamma }</script><p>其中$I$为图像像素值，$\gamma$为校正系数。<br>由幂函数的性质容易推得Gamma校正的作用，这里以灰度图像（即仅有黑白，单通道）为例做一个简单解释。</p><img src="/computer-vision20200119195116/Gamma校正.png" title="Gamma校正"><p>当$\gamma$小于1时，在低灰度值区域内，动态范围变大，图像对比度增强；在高灰度值区域，动态范围变小，图像对比度降低。图像的整体灰度值变大，如下面中间的图片（左边是原图）。<br>当$\gamma$大于1时，在低灰度值区域内，动态范围变小，图像对比度降低；在高灰度值区域，动态范围变大，图像对比度提高。图像的整体灰度值变小，如下面右边的图片。</p><img src="/computer-vision20200119195116/校正效果.png" title="校正效果"><blockquote><p>注：灰度值也称灰度等级，范围一般从0到255，白色为255，黑色为0。</p></blockquote><p>然而，是否使用Gamma校正还要视具体情况而定，当涉及大量的类内颜色变化时，比如斑马等自身就颜色变化丰富的物体，不校正效果会更好。<br>上面的例子使用的是灰度图像，事实上，RGB彩色图（三通道）的performance会更好一些。可以看一下下面的对比图。</p><img src="/computer-vision20200119195116/灰度图和彩色图的表现.png" title="灰度图和彩色图的表现"></li><li><h2 id="Image-segmentation"><a href="#Image-segmentation" class="headerlink" title="Image segmentation"></a>Image segmentation</h2>分割图像这一步在论文的流程图中没有，我觉得有必要说一下，因此自行添加了一步。<br>这里先说一下为什么要进行图像分割。如果对一大幅图片直接提取特征，往往得不到好的效果。因为如果提取区域比较大，那么两个完全不同的图像，也可能提取出相似的HOG特征。但这种可能性在较小的区域就很小。此外，当我们把图像分割成很多区块（patch）然后对每个区块提取特征时，这其中也包含了几何上的位置特性。例如，正面的人脸，左上部分的图像区块提取的HOG特征一般是和眼睛的HOG特征相符合的。<br>HOG的图像分割策略一般有overlap和non-overlap两种，如下图所示。第一张图是overlap的情况，指的是分割出的区块会有互相重合的区域。而non-overlap指的是区块不交叠，没有重合区域。 <img src="/computer-vision20200119195116/overlap.jpg" title="overlap"> <img src="/computer-vision20200119195116/non-overlap.jpg" title="non-overlap"> overlap的优点在于这种分割方式可以防止对一些物体的切割，例如，如果分割的时候正好把一个眼睛从中间切割分到了两个patch中，那么提取完HOG特征之后，可能会影响接下来的分类效果。但是如果两个patch之间有一个overlap，那么这里的三个patch至少有一个内会保留完整的眼睛。overlap的缺点在于计算量大，因为重叠区域的像素需要重复计算。<br>non-overlap恰恰相反，其缺点如上所述，就是有时会将一个连续的物体切割开，得到不太好的HOG特征。但它的优点是计算量小，尤其是与图像金字塔（Image Pyramid，详见<a href="https://gsy00517.github.io/computer-vision20200119231033/" target="_blank">computer-vision笔记：图像金字塔与高斯滤波器</a>）相结合时，这个优点更为明显。<br>在这里，我们用overlap的策略将图像分割成一个个互相重叠的block。也就是说，每个cell的直方图都会被多次用于最终的特征描述子的计算。虽然这看起来有冗余，但可以显著地提升性能。 <img src="/computer-vision20200119195116/block＆cell.jpg" title="block＆cell"></li><li><h2 id="Compute-gradients"><a href="#Compute-gradients" class="headerlink" title="Compute gradients"></a>Compute gradients</h2>接下来就是计算图像每个像素点梯度的幅值和方向。<script type="math/tex;mode=display">G_{x}\left ( x,y \right )=I\left ( x+1,y \right )-I\left ( x-1,y \right )</script>$G_{y}\left ( x,y \right )$算法同理。<script type="math/tex;mode=display">G\left ( x,y \right )=\sqrt{G_{x}\left ( x,y \right )^{2}+G_{y}\left ( x,y \right )^{2}}</script><script type="math/tex;mode=display">\alpha =\arctan \frac{G_{y}\left ( x,y \right )}{G_{x}\left ( x,y \right )}</script></li><li><h2 id="Weighted-vote-into-spatial-＆-orientation-cells"><a href="#Weighted-vote-into-spatial-＆-orientation-cells" class="headerlink" title="Weighted vote into spatial ＆ orientation cells"></a>Weighted vote into spatial ＆ orientation cells</h2>将图像划分成小的cell（矩形或者环形），然后统计每一个cell的梯度直方图，即可以得到一个cell的描述符。<br>注意，这里我们在一个block内划分4个cell，即2×2个cell组成一个block，将一个block内每个cell的描述符串联起来即可得到一个block的HOG描述符。<br>统计梯度直方图的方式是利用刚才计算得出的cell中每一个像素点的梯度进行加权投票。我们一般考虑采用9个bin的直方图来统计一个cell中像素的梯度信息，即将cell的梯度方向0~180°（无向）或0~360°（考虑正负）分成9个bin，如下图所示： <img src="/computer-vision20200119195116/直方图.png" title="直方图"> 如果cell中某一像素的梯度方向是20~40°，那么上面这个直方图第2个bin的计数就要加1，这样对cell中的每一个像素用梯度方向在直方图中进行加权投影（权值大小等于梯度幅值），将其映射到对应角度范围的bin内，就可以得到这个cell的梯度方向直方图了，即该cell对应的一个9维特征向量。若梯度方向位于相邻bin的交界处（如20°、40°等），需要对其进行方向和位置上的双线性插值。 <img src="/computer-vision20200119195116/加权投票.jpg" title="加权投票"> 上图是一个比较直观的加权投票的演示，这里将投影在交界处的梯度的幅值对半分至两侧的bin。<br>事实上，我们可以采用幅值本身或者它的函数（幅值的平方根、幅值的平方、幅值的截断形式）来表示权值，但经实际测试表明：使用幅值来表示权值能获得最佳的效果。采用梯度幅值作为权重，可以使那些比较明显的边缘的方向信息对特征表达影响增大，这样比较合理，因为HOG特征主要就是依靠这些边缘纹理。<br>经实验还发现，采用无向的梯度（即0~180°）和9个bin的直方图，能在行人检测试验中取得最佳的效果。</li><li><h2 id="Contrast-normalize-over-overlapping-spatial-blocks"><a href="#Contrast-normalize-over-overlapping-spatial-blocks" class="headerlink" title="Contrast normalize over overlapping spatial blocks"></a>Contrast normalize over overlapping spatial blocks</h2>由于局部光照的变化，以及前景背景对比度的变化，使得梯度强度的变化范围非常大，这就需要对梯度做局部对比度归一化。归一化能够进一步对光照、阴影、边缘进行压缩，使得特征向量对光照、阴影和边缘变化具有鲁棒性。<br>我们之前已将2x2的cell组成了更大的block，现在要做的就是针对每个block进行对比度归一化。<br>通常使用的HOG结构大致有三种：矩形HOG（R-HOG），圆形HOG（C-HOG）和中心环绕HOG。它们的单位都是block。实验证明，矩形HOG和圆形HOG的检测效果基本一致，而环绕形HOG效果相对差一些。 <img src="/computer-vision20200119195116/区间形状.png" title="区间形状"> 我们一般根据如下公式对block进行对比度归一化：<script type="math/tex;mode=display">v=\frac{v}{\sqrt{\left \| v \right \|_{2}^{2}+\xi ^{2}}}</script>这里$v$是该block未经归一化的特征向量，比如这里一个block含2x2个cell，每个cell对应一个9维的特征向量，那么这个block的特征向量的长度就是2x2x9。这里$\xi$是一个很小的数，主要是为了防止分母等于零而引入的。</li><li><h2 id="Collect-HOG’s-over-detection-window"><a href="#Collect-HOG’s-over-detection-window" class="headerlink" title="Collect HOG’s over detection window"></a>Collect HOG’s over detection window</h2>我们先review一下，上面我们首先把样本图片分割为若干个像素的cell，然后把梯度方向划分为9个区间，在每个cell里面对所有像素的梯度方向在各个区间进行直方图统计，得到一个9维的特征向量；然后我们使每相邻4个cell构成一个block，把一个block内的特征向量串联起来得到一个36（即2x2x9）维的特征向量。<br>接下来，我们用block对样本图像进行扫描，stride为一个cell的大小，扫描完成后，我们将扫描过程中每个block的特征向量（即上述36维的）串联起来，得到样本的特征向量，也就是可以输入到后面分类器的HOG特征描述子。<br>如此，就完成了HOG特征的提取。</li></ol><hr><h1 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h1><h2 id="优点"><a href="#优点" class="headerlink" title="优点"></a>优点</h2><ol><li>HOG特性能较好地捕捉局部形状信息，对几何和光学变化都有很好的不变性。</li><li>HOG特征是在密集采样的图像块中求取的，在计算得到的HOG特征向量中隐含了该块与检测窗口之间的几何空间位置关系。</li></ol><h2 id="不足"><a href="#不足" class="headerlink" title="不足"></a>不足</h2><ol><li>很难处理遮挡问题，人体姿势动作幅度过大或物体方向改变时不易检测（这个问题后来在DPM中采用可变形部件模型的方法得到了改善）。</li><li>没有选取主方向，也没有旋转梯度方向直方图，因而本身不具有旋转不变性（较大的方向变化），其旋转不变性是通过采用不同旋转方向的训练样本来实现的。</li><li>本身不具有尺度不变性，其尺度不变性是通过缩放检测窗口图像的大小（如Image Pyramid）来实现的。</li><li>由于梯度的性质，HOG对噪点相当敏感，在实际应用中，在block和cell划分之后，对于得到各个区域，有时候还会做一次高斯平滑去除噪点。但又由于HOG特征是基于边缘的，平滑操作会降低边缘信息的对比度，从而减少图像中的有用信息，因此还需好好做个balance。</li></ol><hr><h1 id="计算机视觉女神"><a href="#计算机视觉女神" class="headerlink" title="计算机视觉女神"></a>计算机视觉女神</h1><p>或许会发现本文中好多的图例都用了同一位女士的脸，这里就扯点题外话。<br>照片中的女子名为Lena Soderberg，对计算机视觉领域有一定的接触的朋友应该对这张照片不会陌生。这张照片是标准的数字图像处理用例，各种算法研究经常会使用这张图作为模板。那为什么要用这幅图呢？<br>David C. Munson在“A Note on Lena”中给出了两条理由：</p><ol><li>首先，该图像中各个频段的能量都很丰富，既有低频（光滑的皮肤），也有高频（帽子上的羽毛），适度地混合了细节、平滑区域、阴影和纹理，很适合来测试各种图像处理算法。</li><li>其次，Lena是位迷人的女子，能有效吸引研究者（大部分为男性）做研究。</li></ol><p>该照片其实是一张于1972年11月出版的Playboy的中间插页。1973年，南加州大学信号图像处理研究所的副教授Alexander和学生一起，为了一个同事的学会论文正忙于寻找一幅好的图片。他们想要一幅具有良好动态范围的人的面部图片用于扫描。这时，不知是谁拿着一本Playboy走进研究室。由于当时实验室里使用的扫描仪（Muirhead wirephoto scanner）分辨率是100行每英寸，试验也仅仅需要一副512X512的图片，所以他们只将图片顶端开始的5.12英寸扫描下来，并切掉肩膀以下的部分。多年以来，由于图像Lena源于Playboy，将其引用于科技文章中饱受争议。Playboy杂志也将未授权的引用告上法庭。但随着时间的流逝，人们渐渐淡忘Lena的来源，Playboy也放松了对此的关注。值得一提的是，Lena也是Playboy发行的最畅销的海报，已经出售7,161,561份。其真正刊登的原图如下。<br><img src="/computer-vision20200119195116/原版照片.jpg" title="原版照片"><br>1997年，在图像科学和技术协会的第50届会议上，Lenna被邀为贵宾出席。在会议上，她忙于签名、拍照以及介绍自我。<br><img src="/computer-vision20200119195116/出席顶会.jpg" title="出席顶会"><br>说实话每次看到这张照片，想起学术界那些大牛们居然能弄出这样一个挺有意思的故事，不免觉得也挺有趣的。在我最喜欢的美剧之一《Silicon Valley》中，也有计算机视觉女神的身影。<br><img src="/computer-vision20200119195116/硅谷S01E02截图.png" title="硅谷S01E02截图"><br>大家对计算机视觉有兴趣的话，其实自己也打印一张摆墙上，这样就可以和懂的小伙伴们一起讨论问题了。<br><img src="/computer-vision20200119195116/没错就是她.png" title="没错就是她"></p><script type="text/javascript" src="/js/src/bai.js"></script><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      
      
        &lt;!-- build time:Tue Jan 28 2020 15:40:41 GMT+0800 (GMT+08:00) --&gt;&lt;p&gt;这几天感觉知识的海洋真的是无边无际的，一旦查资料就意味着要查更多的资料，不懂的东西简直一个接着一个。希望某天能对这些知识有个大概的把握吧，废话不
      
    
    </summary>
    
    
      <category term="人工智能" scheme="https://gsy00517.github.io/categories/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/"/>
    
    
      <category term="计算机视觉" scheme="https://gsy00517.github.io/tags/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/"/>
    
  </entry>
  
  <entry>
    <title>machine learning笔记：闭式解</title>
    <link href="https://gsy00517.github.io/machine-learning20200119145637/"/>
    <id>https://gsy00517.github.io/machine-learning20200119145637/</id>
    <published>2020-01-19T06:56:37.000Z</published>
    <updated>2020-01-19T07:10:07.808Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Tue Jan 28 2020 15:40:41 GMT+0800 (GMT+08:00) --><p>看论文总是会看到许多新奇的名词，比如“one-shot learning”（单样本学习）即仅从一个或者很少的样本中学习训练。还有诸如“closed-form”、“closed-form solution”，看的我真是很纳闷，这里就结合资料好好理理清楚。</p><p><strong>References</strong>：</p><p>电子文献：<br><a href="https://blog.csdn.net/yy16808/article/details/76493384" target="_blank" rel="noopener">https://blog.csdn.net/yy16808/article/details/76493384</a><br><a href="https://blog.csdn.net/langjueyun2010/article/details/80348449" target="_blank" rel="noopener">https://blog.csdn.net/langjueyun2010/article/details/80348449</a></p><hr><h1 id="解析解与数值解"><a href="#解析解与数值解" class="headerlink" title="解析解与数值解"></a>解析解与数值解</h1><p>这里先介绍两个相对应的数学概念：解析解与数值解。<br>直接上例子：解$x^{2}=3$。<br>那么这题的解析解是：$x=\sqrt{3}$。<br>数值解为：$x=1.732$。<br>简而言之，解析解就是给出解的具体函数形式，从解的表达式中就可以算出任何对应值，好像就是小学所谓的公式解；而数值解就是直接用数值方法求出具体的解。</p><hr><h1 id="闭式解"><a href="#闭式解" class="headerlink" title="闭式解"></a>闭式解</h1><p>实际上，闭式解也被称为解析解。由于解析解为一封闭形式（closed-form）的函数，因此对任一独立变量，我们皆可将其带入解析函数求得正确的相依变量。即解可以表达为一个函数形式，带入变量即可得到解。<br>这就写完了，似乎很简单。</p><script type="text/javascript" src="/js/src/bai.js"></script><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      
      
        &lt;!-- build time:Tue Jan 28 2020 15:40:41 GMT+0800 (GMT+08:00) --&gt;&lt;p&gt;看论文总是会看到许多新奇的名词，比如“one-shot learning”（单样本学习）即仅从一个或者很少的样本中学习训练。还有诸如“close
      
    
    </summary>
    
    
      <category term="知识点与小技巧" scheme="https://gsy00517.github.io/categories/%E7%9F%A5%E8%AF%86%E7%82%B9%E4%B8%8E%E5%B0%8F%E6%8A%80%E5%B7%A7/"/>
    
    
      <category term="机器学习" scheme="https://gsy00517.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>computer vision笔记：Peak-to-Sidelobe Ratio应用于目标跟踪</title>
    <link href="https://gsy00517.github.io/computer-vision20200118213942/"/>
    <id>https://gsy00517.github.io/computer-vision20200118213942/</id>
    <published>2020-01-18T13:39:42.000Z</published>
    <updated>2020-01-19T11:52:40.496Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Tue Jan 28 2020 15:40:41 GMT+0800 (GMT+08:00) --><p>在Visual Object Tracking using Adaptive Correlation Filters一文中，我看到这样一句话：“The Peak-to-Sidelobe Ratio(PSR), which measures the strength of a correlation peak, can be used to detect occlusions or tracking failure, to stop the online update, and to reacquire the track if the object reappears with a similar appearance.”其大意为：用来衡量相关峰值强度的Peak-to-Sidelobe Ratio可以用于检测遮挡或者跟踪失误、停止实时更新和在相似外观再次出现时重新获取跟踪路径。我读完在想：这个PSR是什么？这么有用。结果百度了半天翻了几页没发现有任何有关它介绍或者解释的文章。于是看了一些英文文献，将自己的一些浅见写下来。</p><p><strong>References</strong>：</p><p>电子文献：<br><a href="https://baike.baidu.com/item/pslr/19735601" target="_blank" rel="noopener">https://baike.baidu.com/item/pslr/19735601</a></p><p>参考文献：<br>[1]Understanding and Diagnosing Visual Tracking Systems<br>[2]Visual Object Tracking using Adaptive Correlation Filters<br>[3]Adaptive Model Update via Fusing Peak-to-sidelobe Ratio and Mean Frame Difference for Visual Tracking</p><hr><h1 id="What"><a href="#What" class="headerlink" title="What"></a>What</h1><p>由于没有找到任何中文翻译，我只能取一个相近的（感觉指的差不多）。在百度百科中，我找到了如下解释：PSLR，峰值旁瓣比，peak side lobe ratio，定义为主瓣峰值强度对于最强旁瓣的峰值强度之比，多用于雷达信号脉冲压缩后对信号的评估。</p><blockquote><p>注意：百度百科内的名词与我遇到的相差了一个“to”，且缩写也不同。</p></blockquote><img src="/computer-vision20200118213942/百科配图.png" title="百科配图"><p>上面是百度百科内的一个配图，根据百科解释，最高的主瓣与第二高的主瓣之差，即是PLSR（峰值旁瓣比），在这里大小为-13.4dB。<br>此外，我还找到了一个称为峰均比（PAPR，peak-to-average power ratio）的概念，它等于波形的振幅和其有效值（RMS）之比，主要是针对功率的，这里就不细说了。<br>后来，我在一篇光学期刊的文章上找到了一个比较可靠的翻译，该文献中有一个名为“峰旁比”的名词，且文献内容与目标追踪相关。因此我暂且称其为峰旁比吧。个人觉得比峰值旁瓣比简洁且好听。</p><hr><h1 id="Why"><a href="#Why" class="headerlink" title="Why"></a>Why</h1><p>其实中文翻译并不重要，重要的是它的作用。<br>在查阅文献的过程中，我看到了这样一个公式：</p><script type="math/tex;mode=display">P_{sr}^{t}=\frac{max(f_{t})-\mu _{t}}{\sigma _{t}}</script><p>其中$P_{sr}^{t}$是此时第t帧的峰旁比，$f_{t}$是对于第t帧分类器预测的响应，$\mu _{t}$和$\sigma _{t}$分别是响应图$f$的均值和方差。<br>根据这种计算方法，我们可以大概分析一下峰旁比的作用。当初看到时，我觉得它与我在物理实验中做过的音叉共振实验中的品质因数有相似之处（品质因数$Q=\frac{f_{0}}{f_{2}-f_{1}}$）。<br>由公式可知，当响应中的峰值较高，且响应分布相对而言集中在峰值及周围时，峰旁比就会较高；反之，峰旁比就会较低。<br>那么什么时候会造成峰旁比较低呢？根据论文描述可以获得提示，当遇到遮挡，或者目标跟丢即响应区内不含目标主体时，就不会出现一个那么明显的峰值响应，同时响应也会较为分散了，此时分母较大、分子较小，峰旁比就会变低。<br>下面也是Visual Object Tracking using Adaptive Correlation Filters中的一张figure（这篇文章提出了MOSSE），我们需要的是a much stronger peak which translates into less drift and fewer dropped tracks。看了这个想必应该知道峰旁比发挥的作用和大致原因了。<br><img src="/computer-vision20200118213942/响应图.png" title="响应图"><br>和上面的峰值旁瓣比、峰均比相比较，显然峰旁比的定义能更好地表征响应的集中程度。</p><hr><h1 id="How"><a href="#How" class="headerlink" title="How"></a>How</h1><p>由峰旁比定义所得出的性质可知，峰旁比可以作为模型预测定位准确性和可信度的判据。我们可以利用峰旁比来调整Model Updater（The model updater controls the strategy and frequency of updating the observation model. It has to strike a balance between model adaptation and drift.）<br>我的想法是，我们可以设置一个threshold，当峰旁比小于这个threshold时，表示模型未能准确定位到目标（可信度较低），这时我们可以停止模型的更新，或者通过减小学习率等方法减慢模型的更新速度以防止模型受背景或者遮挡物较大影响，而当目标再次出现时难以复原导致最终完全跟丢的问题；而当峰旁比大于这个threshold时，我们可以实时更新模型，或者运用较大的学习率（也可以根据峰旁比将学习率划分成几个等级）。</p><script type="text/javascript" src="/js/src/bai.js"></script><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      
      
        &lt;!-- build time:Tue Jan 28 2020 15:40:41 GMT+0800 (GMT+08:00) --&gt;&lt;p&gt;在Visual Object Tracking using Adaptive Correlation Filters一文中，我看到这样一句话：“
      
    
    </summary>
    
    
      <category term="人工智能" scheme="https://gsy00517.github.io/categories/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/"/>
    
    
      <category term="计算机视觉" scheme="https://gsy00517.github.io/tags/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/"/>
    
  </entry>
  
  <entry>
    <title>machine learning笔记：数据采样之正样本和负样本</title>
    <link href="https://gsy00517.github.io/machine-learning20200118112156/"/>
    <id>https://gsy00517.github.io/machine-learning20200118112156/</id>
    <published>2020-01-18T03:21:56.000Z</published>
    <updated>2020-01-20T05:28:14.019Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Tue Jan 28 2020 15:40:41 GMT+0800 (GMT+08:00) --><p>今天在看最小二乘回归（least squares regression）时看到作者把positive examples设成1，把negative examples设成0。感觉对这个概念既熟悉又陌生，查了一下之后一下子想起来了。在机器学习中，数据预处理一般包括数据清洗、数据集成、数据采样。而正负样本涉及到了数据采样的问题，因此后面也提一下。</p><hr><h1 id="正样本和负样本"><a href="#正样本和负样本" class="headerlink" title="正样本和负样本"></a>正样本和负样本</h1><p>简单来说，和概率论中类似，一般我们看一个问题时，只关注一个事件（希望它发生或者成功，并对其进行分析计算），而正样本就是属于我们关注的这一类别的样本，负样本就是指不属于该类别的样本。</p><hr><h1 id="数据采样平衡"><a href="#数据采样平衡" class="headerlink" title="数据采样平衡"></a>数据采样平衡</h1><p>一般来说，比如我们训练分类器时，希望样本中正负样本的比例是接近于1:1的。因为如果正样本占比很大（比如90%）或者负样本占比远超正样本，那么训练结果可想而知，获得的分类器在测试中的效果会很差。<br>针对这种数据不平衡的问题，有以下三种solution：</p><ol><li><h2 id="过采样（over-sampling）"><a href="#过采样（over-sampling）" class="headerlink" title="过采样（over-sampling）"></a>过采样（over-sampling）</h2>这是一种较为直接的办法，即通过随机复制少数类来增加其中的实例数量，从而可增加样本中少数类的代表性。</li><li><h2 id="欠采样（under-sampling）"><a href="#欠采样（under-sampling）" class="headerlink" title="欠采样（under-sampling）"></a>欠采样（under-sampling）</h2>这种方法也比较直接，即通过随机消除占多数类的样本来平衡类分布，直到多数类和少数类实现平衡。</li><li><h2 id="获取更多样本"><a href="#获取更多样本" class="headerlink" title="获取更多样本"></a>获取更多样本</h2>上面的两种方法比较直接方便，但也存在弊端，比如过采样可能会导致过拟合，欠采样可能无法很好地利用有限的数据（这也可能会造成过拟合）。因此最好还是获取更多的样本来补充，我认为主要有下面两种方法：<ol><li><h3 id="采集"><a href="#采集" class="headerlink" title="采集"></a>采集</h3>例如在海贼王漫画的样本中，我们要进行20x20大小的海贼检测，那么为了获取尽可能多的负样本，我们可以截取一张1000x1000大小的海王类图像，将其拆分为20x20大小的片段加入到负样本中（即50x50地进行分割）。</li><li><h3 id="生成"><a href="#生成" class="headerlink" title="生成"></a>生成</h3>为了获得更多负样本，我们也可将前面1000x1000的海王类图像先拆分为10x10大小，这就比之前多出了4倍的负样本图像。不过要注意的是，为了保持大小的一致，还需进一步将其拉伸至20x20的大小。<img src="/machine-learning20200118112156/这么大够分了.png" title="这么大够分了"> 当然，其实不需要从体积上达到这么大的比例，关键是像素尺寸的匹配。</li></ol></li></ol><script type="text/javascript" src="/js/src/bai.js"></script><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      
      
        &lt;!-- build time:Tue Jan 28 2020 15:40:41 GMT+0800 (GMT+08:00) --&gt;&lt;p&gt;今天在看最小二乘回归（least squares regression）时看到作者把positive examples设成1，把negative
      
    
    </summary>
    
    
      <category term="人工智能" scheme="https://gsy00517.github.io/categories/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/"/>
    
    
      <category term="机器学习" scheme="https://gsy00517.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="海贼王" scheme="https://gsy00517.github.io/tags/%E6%B5%B7%E8%B4%BC%E7%8E%8B/"/>
    
  </entry>
  
  <entry>
    <title>ubuntu笔记：删除文件中的问题</title>
    <link href="https://gsy00517.github.io/ubuntu20200117213946/"/>
    <id>https://gsy00517.github.io/ubuntu20200117213946/</id>
    <published>2020-01-17T13:39:46.000Z</published>
    <updated>2020-01-24T10:32:53.492Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Tue Jan 28 2020 15:40:41 GMT+0800 (GMT+08:00) --><p>大家可以看看我删个文件多么曲折：<br><img src="/ubuntu20200117213946/递归删除文件.png" title="递归删除文件"><br>献丑了哈哈哈，这里就对这个过程中涉及到的一些问题做一个总结吧。</p><hr><h1 id="目录"><a href="#目录" class="headerlink" title="目录"></a>目录</h1><p>首先我<code>cd /</code>进入到根目录，然后我每一步<code>ls</code>列出目录中的文件及子目录，一步一个脚印找到了我要删的文件——MATLAB，emmm我不想解释为什么是它。<br>然后我想当然的想remove掉这个文件，结果发现权限不够。这里其实可以<code>ls -l</code>以列表的形式查看目录中的文件及子目录并且列出每个文件拥有者、所属组、其他用户各自的权限的。<br>后面我又使用<code>cd ../</code>来回到上一级目录，这是为了怕自己搞错目录，怕删高了一级酿成惨剧。</p><hr><h1 id="权限"><a href="#权限" class="headerlink" title="权限"></a>权限</h1><p>它说我没权限，于是就<code>sudo</code>临时给个5分钟的root权限呗。本来还想<code>sudo su</code>进入root的（可以用ctrl+D退出），那简直杀鸡用牛刀了。</p><hr><h1 id="删除文件-目录"><a href="#删除文件-目录" class="headerlink" title="删除文件/目录"></a>删除文件/目录</h1><p>一开始用<code>rm</code>，它提示我是一个目录，于是我使用了<code>rmdir</code>，但它的作用是删除一个空目录，而我的目录内还有文件。<br>于是我使用<code>sudo rm folder_name -R</code>即递归删除文件的方法来从里到外把这个目录中的文件都删了。<br>其实好像也可以<code>sudo rm -rf folder_name</code>强制删除，这里<code>-r</code>和<code>-R</code>一样，都是递归的意思，<code>-f</code>就是强制执行无需确认。但是由于牢记linux最大禁忌<code>rm -rf /*</code>（真正的从删库到跑路），对这个命令还是比较怕的，于是就采取了前者。执行完之后再<code>ls</code>看了一下，发现已成功删除，<code>df</code>查看空间分配，内存使用也回来了不少。</p><blockquote><p>补充：<code>-R</code>递归也有许多别的妙用，比如可以通过<code>sudo chmod a+rw file_name -R</code>来一次性修改一个文件夹内所有文件的权限。</p></blockquote><script type="text/javascript" src="/js/src/bai.js"></script><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      
      
        &lt;!-- build time:Tue Jan 28 2020 15:40:41 GMT+0800 (GMT+08:00) --&gt;&lt;p&gt;大家可以看看我删个文件多么曲折：&lt;br&gt;&lt;img src=&quot;/ubuntu20200117213946/递归删除文件.png&quot; title=&quot;递
      
    
    </summary>
    
    
      <category term="操作和使用" scheme="https://gsy00517.github.io/categories/%E6%93%8D%E4%BD%9C%E5%92%8C%E4%BD%BF%E7%94%A8/"/>
    
    
      <category term="命令操作" scheme="https://gsy00517.github.io/tags/%E5%91%BD%E4%BB%A4%E6%93%8D%E4%BD%9C/"/>
    
      <category term="ubuntu" scheme="https://gsy00517.github.io/tags/ubuntu/"/>
    
  </entry>
  
  <entry>
    <title>ubuntu笔记：apt包管理以及如何更新软件列表</title>
    <link href="https://gsy00517.github.io/ubuntu20200117094401/"/>
    <id>https://gsy00517.github.io/ubuntu20200117094401/</id>
    <published>2020-01-17T01:44:01.000Z</published>
    <updated>2020-01-27T08:24:08.091Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Tue Jan 28 2020 15:40:41 GMT+0800 (GMT+08:00) --><p>ubuntu有时在用户登录后会提示有软件包更新，每次更新之后按提示重启，你就会看到一个类似于安全模式下大写的GNU GRUB（一个多操作系统启动程序），虽然这没什么问题，但是我在想能不能自主地去更新呢？</p><p><strong>References</strong>：</p><p>电子文献：<br><a href="https://birdteam.net/122231" target="_blank" rel="noopener">https://birdteam.net/122231</a><br><a href="https://blog.csdn.net/a3192048/article/details/86618314" target="_blank" rel="noopener">https://blog.csdn.net/a3192048/article/details/86618314</a></p><hr><h1 id="apt-get"><a href="#apt-get" class="headerlink" title="apt-get"></a>apt-get</h1><p>这个有点类似于windows中的<code>dism</code>命令，可以用于安装、更新、卸载软件，大部分操作需要root权限，因此使用命令时别忘了授权。<br>首先介绍一下它的常见用法：</p><ol><li><h2 id="安装"><a href="#安装" class="headerlink" title="安装"></a>安装</h2><p>使用如下命令安装名为xxx的软件：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo apt-get install xxx</span><br></pre></td></tr></table></figure></li><li><h2 id="卸载"><a href="#卸载" class="headerlink" title="卸载"></a>卸载</h2><p>使用如下命令卸载名为xxx的软件：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo apt-get remove xxx</span><br></pre></td></tr></table></figure><blockquote><p>注意：切忌卸载关键的软件包，比如coreutils。</p></blockquote></li><li><h2 id="更新"><a href="#更新" class="headerlink" title="更新"></a>更新</h2>本文重点来了，<code>apt-get</code>相关升级更新命令有下面这四个：<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">sudo apt-get update</span><br><span class="line">#更新软件源缓存，从服务器更新可用的软件列表，一般在安装软件时引入新的软件仓库之后使用</span><br><span class="line"></span><br><span class="line">sudo apt-get upgrade</span><br><span class="line">#更新系统，即根据列表更新已安装的软件包，既不会删除在列表中已经不存在了的软件，也不会安装有依赖需求但尚未安装的软件</span><br><span class="line"></span><br><span class="line">sudo apt-get full-upgrade</span><br><span class="line">#根据列表更新已安装的软件包，可能会为了解决软件包冲突而删除一些已安装的软件</span><br><span class="line"></span><br><span class="line">sudo apt-get dist-upgrade</span><br><span class="line">#更新系统版本，也是根据列表更新已安装的软件包，可能会为了解决软件包冲突而删除一些已安装的软件，不同于full-upgrade的dist-upgrade也可能会为了解决软件包依赖问题安装新的软件包</span><br></pre></td></tr></table></figure></li></ol><hr><h1 id="更新软件列表"><a href="#更新软件列表" class="headerlink" title="更新软件列表"></a>更新软件列表</h1><p>当我们想自主更新软件包时，可以依次执行下面两条命令：<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">sudo apt-get upgrade</span><br><span class="line">sudo apt-get dist-upgrade //谨慎执行</span><br></pre></td></tr></table></figure><p></p><p>这两条命令其实比较类似，不同的是当相依性问题时，<code>upgrade</code>时此package就不会被升级而保留下来；而<code>dist-upgrade</code>相对“智能”，若遇到相依性问题，需要安装或者移除新的package时，<code>dist-upgrade</code>命令就会试着去安装或者移除它，这就可能以牺牲某些非重要软件包为代价来升级某些非常重要的软件包，个人认为存在一定风险。</p><hr><h1 id="apt"><a href="#apt" class="headerlink" title="apt"></a>apt</h1><p>在根据各类教程安装各个软件时，我开始注意到有时候<code>apt-get</code>的位置被<code>apt</code>代替了。随着使用量的增加，这个疑惑越来越大，因此我决定搞搞清楚。<br>其实，<code>apt</code>命令是在ubuntu16.04发布时引入的。它具有更精减但足够的命令选项，而且具有更为有效的参数选项的组织方式。实际上，虽然不是一个东西，但完完全全可以认为<code>apt</code>和<code>apt-get</code>是等价的，其格式语法几乎完全统一，在使用时不会出现不同。目前<code>apt</code>命令还在不断地发展，而<code>apt-get</code>比<code>apt</code>有更多、更细化的操作功能，有时对于一些低级操作，仍需使用<code>apt-get</code>。<br>下表是<code>apt</code>命令与<code>apt-get</code>等命令的对比，可以看到在普通使用时是完全一样的。</p><table border="1"><tr><td>apt命令</td><td>等效命令</td><td>功能</td></tr><tr><td>apt install</td><td>apt-get install</td><td>安装软件包</td></tr><tr><td>apt remove</td><td>apt-get remove</td><td>移除软件包</td></tr><tr><td>apt purge</td><td>apt-get purge</td><td>移除软件包及配置文件</td></tr><tr><td>apt update</td><td>apt-get update</td><td>更新软件列表</td></tr><tr><td>apt upgrade</td><td>apt-get upgrade</td><td>升级所有可升级的软件包</td></tr><tr><td>apt autoremove</td><td>apt-get autoremove</td><td>自动删除不需要的包</td></tr><tr><td>apt full-upgrade</td><td>apt-get full-upgrade</td><td>在升级软件包时自动处理依赖关系</td></tr><tr><td>apt search</td><td>apt-get search</td><td>搜索应用程序</td></tr><tr><td>apt show</td><td>apt-get show</td><td>显示软件包信息</td></tr></table><p>此外，<code>apt</code>还有一些自己的命令，比如<code>apt list</code>列出包含条件的包（已安装，可升级等）；<code>apt edit-sources</code>编辑源列表。</p><script type="text/javascript" src="/js/src/bai.js"></script><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      
      
        &lt;!-- build time:Tue Jan 28 2020 15:40:41 GMT+0800 (GMT+08:00) --&gt;&lt;p&gt;ubuntu有时在用户登录后会提示有软件包更新，每次更新之后按提示重启，你就会看到一个类似于安全模式下大写的GNU GRUB（一个多操作系统启动
      
    
    </summary>
    
    
      <category term="操作和使用" scheme="https://gsy00517.github.io/categories/%E6%93%8D%E4%BD%9C%E5%92%8C%E4%BD%BF%E7%94%A8/"/>
    
    
      <category term="命令操作" scheme="https://gsy00517.github.io/tags/%E5%91%BD%E4%BB%A4%E6%93%8D%E4%BD%9C/"/>
    
      <category term="ubuntu" scheme="https://gsy00517.github.io/tags/ubuntu/"/>
    
  </entry>
  
  <entry>
    <title>ubuntu笔记：双系统下时间差问题的解决</title>
    <link href="https://gsy00517.github.io/ubuntu20200117085337/"/>
    <id>https://gsy00517.github.io/ubuntu20200117085337/</id>
    <published>2020-01-17T00:53:37.000Z</published>
    <updated>2020-01-19T00:26:43.668Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Tue Jan 28 2020 15:40:41 GMT+0800 (GMT+08:00) --><p>第一次在电脑加装ubuntu双系统后，就存在ubuntu比windows系统时间慢8个小时的问题。当时搞了一会好像也解决了。然而，在我重装了ubuntu系统之后（详见<a href="https://gsy00517.github.io/ubuntu20190914100050/" target="_blank">ubuntu笔记：重装ubuntu——记一段辛酸血泪史</a>），这个问题又出现了。一时间得不到很好地解决，也就没管。最近强迫症犯了，花了点功夫终于搞定了，决定记录在此。</p><p><strong>References</strong>：</p><p>电子文献：<br><a href="http://doc.ntp.org/4.1.1/ntpdate.htm" target="_blank" rel="noopener">http://doc.ntp.org/4.1.1/ntpdate.htm</a><br><a href="http://doc.ntp.org/4.1.1/ntpd.htm" target="_blank" rel="noopener">http://doc.ntp.org/4.1.1/ntpd.htm</a><br><a href="https://blog.csdn.net/vic_qxz/article/details/80344855" target="_blank" rel="noopener">https://blog.csdn.net/vic_qxz/article/details/80344855</a></p><hr><h1 id="windows、ubuntu系统时间"><a href="#windows、ubuntu系统时间" class="headerlink" title="windows、ubuntu系统时间"></a>windows、ubuntu系统时间</h1><p>在windows中，系统时间的设置较为简单。而且设置后，系统时间会自动保存在bios的时钟里面，当启动计算机时，系统会自动在bios里面读取硬件时间，以保证时间不间断。<br>但在ubuntu linux默认情况下，系统时间和硬件时间，并不会自动同步。在ubuntu linux运行过程中，系统时间和硬件时间以异步的方式运行，互不干扰。硬件时间是靠bios电池来维持运行的，而系统时间是用CPU tick来维持的。在系统开机时，会自动从bios中取得硬件时间，设置为系统时间。<br>这样一来就不奇怪了，中国的时区是东八区（GMT+8），因此ubuntu每次读入的是格林威治标准时间并直接将其设置为系统时间，而windows则会加上8:00调整。<br>因此我解决的思路如下：考虑到windows下时间调整更方便，我就优先调整ubuntu的系统时间，将其系统时间（即本电脑的硬件时间）设为GMT+8，然后再在windows系统中取消自动添加8小时的自动调整，即让两个系统以同样的方法从硬件时间设置系统时间。</p><hr><h1 id="ntpd"><a href="#ntpd" class="headerlink" title="ntpd"></a>ntpd</h1><p>这里先介绍一下ntpd（Network Time Protocol (NTP) daemon），如官方文档所说，它的作用是sets and maintains the system time of day in synchronism with Internet standard time servers。因此，我们可以通过ntpdate命令进行设置，其基本格式如下：<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ntpdate [ -bBdoqsuv ] [ -a key ] [ -e authdelay ] [ -k keyfile ] [ -o version ] [ -p samples ] [ -t timeout ] server [ ... ]</span><br></pre></td></tr></table></figure><p></p><p>这里我们不需要用到上面的这些额外选项，因此不一一介绍了。</p><hr><h1 id="solution"><a href="#solution" class="headerlink" title="solution"></a>solution</h1><ol><li><h2 id="安装"><a href="#安装" class="headerlink" title="安装"></a>安装</h2><p>如果还没有安装ntpdate的话，可以先执行该条命令。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo apt-get install ntpdate</span><br></pre></td></tr></table></figure><blockquote><p>注意：apt-get大部分操作都需要root权限，别忘了sudo赋予权限。我有一回忘记sudo了结果搞了半天不知所以…真的太蠢了。</p></blockquote></li><li><h2 id="从服务器校准时间"><a href="#从服务器校准时间" class="headerlink" title="从服务器校准时间"></a>从服务器校准时间</h2><p>这里我使用的时间服务器是<code>time.windows.com</code>，好像也可以用苹果的<code>time.apple.com</code>或者阿里云的<code>time.pool.aliyun.com</code>。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo ntpdate time.windows.com</span><br></pre></td></tr></table></figure><p>格式参考上文。在执行之后发现有0.005秒的微小偏差，因此感觉还是比较可靠的。</p></li><li><h2 id="把时间同步到硬件上"><a href="#把时间同步到硬件上" class="headerlink" title="把时间同步到硬件上"></a>把时间同步到硬件上</h2><p>同步系统时间和硬件时间，可以使用hwclock命令。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo hwclock --localtime --systohc</span><br></pre></td></tr></table></figure><p>这里的sysyohc即系统时间（sys）写到（to）硬件时间（hard clock）。<br>这时ubuntu这边已经解决了，但如果重启打开windows，会发现时间快了8小时，原因之前解释过，因为自动加了8小时，所以还要作下面的调整。</p></li><li><h2 id="调整windows"><a href="#调整windows" class="headerlink" title="调整windows"></a>调整windows</h2>打开windows，调整日期/时间，把时区改到：(UTC)协调世界时。如此一来，windows上的系统时间也是硬件时间了。双系统的系统时间设置方式一致，时间准确，大功告成。<blockquote><p>注：如果windows中时间没有问题，那就无需调整时区。总之就是先设置好ubuntu的，然后再在windows里调整，因为windows下更好调整。似乎windows会自动更正系统时间，所以经以上4步操作后过段时间需要把时区调整回来。</p></blockquote></li></ol><script type="text/javascript" src="/js/src/bai.js"></script><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      
      
        &lt;!-- build time:Tue Jan 28 2020 15:40:41 GMT+0800 (GMT+08:00) --&gt;&lt;p&gt;第一次在电脑加装ubuntu双系统后，就存在ubuntu比windows系统时间慢8个小时的问题。当时搞了一会好像也解决了。然而，在我重装了ub
      
    
    </summary>
    
    
      <category term="操作和使用" scheme="https://gsy00517.github.io/categories/%E6%93%8D%E4%BD%9C%E5%92%8C%E4%BD%BF%E7%94%A8/"/>
    
    
      <category term="命令操作" scheme="https://gsy00517.github.io/tags/%E5%91%BD%E4%BB%A4%E6%93%8D%E4%BD%9C/"/>
    
      <category term="ubuntu" scheme="https://gsy00517.github.io/tags/ubuntu/"/>
    
  </entry>
  
  <entry>
    <title>linear algebra笔记：循环矩阵</title>
    <link href="https://gsy00517.github.io/linear-algebra20200116095725/"/>
    <id>https://gsy00517.github.io/linear-algebra20200116095725/</id>
    <published>2020-01-16T01:57:25.000Z</published>
    <updated>2020-01-20T05:51:26.233Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Tue Jan 28 2020 15:40:41 GMT+0800 (GMT+08:00) --><p>循环矩阵是我在看相关滤波时遇到的一个terminology，通过一定的了解之后发现其具有许多有用的性质。在目标跟踪领域，循环矩阵的引入对速度的提升是非常大的。关于相关滤波，由于现在了解还不够全面和深入，暂时不提及。本文主要就循环矩阵的概念和性质做一个总结。</p><p><strong>References</strong>：</p><p>电子文献：<br><a href="https://www.cnblogs.com/cj-xxz/p/10323711.html" target="_blank" rel="noopener">https://www.cnblogs.com/cj-xxz/p/10323711.html</a><br><a href="https://blog.csdn.net/shenxiaolu1984/article/details/50884830" target="_blank" rel="noopener">https://blog.csdn.net/shenxiaolu1984/article/details/50884830</a></p><p>参考文献：<br>[1]High-Speed Tracking with Kernelized Correlation Filters</p><hr><h1 id="循环矩阵（Circulant-Matrices）"><a href="#循环矩阵（Circulant-Matrices）" class="headerlink" title="循环矩阵（Circulant Matrices）"></a>循环矩阵（Circulant Matrices）</h1><p>任意循环矩阵可以被傅里叶变换矩阵对角化。（All circulant matrices are made diagonal by the Discrete Fourier Transform (DFT), regardless of the generating vector x.）我们在文献中往往会看到这样一个变换：</p><script type="math/tex;mode=display">X=C(x)=F\cdot diag(\widehat{x})\cdot F^{H}</script><p>下面的$X$它就是一个循环矩阵，它是由它的第一行$x=(x_{1},x_{2},…,x_{n})$的向量组每次经过一个循环位移，得到的一个循环矩阵。其中$\widehat{x}$（读作x hat）为原向量$x$的傅里叶变换；$F$是傅里叶变换矩阵，$F^{H}$表示共轭转置。<br>换句话说，循环矩阵$X$相似于对角阵，其特征值是$\widehat{x}$的元素。以长度为3的$x$为例，其生成的循环矩阵为：</p><script type="math/tex;mode=display">X=C(x)=\begin{bmatrix} x_{1} & x_{2} & x_{3}\\ x_{3} & x_{1} & x_{2}\\ x_{2} & x_{3} & x_{1} \end{bmatrix}</script><p>这样的一个矩阵它有一个特别好的性质，就是能够通过它的第一行的生成向量来做来进行对角化。通过这个式子，我们能够把$X$循环矩阵进行对角化，如此把它转换到傅里叶域之后，用离散傅里叶变化来做运算时，对速度的提升是非常大的，这将在后文进一步说明。</p><blockquote><p>关于这里的对角化、傅里叶变换矩阵可以看后文，在此先跳过。</p></blockquote><p>这里有必要列出循环矩阵的两个重要公式，这两个性质是比较常用和有用的：</p><ol><li><h2 id="卷积"><a href="#卷积" class="headerlink" title="卷积"></a>卷积</h2>循环矩阵乘向量等价于生成向量的逆序和该向量卷积，可进一步转化为傅里叶变换相乘。 <img src="/linear-algebra20200116095725/重要公式1.png" title="重要公式1"> 这里$\overline{x}$表示$x$的逆序排列，$*$表示共轭。<blockquote><p>注意：卷积本身也包含逆序操作。此外，这里最后一个等号利用了信号与系统中的“时域卷积，频域相乘”，即时域卷积定理，它表明两信号在时域的卷积积分对应于在频域中该两信号的傅里叶变换的乘积。</p></blockquote></li><li><h2 id="相乘"><a href="#相乘" class="headerlink" title="相乘"></a>相乘</h2>循环矩阵的乘积仍是循环矩阵，所以我们只要维护循环矩阵的第一行，就可以以较低的复杂度维护循环矩阵的乘积。 <img src="/linear-algebra20200116095725/重要公式2.png" title="重要公式2"> 公式中最终所得的乘积也是循环矩阵，其生成向量是原生成向量对位相乘的傅里叶逆变换。</li></ol><p>用了上述循环矩阵的性质之后，我们就可以使得原来两个矩阵相乘的时间复杂度$O(K^{3})$能够降到$O(Klog(K))$（反向傅里叶的复杂度（$O(Klog(K))$）加上向量点乘的复杂度（$K$）），速度的提升是非常明显的。</p><blockquote><p>注：这里K表示的是矩阵的尺寸。</p></blockquote><p>在非线形的情况下，当引入了核之后，也可以得到同样的一个情况。此时需要这个核满足一定的条件，它是可以具备循环矩阵的一些性质的，例如常用的高斯核、线性核都满足这个条件，因此可以直接拿来用。</p><hr><h1 id="傅里叶变换矩阵（DFT-matrix）"><a href="#傅里叶变换矩阵（DFT-matrix）" class="headerlink" title="傅里叶变换矩阵（DFT matrix）"></a>傅里叶变换矩阵（DFT matrix）</h1><p>关于离散傅里叶矩阵$F$这里涉及较多的数学，想看详细推导可以参考文首给出的第二个参考链接。这里把比较关键的结论部分截了过来。<br><img src="/linear-algebra20200116095725/傅里叶变换矩阵.png" title="傅里叶变换矩阵"><br>$F$在这里是一个奇异矩阵（方阵且行列式等于零），它可以对任意输入向量进行傅里叶变换，这是因为傅里叶变换具有线性性。</p><hr><h1 id="矩阵快速幂"><a href="#矩阵快速幂" class="headerlink" title="矩阵快速幂"></a>矩阵快速幂</h1><p>在本文前的第一个参考链接中，作者是用矩阵快速幂引入的，那么这里我也简单谈一下快速幂。<br>顾名思义，快速幂就是快速算某个数的多少次幂。<br>我们知道，对于任何一个整数，都能用二进制来表示。那么对于$a^{n}$，$n$也一定可以用二进制来表示。<br>那么问题来了，如何计算某个数较大的次幂呢？<br>比如计算$a^{156}$，我们可以利用除二取余、倒序排列、高位补零的方法得到$(156)_{10}=(10011100)_{2}$。<br>如此可以推导：</p><script type="math/tex;mode=display">Ans=a^{(156)_{10}}=a^{(10011100)_{2}}</script><script type="math/tex;mode=display">=a^{2^{7}*1+2^{6}*0+2^{5}*0+2^{4}*1+2^{3}*1+2^{2}*1+2^{1}*0+2^{0}*0}</script><script type="math/tex;mode=display">=(a^{2^{7}})*(a^{2^{4}})*(a^{2^{3}})*(a^{2^{2}})</script><p>这样一来，原本要进行$156-1=155$次乘法运算，现在运算量级相当于该幂的二进制数表示中1的个数。其时间复杂度为$O(log_{2}n)$，与朴素的$O(n)$相比，效率有了极大地提高。<br>以上就是一般的快速幂的基本套路。相对于一般的快速幂，矩阵快速幂仅仅是把他的底数和乘数换成了矩阵形式。其主要方法就是：通过把数放到矩阵的不同位置，然后把普通递推式构造成类似于“矩阵的等比数列”，最后快速幂求解递推式。<br>矩阵快速幂主要用于求一个很复杂的递推式中的某一项问题。<br>递推矩阵（关系矩阵）的构造，也是矩阵快速幂的难点，一般是由原始的递推公式推导或者配凑得出，网上有许多ACM的赛题解答，可以看几道理解一下思路。</p><script type="text/javascript" src="/js/src/bai.js"></script><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      
      
        &lt;!-- build time:Tue Jan 28 2020 15:40:41 GMT+0800 (GMT+08:00) --&gt;&lt;p&gt;循环矩阵是我在看相关滤波时遇到的一个terminology，通过一定的了解之后发现其具有许多有用的性质。在目标跟踪领域，循环矩阵的引入对速度的提
      
    
    </summary>
    
    
      <category term="知识点与小技巧" scheme="https://gsy00517.github.io/categories/%E7%9F%A5%E8%AF%86%E7%82%B9%E4%B8%8E%E5%B0%8F%E6%8A%80%E5%B7%A7/"/>
    
    
      <category term="线性代数" scheme="https://gsy00517.github.io/tags/%E7%BA%BF%E6%80%A7%E4%BB%A3%E6%95%B0/"/>
    
  </entry>
  
  <entry>
    <title>linear algebra笔记：二维仿射变换</title>
    <link href="https://gsy00517.github.io/linear-algebra20200116084728/"/>
    <id>https://gsy00517.github.io/linear-algebra20200116084728/</id>
    <published>2020-01-16T00:47:28.000Z</published>
    <updated>2020-01-16T01:56:40.550Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Tue Jan 28 2020 15:40:41 GMT+0800 (GMT+08:00) --><p>之前在<a href="https://gsy00517.github.io/deep-learning20191001151454/" target="_blank">deep-learning笔记：学习率衰减与批归一化</a>一文中，我已经对仿射变换作了简单的介绍。但这里我想提出来单独对其做一个小归纳。</p><hr><h1 id="应用"><a href="#应用" class="headerlink" title="应用"></a>应用</h1><p>仿射变换在计算机科学中有丰富的运用。例如，在计算机图形学中，它可以用于在较小或较大的屏幕上显示图形内容时简单地重新缩放图形内容。此外，它也可以应用于扭曲一个图像到另一个图像平面。<br>另一个重要的应用是训练深层神经网络时用于扩充数据集。训练深度模型需要大量的数据。在几乎所有的情况下，模型都受益于更高的泛化性能，因为有更多的训练图像。人工生成更多数据的一种方法就是对输入数据随机应用仿射变换（数据增强）。<br><img src="/linear-algebra20200116084728/图形变换.jpg" title="图形变换"></p><hr><h1 id="仿射变换"><a href="#仿射变换" class="headerlink" title="仿射变换"></a>仿射变换</h1><p>二维仿射变换可以用下面这个公式来表示：</p><script type="math/tex;mode=display">x'=Ax</script><p>其中$A$是在齐次坐标系中的3x3矩阵，$x$是在齐次坐标系中$(x，y，1)$形式的向量。这个公式表示$A$将一个任意向量$x$映射到另一个向量$x’$。<br>一般来说，仿射变换有6个自由度。根据参数的值，它将在矩阵乘法后扭曲任何图像。变换后的图像保留了原始图像中的平行直线（考虑剪切）。本质上，满足这两个条件的任何变换都是仿射的。它保持了二维图形的“平直性”、“平行性”和“共线比例不变性”，非共线的三对对应点可以确定一个唯一的仿射变换。<br>下面一些特殊形式的$A$，如下图所示，从上到下分别是：缩放、平移和旋转。<br><img src="/linear-algebra20200116084728/特殊形式.jpg" title="特殊形式"><br>上述仿射变换的一个非常有用的性质是它们是线性函数。它们保留了乘法和加法运算，并遵循叠加原理。<br><img src="/linear-algebra20200116084728/性质.jpg" title="性质"><br>换言之，我们可以组合2个或更多的变换：向量加法表示平移，矩阵乘法表示线性映射，只要我们用齐次坐标表示它们。即利用这个性质，我们可以将二维仿射变换视为线性变换R和平移变换T的叠加，具体可以看一下之前的文章。<br>举个例子，我们可以将旋转和平移如下表示：<br></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">A = array([[cos(angle), -sin(angle), tx],</span><br><span class="line">           [sin(angle), cos(angle),  ty],</span><br><span class="line">           [<span class="number">0</span>,          <span class="number">0</span>,           <span class="number">1</span>]])</span><br></pre></td></tr></table></figure><p></p><p>如果理解了的话，你会发现各种各样的变化其实还挺繁琐的。不过请放心，大多数开发人员和研究人员通常省去了编写所有这些变换的麻烦，而只需依赖优化的库来执行任务。在OpenCV中进行仿射变换非常简单，如果今后遇到的比较多再做整理。</p><hr><h1 id="分享"><a href="#分享" class="headerlink" title="分享"></a>分享</h1><p>之前在<a href="https://gsy00517.github.io/artificial-intelligence20191007232512/" target="_blank">artificial-intelligence笔记：吴恩达——阅读论文的建议</a>一文中提到以后会分享几个觉得有价值的公众号的，这里就再分享一个吧。本文中的图片均来自该公众号的推文。<br><img src="/linear-algebra20200116084728/磬创AI.JPG" title="磬创AI"></p><script type="text/javascript" src="/js/src/bai.js"></script><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      
      
        &lt;!-- build time:Tue Jan 28 2020 15:40:41 GMT+0800 (GMT+08:00) --&gt;&lt;p&gt;之前在&lt;a href=&quot;https://gsy00517.github.io/deep-learning20191001151454/&quot; tar
      
    
    </summary>
    
    
      <category term="知识点与小技巧" scheme="https://gsy00517.github.io/categories/%E7%9F%A5%E8%AF%86%E7%82%B9%E4%B8%8E%E5%B0%8F%E6%8A%80%E5%B7%A7/"/>
    
    
      <category term="线性代数" scheme="https://gsy00517.github.io/tags/%E7%BA%BF%E6%80%A7%E4%BB%A3%E6%95%B0/"/>
    
  </entry>
  
  <entry>
    <title>matlab笔记：安装MinGW编译器</title>
    <link href="https://gsy00517.github.io/matlab20200115222641/"/>
    <id>https://gsy00517.github.io/matlab20200115222641/</id>
    <published>2020-01-15T14:26:41.000Z</published>
    <updated>2020-01-21T11:59:48.212Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Tue Jan 28 2020 15:40:41 GMT+0800 (GMT+08:00) --><p>因为目标追踪领域最著名的比赛VOT（Visual Object Tracking），同时也拥有一个非常重要的数据集和一套比较权威的评价指标，基于的是matlab，因此我又开始用起了matlab（这么看下来貌似matlab要成我本学期用的最多的语言了）。当我download官方的toolkit之后，按着document一路比较顺利地操作了下来，结果突然遇到一个报错，说我没有C和C++编译器。WTF？我用了这么久居然都没发现这个问题…<br>本以为按照提示就能很快解决，结果这个问题折腾了我一整个晚上。好吧既然被折磨得这么惨那我还是本着逢血泪必写博的原则在这里写一下吧。<br>不过不得不说，最后成功的时候真的还是挺爽的哈哈！<br><img src="/matlab20200115222641/开心.jpg" title="开心"></p><p><strong>References</strong>：</p><p>电子文献：<br><a href="https://ww2.mathworks.cn/help/matlab/matlab_external/compiling-c-mex-files-with-mingw.html?requestedDomain=uk.mathworks.com" target="_blank" rel="noopener">https://ww2.mathworks.cn/help/matlab/matlab_external/compiling-c-mex-files-with-mingw.html?requestedDomain=uk.mathworks.com</a><br><a href="https://ww2.mathworks.cn/matlabcentral/fileexchange/52848-matlab-support-for-mingw-w64-c-c-compiler" target="_blank" rel="noopener">https://ww2.mathworks.cn/matlabcentral/fileexchange/52848-matlab-support-for-mingw-w64-c-c-compiler</a><br><a href="https://www.cnblogs.com/Vae1990Silence/p/10102375.html" target="_blank" rel="noopener">https://www.cnblogs.com/Vae1990Silence/p/10102375.html</a><br><a href="https://blog.csdn.net/fly910905/article/details/86222946" target="_blank" rel="noopener">https://blog.csdn.net/fly910905/article/details/86222946</a></p><hr><h1 id="MinGW"><a href="#MinGW" class="headerlink" title="MinGW"></a>MinGW</h1><p>MinGW，是Minimalist GNU for Windows的缩写。它是一个可自由使用和自由发布的Windows特定头文件和使用GNU工具集导入库的集合，允许你在GNU/Linux和Windows平台生成本地的Windows程序而不需要第三方C运行时（C Runtime）库。<br>当初报错的时候，我也是很诧异，因为之前使用CodeBlocks和Visual Studio的时候明明是有的。而这次在matlab中编译C/C++时怎么就找不到了。<br>因为之前使用CodeBlocks也有找不到的情况，我当时是重装了一遍CodeBlocks解决问题的，因此我上网开了一下有没有类似的方法。按道理来说已经装了VS是可以找到的，但好像也存在即使安装了VS、matlab还是找不到编译器的情况。可以使用mex看一下具体是哪些路径没有匹配上，似乎可以通过修改注册表的方法解决，但我没有尝试。</p><blockquote><p>注：matlab调用C/C++的方式主要有两种：利用MEX技术和调用C/C++动态连接库。MEX是Matlab Executable的缩写，它是一种“可在matlab中调用的C（或Fortran）语言衍生程序”。后文中还会用到。</p></blockquote><hr><h1 id="失败的方法"><a href="#失败的方法" class="headerlink" title="失败的方法"></a>失败的方法</h1><p>毕竟是花了一个晚上，看了mathworks上网友们的各种solution，试错了许多方法，这里记录两个貌似要成功的方法（其实最后还是失败了），或许会有参考价值。<br>这里有一个被许多网友强调、要注意的是：下载后的mingw文件（没错它只有15kb看上去好假）要在打开的matlab中，找到相应的下载目录，右键点击然后选择下载并安装（download and install），否则似乎会出错。</p><ol><li><h2 id="禁用IPv6"><a href="#禁用IPv6" class="headerlink" title="禁用IPv6"></a>禁用IPv6</h2><p>IPv6，顾名思义，就是IP地址的第6版协议。<br>我们现在用的是IPv4，它的地址是32位，总数有43亿个左右，还要减去内网专用的192、170地址段，这样一来就更少了。<br>然而，IPv6的地址是128位的，大概是43亿的4次方，地址极为丰富。<br>网友Kshitij Mall给出了这样一个解决思路：</p><ol><li>首先打开控制面板中的编辑系统环境变量。</li><li>在高级选项中，点击环境变量。</li><li><p>在系统变量栏中添加如下两个变量：（1）variable name：“JAVA_TOOL_OPTIONS”；value：“-Djava.net.preferIPv4Stack=true”；（2）variable name：“JAVA_OPTIONS”；value：“-Djava.net.preferIPv4Stack=true”。另外根据该网友所述，安装完成后可以删去这两个环境变量。</p><blockquote><p>注意：仅输入引号内部的内容。</p></blockquote><p>java文档指示，设置jvm属性java.net.preferIPv4Stack为true时，就可以禁用IPv6。反之，若设为0，则启用。</p><blockquote><p>注：禁用设置时不需要重启系统。</p></blockquote><p>由于该网友的系统配置和我相同（MATLAB 2019a on Windows 10 system with 64 bits），于是我毫不犹豫地首先尝试了他的方法，不知道是卡进去的原因还是为何，我成功看到了协议界面（原本一直卡在附加功能管理器加载的空白界面），但是最终开始下载支持包失败。</p></li></ol></li><li><h2 id="关闭防火墙"><a href="#关闭防火墙" class="headerlink" title="关闭防火墙"></a>关闭防火墙</h2>失败之后，我继续看评论，看到一个网友感谢另一个网友提供的solution，我非常激动，感觉自己也要跟着解决了。<br>似乎好像取得了一定的进展，但是在下载第三方包的时候还是卡住了（3rt party download error），我浏览了大多数网友的评论，貌似大家基本上都卡在了这里。</li></ol><hr><h1 id="成功的方法"><a href="#成功的方法" class="headerlink" title="成功的方法"></a>成功的方法</h1><p>当我快要绝望的时候，突然看到了评论区感谢三连，都是感谢同一个人的。这已经是2018年的一个回复了，看评论发现他们使用的是Windows 7系统，但我决定还是试一下，结果真的成功了，非常感谢最初的solution提供者pawan singh！<br>具体方法如下：</p><ol><li>到<a href="http://tdm-gcc.tdragon.net/download" target="_blank">这个网址</a>下载合适的TDM-GCC。</li><li>下载之后，create一个新的到设定的安装路径中。<blockquote><p>注意：根据matlab文档（文首第一个参考链接）。MinGW的安装文件夹名称不能包含空格。例如，不要使用：C:\Program Files\mingw-64。应改用：C:\mingw-64。我建议直接装在C盘下面，默认似乎也是这样，维持不变即可。</p></blockquote></li><li>与之前修改系统变量方式类似。添加新的系统变量名为<code>MW_MINGW64_LOC</code>，值为MinGW-w64编译器的安装位置，于我是<code>C:\TDM-GCC-64</code>。最后别忘了确定设置。</li><li>在matlab命令行内执行命令：<code>setenv(&#39;MW_MINGW64_LOC&#39;, &#39;path&#39;)</code>，folder为TDM-GCC的安装位置，要加单引号。例如我是：<code>setenv(&#39;MW_MINGW64_LOC&#39;, &#39;C:\TDM-GCC-64&#39;)</code>。</li><li>可以继续在命令行中执行命令：<code>mex -setup</code>。若没有报错，则表明成功了。</li></ol><hr><h1 id="然而"><a href="#然而" class="headerlink" title="然而"></a>然而</h1><p>英语老师说，however后面的往往是重点，那么我这里就however一下。<br>上面的方法问题是没有，但是使用的时候有可能会收到警告：使用的是不受支持的MinGW编译器版本。<br>如果没有收到这个警告，那么就万事大吉，如果有的话，能运行的话依旧还是万事大吉。<br>But如果真的因此而运行出错，或者看着warning心里实在不舒服的话，可以看一下我后来写的<a href="https://gsy00517.github.io/matlab20200121194751/" target="_blank">matlab笔记：MEX文件函数使用中的问题</a>。</p><script type="text/javascript" src="/js/src/bai.js"></script><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      
      
        &lt;!-- build time:Tue Jan 28 2020 15:40:41 GMT+0800 (GMT+08:00) --&gt;&lt;p&gt;因为目标追踪领域最著名的比赛VOT（Visual Object Tracking），同时也拥有一个非常重要的数据集和一套比较权威的评价指标，基于
      
    
    </summary>
    
    
      <category term="环境配置" scheme="https://gsy00517.github.io/categories/%E7%8E%AF%E5%A2%83%E9%85%8D%E7%BD%AE/"/>
    
    
      <category term="安装教程" scheme="https://gsy00517.github.io/tags/%E5%AE%89%E8%A3%85%E6%95%99%E7%A8%8B/"/>
    
      <category term="matlab" scheme="https://gsy00517.github.io/tags/matlab/"/>
    
  </entry>
  
  <entry>
    <title>python笔记：pandas基本使用</title>
    <link href="https://gsy00517.github.io/python20200113202851/"/>
    <id>https://gsy00517.github.io/python20200113202851/</id>
    <published>2020-01-13T12:28:51.000Z</published>
    <updated>2020-01-26T03:30:42.114Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Tue Jan 28 2020 15:40:41 GMT+0800 (GMT+08:00) --><p>在python中，Pandas可以说是最实用的库之一，它提供了非常丰富的数据读写方法。可以看一下Pandas中文网提供的<a href="https://www.pypandas.cn/docs/" target="_blank">Pandas参考文档</a>中对所有I/O函数的总结。<br><img src="/python20200113202851/各类文件的读写.png" title="各类文件的读写"><br>Pandas是一个开源的，BSD许可的库，为python提供高性能、易于使用的数据结构和数据分析工具。它的使用基础是Numpy（提供高性能的矩阵运算）；可以用于数据挖掘和数据分析，同时也提供数据清洗的功能。<br>本文就对Pandas的基本使用做一个简单的归纳，所有代码可以从上往下按顺序依次执行。</p><p><strong>References</strong>：</p><p>电子文献：<br><a href="https://www.cnblogs.com/chenhuabin/p/11477076.html" target="_blank" rel="noopener">https://www.cnblogs.com/chenhuabin/p/11477076.html</a><br><a href="https://blog.csdn.net/weixin_39791387/article/details/81487549" target="_blank" rel="noopener">https://blog.csdn.net/weixin_39791387/article/details/81487549</a><br><a href="https://kanoki.org/2019/09/16/dataframe-visualization-with-pandas-plot/" target="_blank" rel="noopener">https://kanoki.org/2019/09/16/dataframe-visualization-with-pandas-plot/</a><br><a href="https://pandas.pydata.org/pandas-docs/stable/getting_started/10min.html" target="_blank" rel="noopener">https://pandas.pydata.org/pandas-docs/stable/getting_started/10min.html</a><br><a href="https://blog.csdn.net/weixin_41712499/article/details/82719987" target="_blank" rel="noopener">https://blog.csdn.net/weixin_41712499/article/details/82719987</a></p><hr><h1 id="csv"><a href="#csv" class="headerlink" title="csv"></a>csv</h1><p>这里我想介绍一下一种新的数据格式：csv。它和excel很像，但又不同于excel。csv主要有如下特点：</p><ol><li>纯文本，使用某个字符集，比如ASCII、Unicode、EBCDIC或GB2312（简体中文环境）等；</li><li>由记录组成（典型的是每行一条记录）；</li><li>每条记录被分隔符（英语：Delimiter）分隔为字段（英语：Field (computer science)）（典型分隔符有逗号、分号或制表符；有时分隔符可以包括可选的空格）；</li><li>每条记录都有同样的字段序列。</li></ol><p>在Pandas的使用以及AI相关竞赛数据集、结果的存储与使用中，csv文件往往承担着主角的位置。</p><hr><h1 id="准备"><a href="#准备" class="headerlink" title="准备"></a>准备</h1><p>在具体使用之前，别忘了先导入所需相应的库。<br></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br></pre></td></tr></table></figure><p></p><p>可以使用<code>pd.__version__</code>来输出版本号，注意，这里的“__”是两个“_”，这个很容易搞错且难以发现。</p><hr><h1 id="两大利器"><a href="#两大利器" class="headerlink" title="两大利器"></a>两大利器</h1><p>Pandas中文网首页，在介绍完Pandas之后，就重点介绍了一下Pandas的两大利器。分别是DataFrame和Series。这里我先介绍一下Seires，DataFrame在后面有更详细的操作。</p><ol><li><h2 id="Series简介"><a href="#Series简介" class="headerlink" title="Series简介"></a>Series简介</h2><p>Series是一种类似于一维数组的对象，是由一组数据(各种NumPy数据类型)以及一组与之相关的数据标签(即索引)组成。仅由一组数据也可产生简单的Series对象。<br>我们可以通过传入一个list的数值来创建一个Series，Pandas会创建一个默认的整数索引：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">s = pd.Series([<span class="number">1</span>, <span class="number">3</span>, <span class="number">5</span>, np.nan, <span class="number">6</span>, <span class="number">8</span>])</span><br><span class="line"></span><br><span class="line">s</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="number">0</span>    <span class="number">1.0</span></span><br><span class="line">    <span class="number">1</span>    <span class="number">3.0</span></span><br><span class="line">    <span class="number">2</span>    <span class="number">5.0</span></span><br><span class="line">    <span class="number">3</span>    NaN</span><br><span class="line">    <span class="number">4</span>    <span class="number">6.0</span></span><br><span class="line">    <span class="number">5</span>    <span class="number">8.0</span></span><br><span class="line">    dtype: float64</span><br></pre></td></tr></table></figure><blockquote><p>注：这里用<code>np.nan</code>来产生NaN，但要注意的是<code>np.nan</code>不是一个“空”对象，即使用<code>np.nan == np.nan</code>来判断将返回False，<code>np.nan</code>的类型为基本数据类型float。<br>若要对某个值进行空值判断，如对<code>np.nan</code>，需要用<code>np.isnan(np.nan)</code>，此时返回为True。</p></blockquote><p>另外，也可以从字典创建Series。</p></li><li><h2 id="DataFrame简介"><a href="#DataFrame简介" class="headerlink" title="DataFrame简介"></a>DataFrame简介</h2><p>DataFrame是Pandas中的一个表格型的数据结构，包含有一组有序的列，每列可以是不同的值类型（数值、字符串、布尔型等），DataFrame即有行索引也有列索引，可以被看做是由Series组成的字典。<br>我们可以通过传入一个numpy数组来创建一个DataFrame，如下面带有一个datetime的索引以及被标注的列：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">dates = pd.date_range(<span class="string">'20130101'</span>, periods = <span class="number">6</span>)</span><br><span class="line"></span><br><span class="line">dates</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>DatetimeIndex([<span class="string">'2013-01-01'</span>, <span class="string">'2013-01-02'</span>, <span class="string">'2013-01-03'</span>, <span class="string">'2013-01-04'</span>,</span><br><span class="line">                   <span class="string">'2013-01-05'</span>, <span class="string">'2013-01-06'</span>],</span><br><span class="line">                  dtype = <span class="string">'datetime64[ns]'</span>, freq = <span class="string">'D'</span>)</span><br><span class="line"></span><br><span class="line">df1 = pd.DataFrame(np.random.randn(<span class="number">6</span>, <span class="number">4</span>), index = dates, columns = list(<span class="string">'ABCD'</span>))</span><br><span class="line"></span><br><span class="line">df1</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>               A         B         C         D</span><br><span class="line"><span class="number">2013</span><span class="number">-01</span><span class="number">-01</span>  <span class="number">0.469112</span> <span class="number">-0.282863</span> <span class="number">-1.509059</span> <span class="number">-1.135632</span></span><br><span class="line"><span class="number">2013</span><span class="number">-01</span><span class="number">-02</span>  <span class="number">1.212112</span> <span class="number">-0.173215</span>  <span class="number">0.119209</span> <span class="number">-1.044236</span></span><br><span class="line"><span class="number">2013</span><span class="number">-01</span><span class="number">-03</span> <span class="number">-0.861849</span> <span class="number">-2.104569</span> <span class="number">-0.494929</span>  <span class="number">1.071804</span></span><br><span class="line"><span class="number">2013</span><span class="number">-01</span><span class="number">-04</span>  <span class="number">0.721555</span> <span class="number">-0.706771</span> <span class="number">-1.039575</span>  <span class="number">0.271860</span></span><br><span class="line"><span class="number">2013</span><span class="number">-01</span><span class="number">-05</span> <span class="number">-0.424972</span>  <span class="number">0.567020</span>  <span class="number">0.276232</span> <span class="number">-1.087401</span></span><br><span class="line"><span class="number">2013</span><span class="number">-01</span><span class="number">-06</span> <span class="number">-0.673690</span>  <span class="number">0.113648</span> <span class="number">-1.478427</span>  <span class="number">0.524988</span></span><br></pre></td></tr></table></figure><blockquote><p>注：上面用<code>pd.data_range()</code>生成了一个时间频率<code>freq = &#39;D&#39;</code>（即天）的日期序列。</p></blockquote><p>我们也可以通过传入一个可以转换为类Series（series-like）的字典对象来创建一个DataFrame：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">df2 = pd.DataFrame(&#123;<span class="string">'A'</span>: <span class="number">1.</span>,</span><br><span class="line">                    <span class="string">'B'</span>: pd.Timestamp(<span class="string">'20130102'</span>),</span><br><span class="line">                    <span class="string">'C'</span>: pd.Series(<span class="number">1</span>, index = list(range(<span class="number">4</span>)), dtype = <span class="string">'float32'</span>),</span><br><span class="line">                    <span class="string">'D'</span>: np.array([<span class="number">3</span>] * <span class="number">4</span>, dtype = <span class="string">'int32'</span>),</span><br><span class="line">                    <span class="string">'E'</span>: pd.Categorical([<span class="string">"test"</span>, <span class="string">"train"</span>, <span class="string">"test"</span>, <span class="string">"train"</span>]),</span><br><span class="line">                    <span class="string">'F'</span>: <span class="string">'foo'</span>&#125;)</span><br><span class="line"></span><br><span class="line">df2</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span> A          B    C  D      E    F</span><br><span class="line"><span class="number">0</span>  <span class="number">1.0</span> <span class="number">2013</span><span class="number">-01</span><span class="number">-02</span>  <span class="number">1.0</span>  <span class="number">3</span>   test  foo</span><br><span class="line"><span class="number">1</span>  <span class="number">1.0</span> <span class="number">2013</span><span class="number">-01</span><span class="number">-02</span>  <span class="number">1.0</span>  <span class="number">3</span>  train  foo</span><br><span class="line"><span class="number">2</span>  <span class="number">1.0</span> <span class="number">2013</span><span class="number">-01</span><span class="number">-02</span>  <span class="number">1.0</span>  <span class="number">3</span>   test  foo</span><br><span class="line"><span class="number">3</span>  <span class="number">1.0</span> <span class="number">2013</span><span class="number">-01</span><span class="number">-02</span>  <span class="number">1.0</span>  <span class="number">3</span>  train  foo</span><br></pre></td></tr></table></figure><p>这里可以使用<code>df2.dtypes</code>来查看不同列的数据类型。</p></li></ol><h1 id="读取"><a href="#读取" class="headerlink" title="读取"></a>读取</h1><p>无论是txt文件还是csv文件，在Pandas中都使用<code>read_csv()</code>读取，当然也使用同一个方法写入到文件，那就是<code>to_csv()</code>方法。<br></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">df = pd.read_csv(abs_path) <span class="comment">#此为绝对路径</span></span><br></pre></td></tr></table></figure><p></p><p>为了提供更加多样化、可定制的功能，read_csv()方法定义了数十个参数，还在大部分参数并不常用，以下是几个比较常用的参数：</p><ol><li><strong>filepath_or_buffer</strong>：文件所在路径，可以是一个描述路径的字符串、pathlib.Path对象、http或ftp的连接，也可以是任何可调用<code>read()</code>的对象。这是唯一一个必传的参数，也就是上面的abs_path。</li><li><strong>encoding</strong>：编码，字符型，通常为utf-8，如果中文读取不正常，可以将encoding设为gbk。当然，也可以直接将对应文件改成utf-8编码。</li><li><strong>header</strong>：整数或者由整数组成的列表，用来指定由哪一行或者哪几行作为列名，默认为<code>header = 0</code>，表示用第一列作为列名。若设置<code>header = 0</code>，则指定第二列作为列名。要注意的是，当指定第一行之后的数据作为列名时，前面的所有行都会被略过。也可以传递一个包含多个整数的列表给header，这样每一列就会有多个列名。如果中间某一行没有指定，那么该行会被略过。例如<code>header = [0, 2]</code>，则原本的第二行会被省去。而当文件中没有列名一行数据时，可以传递<code>header = None</code>，表示不从文件数据中指定行作为列名，这时Pandas会自动生成从零开始的序列作为列名。</li><li><strong>names</strong>：接着上面的header，很快就想到是不是可以自己设置列名。names就可以用来生成一个列表，为数据额外指定列名。例如：<code>df = pd.read_csv(&#39;abs_path, names=[&#39;第一列&#39;, &#39;第二列&#39;, &#39;第三列&#39;, &#39;第四列&#39;])</code>。</li></ol><p>在数据读取完毕之后，我们可以使用如下代码来快速查看数据是否正确地导入了。<br></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">df.head() <span class="comment">#看一下导入后df（DataFrame）的前几行，可在括号内输入数字来设定具体显示几行，默认5行</span></span><br><span class="line">df.tail() <span class="comment">#类似，查看后几行</span></span><br><span class="line"></span><br><span class="line">type(df) <span class="comment">#查看类型，DataFrame的输出应该是pandas.core.frame.DataFrame</span></span><br></pre></td></tr></table></figure><p></p><hr><h1 id="DataFrame"><a href="#DataFrame" class="headerlink" title="DataFrame"></a>DataFrame</h1><p>DataFrame的介绍在前面的简介已经写过，这里就不赘述了。事实上，Pandas中的DataFrame的操作，有很大一部分跟numpy中的二维数组的操作是近似的。<br>在上面的读取处理之后，我们下面对其进行一些简单的操作：</p><ol><li><h2 id="查看"><a href="#查看" class="headerlink" title="查看"></a>查看</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">df.head() <span class="comment">#上文已提及</span></span><br><span class="line">df.tail()</span><br><span class="line"></span><br><span class="line"><span class="comment">#查看列名</span></span><br><span class="line">print(df.columns)</span><br><span class="line"></span><br><span class="line"><span class="comment">#查看索引</span></span><br><span class="line">print(df.index)</span><br><span class="line"></span><br><span class="line"><span class="comment">#查看各列的数据格式</span></span><br><span class="line">print(df.dtypes)</span><br><span class="line"></span><br><span class="line"><span class="comment">#查看整个DataFrame的属性信息</span></span><br><span class="line">print(df.info())</span><br><span class="line"></span><br><span class="line"><span class="comment">#访问对应行</span></span><br><span class="line">df.loc[<span class="number">0</span>] <span class="comment">#这里访问了第一行，将显示列名和对应每一列第一行的数据</span></span><br><span class="line"><span class="comment">#具体有关索引请看后文</span></span><br></pre></td></tr></table></figure></li><li><h2 id="筛选"><a href="#筛选" class="headerlink" title="筛选"></a>筛选</h2><p>在numpy中，我们可以这样判断一个数组中每一个数和对应数值的比较结果：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">a = np.array(range(<span class="number">10</span>))</span><br><span class="line">a &gt; <span class="number">3</span></span><br></pre></td></tr></table></figure><p>输出将是一串布尔型（True、False）的array。<br>而在DataFrame中，我们可以用类似的方法通过指定列来进行筛选：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#筛选第二列中数值大于80</span></span><br><span class="line">df[df.第二列 &gt; <span class="number">80</span>]</span><br></pre></td></tr></table></figure><p>这样就会得到只用符合条件数据的对应行的一个DataFrame。<br>我们也可以使用<code>df[(df.第一列 &gt; 80) &amp; (df.第二列 &gt; 80) &amp; (df.第三列 &gt; 80)]</code>来进行多条件的复杂筛选。<br>此外，我们可以直接根据列名提取出一个新的表格：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">new = df[[<span class="string">'第一列'</span>, <span class="string">'第二列'</span>]] <span class="comment">#new为仅由第一列和第二列组成的一个新的DataFrame</span></span><br></pre></td></tr></table></figure></li><li><h2 id="排序"><a href="#排序" class="headerlink" title="排序"></a>排序</h2><p>可以使用如下代码根据单列或者多列的值对数据进行排序：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">df.sort_values([<span class="string">'第二列'</span>, <span class="string">'第一列'</span>, <span class="string">'第三列'</span>], ascending = [<span class="literal">True</span>, <span class="literal">True</span>, <span class="literal">True</span>])</span><br><span class="line"><span class="comment">#使用df.sort_values(['第二列', '第一列', '第三列']).head()查看排序完后前几行的结果</span></span><br></pre></td></tr></table></figure><p>这里排序的规则是：根据设置的顺序（这里是先按第二列排），从小到大升序对所有数据进行排序。其中ascending是设置升序（默认True）和降序（False）。若仅选择单列，则无需添加<code>[]</code>，这里<code>[]</code>的作用是把选择的行列转换为列表。</p></li><li><h2 id="重命名"><a href="#重命名" class="headerlink" title="重命名"></a>重命名</h2><p>如果觉得我前面取得列名称不好听，可以使用下面这个代码来改成需要的名字：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">df.rename(columns = &#123;<span class="string">'第一列'</span>: <span class="string">'好听的第一列'</span>, <span class="string">'第二列'</span>: <span class="string">'好听的第二列'</span>, <span class="string">'第三列'</span>: <span class="string">'好听的第三列'</span>, <span class="string">'第四列'</span>: <span class="string">'好听的第四列'</span>,&#125;, inplace = <span class="literal">True</span>)</span><br></pre></td></tr></table></figure><p>这里用到了字典。</p></li><li><h2 id="索引"><a href="#索引" class="headerlink" title="索引"></a>索引</h2><p>前面提到了使用索引来查看第一行，可当没有数字索引，例如我们通过<code>df = pd.DataFrame(scores, index = [&#39;one&#39;, &#39;two&#39;, &#39;three&#39;])</code>把index设为one、two、three时，<code>df.loc[0]</code>就失效了。因此有下面几种处理方法：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#访问index为“one”的行</span></span><br><span class="line">df.loc[<span class="string">'one'</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment">#访问实实在在所谓的第几行（无论index为何）</span></span><br><span class="line">df.iloc[<span class="number">0</span>] <span class="comment">#注意0指的是第一行</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#ix合并了loc和iloc的功能，当索引为数字索引的时候，ix和loc是等价的</span></span><br><span class="line">df.ix[<span class="number">0</span>] <span class="comment">#访问第一行</span></span><br><span class="line">df.ix[<span class="string">'one'</span>] <span class="comment">#访问“one”行，这里也指的是第一行</span></span><br></pre></td></tr></table></figure></li><li><h2 id="切片"><a href="#切片" class="headerlink" title="切片"></a>切片</h2><p>类似的，DataFrame也支持切片操作，但还是需要注意的。<br>这里总结两种切片方式：</p><ol><li><h3 id="利用索引"><a href="#利用索引" class="headerlink" title="利用索引"></a>利用索引</h3>即使用<code>df.loc[:2]</code>or<code>df.ix[:2]</code>等索引方式，这里这样的话输出为前三行。</li><li><h3 id="直接切片"><a href="#直接切片" class="headerlink" title="直接切片"></a>直接切片</h3>这种方法只能在访问多行数据时使用，例如<code>df[:2]</code>将输出前两行，注意，这里比上面的方法要少一行。此外，值得强调的是，用这种方法访问单行数据是禁止的，例如不能使用<code>df[0]</code>来访问第一行数据。</li></ol></li><li><h2 id="插入"><a href="#插入" class="headerlink" title="插入"></a>插入</h2><p>上面的索引还有一种用途，就是可以用于插入指定index的新行。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">df.loc[<span class="string">'new_index'</span>] = [<span class="string">'one'</span>, <span class="string">'piece'</span>, <span class="string">'is'</span>, <span class="string">'true'</span>]</span><br></pre></td></tr></table></figure></li><li><h2 id="删除"><a href="#删除" class="headerlink" title="删除"></a>删除</h2><p>上面插入的那行中我说了“大秘宝是真实存在的”（海贼迷懂），下面我想把这句话所在的行删了，可以使用<code>df.dtop()</code>来完成。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">df = df.drop(<span class="string">'new_index'</span>)</span><br></pre></td></tr></table></figure></li><li><h2 id="数组"><a href="#数组" class="headerlink" title="数组"></a>数组</h2><p>我们可以使用<code>df.第一列.values</code>以array的形式输出指定列的所用值。<br>基于此，我们可以使用<code>df.第一列.value_counts()</code>来做简单的统计，也就是对该列中每一个出现数字作频次的统计。<br>我们还可以直接对DataFrame做计算，例如：<code>df * n</code>（n为具体数值），结果就是对表中的每一个数值都乘上对应的倍数。</p></li><li><h2 id="元素操作"><a href="#元素操作" class="headerlink" title="元素操作"></a>元素操作</h2><ol><li><h3 id="map函数"><a href="#map函数" class="headerlink" title="map函数"></a>map函数</h3><p>map()是python自带的方法, 可以对DataFrame某列内的元素进行操作。<br>下面是一种使用实例：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">func</span><span class="params">(grade)</span>:</span></span><br><span class="line"><span class="keyword">if</span> grade &gt;= <span class="number">80</span>:</span><br><span class="line">    <span class="keyword">return</span> <span class="string">"A"</span></span><br><span class="line"><span class="keyword">elif</span> grade &gt;= <span class="number">70</span>:</span><br><span class="line">    <span class="keyword">return</span> <span class="string">"B"</span></span><br><span class="line"><span class="keyword">elif</span> grade &gt;= <span class="number">60</span>:</span><br><span class="line">    <span class="keyword">return</span> <span class="string">"C"</span></span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">    <span class="keyword">return</span> <span class="string">"D"</span></span><br><span class="line"></span><br><span class="line">df[<span class="string">'评级'</span>] = df.第一列.map(func)</span><br></pre></td></tr></table></figure><p>这样DataFrame后面会自动添加一列名为“评级”，并根据第一列来生成数据填入。</p></li><li><h3 id="apply函数"><a href="#apply函数" class="headerlink" title="apply函数"></a>apply函数</h3><p>当我们需要进行根据多列生成新的一个列的操作时，就需要用到apply。其用法简单示例如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">df[<span class="string">'求和'</span>] = df.apply(<span class="keyword">lambda</span> x: x.第一列 + x.第二列, axis = <span class="number">1</span>)</span><br></pre></td></tr></table></figure></li><li><h3 id="applymap函数"><a href="#applymap函数" class="headerlink" title="applymap函数"></a>applymap函数</h3><p>applymap时对dataframe中所有的数据进行操作的一个函数，非常重要。例如，我要让之前所用的score和grade都变成scroe+或者grade+，那么我就可以这样：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">df.applymap(<span class="keyword">lambda</span> x: str(x) + <span class="string">'+'</span>)</span><br></pre></td></tr></table></figure><p>如果是成绩单的话，那么这样操作之后打出来就会好看些啦，哈哈。</p></li></ol></li><li><h2 id="plot"><a href="#plot" class="headerlink" title="plot"></a>plot</h2>数据可视化本来是一个非常复杂的过程，但Pandas数据帧plot函数的出现，使得创建可视化图形变得很容易。<br>这个函数的具体使用可以访问文首给出的第三个参考链接，为一个印度小哥利用kaggle上的<a href="https://www.kaggle.com/PromptCloudHQ/world-happiness-report-2019/version/1" target="_blank">数据</a>对<code>df.plot()</code>做的一个非常详尽的介绍。<br>有机会的话我会结合matplotlib对其做一个搬运与总结，先留个坑。</li><li><h2 id="统计"><a href="#统计" class="headerlink" title="统计"></a>统计</h2>我们可以使用<code>df.describe()</code>对数据进行快速统计汇总。输出将包括：count、mean、std、min、25%、50%、75%和max。<br>通过<code>df.mean()</code>我们可以按列求均值，如果想要按行求均值，可以使用<code>df.mean(1)</code>。</li><li><h2 id="高阶"><a href="#高阶" class="headerlink" title="高阶"></a>高阶</h2>此外还有使用<code>df.T</code>进行转置，<code>df.dropna(how = &#39;any&#39;)</code>删除所有具有缺失值的数据，<code>df.fillna(value = 5)</code>填充所有缺失数据等高阶用法。详细的可以查阅前面给的参考链接<a href="https://pandas.pydata.org/pandas-docs/stable/getting_started/10min.html" target="_blank">10 minutes to pandas</a>，至于更高阶的，可以看一下cookbook，不过一般还是在运用的过程中遇到需求再查找，一下子记不住那么多的。</li></ol><hr><h1 id="写入"><a href="#写入" class="headerlink" title="写入"></a>写入</h1><p>通过<code>to_csv()</code>可以将Pandas数据写入到文本文件中，和读取<code>read_csv()</code>类似，它也有几个常用参数：</p><ol><li><strong>path_or_buf</strong>：表示路径的字符串或者文件句柄，也是必需的。例如：<code>df.to_csv(abs_path)</code>。要注意的是，这里如果abs_path对应的文件不存在，则会新建abs_path的同名文件后再写入，如果本来已存在该文件，则会自动清空该文件后再写入。</li><li><strong>sep</strong>：分隔符，默认为逗号。当写入txt文件时，就需要这个参数来确定数据之间的分隔符了。</li><li><strong>header</strong>：元素为字符串的列表或布尔型数据。当为列表时表示重新指定列名，当为布尔型时，表示是否写入列名。这和读取时的使用基本类似。</li><li><strong>columns</strong>：后接一个列表，用于重新指定写入文件中列的顺序。例如：<code>df.to_csv(abs_path, columns = [&#39;第四列&#39;, &#39;第二列&#39;, &#39;第三列&#39;, &#39;第一列&#39;])</code>。</li><li><strong>index_label</strong>：字符串或布尔型变量，设置索引列列名。原本的索引是空的，使用这个参数就可以给索引添加一个列名。如果觉得不需要添加，同时空着不好看（空的话还是会有分隔符），可以设置为False去掉（同时也将不显示分隔符）。</li><li><strong>index</strong>：布尔型，是否写入索引列，默认为True。</li><li><strong>encoding</strong>：写入时所用的编码，默认是utf-8。这个和上述的许多参数其实保持默认即可。</li></ol><hr><h1 id="匿名函数"><a href="#匿名函数" class="headerlink" title="匿名函数"></a>匿名函数</h1><p>在上面DataFrame一节的最后，我用到了两个匿名函数，这里我想举个例子来简单展示一下匿名函数的使用方法，的确很好用！<br>当我们对一个数进行操作时，若使用函数，一般会：<br></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">func</span><span class="params">(number)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> number + <span class="number">10</span></span><br></pre></td></tr></table></figure><p></p><p>这样看上去就有点费代码了，因此有下面的等价匿名函数可以替代：<br></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">func = <span class="keyword">lambda</span> number: number + <span class="number">10</span></span><br></pre></td></tr></table></figure><p></p><p>当然假如想追求代码行数的话也不拦着你~</p><hr><h1 id="推荐"><a href="#推荐" class="headerlink" title="推荐"></a>推荐</h1><p>最近看到了DataWhale的一篇<a href="https://mp.weixin.qq.com/s/XmnWvnMNobuF-92vbppNbQ" target="_blank">文章</a>，也总结的挺好，在这里推荐一下。</p><script type="text/javascript" src="/js/src/bai.js"></script><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      
      
        &lt;!-- build time:Tue Jan 28 2020 15:40:41 GMT+0800 (GMT+08:00) --&gt;&lt;p&gt;在python中，Pandas可以说是最实用的库之一，它提供了非常丰富的数据读写方法。可以看一下Pandas中文网提供的&lt;a href=&quot;htt
      
    
    </summary>
    
    
      <category term="程序与设计" scheme="https://gsy00517.github.io/categories/%E7%A8%8B%E5%BA%8F%E4%B8%8E%E8%AE%BE%E8%AE%A1/"/>
    
    
      <category term="代码实现" scheme="https://gsy00517.github.io/tags/%E4%BB%A3%E7%A0%81%E5%AE%9E%E7%8E%B0/"/>
    
      <category term="python" scheme="https://gsy00517.github.io/tags/python/"/>
    
  </entry>
  
  <entry>
    <title>deep learning笔记：记首次ResNet实战</title>
    <link href="https://gsy00517.github.io/deep-learning20200113174731/"/>
    <id>https://gsy00517.github.io/deep-learning20200113174731/</id>
    <published>2020-01-13T09:47:31.000Z</published>
    <updated>2020-01-19T00:23:36.612Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Tue Jan 28 2020 15:40:41 GMT+0800 (GMT+08:00) --><p>实践出真知。在之前的博文<a href="https://gsy00517.github.io/deep-learning20191001184216/" target="_blank">deep-learning笔记：使网络能够更深——ResNet简介与pytorch实现</a>中，我对何凯明大神的CVPR最佳论文中提出的残差网络做了简单介绍。而就在第二年（2016年），何凯明的团队就发表了<a href="1603.05027.pdf" target="_blank">“Identity Mappings in Deep Residual Networks”</a>这篇文章，分析了ResNet成功的关键因素——residual block背后的算法，并对residual block以及after-addition activation进行改进，通过一系列的ablation experiments验证了，在residual block和after-addition activation上都使用identity mapping（恒等映射）时，能对模型训练产生很好的效果。不知道为什么，我今天从arXiv上download这篇paper的时候发现上不去了，莫非现在上arXiv也要科学上网了？<br>本次实战主要是基于之前的ResNet实现和flyAI平台，并结合上面提到的何凯明团队分析ResNet的论文做出一些改进，并检验效果。</p><p><strong>References</strong>：</p><p>电子文献：<br><a href="https://blog.csdn.net/Sandwichsauce/article/details/89162570" target="_blank" rel="noopener">https://blog.csdn.net/Sandwichsauce/article/details/89162570</a><br><a href="https://www.jianshu.com/p/184799230f20" target="_blank" rel="noopener">https://www.jianshu.com/p/184799230f20</a><br><a href="https://blog.csdn.net/wspba/article/details/60572886" target="_blank" rel="noopener">https://blog.csdn.net/wspba/article/details/60572886</a><br><a href="https://www.cnblogs.com/4991tcl/p/10395574.html" target="_blank" rel="noopener">https://www.cnblogs.com/4991tcl/p/10395574.html</a><br><a href="https://blog.csdn.net/DuinoDu/article/details/80435127" target="_blank" rel="noopener">https://blog.csdn.net/DuinoDu/article/details/80435127</a></p><p>参考文献：<br>[1]Identity Mappings in Deep Residual Networks</p><hr><h1 id="ablation-experiments"><a href="#ablation-experiments" class="headerlink" title="ablation experiments"></a>ablation experiments</h1><p>在上面我提到了这个名词，中文翻译是“消融实验”。或许在阅读论文的过程中会接触到这个名词，如果仅根据字面翻译的话或许会很纳闷。<br>在查找了一定的资料后，我对这种方法有了大致地了解。<br>ablation的原本释义是通过机械方法切除身体组织，如手术，从身体中去除尤指器官以及异常生长的有害物质。<br>事实上，这种方法类似于物理实验中的控制变量法，即当在一个新提出的模型中同时改变了多个条件或者参数，那么为了分析和检验，在接下去的消融实验中，会一一控制每个条件或者参数不变，来根据结果分析到底是哪个条件或者参数对模型的优化、影响更大。<br>在机器学习、特别是复杂的深度神经网络的背景下，科研工作者们已经采用“消融研究”来描述去除网络的某些部分的过程，以便更好地理解网络的行为。</p><hr><h1 id="ResNet的分析与改进"><a href="#ResNet的分析与改进" class="headerlink" title="ResNet的分析与改进"></a>ResNet的分析与改进</h1><ol><li><h2 id="残差单元"><a href="#残差单元" class="headerlink" title="残差单元"></a>残差单元</h2>在2015年ResNet首次发布的时候，设计的残差单元在最后的输出之前是要经过一个激活函数的。而在2016年新提出的残差单元中，去掉了这个激活函数，并通过实验证明新提出的残差单元训练更简单。<img src="/deep-learning20200113174731/新的残差单元.png" title="新的残差单元"> 这种新的构造的关键在于不仅仅是在残差单元的内部，而是在整个网络中创建一个“直接”的计算传播路径来分析深度残差网络。通过构造这样一个“干净”的信息通路，可以在前向和反向传播阶段，使信号能够直接的从一个单元传递到其他任意一个单元。实验表明，当框架接近于上面的状态时，训练会变得更加简单。</li><li><h2 id="shortcut"><a href="#shortcut" class="headerlink" title="shortcut"></a>shortcut</h2>对于恒等跳跃连接$h(x_{l})=x_{l}$，作者设计了5种新的连接方式来与原本的方式作对比，设计以及实验结果如下所示：<img src="/deep-learning20200113174731/恒等跳跃连接设计方案.png" title="恒等跳跃连接设计方案"> <img src="/deep-learning20200113174731/恒等跳跃连接实验结果.png" title="恒等跳跃连接实验结果"> 其中fail表示测试误差超过了20%。实验结果表明，原本的连接方式误差衰减最快，同时误差也最低，而其他形式的shortcut都产生了较大的损失和误差。<br>作者认为，shortcut连接中的操作 (缩放、门控、1×1的卷积以及dropout) 会阻碍信息的传递，以致于对优化造成困难。<br>此外，虽然1×1的卷积shortcut连接引入了更多的参数，本应该比恒等shortcut连接具有更加强大的表达能力。但是它的效果并不好，这表明了这些模型退化问题的原因是优化问题，而不是表达能力的问题。</li><li><h2 id="激活函数"><a href="#激活函数" class="headerlink" title="激活函数"></a>激活函数</h2>对于激活函数的设置，作者设计了如下几种方式进行比较：<img src="/deep-learning20200113174731/激活函数设置.png" title="激活函数设置"> 在这里，作者将激活项分为了预激活（pre-activation）和后激活（post-activation）。通过实验可以发现，将ReLU和BN都放在预激活中，即full pre-activation最为有效。</li></ol><hr><h1 id="ResNet实战"><a href="#ResNet实战" class="headerlink" title="ResNet实战"></a>ResNet实战</h1><p>根据论文中的实验结果，我使用了新的残差模块进行实践。并结合在<a href="https://gsy00517.github.io/deep-learning20191001151454/" target="_blank">deep-learning笔记：学习率衰减与批归一化</a>中的分析总结对BN层的位置选取作了简单调整。在本次实验中，我尝试使用了StepLR阶梯式衰减和连续衰减两种学习率衰减方式，事实证明，使用StepLR阶梯式衰减的效果在这里要略好一些（连续衰减前期学得太快，后面大半部分都学不动了…）。<br>首次训练的结果并不理想，于是我加大了学习率每次衰减的幅度，即让最后阶段的学习率更小，这使我的模型的评分提高了不少。<br>由于训练资源有限，我没能进行更深（仅设置了10层左右）、更久（每次仅进行20个epoch）的训练，但在每个batch中，最高的accuracy也能达到65%左右，平均大约能超过50%。相比之前使用浅层网络仅能达到20%左右的accuracy，这已经提升不少了。然而最终的打分还是没有显著提高，因此我思考是否存在过拟合的问题。为此我尝试着在全连接层和捷径连接中加入dropout正则化来提高在测试集中的泛化能力，结果最终打分仅提高了0.1，而训练时间稍短。由于我除了dropout之外并没有改变网络的层数等影响参数量的因素，因此似乎与何大神在论文中original版和dropout版shortchut的比较有一些矛盾，但的确还是说明了dropout在这里的作用微乎其微，优化模型时可以排除至考虑范围之外了。</p><hr><h1 id="遇到的问题"><a href="#遇到的问题" class="headerlink" title="遇到的问题"></a>遇到的问题</h1><ol><li><h2 id="TabError-inconsistent-use-of-tabs-and-spaces-in-indentation"><a href="#TabError-inconsistent-use-of-tabs-and-spaces-in-indentation" class="headerlink" title="TabError: inconsistent use of tabs and spaces in indentation"></a>TabError: inconsistent use of tabs and spaces in indentation</h2>当我在flyAI提供的窗口中修改代码并提交GPU训练时，就出现了这个报错。它说我在缩进时错误的使用了制表符和空格。于是我只好把报错处的缩进删除并重敲tab缩进，问题就得到了解决。<br>如果使用PyCharm等IDE的话，这个错误会直接显示出来，即在缩进处会有灰色的颜色警告，将光标移过去就会有具体报错。这就省得提交GPU之后才能收到报错，所以以后写代码、改代码能用IDE还是用起来好啦。</li><li><h2 id="RuntimeError-one-of-the-variables-needed-for-gradient-computation-has-been-modified-by-an-inplace-operation"><a href="#RuntimeError-one-of-the-variables-needed-for-gradient-computation-has-been-modified-by-an-inplace-operation" class="headerlink" title="RuntimeError: one of the variables needed for gradient computation has been modified by an inplace operation"></a>RuntimeError: one of the variables needed for gradient computation has been modified by an inplace operation</h2>这是在shortcut残差连接时遇到的一个报错，上网后发现原因很简单：版本问题。在新版的pytorch中，由于0.4.0之后把Varible和Tensor融合为一个Tensor，因此inplace操作在之前对Varible时还能用，但现在只有Tensor，就会出错了。<br>解决的办法是将<code>x += self.shortcut(x1)</code>替换成<code>x = x + self.shortcut(x1)</code>。<br>若网络很大，找起来很麻烦，可以在网络的中间变量加一句<code>x.backward()</code>，看会不会报错，如果不会的话，那就说明至少这之前是没毛病的。</li><li><h2 id="张量第一维是batch-size"><a href="#张量第一维是batch-size" class="headerlink" title="张量第一维是batch size"></a>张量第一维是batch size</h2><p>起初，我根据输入的torch.Size([64, 1, 128, 128])，使用如下函数将输出拍平成1维的：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">num_flat_features</span><span class="params">(self, x)</span>:</span></span><br><span class="line">    size = x.size()[<span class="number">0</span>:]</span><br><span class="line">    num_features = <span class="number">1</span></span><br><span class="line">    <span class="keyword">for</span> s <span class="keyword">in</span> size:</span><br><span class="line">        num_features *= s</span><br><span class="line">    <span class="keyword">return</span> num_features</span><br></pre></td></tr></table></figure><p>同时，为了匹配，我将第一个全连接层的输入乘上了64。其实这个时候我已经开始怀疑这个64是哪来的了，为什么这个张量第一维尺度有64。<br>直到后来平台报错，我才意识到这个表示的不是数据的维度，而是我设计的batch size。<br>为此我将上面的代码调整如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">num_flat_features</span><span class="params">(self, x)</span>:</span></span><br><span class="line">    size = x.size()[<span class="number">1</span>:]</span><br><span class="line">    num_features = <span class="number">1</span></span><br><span class="line">    <span class="keyword">for</span> s <span class="keyword">in</span> size:</span><br><span class="line">        num_features *= s</span><br><span class="line">    <span class="keyword">return</span> num_features</span><br></pre></td></tr></table></figure><p>如此，问题得到解决，最终的输出应该是batch size乘上总类别数的一个张量。</p></li></ol><hr><h1 id="arXiv"><a href="#arXiv" class="headerlink" title="arXiv"></a>arXiv</h1><p>文前提到了上arXiv下论文要科学上网的事情，后来我发现了一个中科院理论物理所的一个<a href="http://xxx.itp.ac.cn/" target="_blank">备选镜像</a>，但是好像不是特别稳定，不过还是先留在这里吧，万一的话可以拿来试试。<br>一般一些科研工作者会在论文发布之前上传到arXiv以防止自己的idea被别人用了。估计主要是为了防止类似牛顿莱布尼兹之争这种事吧。</p><script type="text/javascript" src="/js/src/bai.js"></script><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      
      
        &lt;!-- build time:Tue Jan 28 2020 15:40:41 GMT+0800 (GMT+08:00) --&gt;&lt;p&gt;实践出真知。在之前的博文&lt;a href=&quot;https://gsy00517.github.io/deep-learning20191001184
      
    
    </summary>
    
    
      <category term="人工智能" scheme="https://gsy00517.github.io/categories/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/"/>
    
    
      <category term="深度学习" scheme="https://gsy00517.github.io/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="论文分享" scheme="https://gsy00517.github.io/tags/%E8%AE%BA%E6%96%87%E5%88%86%E4%BA%AB/"/>
    
      <category term="踩坑血泪" scheme="https://gsy00517.github.io/tags/%E8%B8%A9%E5%9D%91%E8%A1%80%E6%B3%AA/"/>
    
  </entry>
  
  <entry>
    <title>python笔记：打印进度</title>
    <link href="https://gsy00517.github.io/python20200108214052/"/>
    <id>https://gsy00517.github.io/python20200108214052/</id>
    <published>2020-01-08T13:40:52.000Z</published>
    <updated>2020-01-19T00:26:17.886Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Tue Jan 28 2020 15:40:41 GMT+0800 (GMT+08:00) --><p>在我们训练模型的时候，我们总希望能够直接看到训练的进度，下面我就总结几个我收集的打印进度的方法。</p><p><strong>References</strong>：</p><p>电子文献：<br><a href="https://blog.csdn.net/u013985241/article/details/86653356" target="_blank" rel="noopener">https://blog.csdn.net/u013985241/article/details/86653356</a><br><a href="https://blog.csdn.net/zkp_987/article/details/81748098" target="_blank" rel="noopener">https://blog.csdn.net/zkp_987/article/details/81748098</a></p><hr><h1 id="利用回车符"><a href="#利用回车符" class="headerlink" title="利用回车符"></a>利用回车符</h1><p>打印百分比应该是最常见的方法，也是我一直使用的。不过如果简单地逐次打印百分比的话，就会占据大量的屏幕空间，甚至装不下而需要手动拖动滚动条，让人眼花缭乱。这时我就想到了利用转义符“\r”，在print完本次的进度之后，下一次直接回车将其清除覆盖，这样就达到了既不占用屏幕又清晰的目的。<br>大致的方法如下：<br></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> time <span class="comment">#这里是为了用来延时，代替训练的时间</span></span><br><span class="line">numOfTimes = <span class="number">200</span> <span class="comment">#总循环次数，可以是总训练数据量等，这里设为200</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(numOfTimes):</span><br><span class="line">    print(<span class="string">"\r"</span>, <span class="string">"progress percentage:&#123;0&#125;%"</span>.format((round(i + <span class="number">1</span>) * <span class="number">100</span> / numOfTimes)), end = <span class="string">""</span>, flush = <span class="literal">True</span>)</span><br><span class="line">    time.sleep(<span class="number">0.02</span>) <span class="comment">#若前面from time import sleep，这里直接sleep(0.02)即可</span></span><br></pre></td></tr></table></figure><p></p><p>这里用到了python的format格式化函数，format中计算出的数值对应的位置是{0}，将在实际print的过程中被替换。<br>此外，这里还用到了round()函数，其作用是返回浮点数的四舍五入值。<br>关于上面在print()函数中出现的flush，文首的参考链接中已给出解释，这里做个搬运：<br>因为print()函数会把内容放到内存中，内存中的内容并不一定能够及时刷新显示到屏幕中。而当我们使用<code>flush = True</code>之后，会在print结束之后，立即将内存中的东西显示到屏幕上，清空缓存。<br>基于上述原理，flush大致有下面两个使用场景：</p><ol><li>在循环中，要想每进行一次循环体，在屏幕上更新打印的内容就得使用flush = True的参数。（我这里就是这种情况）</li><li>打开一个文件，向其写入字符串，在关闭文件f.close()之前 打开文件是看不到写入的字符的。因此，如果要想在关闭之前实时地看到写入的字符串，那么就应该使用<code>flush = True</code>。</li></ol><hr><h1 id="利用tqdm库"><a href="#利用tqdm库" class="headerlink" title="利用tqdm库"></a>利用tqdm库</h1><p>有需求就有市场，一搜果然还是有库能满足我的需求的。tqdm就是其中之一，它是一个快速，可扩展的python进度条，可以在python长循环中添加一个进度提示信息。<br>大致用法如下：<br></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tqdm</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line">numOfTimes = <span class="number">200</span> <span class="comment">#总循环次数，可以是总训练数据量等，这里设为200</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> tqdm.tqdm(range(numOfTimes)):</span><br><span class="line">    time.sleep(<span class="number">0.02</span>) <span class="comment">#代替训练等耗时过程</span></span><br><span class="line">    <span class="keyword">pass</span></span><br></pre></td></tr></table></figure><p></p><p>也可以直接<code>from tqdm import tqdm</code>，这样后面就不需要<code>tqdm.tqdm</code>了。</p><hr><h1 id="利用progressbar"><a href="#利用progressbar" class="headerlink" title="利用progressbar"></a>利用progressbar</h1><p>库如其名，这个库就是用来做进度条的。如果没有的话，它和tqdm都可以使用pip来安装。<br></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> progressbar</span><br><span class="line"><span class="keyword">from</span> time <span class="keyword">import</span> sleep</span><br><span class="line">numOfTimes = <span class="number">200</span> <span class="comment">#总循环次数，可以是总训练数据量等，这里设为200</span></span><br><span class="line">progress = progressbar.ProgressBar()</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> progress(range(numOfTimes)):</span><br><span class="line">    sleep(<span class="number">0.02</span>)</span><br></pre></td></tr></table></figure><p></p><script type="text/javascript" src="/js/src/bai.js"></script><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      
      
        &lt;!-- build time:Tue Jan 28 2020 15:40:41 GMT+0800 (GMT+08:00) --&gt;&lt;p&gt;在我们训练模型的时候，我们总希望能够直接看到训练的进度，下面我就总结几个我收集的打印进度的方法。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Reference
      
    
    </summary>
    
    
      <category term="程序与设计" scheme="https://gsy00517.github.io/categories/%E7%A8%8B%E5%BA%8F%E4%B8%8E%E8%AE%BE%E8%AE%A1/"/>
    
    
      <category term="代码实现" scheme="https://gsy00517.github.io/tags/%E4%BB%A3%E7%A0%81%E5%AE%9E%E7%8E%B0/"/>
    
      <category term="python" scheme="https://gsy00517.github.io/tags/python/"/>
    
  </entry>
  
  <entry>
    <title>circuit笔记：有线双工对讲机的电子线路设计</title>
    <link href="https://gsy00517.github.io/circuit20200107222439/"/>
    <id>https://gsy00517.github.io/circuit20200107222439/</id>
    <published>2020-01-07T14:24:39.000Z</published>
    <updated>2020-01-19T00:23:09.048Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Tue Jan 28 2020 15:40:41 GMT+0800 (GMT+08:00) --><p>模拟电路实验是我进入大学本科以来第一个付出大量课外时间的实验课，其最后的大项目是让我们自行设计一个电路，而我们小组选择的是有线双工对讲机。今天我就简单对我们小组的设计方案做一个整理。在这里还是非常感谢组员们的共同努力和巨大帮助！</p><p><strong>References</strong>：</p><p>电子文献：<br><a href="http://www.ttic.cc/file/TDA1013B_76329.html.com" target="_blank" rel="noopener">http://www.ttic.cc/file/TDA1013B_76329.html.com</a><br><a href="https://wenku.baidu.com/view/c8b016e7ed630b1c58eeb520.html" target="_blank" rel="noopener">https://wenku.baidu.com/view/c8b016e7ed630b1c58eeb520.html</a><br><a href="https://tech.hqew.com/fangan_1909806" target="_blank" rel="noopener">https://tech.hqew.com/fangan_1909806</a></p><hr><h1 id="设计目的"><a href="#设计目的" class="headerlink" title="设计目的"></a>设计目的</h1><p>有线对讲机是用导线直接连接进行通话，而双工通信则是像电话机一样同时进行双方的“听”和“讲”。此外，我们希望可以具有音量可调节、消侧音等一些对讲机需要的功能。</p><hr><h1 id="设计思路"><a href="#设计思路" class="headerlink" title="设计思路"></a>设计思路</h1><ol><li>利用驻极体话筒将声音信号转化为微弱的电信号。</li><li>通过反相比例放大器将微弱的电信号放大。</li><li>利用相位抵消法实现消侧音。</li><li>只使用一根传输线进行信号互传。</li><li>利用电压跟随器避免远距离导线传输时衰减过大的问题。</li><li>添加了低通滤波器电路，滤去高频的噪音信号。</li><li>使用TDA1013B进行功率放大并将信号传输到扬声器。</li><li>最后由扬声器将电信号转化成声音信号，发出声音。</li></ol><p>设计有线双工对讲机的思路可以用如下所示的系统图表示。主要由弱声音采集、前置运算放大器、消侧音电路、减小信号衰减电路、低通滤波器电路、功率放大电路、扬声器等模块组成。<br><img src="/circuit20200107222439/系统图.png" title="系统图"></p><hr><h1 id="基本原理"><a href="#基本原理" class="headerlink" title="基本原理"></a>基本原理</h1><ol><li><h2 id="驻极体话筒"><a href="#驻极体话筒" class="headerlink" title="驻极体话筒"></a>驻极体话筒</h2><img src="/circuit20200107222439/驻极体话筒.jpg" title="驻极体话筒"> 话筒的基本结构由一片单面涂有金属的驻极体薄膜与一个上面有若干小孔的金属电极（背称为背电极）构成。驻极体面与背电极相对，中间有一个极小的空气隙，形成一个以空气隙和驻极体作绝缘介质，以背电极和驻极体上的金属层作为两个电极构成一个平板电容器。电容的两极之间有输出电极。<br>由于驻极体薄膜上分布有自由电荷，当声波引起驻极体薄膜振动而产生位移时；改变了电容两极板之间的距离，从而引起电容的容量发生变化，由于驻极体上的电荷数始终保持恒定，根据公式：Q =CU 所以当C变化时必然引起电容器两端电压U的变化，从而输出电信号，实现声电的变换。<br>不管是源极输出或漏极输出，驻极体话筒必须提供直流电压才能工作，因为它内部装有场效应管。</li><li><h2 id="反向比例放大"><a href="#反向比例放大" class="headerlink" title="反向比例放大"></a>反向比例放大</h2><img src="/circuit20200107222439/反向比例放大.png" title="反向比例放大"> 相比例放大电路的原理如上图所示，输出信号电压增益为R2与R1之比，相位反相变化180°（后面会再次反相）。在本实验中，我们将两个电阻的比值调整在10~100之间，即放大比例为10~100。</li><li><h2 id="消侧音"><a href="#消侧音" class="headerlink" title="消侧音"></a>消侧音</h2>在模拟音频收发信号共用一个信道的对讲系统中，为减小侧音对通话效果的影响，防止侧音的干扰，所有对讲设备均需增加消侧音电路。一方面让音频发送信号按一定比例出现在传输线上，另一方面让本方音频接收电路获得的信号足够小，不至于说话者从己方喇叭听到自己的声音，提高通话的质量。 <img src="/circuit20200107222439/串联分压电路.png" title="串联分压电路"> 在上图所示的串联分压电路中，R1、R2为纯电阻，v1、v2为输入电压，vo为输出电压，据叠加定理：<script type="math/tex;mode=display">v_{o}=\frac{v_{1}R_{2}+v_{2}R_{1}}{R_{1}+R_{2}}</script>令vo=0，则v1R2+v2R1=0，即：<script type="math/tex;mode=display">\frac{v_{1}}{v_{2}}=-\frac{R_{1}}{R_{2}}</script>特别地，当R1=R2时，v1=-v2。<br>由上式可见，欲使vo=0，v1，v2须满足2个条件：<ol><li>每个频率分量的相位相反。</li><li>每个频率分量幅度呈一定比例且比例相同。<br>下面是该方法的一种实现方式的电路图：<img src="/circuit20200107222439/实现电路图.png" title="实现电路图"> 根据文首所列的参考资料，我们找到了另一种原理相同且成本更低的实现方式，如下图所示。三极管发射极和集电极的信号反相且幅度相同，可以相叠加后将信号消去，一个三极管的作用相当于上述方法中的U1和U2。图中三级管的偏置电路没有画出，C1和C2将直流分量同传输线隔离开。需要特别提出的是，如果可调电阻P1足够大，从而对三极管的偏置影响足够小，可将C2去掉，可调电阻P1直接和三极管的c，e极并联。<img src="/circuit20200107222439/优化方案.png" title="优化方案"> 需要注意的是，在这里传送到对方的信号的相位再次反相，与原始信号相一致。<br>实验中，三极管采用9013，电位器采用104。为了使三极管正常工作，在三极管B端由R1和R3分压，使三极管的静态工作点VCQ≥4.5V，则令VEQ=VCC-VCQ，需要VR2=VR4，因此选R2=R4=1kΩ，则由IEQ=ICQ得VR2=VR4。由于有VBE=0.7V，则VBQ=VEQ+0.7，VBQ+VR1=VCC=12V。此时若ICQ=4mA，经计算，则R1与R3之比约为1.5。我们最终选取R1=6.7kΩ，R3=4.7kΩ来分压。<img src="/circuit20200107222439/采用电路图.png" title="采用电路图"></li></ol></li><li><h2 id="减小信号衰减"><a href="#减小信号衰减" class="headerlink" title="减小信号衰减"></a>减小信号衰减</h2>由于我们的目标是实现长导线远程传输信号，因此导线上的电阻是不可忽略的，这就导致了信号衰减的问题。在弱电的情况下，解决导线上信号衰减的方法有选取更优质的导线、改用电流信号输出、增大接收端的输入电阻等。<br>利用电压跟随器输入电阻高的特性，我们决定在信号接收的两端分别添加一个电压跟随器（如下图所示）来抑制信号传输的衰减。 <img src="/circuit20200107222439/电压跟随器.png" title="电压跟随器"></li><li><h2 id="低通滤波"><a href="#低通滤波" class="headerlink" title="低通滤波"></a>低通滤波</h2>人耳可以听到20HZ到20kHZ的音频信号，而人正常对话所发出的声音频率约为300HZ—3000HZ，频率较低，因此我们设计一个低通语音滤波器来滤除杂音，提高声音清晰度。<br>我们的目的是设计一个低频增益A0=2，Q≈1（品质因数，越小则通带或阻带越平坦，电路的稳定性越好）, fH=3kHz，图如下所示是一个二阶压控电压源低通滤波器。 <img src="/circuit20200107222439/低通滤波电路.png" title="低通滤波电路"> 根据该电路低频增益A0=K=1+R27/R28=2，可知R27=R28,因此我们选R27=R28=10kΩ。<br>根据fC=ωC/2π，当R25=R26=R，C10=C12=C时，有ωC=1/(RC)。<br>因此fC=1/2πRC。由所需上限截止频率fH为3kHz，我们选择C=0.01μF，算出R=1/(2πfC) ≈5.3kΩ。这里我们选用4.7kΩ的电阻，可实现近似的功能。</li><li><h2 id="使用TDA1013B功率放大"><a href="#使用TDA1013B功率放大" class="headerlink" title="使用TDA1013B功率放大"></a>使用TDA1013B功率放大</h2>在最后的输出之前，我们需要对信号进行功率的放大。在查阅相关资料之后，我们发现TDA1013B比较符合我们的功能需求。<br>TDA1013B是一个音频功率放大器集成电路，内部具有按对数曲线变化的直流音量控制电路，控制范围可达80dB，它具有很宽的电源电压范围（10V~40V），输出功率位4W~10W，是理想的音频功率放大器。<br>根据文首列出的数据手册，TDA1013B的伴音电路连接方式如下。 <img src="/circuit20200107222439/手册提供连接方式.png" title="手册提供连接方式"> 其中各个引脚的功能分别是：<br>1脚：电源地。<br>2脚：放大器输出，这里作伴音输出。<br>3脚：电源。<br>4脚：电源。<br>5脚：功放输入。<br>6脚：控制单元输出。<br>7脚：控制电压，这里可用于音量控制。<br>8脚：控制单元输入，这里输入音频。<br>9脚：信号地。<br>通过分析和查阅资料（见本文参考），我们确定了芯片的连接方式如下图所示。 <img src="/circuit20200107222439/最终采用连接方式.png" title="最终采用连接方式"></li></ol><hr><h1 id="设计实现"><a href="#设计实现" class="headerlink" title="设计实现"></a>设计实现</h1><p>如下是我们的仿真总电路图（还缺少最后的两级功率放大电路）。我们使用的是multisim14.0，由于软件的库中没有TDA1013B芯片，且声音信号难以在仿真软件中模拟，因此我们在仿真模拟阶段选择分模块检验功能的实现效果。<br><img src="/circuit20200107222439/总电路图.png" title="总电路图"><br>我们在第一级放大电路前后使用示波器检测放大效果，我们输入100mV峰峰值、1000Hz频率的交流信号，得到输出如下，其中通道A为放大之后的的信号，通道B显示的是放大之前的信号。<br><img src="/circuit20200107222439/放大效果.png" title="放大效果"><br>我们还检测了声音低通滤波器的功能实现情况，我们将对应的模块分离出来，利用波特测试仪画出该电路的波特图，结果如下所示。分别使用了对数和线性的横坐标轴（频率），且分别设扫描上限为100kHz和20kHz，由图易知，在3kHz左右处，增益开始下降，基本符合我们的设计要求。<br><img src="/circuit20200107222439/对数水平轴波特图.png" title="对数水平轴波特图"><br><img src="/circuit20200107222439/线性水平轴波特图.png" title="线性水平轴波特图"></p><script type="text/javascript" src="/js/src/bai.js"></script><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      
      
        &lt;!-- build time:Tue Jan 28 2020 15:40:41 GMT+0800 (GMT+08:00) --&gt;&lt;p&gt;模拟电路实验是我进入大学本科以来第一个付出大量课外时间的实验课，其最后的大项目是让我们自行设计一个电路，而我们小组选择的是有线双工对讲机。今天我
      
    
    </summary>
    
    
      <category term="程序与设计" scheme="https://gsy00517.github.io/categories/%E7%A8%8B%E5%BA%8F%E4%B8%8E%E8%AE%BE%E8%AE%A1/"/>
    
    
      <category term="模拟电路" scheme="https://gsy00517.github.io/tags/%E6%A8%A1%E6%8B%9F%E7%94%B5%E8%B7%AF/"/>
    
  </entry>
  
  <entry>
    <title>verilog笔记：频率计实现与verilog中timescale的解释</title>
    <link href="https://gsy00517.github.io/verilog20200107215404/"/>
    <id>https://gsy00517.github.io/verilog20200107215404/</id>
    <published>2020-01-07T13:54:04.000Z</published>
    <updated>2020-01-19T00:27:07.649Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Tue Jan 28 2020 15:40:41 GMT+0800 (GMT+08:00) --><p>之前每次创建完一个新的文件，文件最上方总会显示一行timescale。当初觉得没什么作用，删了之后依旧没有任何问题便没把它当回事，直到后来做频率计数器的时候我才决定一探究竟。那么这篇文章也就一并谈一下这个问题。</p><p><strong>References</strong>：</p><p>电子文献：<br><a href="https://blog.csdn.net/m0_37652453/article/details/90301902" target="_blank" rel="noopener">https://blog.csdn.net/m0_37652453/article/details/90301902</a><br><a href="https://blog.csdn.net/ciscomonkey/article/details/83661395" target="_blank" rel="noopener">https://blog.csdn.net/ciscomonkey/article/details/83661395</a><br><a href="https://blog.csdn.net/qq_16923717/article/details/81099833" target="_blank" rel="noopener">https://blog.csdn.net/qq_16923717/article/details/81099833</a></p><hr><h1 id="基本原理"><a href="#基本原理" class="headerlink" title="基本原理"></a>基本原理</h1><p>一个简易的频率计数器主要由分频器和计数器构成，其基本原理就是记录由分频器得到的一段时间内被测信号上升沿的个数，从而求得被测信号的频率。</p><hr><h1 id="控制信号转换模块"><a href="#控制信号转换模块" class="headerlink" title="控制信号转换模块"></a>控制信号转换模块</h1><figure class="highlight verilog"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">`<span class="meta-keyword">timescale</span> 1ns / 1ps //unit 1ns, precision 1ps</span></span><br><span class="line"><span class="keyword">module</span> control(</span><br><span class="line">    <span class="keyword">output</span> <span class="keyword">reg</span> Cnt_EN, <span class="comment">//enable the counter count, so that the counting period can be controled</span></span><br><span class="line">    <span class="keyword">output</span> <span class="keyword">wire</span> Cnt_CR, <span class="comment">//clear the counter every time when the measure begins</span></span><br><span class="line">    <span class="keyword">output</span> <span class="keyword">wire</span> Latch_Sig, <span class="comment">//at its posedge, the value of the counter will be stored/latched</span></span><br><span class="line">    <span class="keyword">input</span> nRST, <span class="comment">//system reset signal</span></span><br><span class="line">    <span class="keyword">input</span> CP <span class="comment">//1Hz standard clock signal</span></span><br><span class="line">    );</span><br><span class="line">    <span class="keyword">always</span> @ (<span class="keyword">posedge</span> CP <span class="keyword">or</span> <span class="keyword">negedge</span> nRST)</span><br><span class="line">        <span class="keyword">begin</span></span><br><span class="line">            <span class="keyword">if</span>(~nRST) <span class="comment">//generate enable counting signal</span></span><br><span class="line">                Cnt_EN = <span class="number">1'b0</span>; <span class="comment">//don't count</span></span><br><span class="line">            <span class="keyword">else</span></span><br><span class="line">                Cnt_EN = ~Cnt_EN; <span class="comment">//two frequency divider for the clock signal</span></span><br><span class="line">        <span class="keyword">end</span></span><br><span class="line">    <span class="keyword">assign</span> Latch_Sig = ~Cnt_EN; <span class="comment">//generate latch signal</span></span><br><span class="line">    <span class="keyword">assign</span> Cnt_CR = nRST &amp; (~CP &amp; Latch_Sig); <span class="comment">//generate the clear signal for the counter</span></span><br><span class="line"><span class="keyword">endmodule</span></span><br></pre></td></tr></table></figure><hr><h1 id="计数模块"><a href="#计数模块" class="headerlink" title="计数模块"></a>计数模块</h1><figure class="highlight verilog"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">`<span class="meta-keyword">timescale</span> 1ns / 1ps //unit 1ns, precision 1ps</span></span><br><span class="line"><span class="keyword">module</span> counter(</span><br><span class="line">    <span class="keyword">output</span> <span class="keyword">reg</span> [<span class="number">3</span>:<span class="number">0</span>] Q,</span><br><span class="line">    <span class="keyword">input</span> CR, EN, CP</span><br><span class="line">    );</span><br><span class="line">    <span class="keyword">always</span> @ (<span class="keyword">posedge</span> CP <span class="keyword">or</span> <span class="keyword">posedge</span> CR)</span><br><span class="line">        <span class="keyword">begin</span></span><br><span class="line">            <span class="keyword">if</span>(CR)</span><br><span class="line">                Q &lt;= <span class="number">4'b0000</span>; <span class="comment">//reset to zero</span></span><br><span class="line">            <span class="keyword">else</span> <span class="keyword">if</span>(~EN)</span><br><span class="line">                Q &lt;= Q; <span class="comment">//stop counting</span></span><br><span class="line">            <span class="keyword">else</span> <span class="keyword">if</span>(Q == <span class="number">4'b1001</span>)</span><br><span class="line">                Q &lt;= <span class="number">4'b0000</span>;</span><br><span class="line">            <span class="keyword">else</span></span><br><span class="line">                Q &lt;= Q + <span class="number">1'b1</span>; <span class="comment">//counting, plus one</span></span><br><span class="line">        <span class="keyword">end</span></span><br><span class="line"><span class="keyword">endmodule</span></span><br></pre></td></tr></table></figure><hr><h1 id="寄存模块"><a href="#寄存模块" class="headerlink" title="寄存模块"></a>寄存模块</h1><figure class="highlight verilog"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">`<span class="meta-keyword">timescale</span> 1ns / 1ps //unit 1ns, precision 1ps</span></span><br><span class="line"><span class="keyword">module</span> Latch(</span><br><span class="line">    <span class="keyword">output</span> <span class="keyword">reg</span> [<span class="number">15</span>:<span class="number">0</span>] Qout,</span><br><span class="line">    <span class="keyword">input</span> [<span class="number">15</span>:<span class="number">0</span>] Din, </span><br><span class="line">    <span class="keyword">input</span> Load, CR</span><br><span class="line">    );</span><br><span class="line">    <span class="keyword">always</span> @ (<span class="keyword">posedge</span> Load <span class="keyword">or</span> <span class="keyword">posedge</span> CR)</span><br><span class="line">        <span class="keyword">if</span>(CR)</span><br><span class="line">            Qout &lt;= <span class="number">16'h0000</span>; <span class="comment">//reset to zero first</span></span><br><span class="line">        <span class="keyword">else</span></span><br><span class="line">            Qout &lt;= Din; </span><br><span class="line"><span class="keyword">endmodule</span></span><br></pre></td></tr></table></figure><hr><h1 id="顶层文件"><a href="#顶层文件" class="headerlink" title="顶层文件"></a>顶层文件</h1><figure class="highlight verilog"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">`<span class="meta-keyword">timescale</span> 1ns / 1ps //unit 1ns, precision 1ps</span></span><br><span class="line"><span class="keyword">module</span> Fre_Measure(</span><br><span class="line">    <span class="keyword">output</span> <span class="keyword">wire</span> [<span class="number">15</span>:<span class="number">0</span>] BCD, <span class="comment">//transfer to display part</span></span><br><span class="line">    <span class="keyword">input</span> <span class="number">_1</span>HzIN, SigIN, nRST_Key,</span><br><span class="line">    <span class="keyword">output</span> <span class="keyword">wire</span> Cnt_EN, Cnt_CR, <span class="comment">//control signals of the counter</span></span><br><span class="line">    <span class="keyword">output</span> <span class="keyword">wire</span> Latch_Sig, <span class="comment">//latch singal</span></span><br><span class="line">    <span class="keyword">output</span> <span class="keyword">wire</span> [<span class="number">15</span>:<span class="number">0</span>] Cnt <span class="comment">//8421BCDcode output</span></span><br><span class="line">    );</span><br><span class="line">    </span><br><span class="line">    <span class="comment">//call control block</span></span><br><span class="line">    control U0(Cnt_EN, Cnt_CR, Latch_Sig, nRST_Key, <span class="number">_1</span>HzIN);</span><br><span class="line">    </span><br><span class="line">    <span class="comment">//measure counter</span></span><br><span class="line">    counter U1(Cnt[<span class="number">3</span>:<span class="number">0</span>], Cnt_CR, Cnt_EN, SigIN);</span><br><span class="line">    counter U2(Cnt[<span class="number">7</span>:<span class="number">4</span>], Cnt_CR, Cnt_EN, ~(Cnt[<span class="number">3</span>:<span class="number">0</span>] == <span class="number">4'h9</span>));</span><br><span class="line">    counter U3(Cnt[<span class="number">11</span>:<span class="number">8</span>], Cnt_CR, Cnt_EN, ~(Cnt[<span class="number">7</span>:<span class="number">0</span>] == <span class="number">8'h99</span>));</span><br><span class="line">    counter U4(Cnt[<span class="number">15</span>:<span class="number">12</span>], Cnt_CR, Cnt_EN, ~(Cnt[<span class="number">11</span>:<span class="number">0</span>] == <span class="number">12'h999</span>));</span><br><span class="line">    </span><br><span class="line">    <span class="comment">//call latch block</span></span><br><span class="line">    Latch U5(BCD, Cnt, Latch_Sig, ~nRST_Key);</span><br><span class="line">    </span><br><span class="line"><span class="keyword">endmodule</span></span><br></pre></td></tr></table></figure><hr><h1 id="仿真文件"><a href="#仿真文件" class="headerlink" title="仿真文件"></a>仿真文件</h1><figure class="highlight verilog"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">`<span class="meta-keyword">timescale</span> 1ns / 1ps //unit 1ns, precision 1ps</span></span><br><span class="line"><span class="keyword">module</span> simulateFile();</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">wire</span> [<span class="number">15</span>:<span class="number">0</span>] BCD; <span class="comment">//transfer to display part</span></span><br><span class="line">    <span class="keyword">wire</span> [<span class="number">15</span>:<span class="number">0</span>] Cnt; <span class="comment">//8421BCDcode output</span></span><br><span class="line">    <span class="keyword">reg</span> CLK, RST, Signal;</span><br><span class="line">    <span class="keyword">parameter</span> T1 = <span class="number">0</span><span class="variable">.1</span>, <span class="comment">//100Hz</span></span><br><span class="line">              T2 = <span class="number">0</span><span class="variable">.01</span>, <span class="comment">//1000Hz</span></span><br><span class="line">              T3 = <span class="number">0</span><span class="variable">.002</span>; <span class="comment">//5000Hz</span></span><br><span class="line">    <span class="keyword">wire</span> Cnt_EN, Cnt_CR; <span class="comment">//control signals of the counter</span></span><br><span class="line">    <span class="keyword">wire</span> Latch_Sig; <span class="comment">//latch singal</span></span><br><span class="line">    </span><br><span class="line">    Fre_Measure Watch(BCD, CLK, Signal, RST, Cnt_EN, Cnt_CR, Latch_Sig, Cnt);</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">initial</span></span><br><span class="line">        <span class="keyword">begin</span></span><br><span class="line">            RST = <span class="number">0</span>;</span><br><span class="line">            CLK = <span class="number">0</span>;</span><br><span class="line">            Signal = <span class="number">0</span>;</span><br><span class="line">        <span class="keyword">end</span></span><br><span class="line">        </span><br><span class="line">    <span class="keyword">always</span></span><br><span class="line">        <span class="keyword">forever</span> #<span class="number">10</span> CLK = ~CLK; <span class="comment">//generate clock signal</span></span><br><span class="line">               </span><br><span class="line">    <span class="keyword">always</span> #T1 Signal = ~Signal; <span class="comment">//T1 or T2 or T3</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">always</span></span><br><span class="line">    <span class="keyword">begin</span></span><br><span class="line">    #<span class="number">10</span> RST = <span class="number">1</span>;</span><br><span class="line">    #<span class="number">200</span> RST = <span class="number">0</span>;</span><br><span class="line">    #<span class="number">10</span> RST = <span class="number">1</span>;</span><br><span class="line">    #<span class="number">200</span> RST = <span class="number">0</span>;</span><br><span class="line">    #<span class="number">10</span> RST = <span class="number">1</span>;</span><br><span class="line">    #<span class="number">200</span> RST = <span class="number">0</span>;</span><br><span class="line">    #<span class="number">10</span> RST = <span class="number">1</span>;</span><br><span class="line">    <span class="keyword">end</span></span><br><span class="line">        </span><br><span class="line"><span class="keyword">endmodule</span></span><br></pre></td></tr></table></figure><hr><h1 id="仿真结果"><a href="#仿真结果" class="headerlink" title="仿真结果"></a>仿真结果</h1><p>以100Hz（T1）为例，输出的仿真结果如下。<br><img src="/verilog20200107215404/100Hz.png" title="100Hz"><br>其中CLK是固定的1Hz基准时钟信号；RST是系统需求的复位按键，当按下复位即RST为下降沿时，可以看到计数器Cnt被清零同时第一排译码输出的BCD码也被清零；Signal为输入的信号，这里我使用的是100Hz的，由我自己设定，由于频率较快，可以看到波形图非常密集；Cnt_EN是计数使能信号，可见在它为高电平时，Cnt随着输入信号一样快速计数；Cnt_CR是清零信号，在每次计数使能的上升沿或者复位的下降沿到来时Cnt_CR置零，也就是对Cnt清零操作；此外，当时钟信号到来时，假如系统不在计数（Cnt_EN=0），那么Latch_Sig将置1，也就是把记录数值存入锁存器。<br>实际上，这种设计方案会存在±1的计数误差，应为输入信号不一定与分频器同周期，即有可能每次测量的起始位置出于输入信号一个周期内的不同状态。</p><hr><h1 id="timescale"><a href="#timescale" class="headerlink" title="timescale"></a>timescale</h1><p>timescale是Verilog中的预编译指令，指定位于它后边的module的时间单位和时间精度，直到遇到新的timescale指令或者resetall指令。它的语法如下：<br></p><figure class="highlight verilog"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">`<span class="meta-keyword">timescale</span> time_unit / time_precision</span></span><br></pre></td></tr></table></figure><p></p><p>假如我们延时x个time_unit，那延时的总时间time = x * time_unit，但最后真正延时的时间是根据time_precision对time进行四舍五入后的结果。</p><blockquote><p>注意：</p><ol><li>time_unit和time_precision只能是1、10和100这三种整数，单位有s、ms、us、ns、ps和fs。</li><li>time_precision必须小于等于time_unit。</li><li>timescale的时间精度设置是会影响仿真时间的，1ps精度可能是1ns精度仿真时间的一倍还多，并且占用更多的内存，所以如果没有必要，应尽量将时间精度设置得更大一些。</li></ol></blockquote><hr><h1 id="仿真时间"><a href="#仿真时间" class="headerlink" title="仿真时间"></a>仿真时间</h1><p>之前进行仿真时，我往往是让它自动运行至系统默认时间然后停止，这就会造成出现好几次重复循环的情况。<br>在Vivado中，窗口上方有三个类似播放器中的按钮，从左往右依次是：复位、不停运行、按指定时长（在后面的栏中设定）运行。<br>此外，如果计算不准时间，可以直接在仿真文件末尾或者想要结束的地方使用<code>$stop</code>或者<code>$finifsh</code>来终止仿真。</p><script type="text/javascript" src="/js/src/bai.js"></script><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      
      
        &lt;!-- build time:Tue Jan 28 2020 15:40:41 GMT+0800 (GMT+08:00) --&gt;&lt;p&gt;之前每次创建完一个新的文件，文件最上方总会显示一行timescale。当初觉得没什么作用，删了之后依旧没有任何问题便没把它当回事，直到后来做频率
      
    
    </summary>
    
    
      <category term="程序与设计" scheme="https://gsy00517.github.io/categories/%E7%A8%8B%E5%BA%8F%E4%B8%8E%E8%AE%BE%E8%AE%A1/"/>
    
    
      <category term="数字电路" scheme="https://gsy00517.github.io/tags/%E6%95%B0%E5%AD%97%E7%94%B5%E8%B7%AF/"/>
    
      <category term="代码实现" scheme="https://gsy00517.github.io/tags/%E4%BB%A3%E7%A0%81%E5%AE%9E%E7%8E%B0/"/>
    
      <category term="verilog" scheme="https://gsy00517.github.io/tags/verilog/"/>
    
      <category term="Vivado" scheme="https://gsy00517.github.io/tags/Vivado/"/>
    
  </entry>
  
  <entry>
    <title>verilog笔记：运动码表的硬件描述语言实现</title>
    <link href="https://gsy00517.github.io/verilog20200107211139/"/>
    <id>https://gsy00517.github.io/verilog20200107211139/</id>
    <published>2020-01-07T13:11:39.000Z</published>
    <updated>2020-01-19T00:27:21.473Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Tue Jan 28 2020 15:40:41 GMT+0800 (GMT+08:00) --><p>继上一篇<a href="https://gsy00517.github.io/logisim20200107121101/" target="_blank">logisim笔记：基本使用及运动码表的实现</a>，我还使用了硬件描述语言对同样需求的运动码表进行了实现，那么就在这里一并也总结一下吧。</p><p><strong>References</strong>：</p><p>电子文献：<br><a href="https://blog.csdn.net/leon_zeng0/article/details/78441871" target="_blank" rel="noopener">https://blog.csdn.net/leon_zeng0/article/details/78441871</a><br><a href="https://www.cnblogs.com/douzi2/p/5147151.html" target="_blank" rel="noopener">https://www.cnblogs.com/douzi2/p/5147151.html</a><br><a href="https://blog.csdn.net/FPGADesigner/article/details/82425612" target="_blank" rel="noopener">https://blog.csdn.net/FPGADesigner/article/details/82425612</a></p><hr><h1 id="整体设计"><a href="#整体设计" class="headerlink" title="整体设计"></a>整体设计</h1><p>由于需求与使用Logisim实现时一致，因此我的设计思路也基本沿用上一篇博文中提到的方案。但是要注意的是，这里的000状态并不在作为按键抬起之后的中间态，而是进入系统时的一个默认初始状态。</p><hr><h1 id="Vivado中一些高亮的含义"><a href="#Vivado中一些高亮的含义" class="headerlink" title="Vivado中一些高亮的含义"></a>Vivado中一些高亮的含义</h1><p>在具体的代码之前，我还想先归纳一下本次实践过程中遇到的和发现的Vivado中一些高亮提醒的含义。</p><ol><li><h2 id="土黄色高亮"><a href="#土黄色高亮" class="headerlink" title="土黄色高亮"></a>土黄色高亮</h2>土黄色高亮出现的原因主要可能是下面三种情况：<ol><li>定义重复。</li><li>定义放在了调用处的后面（identifier used before its declaration）。</li><li>声明残缺（empty statement）。</li></ol></li><li><h2 id="蓝色高亮"><a href="#蓝色高亮" class="headerlink" title="蓝色高亮"></a>蓝色高亮</h2><ol><li>含有undeclared symbol。</li><li>和上面土黄色高亮相搭配出现，有定义重复时指明重复定义的位置。</li></ol></li></ol><hr><h1 id="16位数值比较器"><a href="#16位数值比较器" class="headerlink" title="16位数值比较器"></a>16位数值比较器</h1><p>该模块用于比较两个16位二进制数的大小，以确定是否需要存入记录的数据。代码如下：<br></p><figure class="highlight verilog"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">module</span> <span class="number">_16</span>bit_Comp(</span><br><span class="line">    <span class="keyword">input</span> [<span class="number">15</span>:<span class="number">0</span>] A,</span><br><span class="line">    <span class="keyword">input</span> [<span class="number">15</span>:<span class="number">0</span>] B,</span><br><span class="line">    <span class="keyword">output</span> <span class="keyword">reg</span> Y</span><br><span class="line">    );</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">always</span> @ (A <span class="keyword">or</span> B)</span><br><span class="line">        <span class="keyword">begin</span></span><br><span class="line">            <span class="keyword">if</span>(A &lt; B)</span><br><span class="line">                Y &lt;= <span class="number">1</span>;</span><br><span class="line">            <span class="keyword">else</span></span><br><span class="line">                Y &lt;= <span class="number">0</span>;</span><br><span class="line">        <span class="keyword">end</span></span><br><span class="line"><span class="keyword">endmodule</span></span><br></pre></td></tr></table></figure><p></p><hr><h1 id="16位寄存器"><a href="#16位寄存器" class="headerlink" title="16位寄存器"></a>16位寄存器</h1><p>TMRecord表示码表暂停时的读数，regRecord表示寄存器中已经存储的记录，初始值为9999。控制信号有使能信号和reset信号。当收到reset信号时，直接将记录改为9999。当有使能信号时，将TMRecord记录下来。代码如下：<br></p><figure class="highlight verilog"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">module</span> <span class="number">_16</span>bit_Reg(</span><br><span class="line">    <span class="keyword">input</span> enable,</span><br><span class="line">    <span class="keyword">input</span> [<span class="number">15</span>:<span class="number">0</span>] TMRecord,</span><br><span class="line">    <span class="keyword">input</span> [<span class="number">15</span>:<span class="number">0</span>] regRecord,</span><br><span class="line">    <span class="keyword">input</span> [<span class="number">15</span>:<span class="number">0</span>] maxDefault,</span><br><span class="line">    <span class="keyword">input</span> reset,</span><br><span class="line">    <span class="keyword">input</span> clock,</span><br><span class="line">    <span class="keyword">output</span> <span class="keyword">reg</span> [<span class="number">15</span>:<span class="number">0</span>] next_record</span><br><span class="line">    );</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">always</span> @ (reset <span class="keyword">or</span> enable)</span><br><span class="line">        <span class="keyword">begin</span></span><br><span class="line">            <span class="keyword">if</span>(reset &amp; enable)</span><br><span class="line">                next_record &lt;= maxDefault;</span><br><span class="line">            <span class="keyword">else</span> <span class="keyword">if</span>(enable)</span><br><span class="line">                next_record &lt;= TMRecord;</span><br><span class="line">            <span class="keyword">else</span></span><br><span class="line">                next_record &lt;= regRecord;</span><br><span class="line">        <span class="keyword">end</span></span><br><span class="line"><span class="keyword">endmodule</span></span><br></pre></td></tr></table></figure><p></p><hr><h1 id="数码管显示驱动"><a href="#数码管显示驱动" class="headerlink" title="数码管显示驱动"></a>数码管显示驱动</h1><p>将BCD码转化为7位二进制数，即对应7段数码管，用于显示。<br></p><figure class="highlight verilog"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">module</span> watchDrive(</span><br><span class="line">    <span class="keyword">input</span> [<span class="number">3</span>:<span class="number">0</span>] BCD,</span><br><span class="line">    <span class="keyword">output</span> <span class="keyword">reg</span> [<span class="number">6</span>:<span class="number">0</span>] light</span><br><span class="line">    );</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">always</span> @ (BCD)</span><br><span class="line">        <span class="keyword">begin</span></span><br><span class="line">            <span class="keyword">case</span>(BCD)</span><br><span class="line">                <span class="number">0</span>: light = <span class="number">7'B0111111</span>;</span><br><span class="line">                <span class="number">1</span>: light = <span class="number">7'B0001001</span>;</span><br><span class="line">                <span class="number">2</span>: light = <span class="number">7'B1011110</span>;</span><br><span class="line">                <span class="number">3</span>: light = <span class="number">7'B1011011</span>;</span><br><span class="line">                <span class="number">4</span>: light = <span class="number">7'B1101001</span>;</span><br><span class="line">                <span class="number">5</span>: light = <span class="number">7'B1110011</span>;</span><br><span class="line">                <span class="number">6</span>: light = <span class="number">7'B1110111</span>;</span><br><span class="line">                <span class="number">7</span>: light = <span class="number">7'B0011001</span>;</span><br><span class="line">                <span class="number">8</span>: light = <span class="number">7'B1111111</span>;</span><br><span class="line">                <span class="number">9</span>: light = <span class="number">7'B1111011</span>;</span><br><span class="line">                <span class="keyword">default</span>: light = <span class="number">7'B0111111</span>;</span><br><span class="line">            <span class="keyword">endcase</span></span><br><span class="line">        <span class="keyword">end</span></span><br><span class="line"><span class="keyword">endmodule</span></span><br></pre></td></tr></table></figure><p></p><hr><h1 id="顶层文件"><a href="#顶层文件" class="headerlink" title="顶层文件"></a>顶层文件</h1><p>该部分主要包括对各个变量的定义和初始化；状态转换，即共设计了5种状态，对应不同的功能，当按下不同按键时，选择对应的状态并作为次态；数码管的显示与进位，即对数码管4个位置依次改动，从低位开始计算，当进位时产生进位信号到下一位。当有重置信号（这里使用的是reset和start的上升沿）时清零。<br></p><figure class="highlight verilog"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">`<span class="meta-keyword">timescale</span> 1ns / 1ps</span></span><br><span class="line"><span class="comment">//top module</span></span><br><span class="line"><span class="keyword">module</span> runningwatch(</span><br><span class="line">    <span class="keyword">input</span> start,</span><br><span class="line">    <span class="keyword">input</span> stop,</span><br><span class="line">    <span class="keyword">input</span> store,</span><br><span class="line">    <span class="keyword">input</span> reset,</span><br><span class="line">    <span class="keyword">input</span> CLK,</span><br><span class="line">    <span class="keyword">output</span> <span class="keyword">reg</span> TM_EN, <span class="comment">//enable the timer</span></span><br><span class="line">    <span class="keyword">output</span> <span class="keyword">reg</span> SD_EN, <span class="comment">//enable register</span></span><br><span class="line">    <span class="keyword">output</span> <span class="keyword">reg</span> DP_SEL, <span class="comment">//the screen show which data</span></span><br><span class="line">    <span class="keyword">output</span> <span class="keyword">reg</span> [<span class="number">15</span>:<span class="number">0</span>] regRecord, <span class="comment">//data the register storing</span></span><br><span class="line">    <span class="keyword">output</span> <span class="keyword">reg</span> [<span class="number">15</span>:<span class="number">0</span>] TMRecord, <span class="comment">//timer data</span></span><br><span class="line">    <span class="keyword">output</span> [<span class="number">6</span>:<span class="number">0</span>] tenmsLight, hunmsLight, onesLight, tensLight</span><br><span class="line">    );</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">reg</span> [<span class="number">3</span>:<span class="number">0</span>] tenMS, hunMS, oneS, tenS; <span class="comment">//four 4bit digits from small to high and each from 0 to 9</span></span><br><span class="line">    <span class="keyword">reg</span> tenmsup, hunmsup, onesup; <span class="comment">//the signals if the bigger digit than itself should add</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment">//allocate state</span></span><br><span class="line">    <span class="keyword">parameter</span> S0 = <span class="number">3'B000</span>;</span><br><span class="line">    <span class="comment">//initial state</span></span><br><span class="line">    <span class="keyword">parameter</span> S1 = <span class="number">3'B001</span>;</span><br><span class="line">    <span class="comment">//TIMING:timer set as 00.00,record does not change,show timer,timer counts,TM_EN = 1, SD_EN = 0, DP_SEL = 0</span></span><br><span class="line">    <span class="keyword">parameter</span> S2 = <span class="number">3'B010</span>;</span><br><span class="line">    <span class="comment">//PAUSE:timer does not change,record does not change,show timer,timer does not count,TM_EN = 0, SD_EN = 0, DP_SEL = 0</span></span><br><span class="line">    <span class="keyword">parameter</span> S3 = <span class="number">3'B011</span>;</span><br><span class="line">    <span class="comment">//UPDATE:timer does not change,record set as timer,show register,timer does not count,TM_EN = 0, SD_EN = 1, DP_SEL = 1</span></span><br><span class="line">    <span class="keyword">parameter</span> S4 = <span class="number">3'B100</span>;</span><br><span class="line">    <span class="comment">//KEEP:timer does not change,record does not change,show register,timer does not count,TM_EN = 0, SD_EN = 0, DP_SEL = 1</span></span><br><span class="line">    <span class="keyword">parameter</span> S5 = <span class="number">3'B101</span>;</span><br><span class="line">    <span class="comment">//RESET:timer set as 00.00,record set as 99.99,show timer,timer does not count,TM_EN = 0, SD_EN = 1, DP_SEL = 0</span></span><br><span class="line">        </span><br><span class="line">    <span class="keyword">reg</span> [<span class="number">2</span>:<span class="number">0</span>] state;</span><br><span class="line">    <span class="keyword">reg</span> [<span class="number">2</span>:<span class="number">0</span>] next_state;</span><br><span class="line">    </span><br><span class="line">    <span class="comment">//reg CLK;</span></span><br><span class="line">    <span class="keyword">initial</span> <span class="comment">//only run once</span></span><br><span class="line">    <span class="keyword">begin</span></span><br><span class="line">        regRecord = <span class="number">16'B0010011100001111</span>;</span><br><span class="line">        TMRecord = <span class="number">16'B0000000000000000</span>;</span><br><span class="line">        </span><br><span class="line">        state = S0;</span><br><span class="line">        </span><br><span class="line">        tenMS = <span class="number">0</span>; hunMS = <span class="number">0</span>; oneS = <span class="number">0</span>; tenS = <span class="number">0</span>;</span><br><span class="line">    <span class="keyword">end</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment">//to judge if store the timer's data</span></span><br><span class="line">    <span class="keyword">wire</span> <span class="keyword">new</span>;</span><br><span class="line">    <span class="keyword">reg</span> newRecord;</span><br><span class="line">    <span class="number">_16</span>bit_Comp comparator(TMRecord, regRecord, <span class="keyword">new</span>); <span class="comment">//compare</span></span><br><span class="line">    <span class="keyword">always</span> @ (<span class="keyword">new</span>)</span><br><span class="line">        newRecord = <span class="keyword">new</span>;</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">reg</span> [<span class="number">15</span>:<span class="number">0</span>] MAX = <span class="number">16'B0010011100001111</span>;</span><br><span class="line">    </span><br><span class="line">    <span class="comment">//sequential logic part</span></span><br><span class="line">    <span class="keyword">always</span> @ (<span class="keyword">posedge</span> CLK) <span class="comment">//update the state at each posedge</span></span><br><span class="line">        <span class="keyword">begin</span></span><br><span class="line">            state &lt;= next_state;</span><br><span class="line">        <span class="keyword">end</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment">//combinatory logic part</span></span><br><span class="line">    <span class="comment">//state transform</span></span><br><span class="line">    <span class="keyword">always</span> @ (state <span class="keyword">or</span> start <span class="keyword">or</span> stop <span class="keyword">or</span> store <span class="keyword">or</span> reset <span class="keyword">or</span> newRecord)</span><br><span class="line">        <span class="keyword">begin</span></span><br><span class="line">            next_state = S0; <span class="comment">//if not press the key, back to the initial state</span></span><br><span class="line">            </span><br><span class="line">            <span class="keyword">case</span>(state)</span><br><span class="line">            </span><br><span class="line">            S0: <span class="comment">//initial state</span></span><br><span class="line">            <span class="keyword">begin</span></span><br><span class="line">                TM_EN = <span class="number">0</span>; SD_EN = <span class="number">0</span>; DP_SEL = <span class="number">0</span>;</span><br><span class="line">                <span class="keyword">if</span>(start)                   <span class="keyword">begin</span> next_state = S1; TM_EN = <span class="number">1</span>; SD_EN = <span class="number">0</span>; DP_SEL = <span class="number">0</span>; <span class="keyword">end</span></span><br><span class="line">                <span class="keyword">else</span> <span class="keyword">if</span>(stop)               <span class="keyword">begin</span> next_state = S2; TM_EN = <span class="number">0</span>; SD_EN = <span class="number">0</span>; DP_SEL = <span class="number">0</span>; <span class="keyword">end</span></span><br><span class="line">                <span class="keyword">else</span> <span class="keyword">if</span>(store &amp; newRecord)  <span class="keyword">begin</span> next_state = S3; TM_EN = <span class="number">0</span>; SD_EN = <span class="number">1</span>; DP_SEL = <span class="number">1</span>; <span class="keyword">end</span></span><br><span class="line">                <span class="keyword">else</span> <span class="keyword">if</span>(store &amp; ~newRecord) <span class="keyword">begin</span> next_state = S4; TM_EN = <span class="number">0</span>; SD_EN = <span class="number">0</span>; DP_SEL = <span class="number">1</span>; <span class="keyword">end</span></span><br><span class="line">                <span class="keyword">else</span> <span class="keyword">if</span>(reset)              <span class="keyword">begin</span> next_state = S5; TM_EN = <span class="number">0</span>; SD_EN = <span class="number">1</span>; DP_SEL = <span class="number">0</span>; <span class="keyword">end</span></span><br><span class="line">                <span class="keyword">else</span>                        <span class="keyword">begin</span> next_state = S0; TM_EN = <span class="number">0</span>; SD_EN = <span class="number">0</span>; DP_SEL = <span class="number">0</span>; <span class="keyword">end</span></span><br><span class="line">            <span class="keyword">end</span></span><br><span class="line">            </span><br><span class="line">            S1: <span class="comment">//TIMING</span></span><br><span class="line">            <span class="keyword">begin</span></span><br><span class="line">                TM_EN = <span class="number">1</span>; SD_EN = <span class="number">0</span>; DP_SEL = <span class="number">0</span>;</span><br><span class="line">                <span class="keyword">if</span>(start)                   <span class="keyword">begin</span> next_state = S1; TM_EN = <span class="number">1</span>; SD_EN = <span class="number">0</span>; DP_SEL = <span class="number">0</span>; <span class="keyword">end</span></span><br><span class="line">                <span class="keyword">else</span> <span class="keyword">if</span>(stop)               <span class="keyword">begin</span> next_state = S2; TM_EN = <span class="number">0</span>; SD_EN = <span class="number">0</span>; DP_SEL = <span class="number">0</span>; <span class="keyword">end</span></span><br><span class="line">                <span class="keyword">else</span> <span class="keyword">if</span>(store &amp; newRecord)  <span class="keyword">begin</span> next_state = S3; TM_EN = <span class="number">0</span>; SD_EN = <span class="number">1</span>; DP_SEL = <span class="number">1</span>; <span class="keyword">end</span></span><br><span class="line">                <span class="keyword">else</span> <span class="keyword">if</span>(store &amp; ~newRecord) <span class="keyword">begin</span> next_state = S4; TM_EN = <span class="number">0</span>; SD_EN = <span class="number">0</span>; DP_SEL = <span class="number">1</span>; <span class="keyword">end</span></span><br><span class="line">                <span class="keyword">else</span> <span class="keyword">if</span>(reset)              <span class="keyword">begin</span> next_state = S5; TM_EN = <span class="number">0</span>; SD_EN = <span class="number">1</span>; DP_SEL = <span class="number">0</span>; <span class="keyword">end</span></span><br><span class="line">                <span class="keyword">else</span>                        <span class="keyword">begin</span> next_state = S0; TM_EN = <span class="number">0</span>; SD_EN = <span class="number">0</span>; DP_SEL = <span class="number">0</span>; <span class="keyword">end</span></span><br><span class="line">            <span class="keyword">end</span></span><br><span class="line"></span><br><span class="line">            S2: <span class="comment">//PAUSE</span></span><br><span class="line">            <span class="keyword">begin</span></span><br><span class="line">                TM_EN = <span class="number">0</span>; SD_EN = <span class="number">0</span>; DP_SEL = <span class="number">0</span>;</span><br><span class="line">                <span class="keyword">if</span>(start)                   <span class="keyword">begin</span> next_state = S1; TM_EN = <span class="number">1</span>; SD_EN = <span class="number">0</span>; DP_SEL = <span class="number">0</span>; <span class="keyword">end</span></span><br><span class="line">                <span class="keyword">else</span> <span class="keyword">if</span>(stop)               <span class="keyword">begin</span> next_state = S2; TM_EN = <span class="number">0</span>; SD_EN = <span class="number">0</span>; DP_SEL = <span class="number">0</span>; <span class="keyword">end</span></span><br><span class="line">                <span class="keyword">else</span> <span class="keyword">if</span>(store &amp; newRecord)  <span class="keyword">begin</span> next_state = S3; TM_EN = <span class="number">0</span>; SD_EN = <span class="number">1</span>; DP_SEL = <span class="number">1</span>; <span class="keyword">end</span></span><br><span class="line">                <span class="keyword">else</span> <span class="keyword">if</span>(store &amp; ~newRecord) <span class="keyword">begin</span> next_state = S4; TM_EN = <span class="number">0</span>; SD_EN = <span class="number">0</span>; DP_SEL = <span class="number">1</span>; <span class="keyword">end</span></span><br><span class="line">                <span class="keyword">else</span> <span class="keyword">if</span>(reset)              <span class="keyword">begin</span> next_state = S5; TM_EN = <span class="number">0</span>; SD_EN = <span class="number">1</span>; DP_SEL = <span class="number">0</span>; <span class="keyword">end</span></span><br><span class="line">                <span class="keyword">else</span>                        <span class="keyword">begin</span> next_state = S0; TM_EN = <span class="number">0</span>; SD_EN = <span class="number">0</span>; DP_SEL = <span class="number">0</span>; <span class="keyword">end</span></span><br><span class="line">            <span class="keyword">end</span></span><br><span class="line">            </span><br><span class="line">            S3: <span class="comment">//UPDATE</span></span><br><span class="line">            <span class="keyword">begin</span></span><br><span class="line">                TM_EN = <span class="number">0</span>; SD_EN = <span class="number">1</span>; DP_SEL = <span class="number">1</span>;</span><br><span class="line">                <span class="keyword">if</span>(start)                   <span class="keyword">begin</span> next_state = S1; TM_EN = <span class="number">1</span>; SD_EN = <span class="number">0</span>; DP_SEL = <span class="number">0</span>; <span class="keyword">end</span></span><br><span class="line">                <span class="keyword">else</span> <span class="keyword">if</span>(stop)               <span class="keyword">begin</span> next_state = S2; TM_EN = <span class="number">0</span>; SD_EN = <span class="number">0</span>; DP_SEL = <span class="number">0</span>; <span class="keyword">end</span></span><br><span class="line">                <span class="keyword">else</span> <span class="keyword">if</span>(store &amp; newRecord)  <span class="keyword">begin</span> next_state = S3; TM_EN = <span class="number">0</span>; SD_EN = <span class="number">1</span>; DP_SEL = <span class="number">1</span>; <span class="keyword">end</span></span><br><span class="line">                <span class="keyword">else</span> <span class="keyword">if</span>(store &amp; ~newRecord) <span class="keyword">begin</span> next_state = S4; TM_EN = <span class="number">0</span>; SD_EN = <span class="number">0</span>; DP_SEL = <span class="number">1</span>; <span class="keyword">end</span></span><br><span class="line">                <span class="keyword">else</span> <span class="keyword">if</span>(reset)              <span class="keyword">begin</span> next_state = S5; TM_EN = <span class="number">0</span>; SD_EN = <span class="number">1</span>; DP_SEL = <span class="number">0</span>; <span class="keyword">end</span></span><br><span class="line">                <span class="keyword">else</span>                        <span class="keyword">begin</span> next_state = S0; TM_EN = <span class="number">0</span>; SD_EN = <span class="number">0</span>; DP_SEL = <span class="number">0</span>; <span class="keyword">end</span></span><br><span class="line">            <span class="keyword">end</span></span><br><span class="line">            </span><br><span class="line">            S4: <span class="comment">//KEEP</span></span><br><span class="line">            <span class="keyword">begin</span></span><br><span class="line">                TM_EN = <span class="number">0</span>; SD_EN = <span class="number">0</span>; DP_SEL = <span class="number">1</span>;</span><br><span class="line">                <span class="keyword">if</span>(start)                   <span class="keyword">begin</span> next_state = S1; TM_EN = <span class="number">1</span>; SD_EN = <span class="number">0</span>; DP_SEL = <span class="number">0</span>; <span class="keyword">end</span></span><br><span class="line">                <span class="keyword">else</span> <span class="keyword">if</span>(stop)               <span class="keyword">begin</span> next_state = S2; TM_EN = <span class="number">0</span>; SD_EN = <span class="number">0</span>; DP_SEL = <span class="number">0</span>; <span class="keyword">end</span></span><br><span class="line">                <span class="keyword">else</span> <span class="keyword">if</span>(store &amp; newRecord)  <span class="keyword">begin</span> next_state = S3; TM_EN = <span class="number">0</span>; SD_EN = <span class="number">1</span>; DP_SEL = <span class="number">1</span>; <span class="keyword">end</span></span><br><span class="line">                <span class="keyword">else</span> <span class="keyword">if</span>(store &amp; ~newRecord) <span class="keyword">begin</span> next_state = S4; TM_EN = <span class="number">0</span>; SD_EN = <span class="number">0</span>; DP_SEL = <span class="number">1</span>; <span class="keyword">end</span></span><br><span class="line">                <span class="keyword">else</span> <span class="keyword">if</span>(reset)              <span class="keyword">begin</span> next_state = S5; TM_EN = <span class="number">0</span>; SD_EN = <span class="number">1</span>; DP_SEL = <span class="number">0</span>; <span class="keyword">end</span></span><br><span class="line">                <span class="keyword">else</span>                        <span class="keyword">begin</span> next_state = S0; TM_EN = <span class="number">0</span>; SD_EN = <span class="number">0</span>; DP_SEL = <span class="number">0</span>; <span class="keyword">end</span></span><br><span class="line">            <span class="keyword">end</span></span><br><span class="line">            </span><br><span class="line">            S5: <span class="comment">//RESET</span></span><br><span class="line">            <span class="keyword">begin</span></span><br><span class="line">                TM_EN = <span class="number">0</span>; SD_EN = <span class="number">1</span>; DP_SEL = <span class="number">0</span>;</span><br><span class="line">                <span class="keyword">if</span>(start)                   <span class="keyword">begin</span> next_state = S1; TM_EN = <span class="number">1</span>; SD_EN = <span class="number">0</span>; DP_SEL = <span class="number">0</span>; <span class="keyword">end</span></span><br><span class="line">                <span class="keyword">else</span> <span class="keyword">if</span>(stop)               <span class="keyword">begin</span> next_state = S2; TM_EN = <span class="number">0</span>; SD_EN = <span class="number">0</span>; DP_SEL = <span class="number">0</span>; <span class="keyword">end</span></span><br><span class="line">                <span class="keyword">else</span> <span class="keyword">if</span>(store &amp; newRecord)  <span class="keyword">begin</span> next_state = S3; TM_EN = <span class="number">0</span>; SD_EN = <span class="number">1</span>; DP_SEL = <span class="number">1</span>; <span class="keyword">end</span></span><br><span class="line">                <span class="keyword">else</span> <span class="keyword">if</span>(store &amp; ~newRecord) <span class="keyword">begin</span> next_state = S4; TM_EN = <span class="number">0</span>; SD_EN = <span class="number">0</span>; DP_SEL = <span class="number">1</span>; <span class="keyword">end</span></span><br><span class="line">                <span class="keyword">else</span> <span class="keyword">if</span>(reset)              <span class="keyword">begin</span> next_state = S5; TM_EN = <span class="number">0</span>; SD_EN = <span class="number">1</span>; DP_SEL = <span class="number">0</span>; <span class="keyword">end</span></span><br><span class="line">                <span class="keyword">else</span>                        <span class="keyword">begin</span> next_state = S0; TM_EN = <span class="number">0</span>; SD_EN = <span class="number">0</span>; DP_SEL = <span class="number">0</span>; <span class="keyword">end</span></span><br><span class="line">            <span class="keyword">end</span></span><br><span class="line">            </span><br><span class="line">            <span class="keyword">default</span>: <span class="keyword">begin</span> next_state = S0; TM_EN = <span class="number">0</span>; SD_EN = <span class="number">0</span>; DP_SEL = <span class="number">0</span>; <span class="keyword">end</span> <span class="comment">//default initial state</span></span><br><span class="line">            <span class="keyword">endcase</span></span><br><span class="line">        <span class="keyword">end</span></span><br><span class="line">        </span><br><span class="line">    <span class="comment">//reset to zero    </span></span><br><span class="line">    <span class="keyword">always</span> @ (<span class="keyword">posedge</span> reset <span class="keyword">or</span> <span class="keyword">posedge</span> start)</span><br><span class="line">        <span class="keyword">begin</span></span><br><span class="line">            tenMS &lt;= <span class="number">0</span>; hunMS &lt;= <span class="number">0</span>; oneS &lt;= <span class="number">0</span>; tenS &lt;= <span class="number">0</span>;</span><br><span class="line">            TMRecord &lt;= <span class="number">0</span>;</span><br><span class="line">        <span class="keyword">end</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment">//the followings are which have stated before:</span></span><br><span class="line">    <span class="comment">//reg [3:0] tenMS, hunMS, oneS, tenS are four 4bit digits from small to high and each from 0 to 9</span></span><br><span class="line">    <span class="comment">//reg tenmsup, hunmsup, onesup are the signals if the bigger digit than itself should add</span></span><br><span class="line">    <span class="comment">//timer, divide into four digits</span></span><br><span class="line">    <span class="comment">//10ms</span></span><br><span class="line">    <span class="keyword">always</span> @ (<span class="keyword">posedge</span> CLK)</span><br><span class="line">        <span class="keyword">begin</span></span><br><span class="line">            <span class="keyword">if</span>(TM_EN)</span><br><span class="line">                <span class="keyword">begin</span></span><br><span class="line">                    TMRecord = TMRecord + <span class="number">1</span>;</span><br><span class="line">                    <span class="keyword">if</span>(tenMS &lt; <span class="number">9</span>)</span><br><span class="line">                    <span class="keyword">begin</span> tenMS &lt;= tenMS + <span class="number">1</span>; tenmsup &lt;= <span class="number">0</span>; <span class="keyword">end</span></span><br><span class="line">                    <span class="keyword">else</span></span><br><span class="line">                    <span class="keyword">begin</span> tenMS &lt;= <span class="number">0</span>; tenmsup &lt;= <span class="number">1</span>; <span class="keyword">end</span></span><br><span class="line">                <span class="keyword">end</span></span><br><span class="line">        <span class="keyword">end</span></span><br><span class="line">    <span class="comment">//100ms</span></span><br><span class="line">    <span class="keyword">always</span> @ (<span class="keyword">posedge</span> tenmsup)</span><br><span class="line">        <span class="keyword">begin</span></span><br><span class="line">            <span class="keyword">if</span>(TM_EN)</span><br><span class="line">                <span class="keyword">begin</span></span><br><span class="line">                    <span class="keyword">if</span>(hunMS &lt; <span class="number">9</span>)</span><br><span class="line">                    <span class="keyword">begin</span> hunMS &lt;= hunMS + <span class="number">1</span>; hunmsup &lt;= <span class="number">0</span>; <span class="keyword">end</span></span><br><span class="line">                    <span class="keyword">else</span></span><br><span class="line">                    <span class="keyword">begin</span> hunMS &lt;= <span class="number">0</span>; hunmsup &lt;= <span class="number">1</span>; <span class="keyword">end</span></span><br><span class="line">                <span class="keyword">end</span></span><br><span class="line">        <span class="keyword">end</span></span><br><span class="line">        </span><br><span class="line">    <span class="comment">//1s</span></span><br><span class="line">    <span class="keyword">always</span> @ (<span class="keyword">posedge</span> hunmsup)</span><br><span class="line">        <span class="keyword">begin</span></span><br><span class="line">            <span class="keyword">if</span>(TM_EN)</span><br><span class="line">                <span class="keyword">begin</span></span><br><span class="line">                    <span class="keyword">if</span>(oneS &lt; <span class="number">9</span>)</span><br><span class="line">                    <span class="keyword">begin</span> oneS &lt;= oneS + <span class="number">1</span>; onesup &lt;= <span class="number">0</span>; <span class="keyword">end</span></span><br><span class="line">                    <span class="keyword">else</span></span><br><span class="line">                    <span class="keyword">begin</span> oneS &lt;= <span class="number">0</span>; onesup &lt;= <span class="number">1</span>; <span class="keyword">end</span></span><br><span class="line">                <span class="keyword">end</span></span><br><span class="line">        <span class="keyword">end</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment">//10s</span></span><br><span class="line">    <span class="keyword">always</span> @ (<span class="keyword">posedge</span> onesup)</span><br><span class="line">        <span class="keyword">begin</span></span><br><span class="line">            <span class="keyword">if</span>(TM_EN)</span><br><span class="line">                <span class="keyword">begin</span></span><br><span class="line">                    <span class="keyword">if</span>(tenS &lt; <span class="number">9</span>)</span><br><span class="line">                    tenS &lt;= oneS + <span class="number">1</span>;</span><br><span class="line">                    <span class="keyword">else</span></span><br><span class="line">                    oneS &lt;= <span class="number">0</span>;</span><br><span class="line">                <span class="keyword">end</span></span><br><span class="line">        <span class="keyword">end</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment">//save to the register</span></span><br><span class="line">    <span class="keyword">wire</span> [<span class="number">15</span>:<span class="number">0</span>] newReg;</span><br><span class="line">    <span class="number">_16</span>bit_Reg register(SD_EN, TMRecord, regRecord, MAX, reset, CLK, newReg);</span><br><span class="line">    <span class="keyword">always</span> @ (newReg)</span><br><span class="line">        regRecord = newReg;</span><br><span class="line">    </span><br><span class="line">    <span class="comment">//change BCD to tube lights</span></span><br><span class="line">    watchDrive TENms(tenMS, tenmsLight);</span><br><span class="line">    watchDrive HUNms(hunMS, hunmsLight);</span><br><span class="line">    watchDrive ONEs(oneS, onesLight);</span><br><span class="line">    watchDrive TENs(tenS, tensLight);</span><br><span class="line">    </span><br><span class="line"><span class="keyword">endmodule</span></span><br></pre></td></tr></table></figure><p></p><hr><h1 id="仿真文件"><a href="#仿真文件" class="headerlink" title="仿真文件"></a>仿真文件</h1><p>最后我们还需要自行编写一个仿真文件。<br></p><figure class="highlight verilog"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">module</span> simulateFile();</span><br><span class="line">    <span class="keyword">reg</span> start, stop, store, reset;</span><br><span class="line">    <span class="keyword">wire</span> TM_EN, SD_EN, DP_SEL;</span><br><span class="line">    <span class="keyword">wire</span> [<span class="number">15</span>:<span class="number">0</span>] regRecord;</span><br><span class="line">    <span class="keyword">wire</span> [<span class="number">15</span>:<span class="number">0</span>] TMRecord;</span><br><span class="line">    <span class="keyword">wire</span> [<span class="number">6</span>:<span class="number">0</span>] tenmsLight;</span><br><span class="line">    <span class="keyword">wire</span> [<span class="number">6</span>:<span class="number">0</span>] hunmsLight;</span><br><span class="line">    <span class="keyword">wire</span> [<span class="number">6</span>:<span class="number">0</span>] onesLight;</span><br><span class="line">    <span class="keyword">wire</span> [<span class="number">6</span>:<span class="number">0</span>] tensLight;</span><br><span class="line">    <span class="keyword">reg</span> CLK;</span><br><span class="line">    </span><br><span class="line">    runningwatch watch(start, stop, store, reset, CLK,</span><br><span class="line">                       TM_EN, SD_EN, DP_SEL, regRecord, TMRecord,</span><br><span class="line">                       tenmsLight, hunmsLight, onesLight, tensLight);</span><br><span class="line">    <span class="keyword">initial</span></span><br><span class="line">        <span class="keyword">begin</span></span><br><span class="line">        CLK = <span class="number">0</span>;</span><br><span class="line">        start = <span class="number">0</span>;</span><br><span class="line">        stop = <span class="number">0</span>;</span><br><span class="line">        store = <span class="number">0</span>;</span><br><span class="line">        reset = <span class="number">0</span>;</span><br><span class="line">        <span class="keyword">end</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">begin</span></span><br><span class="line">        <span class="keyword">always</span></span><br><span class="line">        <span class="comment">//generate clock signal</span></span><br><span class="line">        <span class="keyword">forever</span> #<span class="number">10</span> CLK = ~CLK;</span><br><span class="line">        </span><br><span class="line">        <span class="comment">//always #100</span></span><br><span class="line">        <span class="comment">//&#123;start, stop, store, reset&#125; = 4'B0000;</span></span><br><span class="line">        <span class="comment">//begin start = 0; stop = 0; store = 0; reset = 0; end</span></span><br><span class="line">        <span class="keyword">always</span></span><br><span class="line">        <span class="keyword">begin</span></span><br><span class="line">        #<span class="number">50</span></span><br><span class="line">        &#123;start, stop, store, reset&#125; = <span class="number">4'B0001</span>;</span><br><span class="line">        #<span class="number">50</span></span><br><span class="line">        &#123;start, stop, store, reset&#125; = <span class="number">4'B1000</span>;</span><br><span class="line">        #<span class="number">500</span></span><br><span class="line">        &#123;start, stop, store, reset&#125; = <span class="number">4'B0100</span>;</span><br><span class="line">        #<span class="number">50</span></span><br><span class="line">        &#123;start, stop, store, reset&#125; = <span class="number">4'B0010</span>;</span><br><span class="line">        #<span class="number">50</span></span><br><span class="line">        &#123;start, stop, store, reset&#125; = <span class="number">4'B1000</span>;</span><br><span class="line">        #<span class="number">50</span></span><br><span class="line">        &#123;start, stop, store, reset&#125; = <span class="number">4'B0100</span>;</span><br><span class="line">        #<span class="number">100</span></span><br><span class="line">        &#123;start, stop, store, reset&#125; = <span class="number">4'B0010</span>;</span><br><span class="line">        #<span class="number">50</span></span><br><span class="line">        &#123;start, stop, store, reset&#125; = <span class="number">4'B0001</span>;</span><br><span class="line">        #<span class="number">50</span></span><br><span class="line">        <span class="built_in">$stop</span>; <span class="comment">//stop simulation</span></span><br><span class="line">        <span class="keyword">end</span></span><br><span class="line">    <span class="keyword">end</span></span><br><span class="line"><span class="keyword">endmodule</span></span><br></pre></td></tr></table></figure><p></p><hr><h1 id="仿真结果"><a href="#仿真结果" class="headerlink" title="仿真结果"></a>仿真结果</h1><p>Run simulation，得到如下输出波形图。<br><img src="/verilog20200107211139/仿真结果.png" title="仿真结果"></p><hr><h1 id="遇到的问题"><a href="#遇到的问题" class="headerlink" title="遇到的问题"></a>遇到的问题</h1><ol><li><h2 id="报错：-Synth-8-462-no-clock-signal-specified-in-event-control"><a href="#报错：-Synth-8-462-no-clock-signal-specified-in-event-control" class="headerlink" title="报错：[Synth 8-462] no clock signal specified in event control"></a>报错：[Synth 8-462] no clock signal specified in event control</h2>我原本直接在顶层文件中定义时钟并使用forever生成连续的时钟信号，结果出现了如上报错。<br>在仿佛调整后，最后发现解决的办法是将时钟信号放在仿真文件里生成然后作为input输入到顶层文件。</li><li><h2 id="使用模块的输出赋值时遇到问题"><a href="#使用模块的输出赋值时遇到问题" class="headerlink" title="使用模块的输出赋值时遇到问题"></a>使用模块的输出赋值时遇到问题</h2>这个问题的主要原因还是因为我对于verilog赋值规则以及变量性质的不熟悉，这里做一个小归纳：<ol><li>给wire赋值必须用assign。</li><li>给reg赋值用always。</li><li>使用非阻塞赋值时，reg不能给wire赋值，反之则可以。</li><li>使用阻塞赋值时，reg可以给wire赋值，反之则不行。</li></ol></li></ol><script type="text/javascript" src="/js/src/bai.js"></script><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      
      
        &lt;!-- build time:Tue Jan 28 2020 15:40:41 GMT+0800 (GMT+08:00) --&gt;&lt;p&gt;继上一篇&lt;a href=&quot;https://gsy00517.github.io/logisim20200107121101/&quot; target=&quot;
      
    
    </summary>
    
    
      <category term="程序与设计" scheme="https://gsy00517.github.io/categories/%E7%A8%8B%E5%BA%8F%E4%B8%8E%E8%AE%BE%E8%AE%A1/"/>
    
    
      <category term="踩坑血泪" scheme="https://gsy00517.github.io/tags/%E8%B8%A9%E5%9D%91%E8%A1%80%E6%B3%AA/"/>
    
      <category term="数字电路" scheme="https://gsy00517.github.io/tags/%E6%95%B0%E5%AD%97%E7%94%B5%E8%B7%AF/"/>
    
      <category term="代码实现" scheme="https://gsy00517.github.io/tags/%E4%BB%A3%E7%A0%81%E5%AE%9E%E7%8E%B0/"/>
    
      <category term="verilog" scheme="https://gsy00517.github.io/tags/verilog/"/>
    
      <category term="Vivado" scheme="https://gsy00517.github.io/tags/Vivado/"/>
    
  </entry>
  
  <entry>
    <title>logisim笔记：基本使用及运动码表的实现</title>
    <link href="https://gsy00517.github.io/logisim20200107121101/"/>
    <id>https://gsy00517.github.io/logisim20200107121101/</id>
    <published>2020-01-07T04:11:01.000Z</published>
    <updated>2020-01-07T14:29:31.608Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Tue Jan 28 2020 15:40:41 GMT+0800 (GMT+08:00) --><p>似乎好久没有写新的博文了。由于今年的春节较往年要早，近一个月来，各门考试接踵而至让我忙得不可开交。虽然非常赶，终于还是考完了。当初想都不敢想的13天考10门的考试周也就这样熬过去了。事实证明，即便是很大的困难，逼一下自己还是能挺过来的。不过最后还是和往年的冬天一样发了个一年一度的高烧，真的难受，以后还得更加注重自己的身体。<br>废话不多说，这几天就打算先把这学期期末阶段一些有意义、有价值的的知识与经历整理记录一下。首先是一个运动码表的大作业，本文主要是使用Logisim对运动码表进行实现的方法与过程。</p><hr><h1 id="需求分析"><a href="#需求分析" class="headerlink" title="需求分析"></a>需求分析</h1><p>该项目要求我们设计一个简单的运动码表，包括四个按键组成的输入模块和四个7段数码管组成的输出模块。四个按键分别是：Start，按下时计时器归零并重新开始计时；Stop，按下时停止计时并显示计时数据；Store，按下时尝试更新系统记录，要求记录当前码表值，若已有记录，则和当前待记录的值作对比，如果计时数据比记录数据要小即用时要短，则更新系统记录并显示；Reset，按下时进行复位，将计时数据置为00.00，系统记录置为99.99。</p><hr><h1 id="Logisim使用"><a href="#Logisim使用" class="headerlink" title="Logisim使用"></a>Logisim使用</h1><p>在具体说明实现的方法之前，我想先把之前整理的一些Logisim的基本使用做一个简短的总结。Logisim的使用其实不难，可以参考网上整理的<a href="https://www.ituring.com.cn/space/personalarticle/178219?pcid=1436" target="_blank">Logisim文档翻译</a>就可以快速入门。实际上，在具备一定数字电路知识的情况下，到处点点也能够学会Logisim的基本使用方法。<br>Logisim为数字电路的设计提供了很大的帮助，我们可以通过填写真值表或者输入逻辑函数快速生成原本手动连线根本无法完成的复杂电路。<br>如果想要获得Logisim最新的优化版本，可以加入华科谭志虎教授创建的计算机硬件系统设计交流群（群号：957283876），此群汇集了多个高校的学生，是一个比较活跃的技术交流群，谭教授秒回且超赞的解答真的忍不住让人点赞！下面是一些我当初刚使用时记录在note上的一些Logisim的使用与常见处理，分享在此。</p><ol><li><h2 id="引脚"><a href="#引脚" class="headerlink" title="引脚"></a>引脚</h2><p>在Logisim中，引脚即Pin，可以使用键盘上、下、左、右光标键来改变引脚的朝向。</p><blockquote><p>注意：这里该表朝向不是按照旋转方向来的，而是按哪个方向就是指向哪个方向。</p></blockquote><p>此外，根据形状的不同，引脚分为输入引脚（较方）和输出（较圆）引脚，可以使用UI左上角最左侧的手型的戳工具点击对应的引脚来修改引脚的值（高电平、低电平、未知x态）。<br>当我们选中引脚时，按下alt+数字组合，就可以改变到对应的位宽。</p></li><li><h2 id="与门"><a href="#与门" class="headerlink" title="与门"></a>与门</h2>选中与门，按下数字键，就修改输入引脚个数。</li><li><h2 id="线颜色的含义"><a href="#线颜色的含义" class="headerlink" title="线颜色的含义"></a>线颜色的含义</h2><ul><li>亮绿色：高电平。</li><li>深绿色：低电平。</li><li>蓝色：未知状态。</li><li>灰色：飞线。</li><li>红色：信号冲突。</li><li>黑色：多位总线。</li><li>橙色：位宽不匹配。</li></ul></li><li><h2 id="时钟"><a href="#时钟" class="headerlink" title="时钟"></a>时钟</h2><ul><li>ctrl+K：驱动与关闭时钟连续运行。</li><li>ctrl+T：驱动时钟单步运行。</li></ul></li><li><h2 id="其它快捷键"><a href="#其它快捷键" class="headerlink" title="其它快捷键"></a>其它快捷键</h2>上面列出的都是一些并比较典型的特例，别的地方使用基本类似，可以多多尝试。下面再列出两个比较常用的快捷键：<ul><li>ctrl+D：创建副本。</li><li>ctrl+Z：撤销。</li></ul></li></ol><hr><h1 id="整体设计"><a href="#整体设计" class="headerlink" title="整体设计"></a>整体设计</h1><p>为了实现运动码表的功能，我们将整个项目拆分成如下几个模块：</p><ul><li>计时模块（包括计时使能模块）。</li><li>码表驱动与显示模块。</li><li>系统记录数据寄存模块。</li><li>码表状态功能控制模块。</li><li>数值比较模块。</li><li>数值选择模块。</li></ul><p>此外，根据需求分析，我们设计了如下6个状态，并构建状态机如下：</p><ul><li><strong>000 中间态</strong>：每次按键弹起后，回到该状态。</li><li><strong>101 复位</strong>：计时变成00.00，记录变成99.99，显示计时数值，时钟不计时。</li><li><strong>001 计时</strong>：计时变成00.00，记录不变，显示计时数值，时钟计时。</li><li><strong>010 停止</strong>：计时不变，记录不变，显示计时数值，时钟不计时。</li><li><strong>011 更新（小于系统记录NewRecord=1）</strong>：计时不变，记录变成计时，显示记录数值，时钟不计时。</li><li><strong>100 不更新（大于等于系统记录NewRecord=0）</strong>：计时不变，记录不变，显示记录数值，时钟不计时。<img src="/logisim20200107121101/状态图.png" title="状态图"></li></ul><p>可得状态转换关系的真值表如下所示：<br><img src="/logisim20200107121101/状态转换真值表.png" title="状态转换真值表"><br>为了实现上述状态转换与控制信号输出，我们设计了如下码表控制电路。<br><img src="/logisim20200107121101/转换控制电路.png" title="转换控制电路"><br>其中SD-EN控制寄存器使能，DP-SEL控制码表显示数值的选择，TM-Reset控制是否复位。</p><hr><h1 id="设计实现"><a href="#设计实现" class="headerlink" title="设计实现"></a>设计实现</h1><img src="/logisim20200107121101/文件内容.png" title="文件内容"><p>这是设计完成后最终circ文件中所包含的内容，下面我简述一下各个模块中的内容及思路。<br>数码管驱动：即将4位2进制数转化成7个二进制信号，驱动7段数码管的亮灭。<br>4位BCD计数器：基于下面的BCD计数器状态转换（即在0-9之间递增循环）和BCD计数器输出函数（即在达到9时输出进位信号）。<br>码表计数器：由四个4位BCD计数器组成。<br>码表显示驱动：即将四个4位二进制组成的数字通过四个7段数码管显示出来，内部基于上面的显示驱动转换电路即数码管驱动的封装。<br>码表控制器：上文的设计中已经提及，基于码表控制器状态转换（输入信号+现态-&gt;次态）和码表控制器输出函数（现态-&gt;控制信号）。<br>计时使能：这里我比较巧妙地运用了D触发器的置位清零两个端，将在后文提及。<br>最后就是运动码表的总电路图：<br><img src="/logisim20200107121101/总电路图.jpg" title="总电路图"></p><hr><h1 id="遇到的问题"><a href="#遇到的问题" class="headerlink" title="遇到的问题"></a>遇到的问题</h1><ol><li><h2 id="复位后自动开始计时"><a href="#复位后自动开始计时" class="headerlink" title="复位后自动开始计时"></a>复位后自动开始计时</h2>这是最让我头疼的一个问题，由于上述状态机的设计方法和Logisim中按键在鼠标松开后自动弹起的功能，导致在我按下Reset清零后系统会自动重新开始计时（因为这时的现态已经不是复位状态），这显然是不符合需求的。为了使计时使能在下一次按键到来之前能保持现态，我将复位控制信号从状态转换电路中单独取出，并使用一个D触发器的置位清零两个端子实现了这个保持功能，效果不错。<img src="/logisim20200107121101/计时使能电路.png" title="计时使能电路"></li><li><h2 id="计时器遇到9时跳跃进位"><a href="#计时器遇到9时跳跃进位" class="headerlink" title="计时器遇到9时跳跃进位"></a>计时器遇到9时跳跃进位</h2>当最初的电路实现完成后，我兴奋地开始计时，结果计时器的花式进位方式顿时让我傻了眼。仔细研究后我发现，我起初设计的电路在进位时只考虑了低一位计数器传来的进位信号，而事实上，高位的进位条件往往是由后几位共同决定的，为此，修改电路如下：<img src="/logisim20200107121101/计时模块.jpg" title="计时模块"> 问题解决。</li><li><h2 id="码表在更新数据的前一个时钟内会显示较大的记录数值"><a href="#码表在更新数据的前一个时钟内会显示较大的记录数值" class="headerlink" title="码表在更新数据的前一个时钟内会显示较大的记录数值"></a>码表在更新数据的前一个时钟内会显示较大的记录数值</h2>这个问题其实不是非常重要，但同班的一位同学还是注意到了这点。也就是说，当计时数据比系统记录要小的时候，系统应该更新记录并显示最好的成绩，然而当按下Store进行存储更新时，在前一个时钟周期内，码表会短暂地先显示原本较大即较差地系统记录，这是不希望出现的。<br>解决的办法可以在寄存器和显示选择器之间添加一个三态门，具体可以看上文中的总电路图。</li></ol><script type="text/javascript" src="/js/src/bai.js"></script><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      
      
        &lt;!-- build time:Tue Jan 28 2020 15:40:41 GMT+0800 (GMT+08:00) --&gt;&lt;p&gt;似乎好久没有写新的博文了。由于今年的春节较往年要早，近一个月来，各门考试接踵而至让我忙得不可开交。虽然非常赶，终于还是考完了。当初想都不敢想的1
      
    
    </summary>
    
    
      <category term="程序与设计" scheme="https://gsy00517.github.io/categories/%E7%A8%8B%E5%BA%8F%E4%B8%8E%E8%AE%BE%E8%AE%A1/"/>
    
    
      <category term="踩坑血泪" scheme="https://gsy00517.github.io/tags/%E8%B8%A9%E5%9D%91%E8%A1%80%E6%B3%AA/"/>
    
      <category term="Logisim" scheme="https://gsy00517.github.io/tags/Logisim/"/>
    
      <category term="数字电路" scheme="https://gsy00517.github.io/tags/%E6%95%B0%E5%AD%97%E7%94%B5%E8%B7%AF/"/>
    
  </entry>
  
  <entry>
    <title>matlab笔记：一个非线性方程问题的多种求解方法</title>
    <link href="https://gsy00517.github.io/matlab20191116003545/"/>
    <id>https://gsy00517.github.io/matlab20191116003545/</id>
    <published>2019-11-15T16:35:45.000Z</published>
    <updated>2020-01-07T14:28:52.724Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Tue Jan 28 2020 15:40:41 GMT+0800 (GMT+08:00) --><p>上周科学计算引论结课了，借着写上机报告的机会，我把书本上所有的求解非线性方程的方法（标量型）都用matlab实现了一下，并对一个实际工程问题进行求解。关于实现过程中遇到的问题以及注意事项，我已写在<a href="https://gsy00517.github.io/matlab20191110193640/" target="_blank">matlab笔记：久未使用之后踩的一堆坑</a>内。</p><hr><h1 id="实际问题"><a href="#实际问题" class="headerlink" title="实际问题"></a>实际问题</h1><p>在电机学中，凸极同步发电机的功角特性可表示为：</p><script type="math/tex;mode=display">P_{em}=\frac{E_{0}V}{x_{d}}sin\theta +V^{2}\left ( \frac{1}{x_{q}}-\frac{1}{x_{d}} \right )sin\theta \cdot cos\theta</script><p>式中，$P_{em}$表示发电机的电磁功率；$E_{0}$表示发电机电势；$V$表示发电机端电压；$x_{q}$表示横轴同步电抗；$x_{d}$表示纵轴同步电抗；$\theta$表示功率角，$\theta \in \left ( 0,\frac{\pi }{2} \right )$。<br>如令$\frac{E_{0}V}{x_{d}}=P_{j}$，$V^{2}\left ( \frac{1}{x_{q}}-\frac{1}{x_{d}} \right )=P_{2e}$，则上式可以简化为：</p><script type="math/tex;mode=display">P_{em}=P_{j}sin\theta +P_{2e}sin\theta \cdot cos\theta</script><p>在电力系统稳定计算中，我们往往要由上式求出功率角$\theta$。我们可以使用几何方法求解，也可以利用迭代法求解该非线性方程。<br>我们将上式变为：</p><script type="math/tex;mode=display">\theta =arcsin\frac{P_{em}}{P_{j}+P_{2e}cos\theta }</script><p>以许实章编《电机学习题集》第367页26-1为例，将$P_{em}=1$，$P_{j}=1.878$，$P_{2e}=0.75$代入，得到方程：</p><script type="math/tex;mode=display">\theta =arcsin\frac{1}{1.878+0.75cos\theta }</script><hr><h1 id="问题求解"><a href="#问题求解" class="headerlink" title="问题求解"></a>问题求解</h1><p>在《计算方法》第二章，我们学习了一些非线性方程的数值解法，这里我们分别使用几何法和迭代法求解上述问题并进行比较。设方程求解的预定精度为$0.001^{\circ}$即$1.7\times 10^{-3}rad$，由闭区间上连续函数的性质和初步估计，可确定方程的解位于区间$[0,\frac{\pi }{6}]$。</p><h2 id="几何方法"><a href="#几何方法" class="headerlink" title="几何方法"></a>几何方法</h2><p>由几何法的求解方法，我们定义函数：</p><script type="math/tex;mode=display">f(\theta )=arcsin\frac{1}{1.878+0.75cos\theta }-\theta</script><p>求解$\theta$即求解$f(\theta )$在$[0,\frac{\pi }{6}]$上的零点。<br>在MATLAB中，将该函数实现如下：<br></p><figure class="highlight matlab"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">function</span> <span class="params">[y]</span> = <span class="title">func</span><span class="params">(x)</span></span></span><br><span class="line"><span class="comment">%几何法函数方程</span></span><br><span class="line"><span class="comment">%统一使用弧度制</span></span><br><span class="line">format long <span class="comment">%为提高精度，保留更多位数</span></span><br><span class="line">y = <span class="built_in">asin</span>(<span class="number">1</span> / (<span class="number">1.878</span> + <span class="number">0.75</span> * <span class="built_in">cos</span>(x))) - x;</span><br><span class="line"><span class="keyword">end</span></span><br></pre></td></tr></table></figure><p></p><p>由于MATLAB计算过程中默认保留$4$位小数，为了提高精度，我使用了<code>format long</code>保留更多有效数字。值得注意的是，计算时统一使用弧度制，待求出解之后再使用<code>rad2deg</code>函数转化为角度。</p><h3 id="二分法"><a href="#二分法" class="headerlink" title="二分法"></a>二分法</h3><p>根据二分法的格式，编写二分法函数如下：<br></p><figure class="highlight matlab"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">function</span> <span class="params">[errors, ans, time]</span> = <span class="title">bisection</span><span class="params">(low, high, max, stopError)</span> </span></span><br><span class="line"><span class="comment">%二分法</span></span><br><span class="line"><span class="comment">%func是待求零点的函数</span></span><br><span class="line"><span class="comment">%low，high分别是解区间的上下限</span></span><br><span class="line"><span class="comment">%max是最多循环步数，防止死循环</span></span><br><span class="line"><span class="comment">%stopError是预定精度，作为终止条件</span></span><br><span class="line"><span class="comment">%errors记录每次循环的误差</span></span><br><span class="line"><span class="comment">%ans记录最终求解结果，表示为角度</span></span><br><span class="line"><span class="comment">%time是总的循环次数</span></span><br><span class="line">format long <span class="comment">%为提高精度，保留更多位数</span></span><br><span class="line">fl = func(low);</span><br><span class="line">fh = func(high);</span><br><span class="line">error = high - low;</span><br><span class="line"><span class="keyword">for</span> <span class="built_in">i</span> = <span class="number">1</span> : <span class="built_in">max</span></span><br><span class="line">    mid = (low + high ) / <span class="number">2</span>;</span><br><span class="line">    fm = func(mid);    </span><br><span class="line">    <span class="keyword">if</span> fm * fl &gt; <span class="number">0</span></span><br><span class="line">        low = mid;</span><br><span class="line">    <span class="keyword">else</span> high = mid;</span><br><span class="line">    <span class="keyword">end</span></span><br><span class="line">error = high - low;</span><br><span class="line">errors(<span class="built_in">i</span>) = error;</span><br><span class="line">    <span class="keyword">if</span> error &lt; stopError, <span class="keyword">break</span>, <span class="keyword">end</span></span><br><span class="line"><span class="keyword">end</span></span><br><span class="line">time = <span class="built_in">i</span>;</span><br><span class="line"><span class="built_in">ans</span> = rad2deg(mid); <span class="comment">%转换成角度</span></span><br><span class="line"><span class="keyword">end</span></span><br></pre></td></tr></table></figure><p></p><p>输入命令<code>[errorsB, ans, time] = bisection(0, pi / 6, 50, 1.7e-5)</code>，求得结果如下：<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">errorsB =</span><br><span class="line"></span><br><span class="line">  列 1 至 5</span><br><span class="line"></span><br><span class="line">   0.261799387799149   0.130899693899575   0.065449846949787   0.032724923474894   0.016362461737447</span><br><span class="line"></span><br><span class="line">  列 6 至 10</span><br><span class="line"></span><br><span class="line">   0.008181230868723   0.004090615434362   0.002045307717181   0.001022653858590   0.000511326929295</span><br><span class="line"></span><br><span class="line">  列 11 至 15</span><br><span class="line"></span><br><span class="line">   0.000255663464648   0.000127831732324   0.000063915866162   0.000031957933081   0.000015978966540</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">ans =</span><br><span class="line"></span><br><span class="line">  22.909240722656250</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">time =</span><br><span class="line"></span><br><span class="line">    15</span><br></pre></td></tr></table></figure><p></p><p>使用二分法共迭代$15$次，求得结果为$22.909240722656250^{\circ}$。此外，迭代误差为$0.000015978966540$，符合预设精度要求。然而，此种方法计算次数较多，因此我又尝试了下面的方法。</p><h3 id="弦截法"><a href="#弦截法" class="headerlink" title="弦截法"></a>弦截法</h3><p>根据弦截法的计算方法，编写弦截法函数如下：<br></p><figure class="highlight matlab"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">function</span> <span class="params">[errors, ans, time]</span> = <span class="title">linecut</span><span class="params">(a, b, max, stopError)</span></span></span><br><span class="line"><span class="comment">%弦截法</span></span><br><span class="line"><span class="comment">%func是待求零点的函数</span></span><br><span class="line"><span class="comment">%a，b是在解的领域取的两点，这里取解区间的两个端点</span></span><br><span class="line"><span class="comment">%max是最多循环步数，防止死循环</span></span><br><span class="line"><span class="comment">%stopError是预定精度，作为终止条件</span></span><br><span class="line"><span class="comment">%errors记录每次循环的误差</span></span><br><span class="line"><span class="comment">%ans记录最终求解结果，表示为角度</span></span><br><span class="line"><span class="comment">%time是总的循环次数</span></span><br><span class="line">format long <span class="comment">%为提高精度，保留更多位数</span></span><br><span class="line">fa = func(a);</span><br><span class="line">fb = func(b);</span><br><span class="line">error = <span class="built_in">abs</span>(a - b); </span><br><span class="line"><span class="keyword">for</span> <span class="built_in">i</span> = <span class="number">1</span> : <span class="built_in">max</span></span><br><span class="line">    x = b - (b - a) * fb / (fb - fa);</span><br><span class="line">    a = b;</span><br><span class="line">    b = x;</span><br><span class="line">    fa = func(a);</span><br><span class="line">    fb = func(b);</span><br><span class="line">    error = <span class="built_in">abs</span>(a - b);</span><br><span class="line">    errors(<span class="built_in">i</span>) = error;</span><br><span class="line">    <span class="keyword">if</span> error &lt; stopError, <span class="keyword">break</span>, <span class="keyword">end</span></span><br><span class="line"><span class="keyword">end</span></span><br><span class="line">time = <span class="built_in">i</span>;</span><br><span class="line"><span class="built_in">ans</span> = rad2deg(b); <span class="comment">%转换成角度</span></span><br><span class="line"><span class="keyword">end</span></span><br></pre></td></tr></table></figure><p></p><p>输入命令<code>[errorsL, ans, time] = linecut(0, pi / 6, 50, 1.7e-5)</code>，求得结果如下：<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">errorsL =</span><br><span class="line"></span><br><span class="line">   0.120609794441825   0.003164447033051   0.000026217912123   0.000000005427336</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">ans =</span><br><span class="line"></span><br><span class="line">  22.909760215805360</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">time =</span><br><span class="line"></span><br><span class="line">     4</span><br></pre></td></tr></table></figure><p></p><p>与上面的二分法相比，弦截法只需计算$4$次，效率大大提高。计算所得结果为$22.909760215805360^{\circ}$，迭代误差为$0.000000005427336$，符合预设精度要求，而且比二分法的最终迭代误差更小，显然可以发现弦截法的收敛速度要快于二分法。下面我再用弦截法的改造方法Steffensen方法进行试验。</p><h3 id="Steffensen方法"><a href="#Steffensen方法" class="headerlink" title="Steffensen方法"></a>Steffensen方法</h3><p>根据Steffensen方法的计算方法，编写函数如下：<br></p><figure class="highlight matlab"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">function</span> <span class="params">[errors, ans, time]</span> = <span class="title">steffensen</span><span class="params">(x, max, stopError)</span></span></span><br><span class="line"><span class="comment">%Steffensen方法</span></span><br><span class="line"><span class="comment">%func是待求零点的函数</span></span><br><span class="line"><span class="comment">%x是初始点</span></span><br><span class="line"><span class="comment">%max是最多循环步数，防止死循环</span></span><br><span class="line"><span class="comment">%stopError是预定精度，作为终止条件</span></span><br><span class="line"><span class="comment">%errors记录每次循环的误差</span></span><br><span class="line"><span class="comment">%ans记录最终求解结果，表示为角度</span></span><br><span class="line"><span class="comment">%time是总的循环次数</span></span><br><span class="line">format long <span class="comment">%为提高精度，保留更多位数</span></span><br><span class="line">f = func(x);</span><br><span class="line"><span class="keyword">for</span> <span class="built_in">i</span> = <span class="number">1</span> : <span class="built_in">max</span></span><br><span class="line">    o = x - f ^ <span class="number">2</span> / (f - func(x - f));</span><br><span class="line">    error = <span class="built_in">abs</span>(x - o);</span><br><span class="line">    x = o;</span><br><span class="line">    f = func(o);</span><br><span class="line">    errors(<span class="built_in">i</span>) = error;</span><br><span class="line">    <span class="keyword">if</span> error &lt; stopError, <span class="keyword">break</span>, <span class="keyword">end</span></span><br><span class="line"><span class="keyword">end</span></span><br><span class="line">time = <span class="built_in">i</span>;</span><br><span class="line"><span class="built_in">ans</span> = rad2deg(o); <span class="comment">%转换成角度</span></span><br><span class="line"><span class="keyword">end</span></span><br></pre></td></tr></table></figure><p></p><p>选取初始点为$0$，输入命令<code>[errorsS1, ans, time] = steffensen(0, 50, 1.7e-5)</code>，求得结果如下：<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">errorsS1 =</span><br><span class="line"></span><br><span class="line">   0.381516143583955   0.018291709371805   0.000042893415627   0.000000000236814</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">ans =</span><br><span class="line"></span><br><span class="line">  22.909760215804820</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">time =</span><br><span class="line"></span><br><span class="line">     4</span><br></pre></td></tr></table></figure><p></p><p>发现计算次数没有比弦截法少，因此修改初值为$\frac{\pi }{6}$，再次计算，得结果：<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">errorsS2 =</span><br><span class="line"></span><br><span class="line">   0.125855705283236   0.002107105082521   0.000000571210576</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">ans =</span><br><span class="line"></span><br><span class="line">  22.909760215802425</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">time =</span><br><span class="line"></span><br><span class="line">     3</span><br></pre></td></tr></table></figure><p></p><p>一般而言，Steffensen方法的收敛速度要快于弦截法，但这与初始点的选取有关。对于这个问题，当设初始点为$0$时，Steffensen方法的迭代次数与弦截法持平；当设初始点为$\frac{\pi }{6}$时，迭代次数才小于弦截法。因此，想让Steffensen方法更快地收敛，需选取合适的初始点。在我看来，Steffensen方法的一大优势就是它是一种单步迭代方法，相比二步迭代方法的弦截法，Steffensen方法只需要一个初值就可以开始迭代。</p><h2 id="Picard迭代法"><a href="#Picard迭代法" class="headerlink" title="Picard迭代法"></a>Picard迭代法</h2><p>上述的方法都是基于几何图形的求解方法，而下面的Picard迭代法则是基于不动点原理给出的。<br>首先，我们编写迭代函数：<br></p><figure class="highlight matlab"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">function</span> <span class="params">[y]</span> = <span class="title">interation</span><span class="params">(x)</span></span></span><br><span class="line"><span class="comment">%迭代函数</span></span><br><span class="line"><span class="comment">%统一使用弧度制</span></span><br><span class="line">format long <span class="comment">%为提高精度，保留更多位数</span></span><br><span class="line">y = <span class="built_in">asin</span>(<span class="number">1</span> / (<span class="number">1.878</span> + <span class="number">0.75</span> * <span class="built_in">cos</span>(x)));</span><br><span class="line"><span class="keyword">end</span></span><br></pre></td></tr></table></figure><p></p><p>我们使用如下命令查看函数图像：<br></p><figure class="highlight matlab"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt; x = <span class="number">0</span> : <span class="built_in">pi</span> / <span class="number">36</span> : <span class="built_in">pi</span> / <span class="number">6</span>;</span><br><span class="line">&gt;&gt; y = <span class="built_in">asin</span>(<span class="number">1</span> * (<span class="number">1.878</span> + <span class="number">0.75</span> * <span class="built_in">cos</span>(x)) .^ (<span class="number">-1</span>));</span><br><span class="line">&gt;&gt; <span class="built_in">plot</span>(x, y)</span><br></pre></td></tr></table></figure><p></p><p>得到：<br><img src="/matlab20191116003545/函数图像.jpg" title="函数图像"><br>易验证，该函数在$[0,\frac{\pi }{6}]$上满足Picard迭代条件。</p><h3 id="Picard迭代法-1"><a href="#Picard迭代法-1" class="headerlink" title="Picard迭代法"></a>Picard迭代法</h3><p>根据Picard迭代法原理，定义Picard迭代函数：<br></p><figure class="highlight matlab"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">function</span> <span class="params">[errors, ans, time]</span> = <span class="title">picard</span><span class="params">(x, max, stopError)</span></span></span><br><span class="line"><span class="comment">%Picard迭代法</span></span><br><span class="line"><span class="comment">%interation是迭代函数</span></span><br><span class="line"><span class="comment">%x是初始点</span></span><br><span class="line"><span class="comment">%max是最多循环步数，防止死循环</span></span><br><span class="line"><span class="comment">%stopError是预定精度，作为终止条件</span></span><br><span class="line"><span class="comment">%errors记录每次循环的误差</span></span><br><span class="line"><span class="comment">%ans记录最终求解结果，表示为角度</span></span><br><span class="line"><span class="comment">%time是总的循环次数</span></span><br><span class="line">format long <span class="comment">%为提高精度，保留更多位数</span></span><br><span class="line"><span class="keyword">for</span> <span class="built_in">i</span> = <span class="number">1</span> : <span class="built_in">max</span></span><br><span class="line">    o = interation(x);</span><br><span class="line">    error = <span class="built_in">abs</span>(x - o);</span><br><span class="line">    x = o;</span><br><span class="line">    errors(<span class="built_in">i</span>) = error;</span><br><span class="line">    <span class="keyword">if</span> error &lt; stopError, <span class="keyword">break</span>, <span class="keyword">end</span></span><br><span class="line"><span class="keyword">end</span></span><br><span class="line">time = <span class="built_in">i</span>;</span><br><span class="line"><span class="built_in">ans</span> = rad2deg(o); <span class="comment">%转换成角度</span></span><br><span class="line"><span class="keyword">end</span></span><br></pre></td></tr></table></figure><p></p><p>输入命令<code>[errorsP, ans, time] = picard(0, 50, 1.7e-5)</code>求解：<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">errorsP =</span><br><span class="line"></span><br><span class="line">   0.390355832559508   0.009044503640668   0.000428788831489   0.000020583068808   0.000000988625736</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">ans =</span><br><span class="line"></span><br><span class="line">  22.909757357777192</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">time =</span><br><span class="line"></span><br><span class="line">     5</span><br></pre></td></tr></table></figure><p></p><p>Picard迭代结果为$22.909757357777192^{\circ}$，且精度符合要求。下面使用Picard迭代法的改进Aitken加速迭代法进行试验。</p><h3 id="Aitken加速迭代法"><a href="#Aitken加速迭代法" class="headerlink" title="Aitken加速迭代法"></a>Aitken加速迭代法</h3><p>编写Aitken加速迭代法函数代码如下：<br></p><figure class="highlight matlab"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">function</span> <span class="params">[errors, ans, time]</span> = <span class="title">aitken</span><span class="params">(x, max, stopError)</span></span></span><br><span class="line"><span class="comment">%Aitken迭代法</span></span><br><span class="line"><span class="comment">%interation是迭代函数</span></span><br><span class="line"><span class="comment">%x是初始点</span></span><br><span class="line"><span class="comment">%max是最多循环步数，防止死循环</span></span><br><span class="line"><span class="comment">%stopError是预定精度，作为终止条件</span></span><br><span class="line"><span class="comment">%errors记录每次循环的误差</span></span><br><span class="line"><span class="comment">%ans记录最终求解结果，表示为角度</span></span><br><span class="line"><span class="comment">%time是总的循环次数</span></span><br><span class="line">format long <span class="comment">%为提高精度，保留更多位数</span></span><br><span class="line"><span class="keyword">for</span> <span class="built_in">i</span> = <span class="number">1</span> : <span class="built_in">max</span></span><br><span class="line">    x1 = interation(x);</span><br><span class="line">    x2 = interation(x1);</span><br><span class="line">    x3 = x2 - (x2 - x1) ^ <span class="number">2</span> / (x2 - <span class="number">2</span> * x1 + x);</span><br><span class="line">    error = <span class="built_in">abs</span>(x3 - x);</span><br><span class="line">    x = x3;</span><br><span class="line">    errors(<span class="built_in">i</span>) = error;</span><br><span class="line">    <span class="keyword">if</span> error &lt; stopError, <span class="keyword">break</span>, <span class="keyword">end</span></span><br><span class="line"><span class="keyword">end</span></span><br><span class="line">time = <span class="built_in">i</span>;</span><br><span class="line"><span class="built_in">ans</span> = rad2deg(x); <span class="comment">%转换成角度</span></span><br><span class="line"><span class="keyword">end</span></span><br></pre></td></tr></table></figure><p></p><p>输入命令<code>[errorsA, ans, time] = aitken(0, 50, 1.7e-5)</code>求解得到：<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">errorsA =</span><br><span class="line"></span><br><span class="line">   0.399614867056990   0.000235879375045   0.000000000176165</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">ans =</span><br><span class="line"></span><br><span class="line">  22.909760215804827</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">time =</span><br><span class="line"></span><br><span class="line">     3</span><br></pre></td></tr></table></figure><p></p><p>可以看到，Aitken加速迭代法仅需3次迭代就得到了符合条件的解，且它的迭代误差只用$0.000000000176165$，小于上述所有的方法，由此该方法的优势得以体现。</p><h2 id="Newton迭代法"><a href="#Newton迭代法" class="headerlink" title="Newton迭代法"></a>Newton迭代法</h2><p>Newton迭代法也是一种求解非线性方程的高效算法，因此我也对其进行实现。<br>这里要用到<code>func</code>的导数，经计算，编写<code>func</code>的导函数为程序<code>df</code>：<br></p><figure class="highlight matlab"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">function</span> <span class="params">[y]</span> = <span class="title">df</span><span class="params">(x)</span></span></span><br><span class="line"><span class="comment">%求导数</span></span><br><span class="line"><span class="comment">%统一使用弧度制</span></span><br><span class="line">format long</span><br><span class="line">y = (<span class="number">0.75</span> * <span class="built_in">sin</span>(x) / (<span class="number">1</span> - (<span class="number">1</span> / (<span class="number">1.878</span> + <span class="number">0.75</span> * <span class="built_in">cos</span>(x))) ^ <span class="number">2</span>) ^ <span class="number">0.5</span>) / (<span class="number">1.878</span> + <span class="number">0.75</span> * <span class="built_in">cos</span>(x)) ^ <span class="number">2</span> - <span class="number">1</span>;</span><br><span class="line"><span class="keyword">end</span></span><br></pre></td></tr></table></figure><p></p><h3 id="Newton迭代法-1"><a href="#Newton迭代法-1" class="headerlink" title="Newton迭代法"></a>Newton迭代法</h3><p>编写Newton迭代法的计算程序如下：<br></p><figure class="highlight matlab"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">function</span> <span class="params">[errors, ans, time]</span> = <span class="title">newton</span><span class="params">(x, max, stopError)</span></span></span><br><span class="line"><span class="comment">%Newton迭代法</span></span><br><span class="line"><span class="comment">%func是待求零点的函数</span></span><br><span class="line"><span class="comment">%x是初始点</span></span><br><span class="line"><span class="comment">%max是最多循环步数，防止死循环</span></span><br><span class="line"><span class="comment">%stopError是预定精度，作为终止条件</span></span><br><span class="line"><span class="comment">%errors记录每次循环的误差</span></span><br><span class="line"><span class="comment">%ans记录最终求解结果，表示为角度</span></span><br><span class="line"><span class="comment">%time是总的循环次数</span></span><br><span class="line">format long <span class="comment">%为提高精度，保留更多位数</span></span><br><span class="line"><span class="keyword">for</span> <span class="built_in">i</span> = <span class="number">1</span> : <span class="built_in">max</span></span><br><span class="line">    o = x - func(x) / df(x);</span><br><span class="line">    error = <span class="built_in">abs</span>(o - x);</span><br><span class="line">    x = o;</span><br><span class="line">    errors(<span class="built_in">i</span>) = error;</span><br><span class="line">    <span class="keyword">if</span> error &lt; stopError, <span class="keyword">break</span>, <span class="keyword">end</span></span><br><span class="line"><span class="keyword">end</span></span><br><span class="line">time = <span class="built_in">i</span>;</span><br><span class="line"><span class="built_in">ans</span> = rad2deg(x); <span class="comment">%转换成角度</span></span><br><span class="line"><span class="keyword">end</span></span><br></pre></td></tr></table></figure><p></p><p>输入命令<code>[errorsN, ans, time] = newton(0, 50, 1.7e-5)</code>进行计算，得解：<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">errorsN =</span><br><span class="line"></span><br><span class="line">   0.390355832559508   0.009488988787132   0.000005925259246</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">ans =</span><br><span class="line"></span><br><span class="line">  22.909760215672179</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">time =</span><br><span class="line"></span><br><span class="line">     3</span><br></pre></td></tr></table></figure><p></p><h3 id="Newton下山法"><a href="#Newton下山法" class="headerlink" title="Newton下山法"></a>Newton下山法</h3><p>为尽可能避免因初值选取不当导致计算过程缓慢收敛或者发散（经上面计算，此问题不存在这种情况），引入下山因子$\lambda \in (0,1]$，得到改进后的Newton下山法，其计算程序如下：<br></p><figure class="highlight matlab"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">function</span> <span class="params">[errors, ans, time]</span> = <span class="title">hill</span><span class="params">(x, max, stopError)</span></span></span><br><span class="line"><span class="comment">%Newton下山法</span></span><br><span class="line"><span class="comment">%   此处显示详细说明</span></span><br><span class="line">format long <span class="comment">%为提高精度，保留更多位数</span></span><br><span class="line">l = <span class="number">1</span>;</span><br><span class="line"><span class="keyword">for</span> <span class="built_in">i</span> = <span class="number">1</span> : <span class="built_in">max</span></span><br><span class="line">    o = x - l * func(x) / df(x);</span><br><span class="line">    <span class="keyword">while</span> <span class="built_in">abs</span>(func(o)) &gt; <span class="built_in">abs</span>(func(x)) <span class="comment">%不满足下山条件</span></span><br><span class="line">        l = l / <span class="number">2</span>;</span><br><span class="line">        o = x - l * func(x) / df(x);</span><br><span class="line">    <span class="keyword">end</span></span><br><span class="line">    error = <span class="built_in">abs</span>(o - x);</span><br><span class="line">    x = o;</span><br><span class="line">    errors(<span class="built_in">i</span>) = error;</span><br><span class="line">    <span class="keyword">if</span> error &lt; stopError, <span class="keyword">break</span>, <span class="keyword">end</span></span><br><span class="line"><span class="keyword">end</span></span><br><span class="line">time = <span class="built_in">i</span>;</span><br><span class="line"><span class="built_in">ans</span> = rad2deg(x); <span class="comment">%转换成角度</span></span><br><span class="line"><span class="keyword">end</span></span><br></pre></td></tr></table></figure><p></p><p>计算得到结果如下：<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">errorsH =</span><br><span class="line"></span><br><span class="line">   0.390355832559508   0.009488988787132   0.000005925259246</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">ans =</span><br><span class="line"></span><br><span class="line">  22.909760215672179</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">time =</span><br><span class="line"></span><br><span class="line">     3</span><br></pre></td></tr></table></figure><p></p><p>由于此问题初值选取得当，即初值取$0$时使用一般Newton迭代法不存在缓慢收敛或者发散的问题，因此Newton下山法在这里并没有发挥作用。可以看到，Newton迭代法仅$3$步就完成了求解的任务，非常高效。</p><hr><h1 id="问题的解"><a href="#问题的解" class="headerlink" title="问题的解"></a>问题的解</h1><p>以上方法都得到了一致的答案，根据精度要求，我们得出此条件下功率角$\theta$为$22.910^{\circ}$，与许实章编《电机学习题集》中的例题答案$22.9^{\circ}$相吻合。</p><hr><h1 id="方法比较"><a href="#方法比较" class="headerlink" title="方法比较"></a>方法比较</h1><p>将上述各种方法的误差记录绘制成图表，由于Newton下山法在该问题中没有发挥作用，因此仅作出Newton迭代法的迭代误差图像。<br><img src="/matlab20191116003545/不同算法收敛速度比较.jpg" title="不同算法收敛速度比较"><br>根据图片，我们可以观察到以上各个方法均收敛。其中，表现较为优越的有Aitken加速迭代法、Newton迭代法和Steffensen迭代法，均用了$3$次迭代就达到了精度要求。然而这里Steffensen法的收敛速度更依赖于初值的选取，当初值选为$0$时，它的收敛速度就类似于弦截法在该问题上的收敛速度了。因此，总的来说，对于这个问题，最高效的算法是Aitken加速迭代法和Newton迭代法。另外，二分法最简单却也最低效，迭代了$15$次才达到预设的精度要求。可见，各种算法的效率大致上与它们的复杂度和高级程度成正比关系。</p><script type="text/javascript" src="/js/src/bai.js"></script><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      
      
        &lt;!-- build time:Tue Jan 28 2020 15:40:41 GMT+0800 (GMT+08:00) --&gt;&lt;p&gt;上周科学计算引论结课了，借着写上机报告的机会，我把书本上所有的求解非线性方程的方法（标量型）都用matlab实现了一下，并对一个实际工程问题进行
      
    
    </summary>
    
    
      <category term="程序与设计" scheme="https://gsy00517.github.io/categories/%E7%A8%8B%E5%BA%8F%E4%B8%8E%E8%AE%BE%E8%AE%A1/"/>
    
    
      <category term="matlab" scheme="https://gsy00517.github.io/tags/matlab/"/>
    
      <category term="代码实现" scheme="https://gsy00517.github.io/tags/%E4%BB%A3%E7%A0%81%E5%AE%9E%E7%8E%B0/"/>
    
  </entry>
  
  <entry>
    <title>verilog笔记：数值比较器实现与vivado使用</title>
    <link href="https://gsy00517.github.io/verilog20191115233116/"/>
    <id>https://gsy00517.github.io/verilog20191115233116/</id>
    <published>2019-11-15T15:31:16.000Z</published>
    <updated>2020-01-19T00:27:14.020Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Tue Jan 28 2020 15:40:41 GMT+0800 (GMT+08:00) --><p>本周数字电路课程老师布置了一个利用verilog语言进行数值比较器波形仿真的作业。可以利用Modelsim或者Vivado实现。由于Vivado默认安装大小就有将近30个GB（2018版好像是27GB左右，2014版是12.68GB，这版本的容量增速跟maltab有得一拼啊），因此之前装了之后不太会使用便又卸了。最近刚好趁着双十一降价给自己的laptop加了一个SSD，因此正好赶快学习一下如何使用。有关如何给笔记本加装SSD的问题，这里有两个视频可以解决，<a href="https://www.bilibili.com/video/av55069565" target="_blank">安装准备与步骤</a>、<a href="https://www.bilibili.com/video/av44978237" target="_blank">安装后点亮磁盘</a>。</p><p><strong>References</strong>：</p><p>电子文献：<br><a href="https://blog.csdn.net/qq_41154156/article/details/80989125" target="_blank" rel="noopener">https://blog.csdn.net/qq_41154156/article/details/80989125</a><br><a href="https://wenku.baidu.com/view/0294cbb3bb4cf7ec4bfed01a.html" target="_blank" rel="noopener">https://wenku.baidu.com/view/0294cbb3bb4cf7ec4bfed01a.html</a></p><hr><h1 id="关于vivado"><a href="#关于vivado" class="headerlink" title="关于vivado"></a>关于vivado</h1><p>相比于Modelsim，Vivado的UI还是要舒服许多的，有点像Multisim之于Pspice。关于Vivado的使用，上面参考的文章中的步骤比较详细，照做一遍之后基本就会了。</p><hr><h1 id="1位数值比较器"><a href="#1位数值比较器" class="headerlink" title="1位数值比较器"></a>1位数值比较器</h1><p>1位数值比较器的逻辑图如下：<br><img src="/verilog20191115233116/1位数值比较器.jpg" title="1位数值比较器"><br>使用verilog代码实现：<br></p><figure class="highlight verilog"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">module</span> <span class="number">_1</span>bit_Comp(</span><br><span class="line">    <span class="keyword">input</span> A,</span><br><span class="line">    <span class="keyword">input</span> B,</span><br><span class="line">    <span class="keyword">output</span> AGB,</span><br><span class="line">    <span class="keyword">output</span> AEB,</span><br><span class="line">    <span class="keyword">output</span> ALB</span><br><span class="line">    );</span><br><span class="line">    <span class="keyword">wire</span> Anot, Bnot;</span><br><span class="line">    <span class="keyword">not</span> n0(Anot, A),</span><br><span class="line">        n1(Bnot, B);</span><br><span class="line">    <span class="keyword">and</span> n2(AGB, A, Bnot),</span><br><span class="line">        n3(ALB, Anot, B);</span><br><span class="line">    <span class="keyword">nor</span> n4(AEB, AGB, ALB);</span><br><span class="line"><span class="keyword">endmodule</span></span><br></pre></td></tr></table></figure><p></p><p>为了输出仿真波形，新建一个仿真文件：<br></p><figure class="highlight verilog"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">module</span> simulateFile();</span><br><span class="line">    <span class="keyword">reg</span> A, B;</span><br><span class="line">    <span class="keyword">wire</span> AGB, AEB, ALB;</span><br><span class="line">    <span class="number">_1</span>bit_Comp u1(A, B, AGB, AEB, ALB);</span><br><span class="line">    <span class="keyword">initial</span> </span><br><span class="line">        <span class="keyword">begin</span> A = <span class="number">0</span>; B = <span class="number">0</span>;</span><br><span class="line">    <span class="keyword">end</span></span><br><span class="line">    <span class="keyword">always</span> #<span class="number">50</span> &#123;A, B&#125; = &#123;A, B&#125; + <span class="number">1</span>;</span><br><span class="line"><span class="keyword">endmodule</span></span><br></pre></td></tr></table></figure><p></p><p>其中，过程赋值语句always只能给寄存器类型变量赋值，因此，在这里A、B要定义为reg类型。<br>这里“#50”表示延时，使用{A, B}使AB变成二进制数，方便生成所有不同的输入，在这里即00、01、10、11。<br>Run Simulation，输出波形：<br><img src="/verilog20191115233116/1位输出波形.png" title="1位输出波形"></p><hr><h1 id="2位数值比较器"><a href="#2位数值比较器" class="headerlink" title="2位数值比较器"></a>2位数值比较器</h1><p>2位数值比较器的逻辑图如下：<br><img src="/verilog20191115233116/2位数值比较器.jpg" title="2位数值比较器"><br>使用verilog代码，调用1位数值比较器，实现2位数值比较器如下：<br></p><figure class="highlight verilog"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">module</span> <span class="number">_2</span>bit_Comp(</span><br><span class="line">    <span class="keyword">input</span> A1,</span><br><span class="line">    <span class="keyword">input</span> A0,</span><br><span class="line">    <span class="keyword">input</span> B1,</span><br><span class="line">    <span class="keyword">input</span> B0,</span><br><span class="line">    <span class="keyword">output</span> FAGB,</span><br><span class="line">    <span class="keyword">output</span> FAEB,</span><br><span class="line">    <span class="keyword">output</span> FALB</span><br><span class="line">    );</span><br><span class="line">    <span class="keyword">wire</span> AGB1, AEB1, ALB1, AGB0, AEB0, ALB0; <span class="comment">//signal inside</span></span><br><span class="line">    <span class="keyword">wire</span> G1O, G2O; <span class="comment">//the output of and gate G1, G2</span></span><br><span class="line">    <span class="number">_1</span>bit_Comp C1(A1, B1, AGB1, AEB1, ALB1); <span class="comment">//Instantiate 1-bit Comparator</span></span><br><span class="line">    <span class="number">_1</span>bit_Comp C0(A0, B0, AGB0, AEB0, ALB0);</span><br><span class="line">    <span class="keyword">and</span> G1(G1O, AEB1, AGB0),</span><br><span class="line">        G2(G2O, AEB1, ALB0),</span><br><span class="line">        G3(FAEB, AEB1, AEB0);</span><br><span class="line">    <span class="keyword">or</span> G4(FAGB, AGB1, G1O);</span><br><span class="line">    <span class="keyword">or</span> G5(FALB, ALB1, G2O);</span><br><span class="line"><span class="keyword">endmodule</span></span><br></pre></td></tr></table></figure><p></p><p>可以使用RTL ANALYSIS来仿真出2位数值比较器的RTL schematic电子原理图。<br><img src="/verilog20191115233116/RTL逻辑图.png" title="RTL逻辑图"><br>类似的，编写仿真文件：<br></p><figure class="highlight verilog"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">module</span> simulateAgain();</span><br><span class="line">    <span class="keyword">reg</span> A1, A0, B1, B0;</span><br><span class="line">    <span class="keyword">wire</span> FAGB, FAEB, FALB;</span><br><span class="line">    <span class="number">_2</span>bit_Comp u2(A1, A0, B1, B0, FAGB, FAEB, FALB);</span><br><span class="line">    <span class="keyword">initial</span></span><br><span class="line">        <span class="keyword">begin</span> A1 = <span class="number">0</span>; A0 = <span class="number">0</span>; B1 = <span class="number">0</span>; B0 = <span class="number">0</span>;</span><br><span class="line">    <span class="keyword">end</span></span><br><span class="line">    <span class="keyword">always</span> #<span class="number">30</span> &#123;A1, A0, B1, B0&#125; = &#123;A1, A0, B1, B0&#125; + <span class="number">1</span>;</span><br><span class="line"><span class="keyword">endmodule</span></span><br></pre></td></tr></table></figure><p></p><p>Run Simulation，输出波形：<br><img src="/verilog20191115233116/2位输出波形.png" title="2位输出波形"></p><hr><h1 id="出现的问题"><a href="#出现的问题" class="headerlink" title="出现的问题"></a>出现的问题</h1><ol><li><h2 id="报错：-Synth-8-439-module’xxx’not-found"><a href="#报错：-Synth-8-439-module’xxx’not-found" class="headerlink" title="报错：[Synth 8-439] module’xxx’not found"></a>报错：[Synth 8-439] module’xxx’not found</h2><p>当初遇到这个问题后，我的第一反应是上网搜索原因。得到的解释有模块未添加、IP未正确设置等。<br>对照网上的解决方案之后，我发现除了我无法理解的，网上所述的问题我都不存在。于是我只好独立进行思考。<br>果然，我还犯不了像网上那样“高级”的错误。错误的原代码如下：</p><figure class="highlight verilog"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">not</span> n0(Anot, A);</span><br><span class="line">    n1(Bnot, B);</span><br></pre></td></tr></table></figure><p>报错：[Synth 8-439] module’n1’not found。<br>当我调用门的时候，由于内部变量换行导致我将逗号误用成了分号，因此导致分号之后的变量not found，修改后错误即可解决。<br>其实，这个问题仔细观察即可发觉，相比于n1，同样格式的n0就没有报错，那么很有可能错误就在两者之间。</p></li><li><h2 id="ERROR-Common-17-39-‘xxx’-failed-due-to-earlier-errors"><a href="#ERROR-Common-17-39-‘xxx’-failed-due-to-earlier-errors" class="headerlink" title="ERROR: [Common 17-39] ‘xxx’ failed due to earlier errors"></a>ERROR: [Common 17-39] ‘xxx’ failed due to earlier errors</h2>这是我在执行仿真文件时遇到的error。仔细检查后，发现错误也与上一个问题相同。由于我在设计完电路后没有Run Synthesis综合并生成网表文件来进行检验，也没有进行其它的仿真操作，因此之前并没能发现这个问题。于是最后当调用该电路的仿真文件开始运行时就会报告这样的错误。</li></ol><hr><h1 id="要注意的点"><a href="#要注意的点" class="headerlink" title="要注意的点"></a>要注意的点</h1><ol><li>和matlab中函数文件的要求类似，verilog定义模块时，需要新建的模块文件名称与模块的文件名称一致。例如，我上面的1位数值比较器module名为_1bit_Comp，那么对应的文件名就应该是_1bit_Comp.v。此外，每个模块应使用一个文件来表示，且一个文件最多能表示一个模块（可以在其中调用其它模块，这点和matlab很像），两者呈一一对应关系。</li><li>新建project时，如要从RTL代码开始综合，就选择RTL project（默认的这个）。要注意的是，下面的“Do not specify sources at this time”（此时不定义源文件）可以勾上。否则，下一步会进入添加source file。</li><li>如果在一个project中已经建立了一个仿真文件，那么当你新建一个仿真文件时，需要建立在create的new file内，这样在后面对不同的仿真文件进行仿真时可以将对应的文件夹依次分别激活。</li><li>在source窗口中，一般情况下，Vivado会自动加粗识别出来的top module，同时对应module名称前面也会有一个二叉树状的图标表示这是顶层模块。有时候，软件也会识别错误或者与实际需求不符，这时候我们可以右键想要置顶的module，在弹出的菜单中点击Set As Top将其设为顶层。</li><li>当在同一个project中创建了多个仿真文件时，如要在进行完一次仿真之后对另一个仿真文件，需要对对应的文件夹进行激活。方法是右键仿真文件，然后在弹出的菜单中点击Make Active即可。<img src="/verilog20191115233116/激活.png" title="激活"></li></ol><hr><h1 id="加装固态盘"><a href="#加装固态盘" class="headerlink" title="加装固态盘"></a>加装固态盘</h1><p>前文提到给笔记本加装SSD，给了两个示范视频，这里我还是想再稍稍补充一下关于加装固态盘的一些事情。<br>首先必须确保自己的本有空位。我使用的是小电池版本，因此有一整个2.5英寸7mm的硬盘位，这个请在决定购买新的硬盘前和卖家自己核对确认。如果还是不放心，那么最好亲自拆开查看，眼见为实嘛。要注意的是，必须使用完全对应规格的螺丝刀（比如我使用的是梅花T5螺丝刀），否则很容易发生滑丝，即螺牙连接处由于受力过大或其它原因导致螺牙磨损而使螺牙无法咬合，这会为今后的拆机带来不必要的麻烦。<br><img src="/verilog20191115233116/闪迪SSD.JPG" title="闪迪SSD"><br><img src="/verilog20191115233116/SATA3数据线.JPG" title="SATA3数据线"><br>上面是我买的固态盘和数据线，相同或者类似机型的可以参考一下。在到货之后我发现，我所买的闪迪SSD较7mm稍薄些，因此平时拿动时（一般较大幅度翻动laptop时）会感到里面有东西松动的响声，不过使用至今没遇到任何问题。另外，为了适配各类硬盘，数据线的长度也有可能不能完全匹配，其实也没有关系，稍稍用力将数据线对应接头按入SATA3接口并用架子将固态盘固定即可。相比移动硬盘的USB接口，内置固态的SATA3读取速度还是相当不错的。</p><script type="text/javascript" src="/js/src/bai.js"></script><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      
      
        &lt;!-- build time:Tue Jan 28 2020 15:40:41 GMT+0800 (GMT+08:00) --&gt;&lt;p&gt;本周数字电路课程老师布置了一个利用verilog语言进行数值比较器波形仿真的作业。可以利用Modelsim或者Vivado实现。由于Vivado
      
    
    </summary>
    
    
      <category term="程序与设计" scheme="https://gsy00517.github.io/categories/%E7%A8%8B%E5%BA%8F%E4%B8%8E%E8%AE%BE%E8%AE%A1/"/>
    
    
      <category term="踩坑血泪" scheme="https://gsy00517.github.io/tags/%E8%B8%A9%E5%9D%91%E8%A1%80%E6%B3%AA/"/>
    
      <category term="数字电路" scheme="https://gsy00517.github.io/tags/%E6%95%B0%E5%AD%97%E7%94%B5%E8%B7%AF/"/>
    
      <category term="代码实现" scheme="https://gsy00517.github.io/tags/%E4%BB%A3%E7%A0%81%E5%AE%9E%E7%8E%B0/"/>
    
      <category term="verilog" scheme="https://gsy00517.github.io/tags/verilog/"/>
    
      <category term="Vivado" scheme="https://gsy00517.github.io/tags/Vivado/"/>
    
  </entry>
  
  <entry>
    <title>markdown笔记：写报告时的一些应用</title>
    <link href="https://gsy00517.github.io/markdown20191110194256/"/>
    <id>https://gsy00517.github.io/markdown20191110194256/</id>
    <published>2019-11-10T11:42:56.000Z</published>
    <updated>2020-01-19T00:25:44.337Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Tue Jan 28 2020 15:40:41 GMT+0800 (GMT+08:00) --><p>在上一篇<a href="https://gsy00517.github.io/matlab20191110193640/" target="_blank">matlab笔记：久未使用之后踩的一堆坑</a>中，我用matlab完成了实验，接下来就是写报告了。然而写博客用惯了markdown，现在极其嫌弃word文档。考虑到这次报告中大量的公式及代码，word文档的观感可想而知，因此果断决定使用markdown来书写实验报告。那么这篇文章就及时跟进一下我在写报告的时候发现的一些之前没注意的新应用吧。</p><p><strong>References</strong>：</p><p>电子文献：<br><a href="https://blog.csdn.net/m0_37925202/article/details/80461714" target="_blank" rel="noopener">https://blog.csdn.net/m0_37925202/article/details/80461714</a><br><a href="https://www.w3school.com.cn/tags/att_p_align.asp" target="_blank" rel="noopener">https://www.w3school.com.cn/tags/att_p_align.asp</a></p><hr><h1 id="居中文字"><a href="#居中文字" class="headerlink" title="居中文字"></a>居中文字</h1><p>之前写markdown的时候一直没有考虑到要把文字居中，而这回报告的标题就有这个要求了。<br>实现的方法有如下两种。<br></p><figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">center</span>&gt;</span>这样就居中了<span class="tag">&lt;/<span class="name">center</span>&gt;</span></span><br></pre></td></tr></table></figure><p></p><figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">p</span> <span class="attr">align</span>=<span class="string">"center"</span>&gt;</span>这样就居中了<span class="tag">&lt;/<span class="name">p</span>&gt;</span></span><br></pre></td></tr></table></figure><p>此外html中的<code>&lt;p&gt;</code>标签的align属性还有其它的用法：<br></p><figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">p</span> <span class="attr">align</span>=<span class="string">"left"</span>&gt;</span> <span class="comment">&lt;!--左对齐--&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">p</span> <span class="attr">align</span>=<span class="string">"right"</span>&gt;</span> <span class="comment">&lt;!--右对齐--&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">p</span> <span class="attr">align</span>=<span class="string">"center"</span>&gt;</span> <span class="comment">&lt;!--居中--&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">p</span> <span class="attr">align</span>=<span class="string">"justify"</span>&gt;</span> <span class="comment">&lt;!--对行进行伸展，使每行都可以有相等的长度（就像在报纸和杂志中）--&gt;</span></span><br></pre></td></tr></table></figure><p></p><hr><h1 id="转pdf文件"><a href="#转pdf文件" class="headerlink" title="转pdf文件"></a>转pdf文件</h1><p>考虑到markdown文件不能作为最终的报告提交，我就想办法将markdown转化为pdf。<br>我首先找到了VScode里面的markdown PDF扩展，然而安装之后有一个东西一直安装失败。<br>这里我想起了之前我在<a href="https://gsy00517.github.io/markdown20190913211144/" target="_blank">markdown笔记：markdown的基本使用</a>中强烈推荐过的typora（我的报告最后就是用它书写的），打开之后，果然没有让我失望。直接点击“文件”“导出”选择PDF，瞬间就完成了pdf文件的转换。此外，typora提供的导出格式实在是丰富，虽然导出的word与原markdown文件的颜值有少许降低，但还是不得不赞叹typora的实用！<br><img src="/markdown20191110194256/导出.png" title="导出"></p><hr><h1 id="行间公式"><a href="#行间公式" class="headerlink" title="行间公式"></a>行间公式</h1><p>刚开始使用typora时，我遇到了一个疑惑：似乎typora不能插入行内公式块。在查找相关资料之后，我终于找到了解决的方案。<br>点击“文件”“偏好设置”，把markdown扩展语法中的内联公式项打上勾。<br><img src="/markdown20191110194256/开启行内公式.png" title="开启行内公式"></p><blockquote><p>补充：当使用<code>$</code>夹着文字而非LaTex格式的公式时，字体会发生变化，如<code>$我会变$我没变</code>，呈现的效果是：$我会变$我没变。</p></blockquote><hr><h1 id="插入图片"><a href="#插入图片" class="headerlink" title="插入图片"></a>插入图片</h1><p>typora插入图片的方法与hexo中类似，也是可以创建一个文件夹存放图片，然后在偏好设置里进行设置。当指定路径之后，typora存放图片的文件夹名可以与markdown文件的名字不一致，而hexo中则需要一致才能够直接用图片名调用。<br><img src="/markdown20191110194256/图片路径设置.png" title="图片路径设置"><br>此外，还是在偏好设置中，我们可以调整字体，我一般使用的是16px。</p><script type="text/javascript" src="/js/src/bai.js"></script><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      
      
        &lt;!-- build time:Tue Jan 28 2020 15:40:41 GMT+0800 (GMT+08:00) --&gt;&lt;p&gt;在上一篇&lt;a href=&quot;https://gsy00517.github.io/matlab20191110193640/&quot; target=&quot;_
      
    
    </summary>
    
    
      <category term="操作和使用" scheme="https://gsy00517.github.io/categories/%E6%93%8D%E4%BD%9C%E5%92%8C%E4%BD%BF%E7%94%A8/"/>
    
    
      <category term="markdown" scheme="https://gsy00517.github.io/tags/markdown/"/>
    
      <category term="typora" scheme="https://gsy00517.github.io/tags/typora/"/>
    
  </entry>
  
  <entry>
    <title>matlab笔记：久未使用之后踩的一堆坑</title>
    <link href="https://gsy00517.github.io/matlab20191110193640/"/>
    <id>https://gsy00517.github.io/matlab20191110193640/</id>
    <published>2019-11-10T11:36:40.000Z</published>
    <updated>2020-01-19T00:26:07.822Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Tue Jan 28 2020 15:40:41 GMT+0800 (GMT+08:00) --><p>本周科学计算引论结课了，就花了一整天时间把要求的实验报告写了。根据考核说明，算法可以使用各种工具、语言来实现，但由于这门课程的上机实验统一使用的是matlab，再加上我上一次使用matlab大量实践是在劳动节的数学建模华中赛了，还是很有必要重拾起来再熟悉一下。于是乎，一大波坑就等着我去填了。</p><p><strong>References</strong>：</p><p>电子文献：<br><a href="https://blog.csdn.net/zhanshen112/article/details/79728887" target="_blank" rel="noopener">https://blog.csdn.net/zhanshen112/article/details/79728887</a></p><hr><h1 id="报错：未找到具有匹配签名的构造函数"><a href="#报错：未找到具有匹配签名的构造函数" class="headerlink" title="报错：未找到具有匹配签名的构造函数"></a>报错：未找到具有匹配签名的构造函数</h1><p>我编写了一个二分法的函数求解非线性方程，然而当我调用的时候，却遇到报错：“未找到具有匹配签名的构造函数”，这是怎么回事呢？<br>我们来冷静分析一下。<br><img src="/matlab20191110193640/冷静分析.jpg" title="冷静分析"><br>识破！<br>原来是我定义的函数名为half，而half也是matlab自带的函数之一，可以使用<code>help functionname</code>查看函数具体的使用方法与功能。<br><img src="/matlab20191110193640/用法查询.png" title="用法查询"><br>调用时默认优先使用自带的函数，因此修改函数名为matlab自带函数之外的即可。</p><hr><h1 id="报错：矩阵维度必须一致"><a href="#报错：矩阵维度必须一致" class="headerlink" title="报错：矩阵维度必须一致"></a>报错：矩阵维度必须一致</h1><p>这是我在画图时遇到的一个问题，首先先补充一下matlab中作函数图像的方法，如下图所示。<br><img src="/matlab20191110193640/matlab作图.png" title="matlab作图"><br>当我尝试使用上述方法作简单的函数图像时，并没有报错，而当我想要作出我实验所需函数（如下所示）的图像时，却出现了“矩阵维度必须一致”的错误信息。<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">y = asin(1 / (1.878 + 0.75 * cos(x)))</span><br></pre></td></tr></table></figure><p></p><p>查阅相关资料，我才发现是乘除的时候出现的问题，为此我将其中的除<code>/</code>修改为乘<code>*</code>，并用<code>.^</code>代替<code>^</code>作乘方计算，即将原函数式修改为：<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">y = asin(1 * (1.878 + 0.75 * cos(x)) .^ (-1))</span><br></pre></td></tr></table></figure><p></p><p>这样，问题就解决了。<br>这里我想强调一下在matlab中<code>^</code>与<code>.^</code>的区别：<code>.^</code>是点乘，而<code>^</code>是乘法。<br>直接用<code>^</code>进行乘法的话，在这里即矩阵乘法，也就是说，必须满足前一个矩阵的列数等于后一个矩阵的行数。<br>而使用<code>.^</code>点乘操作，是使每一个元素相乘，也就是向量或者矩阵中对应元素相乘，也很好记忆，加个点就是点乘。</p><hr><h1 id="函数输出只有一个"><a href="#函数输出只有一个" class="headerlink" title="函数输出只有一个"></a>函数输出只有一个</h1><p>这是一个很愚蠢的问题，显然我好久没用了，因为matlab不必使用return返回结果，在函数声明的第一句就确定了返回值的数量和顺序。因此在调用函数的时候，必须也提供对应的变量去接收返回值，否则只能得到第一个返回的元素。</p><hr><h1 id="使用diff求导不是导数值"><a href="#使用diff求导不是导数值" class="headerlink" title="使用diff求导不是导数值"></a>使用diff求导不是导数值</h1><p>用惯了pytorch，总想着能够自动求导，一查matlab还真有这么一个函数，即diff函数。然而，事实证明它不是我想要的。<br>我们可以在命令行中使用这一个函数：</p><ol><li>声明变量x：<code>syms x</code>。它代表着声明符号变量x，只有声明了符号变量才可以进行符号运算，包括求导。</li><li>定义一个需要求导的函数：<code>f(x) = sin(x) + x ^ 2</code>。</li><li>使用diff函数求导：<code>diff(f(x))</code>。也可以对已经定义好的m文件中的函数直接求导。<br>这里，我们会得到ans为<code>2*x + cos(x)</code>。</li><li>如果想pretty一些，可以使用pretty函数将结果转化成书面格式：<code>pretty(ans)</code>。</li></ol><p>然而，当我代入不同的具体数值想得到函数的导数值的时候，发现输出的结果却是0。<br>使用help查阅diff函数的用法，得到的说明是：<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">此MATLAB函数计算沿大小不等于1的第一个数组维度的X相邻元素之间的差分：</span><br><span class="line">    Y = diff(X)</span><br><span class="line">    Y = diff(X,n)</span><br><span class="line">    Y = diff(X,n,dim)</span><br></pre></td></tr></table></figure><p></p><p>原来这个函数的主要用法是对向量或者矩阵中的元素进行差分计算，当用它来求导时，得到的只是一个表达式，且函数一但复杂，得到的就是一个参数众多的逼近格式。<br>唉，总而言之，还是老老实实地拿起笔自己算出导函数吧。都用矩阵实验室（matlab）了，手动求个导还是得会的呀。</p><hr><h1 id="角度制弧度制互换"><a href="#角度制弧度制互换" class="headerlink" title="角度制弧度制互换"></a>角度制弧度制互换</h1><p>无论使用计算器还是编程计算，这都是一个需要注意的点，matlab默认使用的是弧度制，在计算出结果之后，可以使用<code>rad2deg</code>函数进行转换。<br>同样的，我推测角度制转弧度制的函数名为<code>deg2rad</code>，一试，果不其然。这个函数还是挺好记的，英文里有许多同音词与字符的妙用，比如这里的to和2的two，还有at和@，感觉既方便又高级。</p><hr><h1 id="保留更多位数"><a href="#保留更多位数" class="headerlink" title="保留更多位数"></a>保留更多位数</h1><p>matlab默认是保留4位有效数字，为了提升计算精度，可以使用<code>format long</code>来增加计算过程中保留的位数。</p><hr><h1 id="常用操作"><a href="#常用操作" class="headerlink" title="常用操作"></a>常用操作</h1><p>难得开一篇写matlab使用的博文，那就在这补上几个我记忆中的较为常用的命令或操作。</p><ol><li>Ctrl+C终止操作。这跟许多地方都一样，在matlab中，Ctrl+C平时可以用来粘贴剪切板上的内容，而在程序运行时，可以使用它来终止运行，这在死循环的时候非常有用。</li><li><code>clc</code>清空命令行。</li><li><code>bench</code>测试性能。这其实不是一个很常用的命令，可以跟朋友输这个命令看看自己电脑的性能，下图是我的结果。需要注意的是，笔记本电脑电池使用模式的不同对这个排名影响还是挺大的，如果想让排名高一些的话，请确保电池开在最佳性能的模式。另外，不同的电脑的比较对象可能会不一样，比如学校的台式机和我的笔记本在这里比较的对象就不一样。<img src="/matlab20191110193640/性能测试.png" title="性能测试"> 常用的还有许多，以后在使用中不断增加，先在这占个位。</li></ol><script type="text/javascript" src="/js/src/bai.js"></script><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      
      
        &lt;!-- build time:Tue Jan 28 2020 15:40:41 GMT+0800 (GMT+08:00) --&gt;&lt;p&gt;本周科学计算引论结课了，就花了一整天时间把要求的实验报告写了。根据考核说明，算法可以使用各种工具、语言来实现，但由于这门课程的上机实验统一使用的
      
    
    </summary>
    
    
      <category term="程序与设计" scheme="https://gsy00517.github.io/categories/%E7%A8%8B%E5%BA%8F%E4%B8%8E%E8%AE%BE%E8%AE%A1/"/>
    
    
      <category term="踩坑血泪" scheme="https://gsy00517.github.io/tags/%E8%B8%A9%E5%9D%91%E8%A1%80%E6%B3%AA/"/>
    
      <category term="matlab" scheme="https://gsy00517.github.io/tags/matlab/"/>
    
  </entry>
  
  <entry>
    <title>front end笔记：使用div实现居中显示</title>
    <link href="https://gsy00517.github.io/front-end20191110165901/"/>
    <id>https://gsy00517.github.io/front-end20191110165901/</id>
    <published>2019-11-10T08:59:01.000Z</published>
    <updated>2020-01-19T00:24:21.292Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Tue Jan 28 2020 15:40:41 GMT+0800 (GMT+08:00) --><p>最近心血来潮花了好久给自己的博客添加了一个粒子时钟，最后想要使它在sidebar中居中显示废了我好大功夫，为了以后不在这上面浪费时间，我决定浪费现在的时间把这个问题记下来。</p><p><strong>References</strong>：</p><p>电子文献：<br><a href="https://www.jianshu.com/p/f0bffc42c1ce" target="_blank" rel="noopener">https://www.jianshu.com/p/f0bffc42c1ce</a><br><a href="https://blog.csdn.net/chwshuang/article/details/52350559" target="_blank" rel="noopener">https://blog.csdn.net/chwshuang/article/details/52350559</a></p><hr><h1 id="swig"><a href="#swig" class="headerlink" title="swig"></a>swig</h1><p>由于我要将我的时钟显示在侧边栏，需要插入到header.swig文件中。hexo博客的源码中有大量这个格式的文件，然而具体的使用方法我也不是很清楚。在查阅了一些文章之后，我对swig有了一个初步的认知。<br>swig是一个JS前端模板引擎，它有如下特点：</p><ul><li>支持大多数主流浏览器。</li><li>表达式兼容性好。</li><li>面向对象的模板继承。</li><li>将过滤器和转换应用到模板中的输出。</li><li>可根据路劲渲染页面。</li><li>支持页面复用。</li><li>支持动态页面。</li><li>可扩展、可定制。</li></ul><p>可以通过<a href="http://vschart.com/compare/swig-template-engine" target="_blank">VSchart</a>将swig与其它前端模板框架进行对比，这个网站由维基支持，可以在里面进行各种对比，非常有意思，在这里推荐一下。<br>关于swig的基本用法，可以在我文首的第一个参考链接中找到，个人认为不搞前端的话大概率是用不到的。</p><hr><h1 id="原本的方法"><a href="#原本的方法" class="headerlink" title="原本的方法"></a>原本的方法</h1><p>当我添加完时钟本地测验时，我发现添加的时钟在侧边栏的位置没有居中。<br><img src="/front-end20191110165901/未居中.png" title="未居中"><br>根据之前学习html的记忆，我尝试了使用<code>&lt;p align=center&gt;&lt;/p&gt;</code>标签包裹我的插入语句，然而并没有达到想要的效果。</p><hr><h1 id="使用div实现居中"><a href="#使用div实现居中" class="headerlink" title="使用div实现居中"></a>使用div实现居中</h1><p>为了达到上述的目的，我使用<code>&lt;div&gt;</code>标签来分割出块，并使用div的属性来实现居中显示的效果。<br></p><figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">div</span> <span class="attr">style</span>=<span class="string">"Text-align:center;width:100%;"</span>&gt;</span></span><br><span class="line">    &#123;% include '../_custom/clock.swig' %&#125;</span><br><span class="line"><span class="tag">&lt;/<span class="name">div</span>&gt;</span></span><br></pre></td></tr></table></figure><p></p><hr><h1 id="使用nav实现隐藏"><a href="#使用nav实现隐藏" class="headerlink" title="使用nav实现隐藏"></a>使用nav实现隐藏</h1><p>本以为大功告成，结果在移动端查看时，发现竖屏显示效果非常煞风景。<br><img src="/front-end20191110165901/不好看.png" title="不好看"><br>其实在电脑端浏览器也可以预览移动端的效果，方法很简单，就是直接将浏览器窗口小化，减小两边间距，网页就会自动变成竖屏显示的状态（除非没有）。此外，当我们F12检查元素或者查看源时，网页也会被挤到一侧从而变成竖屏显示的状态。</p><blockquote><p>注：这里补充一下检查元素和查看源之间的区别，一般的浏览器右键都会有这两个功能，表面上看起来似乎也差不多，但是它们还是有区别的。<br>检查元素看的是渲染过的最终代码，可以做到定位网页元素、实时监控网页元素属性变化的功能，可以及时调试、修改、定位、追踪检查、查看嵌套，修改样式和查看js动态输出信息。这让我想起了自己当初就是这样直接修改四级成绩，然后骗朋友的，不知道的人还真的想不出这原因，就以为的确是真的啦哈哈。<br>另一方面，查看源只是把网页输出的源代码，即就是别人服务器发送到浏览器的原封不动的代码直接打开，既不能动态变化，也不能修改。</p></blockquote><p>为了解决这个问题，追求美观，我就想到可以把时钟和标签、归档、分类等菜单中的索引一起，在竖屏状态下不点击时就不显示。在分析了header.swig中菜单部分的源码之后，我注意到一个标签<code>&lt;nav&gt;</code>，它是是HTML5的新标签，可以标注一个导航链接的区域。于是，我将插入时钟的语句移入nav所包裹的块中，就完美达到了我的需求。<br><img src="/front-end20191110165901/solution.png" title="solution"></p><hr><h1 id="意外的问题"><a href="#意外的问题" class="headerlink" title="意外的问题"></a>意外的问题</h1><p>在我写这篇博文的时候，出现了一个奇怪的问题。每当我想要本地预览（部署应该也会出现这个问题），都会报错：“Nunjucks Error: [Line 17, Column 239] unexpected token: }”，这就让我非常的苦恼。<br>根据错误信息，我开始一个一个寻找我文中的花括号。在反复删减和搜索相关问题之后，我发现是我插入在行间的一个include的swig语句惹的祸（我要是写出来又报错，插入在段间就没问题，可以到上文找）。<br>这类异常一般是文章中使用了大括号{}这个特殊字符，且没有转义导致编译不通过，解决的办法是使用<code>&amp;#123; &amp;#125;</code>对大的花括号进行转换。</p><blockquote><p>补充：小的圆括号可用<code>&amp;#40; &amp;#41;</code>进行转换。</p></blockquote><p>没有这类问题当然再好不过啦，如果出现了，可以试试上面的方法。这类涉及转义的符号还是得熟悉其规则，避免老是出错。</p><script type="text/javascript" src="/js/src/bai.js"></script><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      
      
        &lt;!-- build time:Tue Jan 28 2020 15:40:41 GMT+0800 (GMT+08:00) --&gt;&lt;p&gt;最近心血来潮花了好久给自己的博客添加了一个粒子时钟，最后想要使它在sidebar中居中显示废了我好大功夫，为了以后不在这上面浪费时间，我决定浪费
      
    
    </summary>
    
    
      <category term="程序与设计" scheme="https://gsy00517.github.io/categories/%E7%A8%8B%E5%BA%8F%E4%B8%8E%E8%AE%BE%E8%AE%A1/"/>
    
    
      <category term="前端" scheme="https://gsy00517.github.io/tags/%E5%89%8D%E7%AB%AF/"/>
    
      <category term="踩坑血泪" scheme="https://gsy00517.github.io/tags/%E8%B8%A9%E5%9D%91%E8%A1%80%E6%B3%AA/"/>
    
  </entry>
  
  <entry>
    <title>kaggle笔记：手写数字识别——使用KNN和CNN尝试MNIST数据集</title>
    <link href="https://gsy00517.github.io/kaggle20191102112435/"/>
    <id>https://gsy00517.github.io/kaggle20191102112435/</id>
    <published>2019-11-02T03:24:35.000Z</published>
    <updated>2020-01-19T00:25:06.435Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Tue Jan 28 2020 15:40:41 GMT+0800 (GMT+08:00) --><p>kaggle是一个著名的数据科学竞赛平台，暑假里我也抽空自己独立完成了三四个getting started级别的比赛。对于MNIST数据集，想必入门计算机视觉的人应该也不会陌生。kaggle上getting started的第一个比赛就是Digit Recognizer：Learn computer vision fundamentals with the famous MNIST data。当时作为入门小白的我，使用了入门级的方法KNN完成了我的第一次机器学习（自认为KNN是最最基础的算法，对它的介绍可见我的另一篇博文<a href="https://gsy00517.github.io/machine-learning20191101192042/" target="_blank">machine-learning笔记：机器学习的几个常见算法及其优缺点</a>，真的非常简单，但也极其笨拙）。而最近我又使用CNN再一次尝试了这个数据集，踩了不少坑，因此想把两次经历统统记录在这，可能会有一些不足之处，留作以后整理优化。</p><p><strong>References</strong>：</p><p>电子文献：<br><a href="https://blog.csdn.net/gybinlero/article/details/79294649" target="_blank" rel="noopener">https://blog.csdn.net/gybinlero/article/details/79294649</a><br><a href="https://blog.csdn.net/qq_43497702/article/details/95005248" target="_blank" rel="noopener">https://blog.csdn.net/qq_43497702/article/details/95005248</a><br><a href="https://blog.csdn.net/a19990412/article/details/90349429" target="_blank" rel="noopener">https://blog.csdn.net/a19990412/article/details/90349429</a></p><hr><h1 id="KNN"><a href="#KNN" class="headerlink" title="KNN"></a>KNN</h1><p>首先导入必要的包，这里基本用不到太多：<br></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> csv</span><br><span class="line"><span class="keyword">import</span> operator</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> matplotlib</span><br><span class="line"><span class="keyword">from</span> matplotlib <span class="keyword">import</span> pyplot <span class="keyword">as</span> plt</span><br><span class="line">%matplotlib inline</span><br></pre></td></tr></table></figure><p></p><p>导入训练数据：<br></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">trainSet = []</span><br><span class="line"><span class="keyword">with</span> open(<span class="string">'train.csv'</span>,<span class="string">'r'</span>) <span class="keyword">as</span> trainFile:</span><br><span class="line">    lines=csv.reader(trainFile)</span><br><span class="line">    <span class="keyword">for</span> line <span class="keyword">in</span> lines:</span><br><span class="line">        trainSet.append(line)</span><br><span class="line">    trainSet.remove(trainSet[<span class="number">0</span>])</span><br><span class="line"></span><br><span class="line">trainSet = np.array(trainSet)</span><br><span class="line">rawTrainLabel = trainSet[:, <span class="number">0</span>] <span class="comment">#分割出训练集标签</span></span><br><span class="line">rawTrainData = trainSet[:, <span class="number">1</span>:] <span class="comment">#分割出训练集数据</span></span><br></pre></td></tr></table></figure><p></p><p>我当时用了一种比较笨拙的办法转换数据类型：<br></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">rawTrainData = np.mat(rawTrainData) <span class="comment">#转化成矩阵，或许不需要</span></span><br><span class="line">m, n = np.shape(rawTrainData)</span><br><span class="line">trainData = np.zeros((m, n)) <span class="comment">#创建初值为0的ndarray</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(m):</span><br><span class="line">    <span class="keyword">for</span> j <span class="keyword">in</span> range(n):</span><br><span class="line">        trainData[i, j] = int(rawTrainData[i, j]) <span class="comment">#转化并赋值</span></span><br><span class="line"></span><br><span class="line">rawTrainLabel = np.mat(rawTrainLabel) <span class="comment">#或许不需要</span></span><br><span class="line">m, n = np.shape(rawTrainLabel)</span><br><span class="line">trainLabel = np.zeros((m, n))</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(m):</span><br><span class="line">    <span class="keyword">for</span> j <span class="keyword">in</span> range(n):</span><br><span class="line">        trainLabel[i, j] = int(rawTrainLabel[i, j])</span><br></pre></td></tr></table></figure><p></p><p>这里我们可以查看以下数据的维度，确保没有出错。<br><img src="/kaggle20191102112435/查看维度.png" title="查看维度"><br>为了方便起见，我们把所有pixel不为0的点都设置为1。<br></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">m, n = np.shape(trainData)</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(m):</span><br><span class="line">    <span class="keyword">for</span> j <span class="keyword">in</span> range(n):</span><br><span class="line">        <span class="keyword">if</span> trainData[i, j] != <span class="number">0</span>:</span><br><span class="line">            trainData[i, j] = <span class="number">1</span></span><br></pre></td></tr></table></figure><p></p><p>仿照训练集的步骤，导入测试集并做相同处理：<br></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">testSet = []</span><br><span class="line"><span class="keyword">with</span> open(<span class="string">'test.csv'</span>,<span class="string">'r'</span>) <span class="keyword">as</span> testFile:</span><br><span class="line">    lines=csv.reader(testFile)</span><br><span class="line">    <span class="keyword">for</span> line <span class="keyword">in</span> lines:</span><br><span class="line">        testSet.append(line)</span><br><span class="line">    testSet.remove(testSet[<span class="number">0</span>])</span><br><span class="line"></span><br><span class="line">testSet = np.array(testSet)</span><br><span class="line">rawTestData = testSet</span><br><span class="line"></span><br><span class="line">rawTestData = np.mat(rawTestData)</span><br><span class="line">m, n = np.shape(rawTestData)</span><br><span class="line">testData = np.zeros((m, n))</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(m):</span><br><span class="line">    <span class="keyword">for</span> j <span class="keyword">in</span> range(n):</span><br><span class="line">        testData[i, j] = int(rawTestData[i, j])</span><br><span class="line"></span><br><span class="line">m, n = np.shape(testData)</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(m):</span><br><span class="line">    <span class="keyword">for</span> j <span class="keyword">in</span> range(n):</span><br><span class="line">        <span class="keyword">if</span> testData[i, j] != <span class="number">0</span>:</span><br><span class="line">            testData[i, j] = <span class="number">1</span></span><br></pre></td></tr></table></figure><p></p><p>同样的，可使用<code>testData.shape</code>查看测试集的维度，保证它是28000*784，由此可知操作无误。<br>接下来，我们定义KNN的分类函数。<br></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">classify</span><span class="params">(inX, dataSet, labels, k)</span>:</span></span><br><span class="line">    inX = np.mat(inX)</span><br><span class="line">    dataSet = np.mat(dataSet)</span><br><span class="line">    labels = np.mat(labels)</span><br><span class="line">    dataSetSize = dataSet.shape[<span class="number">0</span>]</span><br><span class="line">    diffMat = np.tile(inX, (dataSetSize, <span class="number">1</span>)) - dataSet     </span><br><span class="line">    sqDiffMat = np.array(diffMat) ** <span class="number">2</span></span><br><span class="line">    sqDistances = sqDiffMat.sum(axis = <span class="number">1</span>)       </span><br><span class="line">    distances = sqDistances ** <span class="number">0.5</span></span><br><span class="line">    sortedDistIndicies = distances.argsort()</span><br><span class="line">    classCount=&#123;&#125;</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(k):</span><br><span class="line">        voteIlabel = labels[<span class="number">0</span>, sortedDistIndicies[i]]</span><br><span class="line">        classCount[voteIlabel] = classCount.get(voteIlabel, <span class="number">0</span>) + <span class="number">1</span></span><br><span class="line">    sortedClassCount = sorted(classCount.iteritems(), key = operator.itemgetter(<span class="number">1</span>), reverse = <span class="literal">True</span>)</span><br><span class="line">    <span class="keyword">return</span> sortedClassCount[<span class="number">0</span>][<span class="number">0</span>]</span><br></pre></td></tr></table></figure><p></p><p>为了更好地分类，这里我们需要选择合适的k值，我选取了4000个样本作为验证机进行尝试，找到误差最小的k值并作为最终的k值输入。<br></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line">trainingTestSize = <span class="number">4000</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#分割出验证集</span></span><br><span class="line">m, n = np.shape(trainLabel)</span><br><span class="line">trainingTrainLabel = np.zeros((m, n - trainingTestSize))</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(m):</span><br><span class="line">    <span class="keyword">for</span> j <span class="keyword">in</span> range(n - trainingTestSize):</span><br><span class="line">        trainingTrainLabel[i, j] = trainLabel[i, j]</span><br><span class="line">        </span><br><span class="line">trainingTestLabel = np.zeros((m, trainingTestSize))</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(m):</span><br><span class="line">    <span class="keyword">for</span> j <span class="keyword">in</span> range(trainingTestSize):</span><br><span class="line">        trainingTestLabel[i, j] = trainLabel[i, n - trainingTestSize + j]</span><br><span class="line">        </span><br><span class="line">m, n = np.shape(trainData)</span><br><span class="line">trainingTrainData = np.zeros((m - trainingTestSize, n))</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(m - trainingTestSize):</span><br><span class="line">    <span class="keyword">for</span> j <span class="keyword">in</span> range(n):</span><br><span class="line">        trainingTrainData[i, j] = trainData[i, j]</span><br><span class="line">        </span><br><span class="line">trainingTestData = np.zeros((trainingTestSize, n))</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(trainingTestSize):</span><br><span class="line">    <span class="keyword">for</span> j <span class="keyword">in</span> range(n):</span><br><span class="line">        trainingTestData[i, j] = trainData[m - trainingTestSize + i, j]</span><br><span class="line"></span><br><span class="line"><span class="comment">#使k值为3到9依次尝试</span></span><br><span class="line">training = []</span><br><span class="line"><span class="keyword">for</span> x <span class="keyword">in</span> range(<span class="number">3</span>, <span class="number">10</span>):</span><br><span class="line">    error = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> y <span class="keyword">in</span> range(trainingTestSize):</span><br><span class="line">        answer = (classify(trainingTestData[y], trainingTrainData, trainingTrainLabel, x))</span><br><span class="line">        <span class="keyword">print</span> <span class="string">'the classifier came back with: %d, %.2f%% has done, the k now is %d'</span> % (answer, (y + (x - <span class="number">3</span>) * trainingTestSize) / float(trainingTestSize * <span class="number">7</span>) * <span class="number">100</span>, x) <span class="comment">#方便知道进度</span></span><br><span class="line">        <span class="keyword">if</span> answer != trainingTestLabel[<span class="number">0</span>, y]:</span><br><span class="line">            error += <span class="number">1</span></span><br><span class="line">    training.append(error)</span><br></pre></td></tr></table></figure><p></p><p>这个过程比较长，结果会得到training的结果是[156, 173, 159, 164, 152, 155, 156]。<br>可以使用<code>plt.plot(training)</code>更直观地查看误差，呈现如下：<br><img src="/kaggle20191102112435/各k值的误差.png" title="各k值的误差"></p><blockquote><p>注意：这里的下标应该加上3才是对应的k值。</p></blockquote><p>可以看图手动选择k值，但由于事先无法把握训练结束的时间，可以编写函数自动选择并使程序继续进行。<br></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">theK = <span class="number">3</span></span><br><span class="line">hasError = training[<span class="number">0</span>]</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">7</span>):</span><br><span class="line">    <span class="keyword">if</span> training[i] &lt; hasError:</span><br><span class="line">        theK = i + <span class="number">3</span></span><br><span class="line">        hasError = training[i]</span><br></pre></td></tr></table></figure><p></p><p>在确定k值后，接下来就是代入测试集进行结果的计算了。由于KNN算法相对而言比较低级，因此就别指望效率了，跑CPU的话整个过程大概需要半天左右。<br></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">m, n = np.shape(testData)</span><br><span class="line">result = []</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(m):</span><br><span class="line">    answer = (classify(testData[i], trainData, trainLabel, theK))</span><br><span class="line">    result.append(answer)</span><br><span class="line">    <span class="keyword">print</span> <span class="string">'the classifier came back with: %d, %.2f%% has done'</span> % (answer, i / float(m) * <span class="number">100</span>)</span><br></pre></td></tr></table></figure><p></p><p>最后，定义一个保存结果的函数，然后<code>saveResult(result)</code>之后，再对csv文件进行处理（后文会提到），然后就可以submit了。<br></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">saveResult</span><span class="params">(result)</span>:</span></span><br><span class="line">    <span class="keyword">with</span> open(<span class="string">'result.csv'</span>, <span class="string">'w'</span>) <span class="keyword">as</span> myFile:      </span><br><span class="line">        myWriter = csv.writer(myFile)</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> result:</span><br><span class="line">            tmp = []</span><br><span class="line">            tmp.append(i)</span><br><span class="line">            myWriter.writerow(tmp)</span><br></pre></td></tr></table></figure><p></p><p>最终此方法在kaggle上获得的score为0.96314，准确率还是挺高的，主要是因为问题相对简单，放到leaderboard上，这结果的排名就要到两千左右了。</p><hr><h1 id="CNN"><a href="#CNN" class="headerlink" title="CNN"></a>CNN</h1><p>在学习了卷积神经网络和pytorch框架之后，我决定使用CNN对这个比赛再进行一次尝试。<br>首先还是导入相关的包。<br></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"><span class="keyword">import</span> torch.optim <span class="keyword">as</span> optim</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> math <span class="keyword">import</span> *</span><br><span class="line"></span><br><span class="line">%matplotlib inline</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> matplotlib.cm <span class="keyword">as</span> cm</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> torch.utils.data <span class="keyword">as</span> Data</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> torch.autograd <span class="keyword">import</span> Variable</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> csv</span><br></pre></td></tr></table></figure><p></p><p>导入训练数据，可以使用<code>train.head()</code>查看导入的结果，便于后续的处理。<br></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">train = pd.read_csv(<span class="string">"train.csv"</span>)</span><br></pre></td></tr></table></figure><p></p><p>对数据进行处理，由于要使用的是CNN，我们必须要把数据整理成能输入的形式，即从数组变成高维张量。<br></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">train_labels = torch.from_numpy(np.array(train.label[:]))</span><br><span class="line"></span><br><span class="line">image_size = train.iloc[:, <span class="number">1</span>:].shape[<span class="number">1</span>]</span><br><span class="line">image_width = image_height = np.ceil(np.sqrt(image_size)).astype(np.uint8)</span><br><span class="line">train_data = torch.FloatTensor(np.array(train.iloc[:, <span class="number">1</span>:]).reshape((<span class="number">-1</span>, <span class="number">1</span>, image_width, image_height))) / <span class="number">255</span> <span class="comment">#灰度压缩，进行归一化</span></span><br></pre></td></tr></table></figure><p></p><blockquote><p>注：reshape中的-1表示自适应，这样我们能让我们更好的变化数据的形式。</p></blockquote><p>我们可以使用matplotlib查看数据处理的结果。<br></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">plt.imshow(train_data[<span class="number">1</span>].numpy().squeeze(), cmap = <span class="string">'gray'</span>)</span><br><span class="line">plt.title(<span class="string">'%i'</span> % train_labels[<span class="number">1</span>])</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p></p><p>可以看到如下图片，可以与plt.title进行核对。<br><img src="/kaggle20191102112435/处理结果.png" title="处理结果"></p><blockquote><p>注：可以用squeeze()函数来降维，例如：从<code>[[1]]</code>—&gt;<code>[1]</code>。<br>与之相反的是便是unsqueeze(dim = 1)，该函数可以使<code>[1]</code>—&gt;<code>[[1]]</code>。</p></blockquote><p>以同样的方式导入并处理测试集。<br></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">test= pd.read_csv(<span class="string">"test.csv"</span>)</span><br><span class="line">test_data = torch.FloatTensor(np.array(test).reshape((<span class="number">-1</span>, <span class="number">1</span>, image_width, image_height))) / <span class="number">255</span></span><br></pre></td></tr></table></figure><p></p><p>接下来我们定义几个超参数，这里将要使用的是小批梯度下降的优化算法，因此定义如下：<br></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#超参数</span></span><br><span class="line">EPOCH = <span class="number">1</span> <span class="comment">#整个数据集循环训练的轮数</span></span><br><span class="line">BATCH_SIZE = <span class="number">10</span> <span class="comment">#每批的样本个数</span></span><br><span class="line">LR = <span class="number">0.01</span> <span class="comment">#学习率</span></span><br></pre></td></tr></table></figure><p></p><p>定义好超参数之后，我们使用Data对数据进行最后的处理。<br></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">trainData = Data.TensorDataset(train_data, train_labels) <span class="comment">#用后会变成元组类型</span></span><br><span class="line"></span><br><span class="line">train_loader = Data.DataLoader(</span><br><span class="line">    dataset = trainData,</span><br><span class="line">    batch_size = BATCH_SIZE,</span><br><span class="line">    shuffle = <span class="literal">True</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure><p></p><p>上面的Data.TensorDataset可以把数据进行打包，以方便我们更好的使用；而Data.DataLoade可以将我们的数据打乱并且分批。要注意的是，这里不要对测试集进行操作，否则最终输出的结果就难以再与原来的顺序匹配了。<br>接下来，我们定义卷积神经网络。<br></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#build CNN</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">CNN</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">        super(CNN, self).__init__()</span><br><span class="line">        <span class="comment">#一个卷积层</span></span><br><span class="line">        self.conv1 = nn.Sequential(</span><br><span class="line">            nn.Conv2d( <span class="comment">#输入(1, 28, 28)</span></span><br><span class="line">                in_channels = <span class="number">1</span>,      <span class="comment">#1个通道</span></span><br><span class="line">                out_channels = <span class="number">16</span>,    <span class="comment">#输出层数</span></span><br><span class="line">                kernel_size = <span class="number">5</span>,      <span class="comment">#过滤器的大小</span></span><br><span class="line">                stride = <span class="number">1</span>,           <span class="comment">#步长</span></span><br><span class="line">                padding = <span class="number">2</span>           <span class="comment">#填白</span></span><br><span class="line">            ), <span class="comment">#输出(16, 28, 28)</span></span><br><span class="line">            nn.ReLU(),</span><br><span class="line">            nn.MaxPool2d(kernel_size = <span class="number">2</span>), <span class="comment">#输出(16, 14, 14)</span></span><br><span class="line">        )</span><br><span class="line">        self.conv2 = nn.Sequential( <span class="comment">#输入(16, 14, 14)</span></span><br><span class="line">            nn.Conv2d(<span class="number">16</span>, <span class="number">32</span>, <span class="number">5</span>, <span class="number">1</span>, <span class="number">2</span>), <span class="comment">#这里用了两个过滤器，将16层变成了32层</span></span><br><span class="line">            nn.ReLU(),</span><br><span class="line">            nn.MaxPool2d(kernel_size = <span class="number">2</span>) <span class="comment">#输出(32, 7, 7)</span></span><br><span class="line">        )</span><br><span class="line">        self.out = nn.Linear(<span class="number">32</span> * <span class="number">7</span> * <span class="number">7</span>, <span class="number">10</span>) <span class="comment">#全连接层，将三维的数据展为2维的数据并输出</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span> <span class="comment">#父类已定义，不能修改名字</span></span><br><span class="line">        x = self.conv1(x)</span><br><span class="line">        x = self.conv2(x)</span><br><span class="line">        x = x.view(x.size(<span class="number">0</span>), <span class="number">-1</span>)</span><br><span class="line">        output = F.softmax(self.out(x))</span><br><span class="line">        <span class="keyword">return</span> output</span><br><span class="line"></span><br><span class="line">cnn = CNN()</span><br><span class="line">optimzer = torch.optim.Adam(cnn.parameters(), lr = LR) <span class="comment">#define optimezer</span></span><br><span class="line">loss_func = nn.CrossEntropyLoss()   <span class="comment">#loss function使用交叉嫡误差</span></span><br><span class="line"></span><br><span class="line">print(cnn)  <span class="comment"># 查看net architecture</span></span><br></pre></td></tr></table></figure><p></p><p>完成以上的操作之后，就可以开始训练了，整个训练时间在CPU上只需要几分钟，这比KNN算法要优越许多。<br></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> range(EPOCH):</span><br><span class="line">    <span class="keyword">for</span> step, (x, y) <span class="keyword">in</span> enumerate(train_loader):</span><br><span class="line">        b_x = Variable(x)</span><br><span class="line">        b_y = Variable(y)</span><br><span class="line">        output = cnn(b_x)</span><br><span class="line">        loss = loss_func(output, b_y) <span class="comment">#cross entropy loss</span></span><br><span class="line">        <span class="comment">#update W</span></span><br><span class="line">        optimzer.zero_grad()</span><br><span class="line">        loss.backward()</span><br><span class="line">        optimzer.step()</span><br><span class="line">        print(<span class="string">'epoch%d'</span> % (epoch + <span class="number">1</span>), <span class="string">'-'</span>, <span class="string">'batch%d'</span> % step, <span class="string">'-'</span>, <span class="string">'loss%f'</span> % loss) <span class="comment">#查看训练过程</span></span><br><span class="line">    print(<span class="string">'No.%depoch is over'</span> % (epoch + <span class="number">1</span>))</span><br></pre></td></tr></table></figure><p></p><p>代入测试集求解：<br></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">output = cnn(test_data[:])</span><br><span class="line"><span class="comment">#print(output)</span></span><br><span class="line"></span><br><span class="line">result = torch.max(output, <span class="number">1</span>)[<span class="number">1</span>].squeeze()</span><br><span class="line"><span class="comment">#print(result)</span></span><br></pre></td></tr></table></figure><p></p><p>仿照KNN中的结果转存函数，定义saveResult函数。<br></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">saveResult</span><span class="params">(result)</span>:</span></span><br><span class="line">    <span class="keyword">with</span> open(<span class="string">'result.csv'</span>, <span class="string">'w'</span>) <span class="keyword">as</span> myFile:      </span><br><span class="line">        myWriter = csv.writer(myFile)</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> result:</span><br><span class="line">            tmp = []</span><br><span class="line">            tmp.append(i)</span><br><span class="line">            myWriter.writerow(tmp)</span><br></pre></td></tr></table></figure><p></p><p>最后使用<code>saveResult(result.numpy())</code>把结果存入csv文件。</p><hr><h1 id="改进"><a href="#改进" class="headerlink" title="改进"></a>改进</h1><p>然而，若使用上述的CNN，得出的结果在leaderboard上会达到两千三百多名，这已经进入所有参赛者的倒数两百名之内了。为什么这个CNN的表现甚至不如我前面的KNN算法呢？我觉得主要有下面三个原因。</p><ol><li><p>首先，由于CNN的参数较多，仅经过1轮epoch应该是不足够把所有参数训练到最优或者接近最优的位置的。个人认为，靠前的数据在参数相对远离最优值时参与训练而在之后不起作用，很有可能导致最后顾此失彼，因此有必要增加epoch使之前的数据多次参与参数的校正。同时，也要增大batch size使每次优化参数使用的样本更多，从而在测试集上表现更好。训练结束后，我发现我的C盘会被占用几个G，不知道是不是出错了，也有可能是参数占用的空间，必须停止kernel才能得到释放（我关闭了VScode后刷新，空间就回来了）。关于内存，这里似乎存在着一个问题，我将在后文阐述。</p><blockquote><p>注：由于VScode前段时间也开始支持ipynb，喜欢高端暗黑科技风又懒得自己修改jupyter notebook的小伙伴可以试一试。</p></blockquote></li><li><p>学习率过大。尽管我这里的学习率设置为0.01，但对于最后的收敛来说或许还是偏大，这就导致了最后会在最优解附近来回抖动而难以接近的问题。关于这个问题，可以到<a href="https://gsy00517.github.io/deep-learning20191001151454/" target="_blank">deep-learning笔记：学习率衰减与批归一化</a>中看看我较为详细的分析与解决方法。</p></li><li>由于训练时间和epoch轮数相对较小，我推测模型可能会存在过拟合的问题。尤其是最后的全连接层，它的结构很容易造成过拟合。关于这个问题，也可以到<a href="https://gsy00517.github.io/machine-learning20191001104538/" target="_blank">machine-learning笔记：过拟合与欠拟合</a>和<a href="https://gsy00517.github.io/machine-learning20190915150339/" target="_blank">machine-learning笔记：机器学习中正则化的理解</a>中看看我较为详细的分析与解决方法。</li></ol><p>针对上述原因，我对我的CNN模型做了如下调整：</p><ol><li><p>首先，增加训练量，调整超参数如下。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#超参数</span></span><br><span class="line">EOPCH = <span class="number">3</span></span><br><span class="line">BATCH_SIZE = <span class="number">50</span></span><br><span class="line">LR = <span class="number">1e-4</span></span><br></pre></td></tr></table></figure></li><li><p>引入dropout随机失活，加强全连接层的鲁棒性，修改网络结构如下。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#build CNN</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">CNN</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">        super(CNN, self).__init__()</span><br><span class="line">        <span class="comment">#一个卷积层</span></span><br><span class="line">        self.conv1 = nn.Sequential(</span><br><span class="line">            nn.Conv2d( <span class="comment">#输入(1, 28, 28)</span></span><br><span class="line">                in_channels = <span class="number">1</span>,      <span class="comment">#1个通道</span></span><br><span class="line">                out_channels = <span class="number">16</span>,    <span class="comment">#输出层数</span></span><br><span class="line">                kernel_size = <span class="number">5</span>,      <span class="comment">#过滤器的大小</span></span><br><span class="line">                stride = <span class="number">1</span>,           <span class="comment">#步长</span></span><br><span class="line">                padding = <span class="number">2</span>           <span class="comment">#填白</span></span><br><span class="line">            ), <span class="comment">#输出(16, 28, 28)</span></span><br><span class="line">            nn.ReLU(),</span><br><span class="line">            nn.MaxPool2d(kernel_size = <span class="number">2</span>), <span class="comment">#输出(16, 14, 14)</span></span><br><span class="line">        )</span><br><span class="line">        self.conv2 = nn.Sequential( <span class="comment">#输入(16, 14, 14)</span></span><br><span class="line">            nn.Conv2d(<span class="number">16</span>, <span class="number">32</span>, <span class="number">5</span>, <span class="number">1</span>, <span class="number">2</span>), <span class="comment">#这里用了两个过滤器，将16层变成了32层</span></span><br><span class="line">            nn.ReLU(),</span><br><span class="line">            nn.MaxPool2d(kernel_size = <span class="number">2</span>) <span class="comment">#输出(32, 7, 7)</span></span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">        self.dropout = nn.Dropout(p = <span class="number">0.5</span>) <span class="comment">#每次减少50%神经元之间的连接</span></span><br><span class="line"></span><br><span class="line">        self.fc = nn.Linear(<span class="number">32</span> * <span class="number">7</span> * <span class="number">7</span>, <span class="number">1024</span>)</span><br><span class="line">        self.out = nn.Linear(<span class="number">1024</span>, <span class="number">10</span>) <span class="comment">#全连接层，将三维的数据展为2维的数据并输出</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        x = self.conv1(x)</span><br><span class="line">        x = self.conv2(x)</span><br><span class="line">        x = x.view(x.size(<span class="number">0</span>), <span class="number">-1</span>)</span><br><span class="line">        x = self.fc(x)</span><br><span class="line">        x = self.dropout(x)</span><br><span class="line">        output = F.softmax(self.out(x))</span><br><span class="line">        <span class="keyword">return</span> output</span><br></pre></td></tr></table></figure></li></ol><p>本想直接使用torch.nn.functional中的dropout函数轻松实现随机失活正则化，但在网上看到这个函数好像有点坑，因此就不以身试坑了，还是在网络初始化中先定义dropout。</p><blockquote><p>注：训练完新定义的网络之后我一直在思考dropout添加的方式与位置。在看了一些资料之后，我认为或许去掉全连接层、保持原来的层数并在softmax之前dropout可能能达到更好的效果。考虑到知乎上有知友提到做研究试验不宜在MNIST这些玩具级别的数据集上进行，因此暂时不再做没有太大意义的调整，今后有空在做改进试验。</p></blockquote><p>经过上面的改进后，我再次训练网络并提交结果，在kaggle上的评分提高至0.97328，大约处在1600名左右，可以继续调整超参数（可以分割验证集寻找）和加深网络结构以取得更高的分数，但我的目的已经达到了。与之前的KNN相比，无论从时间效率还是准确率，CNN都有很大的进步，这也体现了深度学习相对于一些经典机器学习算法的优势。</p><hr><h1 id="出现的问题"><a href="#出现的问题" class="headerlink" title="出现的问题"></a>出现的问题</h1><p>由于这个最后的网络是我重复构建之后完成的，因此下列部分问题可能不存在于我上面的代码中，但我还是想汇总在这，以防之后继续踩相同的坑。</p><ol><li><h2 id="报错element-0-of-tensors-does-not-require-grad-and-does-not-have-a-grad-fn"><a href="#报错element-0-of-tensors-does-not-require-grad-and-does-not-have-a-grad-fn" class="headerlink" title="报错element 0 of tensors does not require grad and does not have a grad_fn"></a>报错element 0 of tensors does not require grad and does not have a grad_fn</h2>pytorch具有自动求导机制，这就省去了我们编写反向传播的代码。每个Variable变量都有两个标志：requires_grad和volatile。出现上述问题的原因是requires_grad = False，修改或者增加（因为默认是false）成True即可。</li><li><h2 id="RuntimeError-Dimension-out-of-range-expected-to-be-in-range-of-1-0-but-got-1"><a href="#RuntimeError-Dimension-out-of-range-expected-to-be-in-range-of-1-0-but-got-1" class="headerlink" title="RuntimeError: Dimension out of range (expected to be in range of [-1, 0], but got 1)"></a>RuntimeError: Dimension out of range (expected to be in range of [-1, 0], but got 1)</h2>这个好像是我在计算交叉熵时遇到的，原因是因为torch的交叉熵的输入第一个位置的输入应该是在每个label下的概率，而不是对应的label，详细分析与举例可参考文首给出的第三个链接。</li><li><h2 id="AttributeError-‘tuple’-object-has-no-attribute-‘numpy’"><a href="#AttributeError-‘tuple’-object-has-no-attribute-‘numpy’" class="headerlink" title="AttributeError: ‘tuple’ object has no attribute ‘numpy’"></a>AttributeError: ‘tuple’ object has no attribute ‘numpy’</h2><p>为了查看数据处理效果，我在数据预处理过程中使用matplotlib绘制出处理后的图像，但是却出现了如上报错，当时的代码如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">plt.imshow(trainData[<span class="number">1</span>].numpy().squeeze(), cmap = <span class="string">'gray'</span>)</span><br><span class="line">plt.title(<span class="string">'%i'</span> % train_labels[<span class="number">1</span>])</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p>查找相关资料之后，我才知道torch.utils.data会把打包的数据变成元组类型，因此我们绘图还是要使用原来train_data中的数据。</p></li><li><h2 id="转存结果时提醒DefaultCPUAllocator-not-enough-memory"><a href="#转存结果时提醒DefaultCPUAllocator-not-enough-memory" class="headerlink" title="转存结果时提醒DefaultCPUAllocator: not enough memory"></a>转存结果时提醒DefaultCPUAllocator: not enough memory</h2><p>由于当初在实现KNN算法转存结果时使用的函数存入csv文件后还要对文件进行空值删除处理，比较麻烦（后文会写具体如何处理），因此我想借用文章顶部给出的第二个链接中提供的方法：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">out = pd.DataFrame(np.array(result), index = range(<span class="number">1</span>, <span class="number">1</span> + len(result)), columns = [<span class="string">'ImageId'</span>, <span class="string">'Label'</span>])</span><br><span class="line"><span class="comment">#torch和pandas的类型不能直接的转换，所以需要借助numpy中间的步骤，将torch的数据转给pandas</span></span><br><span class="line">out.to_csv(<span class="string">'result.csv'</span>, header = <span class="literal">None</span>)</span><br></pre></td></tr></table></figure><p>结果出现如下错误：</p><img src="/kaggle20191102112435/内存不够？.png" title="内存不够？"><p>我好歹也是八千多买的DELL旗舰本，8G内存，它居然说我不够让我换块新的RAM？什么情况…<br>尝试许久，我怀疑是训练得到的参数占用了我的内存，那只好先把训练出的result保存下来，再导入到csv文件。<br>最后我还是选择自己手动处理csv文件中的空值，应该有其它的转存csv文件的方法或者上述问题的解决措施，留待以后实践过程中发现解决，也欢迎大家不吝赐教。</p></li></ol><hr><h1 id="excel-csv快速删除空白行"><a href="#excel-csv快速删除空白行" class="headerlink" title="excel/csv快速删除空白行"></a>excel/csv快速删除空白行</h1><p>如果你使用的是我的saveResult函数或者类似，你就很有可能发现更新后的csv文件中数据之间双数行都是留空的，即一列数据之间都有空白行相隔，那么可以使用如下方法快速删除空白行。</p><ol><li>选中对应列或者区域。</li><li>在“开始”工具栏中找到“查找与选择”功能并点击。</li><li>在下拉菜单中，点击“定位条件”选项。</li><li>在打开的定位条件窗口中，选择“空值”并确定。</li><li>待电脑为你选中所有空值后，任意右键一个被选中的空白行，在弹出的菜单中点击“删除”。</li><li>如果数据量比较大，这时候会有一个处理时间可能会比较长的提醒弹出，确认即可。</li><li>等待处理完毕。</li></ol><script type="text/javascript" src="/js/src/bai.js"></script><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      
      
        &lt;!-- build time:Tue Jan 28 2020 15:40:41 GMT+0800 (GMT+08:00) --&gt;&lt;p&gt;kaggle是一个著名的数据科学竞赛平台，暑假里我也抽空自己独立完成了三四个getting started级别的比赛。对于MNIST数据集，想必
      
    
    </summary>
    
    
      <category term="人工智能" scheme="https://gsy00517.github.io/categories/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/"/>
    
    
      <category term="机器学习" scheme="https://gsy00517.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="深度学习" scheme="https://gsy00517.github.io/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="踩坑血泪" scheme="https://gsy00517.github.io/tags/%E8%B8%A9%E5%9D%91%E8%A1%80%E6%B3%AA/"/>
    
      <category term="代码实现" scheme="https://gsy00517.github.io/tags/%E4%BB%A3%E7%A0%81%E5%AE%9E%E7%8E%B0/"/>
    
      <category term="pytorch" scheme="https://gsy00517.github.io/tags/pytorch/"/>
    
  </entry>
  
  <entry>
    <title>hexo笔记：SEO优化</title>
    <link href="https://gsy00517.github.io/hexo20191101212014/"/>
    <id>https://gsy00517.github.io/hexo20191101212014/</id>
    <published>2019-11-01T13:20:14.000Z</published>
    <updated>2020-01-19T00:24:47.030Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Tue Jan 28 2020 15:40:41 GMT+0800 (GMT+08:00) --><p>在<a href="https://gsy00517.github.io/toefl20191019150438/" target="_blank">toefl笔记：首考考托福——记一次裸考经历</a>文章末尾我曾提到在被百度收录之后要好好做SEO，这段时间我也的确有所尝试与改进，因此在本文中将一些我认为比较有效的或者依旧存疑的SEO优化方法写下来，供日后参考深究。</p><p><strong>References</strong>：</p><p>电子文献：<br><a href="https://blog.csdn.net/lzy98/article/details/81140704" target="_blank" rel="noopener">https://blog.csdn.net/lzy98/article/details/81140704</a><br><a href="https://www.jianshu.com/p/86557c34b671" target="_blank" rel="noopener">https://www.jianshu.com/p/86557c34b671</a><br><a href="https://baijiahao.baidu.com/s?id=1616368344109675728&amp;wfr=spider&amp;for=pc" target="_blank" rel="noopener">https://baijiahao.baidu.com/s?id=1616368344109675728&amp;wfr=spider&amp;for=pc</a><br><a href="https://www.jianshu.com/p/7e1166eb412a" target="_blank" rel="noopener">https://www.jianshu.com/p/7e1166eb412a</a><br><a href="https://baijiahao.baidu.com/s?id=1597172076743185609&amp;wfr=spider&amp;for=pc" target="_blank" rel="noopener">https://baijiahao.baidu.com/s?id=1597172076743185609&amp;wfr=spider&amp;for=pc</a></p><hr><h1 id="SEO"><a href="#SEO" class="headerlink" title="SEO"></a>SEO</h1><p>之前在知乎上碰巧看到一篇别人是如何推广自己的博客的文章，里面就提到了SEO这个概念。我当时也很好奇，百度之后才发现它完全不同于CEO、CTO等概念。SEO（Search Engine Optimization），汉译为搜索引擎优化。它是一种方式，即利用搜索引擎的规则提高网站在有关搜索引擎内的自然排名。通俗的讲就是post的内容更容易被搜索引擎搜索到或者收录，且在搜索结果列表中显示靠前。<br>看了一圈，SEO的办法真的是多种多样，下面我就简单记录一部分我试过的方法。</p><hr><h1 id="优化url"><a href="#优化url" class="headerlink" title="优化url"></a>优化url</h1><p>同样在站点配置文件下面，可以找到站点的url设置。<br>如果你尚未更改过，你会发现默认的url是<code>http://yoursite.com</code>，我在这里吃了不少亏，之前苹果上add to favorites、RSS订阅后点开的链接以及copyright的链接都会直接跳转到yoursite而非我的博文链接。<br>SEO搜索引擎优化认为，网站的最佳结构是用户从首页点击三次就可以到达任何一个页面，但是我们使用hexo编译的站点默认打开文章的url是：sitename/year/mounth/day/title四层的结构，这样的url结构很不利于SEO，爬虫就会经常爬不到我们的文章，于是，我们可以将url直接改成sitename/title的形式，并且title最好是用英文（中文的url会出现好多乱码，我这方面还有待改进）。<br>基于以上原因，我在根目录的站点配置文件下修改url设置如下（注释中是默认的）：<br><img src="/hexo20191101212014/优化url.png" title="优化url"><br>如此，再次添加RSS订阅，就可以跟yoursite这个鬼地方say goodbye啦。<br>对permalink的修改将会是你的站点的一次巨大的变动，会造成大量的死链。死链会造成搜索引擎对网站的评分降低并有可能会降权。我们可以直接百度搜索“site:url”（url即你的站点网址）查看已经被搜索引擎收录的网址。如下图所示，目前我已被收录了四个，其中前两个经此番调整已成为死链。<br><img src="/hexo20191101212014/查看被收录的网页.png" title="查看被收录的网页"><br><img src="/hexo20191101212014/死链.png" title="死链"><br>这时我们可以在百度站长平台中提交死链，由于死链文件制作稍较复杂，我们可以选择规则提交的方式提交死链（处理死链过程较长，我提交的死链目前还在处理中）。<br>很重要的是，我们需要在自己的所有博文中修改链接，我使用了VScode的搜索关键字符功能对所有markdown文件进行了修改，效率相对较高。此外，如果使用了leancloud等第三方服务，那么也需要修改对应的url与新的相匹配，否则会造成原来数据的丢弃，还是挺可惜的。</p><hr><h1 id="压缩文件"><a href="#压缩文件" class="headerlink" title="压缩文件"></a>压缩文件</h1><p>关于压缩的方法，网上有很多，可以选择简易的应用。我选择的是用hexo-neat，安装插件后在站点配置文件添加如下设置，效果不错。<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"># hexo-neat</span><br><span class="line"># 博文压缩</span><br><span class="line">neat_enable: true</span><br><span class="line"># 压缩html</span><br><span class="line">neat_html:</span><br><span class="line">  enable: true</span><br><span class="line">  exclude:</span><br><span class="line"># 压缩css  </span><br><span class="line">neat_css:</span><br><span class="line">  enable: true</span><br><span class="line">  exclude:</span><br><span class="line">    - &apos;**/*.min.css&apos;</span><br><span class="line"># 压缩js</span><br><span class="line">neat_js:</span><br><span class="line">  enable: true</span><br><span class="line">  mangle: true</span><br><span class="line">  output:</span><br><span class="line">  compress:</span><br><span class="line">  exclude:</span><br><span class="line">    - &apos;**/*.min.js&apos;</span><br><span class="line">    - &apos;**/jquery.fancybox.pack.js&apos;</span><br><span class="line">    - &apos;**/index.js&apos;</span><br></pre></td></tr></table></figure><p></p><p>添加完成之后，每次generate你就会在git bash终端看到neat压缩的反馈信息。<br>另外也有和很多网友使用的是gulp压缩，设置也很简便且有效。<br>压缩网站文件不仅可以提高访问加载的速度，同时减少了大量空白符，对SEO也是有不小的帮助的，推荐尝试。</p><hr><h1 id="主动推送"><a href="#主动推送" class="headerlink" title="主动推送"></a>主动推送</h1><p>首先在根目录下安装插件<code>npm install hexo-baidu-url-submit --save</code>。<br>在根目录站点配置文件中新增如下字段：<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">baidu_url_submit:</span><br><span class="line">  count: 100 # 提交最新的一个链接</span><br><span class="line">  host: gsy00517.github.io # 在百度站长平台中注册的域名</span><br><span class="line">  token: lY..........Fk # 请注意这是您的秘钥，所以请不要把博客源代码发布在公众仓库里!</span><br><span class="line">  path: baidu_urls.txt # 文本文档的地址，新链接会保存在此文本文档里</span><br></pre></td></tr></table></figure><p></p><p>域名和秘钥可以在站长工具平台的连接提交中的接口调用地址中找到，即对应host与token后面的字段。<br>再把主题配置文件中的deploy修改如下：<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"># Deployment</span><br><span class="line">## Docs: https://hexo.io/docs/deployment.html</span><br><span class="line">deploy:</span><br><span class="line">- type: git</span><br><span class="line">  repo: </span><br><span class="line">    github: git@github.com:Gsy00517/Gsy00517.github.io.git</span><br><span class="line">    coding: git@git.dev.tencent.com:gsy00517/gsy00517.git</span><br><span class="line">  branch: master</span><br><span class="line">- type: baidu_url_submitter</span><br></pre></td></tr></table></figure><p></p><blockquote><p>注意：必须严格按照上述格式，否则无法deploy。</p></blockquote><p>这样以后每次执行<code>hexo d</code>，新的链接就会主动推送给百度，然后百度就会更快地派爬虫来发现你站点中的新链接，可以在第一时间收录新建的链接。</p><hr><h1 id="自动推送"><a href="#自动推送" class="headerlink" title="自动推送"></a>自动推送</h1><p>自动推送是百度搜索资源平台为提高站点新增网页发现速度推出的工具，安装自动推送JS代码的网页，在页面被访问时，页面url将立即被推送给百度。详情可以查看百度的<a href="https://ziyuan.baidu.com/college/courseinfo?id=267&page=2#h2_article_title18" target="_blank">站长工具平台使用帮助</a>。事实上，如果已经实行了主动推送，那么自动推送其实不是那么必要，因为主动推送是在生成url的时候第一时间进行推送，之后访问页面时进行的自动推送就显得晚了一步。不同推送方式的效果大概是：主动推送&gt;自动推送&gt;sitemap。<br>下面还是写一下自动推送的实现方法。<br>在blog\themes\next\source\js\src目录下，创建名为bai.js的文件，并根据百度提供的<a href="https://ziyuan.baidu.com/college/courseinfo?id=267&page=2#h2_article_title19" target="_blank">自动推送功能方法</a>添加以下代码：<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">&lt;script&gt;</span><br><span class="line">(function()&#123;</span><br><span class="line">    var bp = document.createElement(&apos;script&apos;);</span><br><span class="line">    var curProtocol = window.location.protocol.split(&apos;:&apos;)[0];</span><br><span class="line">    if (curProtocol === &apos;https&apos;) &#123;</span><br><span class="line">        bp.src = &apos;https://zz.bdstatic.com/linksubmit/push.js&apos;;</span><br><span class="line">    &#125;</span><br><span class="line">    else &#123;</span><br><span class="line">        bp.src = &apos;http://push.zhanzhang.baidu.com/push.js&apos;;</span><br><span class="line">    &#125;</span><br><span class="line">    var s = document.getElementsByTagName(&quot;script&quot;)[0];</span><br><span class="line">    s.parentNode.insertBefore(bp, s);</span><br><span class="line">&#125;)();</span><br><span class="line">&lt;/script&gt;</span><br></pre></td></tr></table></figure><p></p><p>此外，还可以blog\scaffolds目录下的模板文件post.md的分隔线之后添加这么一行：<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&lt;script type=&quot;text/javascript&quot; src=&quot;/js/src/bai.js&quot;&gt;&lt;/script&gt;</span><br></pre></td></tr></table></figure><p></p><p>这样以后每次创建新的文章就会自动在文末添加这行代码，即在生成的模板中包含这行代码。<br>如此，只要访问你的这个页面，它就会自动向百度推送你的这个网页。</p><hr><h1 id="sitemap"><a href="#sitemap" class="headerlink" title="sitemap"></a>sitemap</h1><p>首先需要安装sitemap站点地图自动生成插件。<br>windows下打开git bash，输入安装命令：<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">npm install hexo-generator-sitemap --save</span><br><span class="line">npm install hexo-generator-baidu-sitemap --save</span><br></pre></td></tr></table></figure><p></p><p>然后在站点配置文件_config.yml中找到如下对应的位置（一般默认有，没有的话可以添加），修改如下：<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"># 自动生成sitemap</span><br><span class="line">sitemap:</span><br><span class="line">  path: sitemap.xml</span><br><span class="line">baidusitemap:</span><br><span class="line">  path: baidusitemap.xml</span><br></pre></td></tr></table></figure><p></p><p>特别要注意的是，上面的path一定要缩进，否则在hexo generate时会无法编译导致报错。（似乎有一些版本的hexo不存在这样的问题，关于版本可以使用<code>hexo version</code>命令查看）<br>这样以后每次generate后都会在public目录下面生成sitemap.xml和baidusitemap.xml两个文件，即你的站点地图。也可以deploy后直接在域名后面加上这两个文件名查看你的站点地图。<br>在百度站长平台中，有sitemap提交的选项，由于我当初提交的网站协议前缀是http，因此xml文件所在的https前缀的链接不属于我提交的网站，而我的github page和coding page都设置了强制https访问。这个问题以后有机会再做解决，不存在这个问题的可以试试提交sitemap。</p><hr><h1 id="疑惑"><a href="#疑惑" class="headerlink" title="疑惑"></a>疑惑</h1><p>在优化的过程中，我发现我的post模板也被改变了（原因目前未知），从原本的：<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">title: &#123;&#123; title &#125;&#125;</span><br><span class="line">date: &#123;&#123; date &#125;&#125;</span><br><span class="line">tags:</span><br><span class="line">copyright: true</span><br><span class="line">top:</span><br></pre></td></tr></table></figure><p></p><p>变成了：<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">---</span><br><span class="line">noteId: &quot;9bfafbb............dd5a3&quot;</span><br><span class="line">tags: []</span><br><span class="line">title:</span><br><span class="line">  [object Object]: null</span><br><span class="line">date:</span><br><span class="line">  [object Object]: null</span><br><span class="line">copyright: true</span><br><span class="line">top: null</span><br><span class="line"></span><br><span class="line">---</span><br></pre></td></tr></table></figure><p></p><p>更奇怪的是，我无法删除noteId并恢复到原来的样式，每次更改保存之后又会自动给我换回来，为了方便使用，我将其修改为：<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">---</span><br><span class="line">noteId: &quot;55c6d..............d6fcf&quot;</span><br><span class="line">title:</span><br><span class="line">  &#123;&#123; title &#125;&#125;</span><br><span class="line">date:</span><br><span class="line">  &#123;&#123; date &#125;&#125;</span><br><span class="line">copyright: true</span><br><span class="line">top:</span><br><span class="line">categories:</span><br><span class="line">tags:</span><br><span class="line">---</span><br></pre></td></tr></table></figure><p></p><p>这样就可以直接提取文章标题和创建时间了。<br>对于noteId的作用，网上也找不到相关信息，可能是类似于网站的ID标识的一个代号吧，我对它之后的改进以及用法可见后文。</p><hr><h1 id="使用noteId改进url"><a href="#使用noteId改进url" class="headerlink" title="使用noteId改进url"></a>使用noteId改进url</h1><p>今天看了几个url中含有noteId的网站，立马想到其实noteId其实可以用来替代url的中文等符号从而消除乱码，这更方便了爬虫的抓取。于是，我把站点配置文件下的url设置修改如下：<br><img src="/hexo20191101212014/修改url设置.png" title="修改url设置"><br>同时我把模板文件post.md修改为：<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">---</span><br><span class="line">noteId: &apos;prefix+time remember to change!!!&apos;</span><br><span class="line">title: </span><br><span class="line">  &#123;&#123; title &#125;&#125;</span><br><span class="line">date: </span><br><span class="line">  &#123;&#123; date &#125;&#125;</span><br><span class="line">copyright: true</span><br><span class="line">top:</span><br><span class="line">categories:</span><br><span class="line">tags:</span><br><span class="line">---</span><br><span class="line"></span><br><span class="line">&lt;script type=&quot;text/javascript&quot; src=&quot;/js/src/bai.js&quot;&gt;&lt;/script&gt;</span><br></pre></td></tr></table></figure><p></p><p>这就把我的博文网址修改成了“关键词+创建时间”的形式，当然要手动更改。<br>同样的，以上的操作也带来了巨大的麻烦。我需要给之前没有生成noteId的博文一一加上noteId，同时也免不了对外部辅助平台和网站内链的大幅度修改。<br>对于检查网站死链，我推荐一个挺实用的轻量工具<a href="https://xenus-link-sleuth.en.softonic.com/" target="_blank">Xenu</a>，下载安装之后，选择file，然后check URL，输入网站地址，即可检查站内所有的连接中是否存在死链。下面是我仅修改了url设置而未更改内链时检测的情况，其中红色的就是死链。<br><img src="/hexo20191101212014/检测死链.png" title="检测死链"></p><hr><h1 id="修改url之后"><a href="#修改url之后" class="headerlink" title="修改url之后"></a>修改url之后</h1><p>大约是在我修改了url格式的两天后，当我再用“site:url”查询收录情况时，我发现被收录的死链已经减少了一个（似乎不是提交死链的原因，因为规则提交的死链还在处理中），然而我之前被收录、修改后原url依然可用的主页和分类页面却也消失了，这就使我非常得纳闷。这几日也查找了许多资料寻找原因，总结如下。<br><img src="/hexo20191101212014/原因汇总.jpg" title="原因汇总"><br>首先，网站url的变动产生大量死链，很有可能会导致网站排名消失，原来积累的权重大大减少甚至清除。还好目前我只是一个新站，倘若已运行并被收录了一段时间，应该要慎重考虑是否是因为网址必须得精简等原因从而放弃网站的排名。要注意的是，如果网站url链接过深，会影响搜索引擎蜘蛛抓取，时间久了，蜘蛛来的次数就会减少，最后导致网站不收录。一般建议扁平化结构，url在三层以内方便蜘蛛爬行，这在上文也提到过。<br>此外，如果是新站的话，收录之后消失也是正常的。事实上，上线6个月之内的网站都可以被称为新站。因为搜索引擎蜘蛛对新站有一个好奇心，发现新鲜的事物喜欢去抓取一下，这就是收录，但是收录之后会有一个审核期，包括这个收录之后又消失的问题，审核期过后如果在数据库找不到相同的信息就会认为这是一篇原创，这个时候再去看收录就又会恢复了。值得注意的是，新站上线短期内，只新增更新内容就行了，不要去改动以前的内容（特别是标题、url等，搜索引擎对这些内容很敏感）以免延长新站考核时间，当网站索引趋于稳定状态后可以适当改动。<br>总而言之，目前没什么好担心的，担心也没有用，还是认认真真好好地写笔记好啦！</p><hr><h1 id="添加robots文件"><a href="#添加robots文件" class="headerlink" title="添加robots文件"></a>添加robots文件</h1><p>Robots协议（也称为爬虫协议、机器人协议等）的全称是“网络爬虫排除标准”（Robots Exclusion Protocol），网站可以通过Robots协议告诉搜索引擎哪些页面可以抓取，哪些页面不能抓取。<br>如果一个网站使用大量的js、flash、ifrmae等内容，或者如果一个网站结构混乱，那么整个网站就会是乱七八糟、毫无章法，不仅用户体验极差，更重要的是蜘蛛也不会喜欢，也没有心思去抓取网站的内容了。<br>robots.txt是搜索引擎蜘蛛访问网站时要查看的第一个文件，并且会根据robots.txt文件的内容来爬行网站。在某种意义上说，它的一个任务就是指导蜘蛛爬行，减少搜索引擎蜘蛛的工作量。<br>当搜索引擎蜘蛛访问网站时，它会首先检查该站点根目录下是否存在robots.txt文件，如果该文件存在，搜索引擎蜘蛛就会按照该文件中的内容来确定爬行的范围；如果该文件不存在，则所有的搜索引擎蜘蛛将能够访问网站上所有没有被口令保护的页面。<br>通常搜索引擎对网站派出的蜘蛛是有配额的，多大规模的网站放出多少蜘蛛。如果我们不配置robots文件，那么蜘蛛来到网站以后会无目的地爬行，造成的一个结果就是，需要它爬行的目录，没有爬行到，不需要爬行的，也就是我们不想被收录的内容却被爬行并放出快照。所以robots文件对于SEO具有重要的意义。<br>如果网站中没有robots.txt文件，则网站中的程序脚本、样式表等一些和网站内容无关的文件或目录即使被搜索引擎蜘蛛爬行，也不会增加网站的收录率和权重，只会浪费服务器资源。此外，搜索引擎派出的蜘蛛资源也是有限的，我们要做的应该是尽量让蜘蛛爬行网站重点文件、目录，最大限度的节约蜘蛛资源。<br>在站点根目录的source文件下添加robots.txt文件，加入如下内容：<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">User-agent: * </span><br><span class="line">Allow: /</span><br><span class="line">Allow: /archives/</span><br><span class="line">Disallow: /categories/</span><br><span class="line">Disallow: /tags/</span><br><span class="line">Disallow: /vendors/</span><br><span class="line">Disallow: /js/</span><br><span class="line">Disallow: /css/</span><br><span class="line">Disallow: /fonts/</span><br><span class="line">Disallow: /vendors/</span><br><span class="line">Disallow: /fancybox/</span><br><span class="line"></span><br><span class="line">Sitemap: https://gsy00517.github.io/sitemap.xml</span><br><span class="line">Sitemap: https://gsy00517.github.io/baidusitemap.xml</span><br></pre></td></tr></table></figure><p></p><p>注意sitemap中要修改成自己的url。<br>另外，可以在站长工具平台检测robots文件。<br><img src="/hexo20191101212014/检测robots.png" title="检测robots"></p><script type="text/javascript" src="/js/src/bai.js"></script><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      
      
        &lt;!-- build time:Tue Jan 28 2020 15:40:41 GMT+0800 (GMT+08:00) --&gt;&lt;p&gt;在&lt;a href=&quot;https://gsy00517.github.io/toefl20191019150438/&quot; target=&quot;_blan
      
    
    </summary>
    
    
      <category term="操作和使用" scheme="https://gsy00517.github.io/categories/%E6%93%8D%E4%BD%9C%E5%92%8C%E4%BD%BF%E7%94%A8/"/>
    
    
      <category term="hexo" scheme="https://gsy00517.github.io/tags/hexo/"/>
    
      <category term="配置优化" scheme="https://gsy00517.github.io/tags/%E9%85%8D%E7%BD%AE%E4%BC%98%E5%8C%96/"/>
    
      <category term="SEO" scheme="https://gsy00517.github.io/tags/SEO/"/>
    
  </entry>
  
  <entry>
    <title>machine learning笔记：机器学习的几个常见算法及其优缺点</title>
    <link href="https://gsy00517.github.io/machine-learning20191101192042/"/>
    <id>https://gsy00517.github.io/machine-learning20191101192042/</id>
    <published>2019-11-01T11:20:42.000Z</published>
    <updated>2020-01-19T07:36:25.647Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Tue Jan 28 2020 15:40:41 GMT+0800 (GMT+08:00) --><p>接触机器学习也有一段较长的时间了，不敢说自己全部掌握甚至精通，但是期间也了解或者尝试了许多机器学习的算法。这次就结合参考资料和我自己的感受小结一下几种机器学习的常见算法及其优点和缺点。</p><hr><h1 id="决策树算法"><a href="#决策树算法" class="headerlink" title="决策树算法"></a>决策树算法</h1><p>学过数据结构中的树应该对这个算法不会感到困惑，下面就简单介绍一下其优缺点。</p><ol><li><h2 id="优点"><a href="#优点" class="headerlink" title="优点"></a>优点</h2><ul><li>易于理解和解释，可以可视化分析，容易提取出规则。</li><li>可以同时处理标称型和数值型数据。</li><li>测试数据集时，运行速度比较快。</li><li>决策树可以很好的扩展到大型数据库中，同时它的大小独立于数据库大小。</li></ul></li><li><h2 id="缺点"><a href="#缺点" class="headerlink" title="缺点"></a>缺点</h2><ul><li>对缺失数据处理比较困难。</li><li>容易出现过拟合问题，容易受到例外的干扰，对测试集非常不友好。</li><li>忽略数据集中属性的相互关联。</li><li>ID3算法计算信息增益时结果偏向数值比较多的特征。</li></ul></li><li><h2 id="改进措施"><a href="#改进措施" class="headerlink" title="改进措施"></a>改进措施</h2><ul><li>对决策树进行剪枝。可以采用交叉验证法和加入正则化的方法。较为理想的决策树是叶子节点数少且深度较小。</li><li>使用基于决策树的combination算法，如bagging算法，randomforest算法，可以解决过拟合的问题。</li></ul></li><li><h2 id="常见算法"><a href="#常见算法" class="headerlink" title="常见算法"></a>常见算法</h2><ol><li><h3 id="C4-5算法"><a href="#C4-5算法" class="headerlink" title="C4.5算法"></a>C4.5算法</h3>ID3算法是以信息论为基础，以信息熵和信息增益度为衡量标准，从而实现对数据的归纳分类。ID3算法计算每个属性的信息增益，并选取具有最高增益的属性作为给定的测试属性。C4.5算法核心思想是ID3算法，是ID3算法的改进，改进方面有：<ul><li>用信息增益率来选择属性，克服了用信息增益选择属性时偏向选择取值多的属性的不足。</li><li>在树构造过程中进行剪枝。</li><li>能处理非离散的数据。</li><li>能处理不完整的数据。<h4 id="优点-1"><a href="#优点-1" class="headerlink" title="优点"></a>优点</h4></li><li>产生的分类规则易于理解，准确率较高。<h4 id="缺点-1"><a href="#缺点-1" class="headerlink" title="缺点"></a>缺点</h4></li><li>在构造树的过程中，需要对数据集进行多次的顺序扫描和排序，因而导致算法的低效。</li><li>C4.5只适合于能够驻留于内存的数据集，当训练集大得无法在内存容纳时程序无法运行。</li></ul></li><li><h3 id="CART分类与回归树"><a href="#CART分类与回归树" class="headerlink" title="CART分类与回归树"></a>CART分类与回归树</h3><p>这是一种决策树分类方法，采用基于最小距离的基尼指数估计函数，用来决定由该子数据集生成的决策树的拓展形。如果目标变量是标称的，称为分类树；如果目标变量是连续的，称为回归树。分类树是使用树结构算法将数据分成离散类的方法。</p><h4 id="优点-2"><a href="#优点-2" class="headerlink" title="优点"></a>优点</h4><ul><li>非常灵活，可以允许有部分错分成本，还可指定先验概率分布，可使用自动的成本复杂性剪枝来得到归纳性更强的树。</li><li>在面对诸如存在缺失值、变量数多等问题时CART显得非常稳健。</li></ul><p>下面对决策树的各种算法做一个小结：<br><table border="1"><tr><td>算法</td><td>支持模型</td><td>树结构</td><td>特征选择</td></tr><tr><td>ID3</td><td>分类</td><td>多叉树</td><td>信息增益</td></tr><tr><td>C4.5</td><td>分类</td><td>多叉树</td><td>信息增益比</td></tr><tr><td>CART</td><td>分类、回归</td><td>二叉树</td><td>基尼系数、均方差</td></tr></table></p><blockquote><p>补充：<br>信息熵：表示随机变量的不确定性，熵越大，不确定性越大。这与物理中的熵性质类似。<br>信息增益：即不确定性减小的幅度。信息增益=信息熵（前）-信息熵（后）。在构造决策树的时候往往选择信息增益大的特征优先作为节点分类标准。<br>信息增益比：由于仅根据信息增益构建决策树，那么三叉树以及多叉树比二叉树的效果一般来说分类效果要好，然而这很有可能会导致过拟合的问题。因此定义信息增益比=惩罚参数*信息增益。当特征个数较多时，惩罚参数较小；当特征个数较少时，惩罚参数较大，从而使信息增益比较大，进而克服信息增益偏向于选取取值较多的特征的问题。总的来说，信息增益比相对于信息增益更客观。<br>基尼系数：表示集合的不确定性，基尼系数越大，则表示不平等程度越高。</p></blockquote></li></ol></li></ol><hr><h1 id="分类算法"><a href="#分类算法" class="headerlink" title="分类算法"></a>分类算法</h1><ol><li><h2 id="KNN算法"><a href="#KNN算法" class="headerlink" title="KNN算法"></a>KNN算法</h2><ol><li><h3 id="优点-3"><a href="#优点-3" class="headerlink" title="优点"></a>优点</h3><ul><li>KNN是一种在线技术，新数据可以直接加入数据集而不必进行重新训练。</li><li>KNN理论简单，容易实现。实际上，KNN没有训练过程，或者说，它的训练过程就是导入数据集。</li></ul></li><li><h3 id="缺点-2"><a href="#缺点-2" class="headerlink" title="缺点"></a>缺点</h3><ul><li>KNN对于样本容量大的数据集计算量比较大，极易引发维度灾难。<img src="/machine-learning20191101192042/维度灾难.png" title="维度灾难"></li><li>样本不平衡时，预测偏差比较大。如：某一类的样本比较少，而其它类样本比较多。</li><li>KNN每一次分类都会重新进行一次全局运算，耗时久。这在实践中会非常有体会，可以参考<a href="https://gsy00517.github.io/kaggle20191102112435/" target="_blank">kaggle笔记：手写数字识别——使用KNN和CNN尝试MNIST数据集</a>。</li><li>在CV领域，KNN已经被完全弃用。这是因为它不适合用来表示图像之间的视觉感知差异，如下图所示，这是CS231n中提到的一个例子，后三张图片经过不同的变换，结果与第一张原图的L2距离居然是一样的，而显然对我们而言这三张图是有很大区别的，在实际应用中往往应该区分开。<img src="/machine-learning20191101192042/不适合表征图像差异.png" title="不适合表征图像差异"></li></ul></li><li><h3 id="应用领域"><a href="#应用领域" class="headerlink" title="应用领域"></a>应用领域</h3><ul><li>文本分类。</li><li>模式识别。</li><li>聚类分析。</li><li>多分类领域。</li></ul></li></ol></li><li><h2 id="支持向量机（SVM）"><a href="#支持向量机（SVM）" class="headerlink" title="支持向量机（SVM）"></a>支持向量机（SVM）</h2>支持向量机是一种基于分类边界的方法。其基本原理是（以二维数据为例）：如果训练数据分布在二维平面上的点，它们按照其分类聚集在不同的区域。基于分类边界的分类算法的目标是：通过训练，找到这些分类之间的边界（直线的称为线性划分，曲线的称为非线性划分）。对于多维数据（如N维），可以将它们视为N维空间中的点，而分类边界就是N维空间中的面，称为超面（超面比N维空间少一维）。线性分类器使用超平面类型的边界，非线性分类器使用超曲面。<br>支持向量机的原理是将低维空间的点映射到高维空间，使它们成为线性可分，再使用线性划分的原理来判断分类边界。在高维空间中是一种线性划分，而在原有的数据空间中，是一种非线性划分。<br>在我的博文<a href="https://gsy00517.github.io/machine-learning20191001093428/" target="_blank">machine-learning笔记：一个支持向量机的问题</a>中，我提及了SVM的简介与一个问题，感兴趣的话可以了解一下。<ol><li><h3 id="优点-4"><a href="#优点-4" class="headerlink" title="优点"></a>优点</h3><ul><li>解决小样本下机器学习问题，相对于其他训练分类算法不需要过多样本。</li><li>解决非线性问题。擅长应付线性不可分，主要用松弛变量（惩罚变量）和核函数来实现。</li><li>无局部极小值问题。（相对于神经网络等算法）</li><li>引入了核函数，可以很好的处理高维数据集。</li><li>泛化能力比较强。结构风险最小，指分类器对问题真实模型的逼近与真实解之间的累计误差。</li></ul></li><li><h3 id="缺点-3"><a href="#缺点-3" class="headerlink" title="缺点"></a>缺点</h3><ul><li>对于核函数的高维映射解释力不强，尤其是径向基函数。</li><li>对缺失数据敏感。</li></ul></li><li><h3 id="应用领域："><a href="#应用领域：" class="headerlink" title="应用领域："></a>应用领域：</h3><ul><li>文本分类。</li><li>图像识别。</li><li>主要二分类领域。</li></ul></li></ol></li><li><h2 id="朴素贝叶斯算法"><a href="#朴素贝叶斯算法" class="headerlink" title="朴素贝叶斯算法"></a>朴素贝叶斯算法</h2>朴素贝叶斯，即naive bayes。说白了就是要“sometimes naive”。<ol><li><h3 id="优点-5"><a href="#优点-5" class="headerlink" title="优点"></a>优点</h3><ul><li>对大数量训练和查询时具有较高的速度。即使使用超大规模的训练集，针对每个项目通常也只会有相对较少的特征数，并且对项目的训练和分类也仅仅是特征概率的数学运算而已。</li><li>支持增量式运算。即可以实时的对新增的样本进行训练。</li><li>朴素贝叶斯对结果解释容易理解。</li></ul></li><li><h3 id="缺点-4"><a href="#缺点-4" class="headerlink" title="缺点"></a>缺点</h3><ul><li>由于使用了样本属性独立性的假设，所以如果样本属性有关联时其效果不好。</li></ul></li><li><h3 id="应用领域-1"><a href="#应用领域-1" class="headerlink" title="应用领域"></a>应用领域</h3><ul><li>文本分类。</li><li>欺诈检测。</li></ul></li></ol></li><li><h2 id="Logistic回归算法"><a href="#Logistic回归算法" class="headerlink" title="Logistic回归算法"></a>Logistic回归算法</h2><ol><li><h3 id="优点-6"><a href="#优点-6" class="headerlink" title="优点"></a>优点</h3><ul><li>计算代价不高，易于理解和实现。</li></ul></li><li><h3 id="缺点-5"><a href="#缺点-5" class="headerlink" title="缺点"></a>缺点</h3><ul><li>容易产生欠拟合。</li><li>分类精度不高。</li></ul></li><li><h3 id="应用领域-2"><a href="#应用领域-2" class="headerlink" title="应用领域"></a>应用领域</h3><ul><li>用于二分类领域，可以得出概率值，适用于根据分类概率排名的领域，如搜索排名等。</li><li>Logistic回归的扩展softmax可以应用于多分类领域，如手写字识别等。</li></ul></li></ol></li></ol><hr><h1 id="聚类算法"><a href="#聚类算法" class="headerlink" title="聚类算法"></a>聚类算法</h1><ol><li><h2 id="K-means算法"><a href="#K-means算法" class="headerlink" title="K-means算法"></a>K-means算法</h2>K-means算法，即K均值算法，是一个简单的聚类算法，把n个对象根据它们的属性分为k个分割，k小于n。算法的核心就是要优化失真函数J，使其收敛到局部最小值但不是全局最小值。它比较适合凸数据集，即任意两个数据点之间的连线都在数据集内部。<ol><li><h3 id="算法流程"><a href="#算法流程" class="headerlink" title="算法流程"></a>算法流程</h3><ol><li>随机选择k个随机的点（称为聚类中心）。</li><li>对数据集中的每个数据点，按照距离k个中心的距离，将其与最近的中心点关联起来，与同一中心点关联的点聚成一类。</li><li>计算每一组的均值，将该组所关联的中心点移到平均值的位置。</li><li>重复第2、3两步，直到中心点不再变化。</li></ol></li><li><h3 id="优点-7"><a href="#优点-7" class="headerlink" title="优点"></a>优点</h3><ul><li>算法速度很快。</li></ul></li><li><h3 id="缺点-6"><a href="#缺点-6" class="headerlink" title="缺点"></a>缺点</h3><ul><li>分组的数目k是一个输入超参数，不合适的k可能返回较差的结果。</li></ul></li></ol></li><li><h2 id="EM最大期望算法"><a href="#EM最大期望算法" class="headerlink" title="EM最大期望算法"></a>EM最大期望算法</h2>EM算法是基于模型的聚类方法，是在概率模型中寻找参数最大似然估计的算法，其中概率模型依赖于无法观测的隐藏变量。E步估计隐含变量，M步估计其他参数，交替将极值推向最大。<br>EM算法比K-means算法计算复杂，收敛也较慢，不适于大规模数据集和高维数据，但比K-means算法计算结果稳定、准确。EM经常用在机器学习和计算机视觉的数据集聚（Data Clustering）领域。</li></ol><hr><h1 id="集成算法（AdaBoost）"><a href="#集成算法（AdaBoost）" class="headerlink" title="集成算法（AdaBoost）"></a>集成算法（AdaBoost）</h1><p>俗话说的好“三个臭皮匠，顶个诸葛亮”，集成算法就是将多个弱分类器集成在一起，构建一个强分类器。事实上，它可能不属于算法，而更像一种优化手段。</p><ol><li><h2 id="优点-8"><a href="#优点-8" class="headerlink" title="优点"></a>优点</h2><ul><li>很好的利用了弱分类器进行级联。</li><li>可以将不同的分类算法作为弱分类器。</li><li>AdaBoost具有很高的精度。</li><li>相对于bagging算法和randomforest算法，AdaBoost充分考虑的每个分类器的权重。</li></ul></li><li><h2 id="缺点-7"><a href="#缺点-7" class="headerlink" title="缺点"></a>缺点</h2><ul><li>AdaBoost迭代次数也就是弱分类器数目不太好设定，可以使用交叉验证来进行确定。</li><li>数据不平衡导致分类精度下降。</li><li>训练比较耗时，每次重新选择当前分类器最好切分点。</li></ul></li><li><h2 id="应用领域-3"><a href="#应用领域-3" class="headerlink" title="应用领域"></a>应用领域</h2><ul><li>模式识别。</li><li>计算机视觉领域。</li><li>二分类和多分类场景。</li></ul></li></ol><hr><h1 id="神经网络算法"><a href="#神经网络算法" class="headerlink" title="神经网络算法"></a>神经网络算法</h1><ol><li><h2 id="优点-9"><a href="#优点-9" class="headerlink" title="优点"></a>优点</h2><ul><li>分类准确度高，学习能力极强。</li><li>对噪声数据鲁棒性和容错性较强。</li><li>有联想能力，能逼近任意非线性关系。</li></ul></li><li><h2 id="缺点-8"><a href="#缺点-8" class="headerlink" title="缺点"></a>缺点</h2><ul><li>神经网络参数较多，权值和阈值。我在训练一个只有四层的CNN时，C盘就被占用了几个G，详细情况可见<a href="https://gsy00517.github.io/kaggle20191102112435/" target="_blank">kaggle笔记：手写数字识别——使用KNN和CNN尝试MNIST数据集</a>。</li><li>黑盒过程，不能观察中间的结果，甚至无法完全理解其是怎么达到效果的。</li><li>学习过程比较长，有可能陷入局部极小值。</li></ul></li><li><h2 id="应用领域-4"><a href="#应用领域-4" class="headerlink" title="应用领域"></a>应用领域</h2><ul><li>计算机视觉。</li><li>自然语言处理。</li><li>语音识别等。</li></ul></li></ol><script type="text/javascript" src="/js/src/bai.js"></script><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      
      
        &lt;!-- build time:Tue Jan 28 2020 15:40:41 GMT+0800 (GMT+08:00) --&gt;&lt;p&gt;接触机器学习也有一段较长的时间了，不敢说自己全部掌握甚至精通，但是期间也了解或者尝试了许多机器学习的算法。这次就结合参考资料和我自己的感受小结一
      
    
    </summary>
    
    
      <category term="人工智能" scheme="https://gsy00517.github.io/categories/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/"/>
    
    
      <category term="机器学习" scheme="https://gsy00517.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>toefl笔记：首考考托福——记一次裸考经历</title>
    <link href="https://gsy00517.github.io/toefl20191019150438/"/>
    <id>https://gsy00517.github.io/toefl20191019150438/</id>
    <published>2019-10-19T07:04:38.000Z</published>
    <updated>2019-11-02T02:23:07.434Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Tue Jan 28 2020 15:40:41 GMT+0800 (GMT+08:00) --><p>今天上午终于把自己开学以来耿耿于怀的托福考完了。目前来看是铁定要二战了，因此下午抽空把这次考试的经历总结一下，方便再战的时候可以吸取经验和教训。</p><hr><h1 id="前期"><a href="#前期" class="headerlink" title="前期"></a>前期</h1><p>大一下学期的时候去听了一个学姐的出国分享会，当然是新东方支持的，讲座已结束就被新东方的老师搞了一波传销。当时也不知道托福的有效期是两年，头脑一热就报了班和十月份的托福。不过后来想想可能暑研和暑校也用得上，或许不亏。<br>于是我抽了个周末去新东方校区做了一个入班小测，很幸运的是我的分能进入强化班，毕竟高中的底子还可以。然而很坑的是，我从同学那得知去年同期的同一个班报名费比我少了将近一千（我报的强化班4400rmb），简直坐地起价啊！可能时间比较早校方不是很担心没人报。<br>然后到了暑假，我开始零零散散地准备。在家做了一套听力一套阅读，心态就崩了，怎么这么难！感觉就是跟四六级不在一个档次上。<br><img src="/toefl20191019150438/啥.JPG" title="啥"><br>于是三分钟热度就被浇没了，之后就只是背单词了，打算等八月上了课听了老师的解题方法再强化练习。<br>到了八月中旬，开始上课了，听了几节课明白题型之后，感觉托福或许也没有想象得那么可怕，熟悉就好。那段时间回去后断断续续刷了几套TPO的阅读和听力部分。<br>然后开学一周，很快就到了国庆。根据我的原计划，身为拖延症重度患者的我打算在国庆力挽狂澜，为此我早早地准备好了新东方的《7天搞定托福高频核心词》，当时觉得时间充裕，计划合理，未来充满希望。然而…<br><img src="/toefl20191019150438/都是假的.jpg" title="都是假的"><br>等到国庆结束，我也明白了一个道理：不能被事物的表面现象所迷惑。不过，乐观的我依然觉得剩下的两周足以完成复习。<br>However，国庆上来劈头盖脸砸过来的课程和任务让我分身乏术。周三晚上的实验课，我做到十一点才回寝室，这更是对我的致命一击。因为回寝太晚，没时间更换被子，导致挨冻一晚上（武汉的天气太怪了）。最后，又是喉咙痛，又是犯鼻炎，那时就感觉托福基本要凉。<br>考前第二天，我又刷了一套新托福的阅读和听力，成绩不是特别理想。由于新东方的TPO加载速度感人，也可能是学校网络的问题，总之直到考试之前，我只在小站刷过三套TPO，在新东方刷过两套老TPO和一套新TPO。<br><img src="/toefl20191019150438/校园网.JPG" title="校园网"><br>甚至直到写这篇文章的标题之前，我都不知道托福的英文拼写是“TOEFL”（之前一直觉得是“TOFEL”）。<br>考试前一天，病情加重，我也就不刷题了，把上课的笔记和题型又好好熟悉了一下就早点休息了。</p><hr><h1 id="考前"><a href="#考前" class="headerlink" title="考前"></a>考前</h1><p>考前问了新东方替我报名的老师，她说要打印确认信。不知道为什么我无法下载确认信成pdf格式的，最后屏幕截图打算打印，早上去考场的时候却忘记了。不过还好最后发现根本不需要确认信。<br>早上提早一个小时出寝室，结果发现找不到租八戒了，不知道为什么周六大家起这么早。最后租了辆摩拜拖着病体艰难地骑到了考场。<br>到了考试的楼下，有一个小姐姐热心地给我指路，不过我马上就发现她别有目的。她让我填一个貌似是培训机构的表格，善良易上当的我稀里糊涂地填了，本来想写个假的电话，结果感冒头很晕也没多想就如实写了，反正我平时也不怎么接陌生电话。<br>坐电梯到了考试的楼层，碰到我们学院一个经常见到的学长在做志愿者。他总是活跃在各种场合，好像是英语协会的，总之看到他也是开心了一小下。之后就是看序号，签到，领钥匙，去存背包。<br>在储物室的时候，一边的考试人员一直重复说“A考场的人可以把水拿出来”，我没听太懂。由于之前问过她我鼻炎犯了可不可以带餐巾纸（她说考场会提供），就不想再问第二次了。之前看别人的考试经历，说中途休息时可以出来喝水吃零食，还可以看写作模板，我以为是可以回到储物室的，后来才发现不能。<br>放完东西，我本来想再看会英语进入一下状态，结果过安检之后就只能一直在里边等了。<br>我们来到一个签承诺书的房间，大家都一排排坐在一种比较矮的长凳上，我拿了一张承诺书和一支笔就往后坐了。其实还可以拿个写字的时候用来垫的板子，我没注意，不过好多人和我一样都是在腿上或者趴凳子上写的。写承诺书的时候我没仔细看黑板上的要求，写错了一次，只好挺无奈地找工作人员换了一张。<br>不一会人就好多了，我发现这次考托福的女生比较多，大约是男生的两倍，这在我们学校很不常见啦。考试的也有大人，在我观察是不是还有培训机构的老师的时候，我的隔壁也是实验班的一个朋友也来考试了。他在C考场，那个考场更大。我的A考场人最少，相对来说环境要理想一些。不过不同考场的同学还是在同一个房间等待的。我继续观察，发现还是有几个大二面孔的，和几个人说了几句，发现还是有不少首考的人的。</p><hr><h1 id="入场"><a href="#入场" class="headerlink" title="入场"></a>入场</h1><p>过了一会有一个男老师进来说一些有关考试的注意事项，说完没多久大家就到隔壁的考试教室刷脸入场了。<br>考场的教室和等候的教室一样，也是黄色的日光灯，看着也挺舒适。入场顺序是按照姓氏的首字母顺序的，我进去的比较早。尽管A考场大概也就二三十个人，但整个入场过程还是挺久的。<br>考官把我领到座位上，虽然是随机抽的但好像我的考位还是我的序号。为我把身份证插在旁边的卡槽里之后，考官又为我输了激活码进入考试界面，然后没说什么就走了。<br>考试的隔间挺好，靠桌子往里坐一点就完全看不到旁边了。首先是确认姓名的界面，然而考官走了我也没处问，担心确认了就直接开始考试了因此久久没敢点。由于别人还在入场，因此我不敢太早开始考试。我回头看了一眼，发现是我进候考室以来就注意到的那个男生。虽然没问过他，但看上去他这次绝对不是一战了。<br>过了一会，我听到有人点鼠标的声音，于是我也鼓足勇气开始点。前面大概有七八的页面都是只需continue的direction界面，而且这个界面是不会自动跳转的，我在这里停留了很久。<br>终于，大概第十个人入场的时候，我听到有人开始试音了。意外的是，第一个开始试音的人居然真的在介绍他生活的城市。哈哈哈看来也是首考的，不知道待会整个考场一齐开始诠释人类的本质的时候，他有什么感想。这里我暗暗庆幸自己报了班。<br>当大家都在诠释人类的本质时，我心里觉得还是挺可乐的。不过就在这时，我听到前面提到的那位久经沙场的老哥也开始复读了，于是我又点了一个continue。<br>每个continue我的拖好久才点，不过入场真的是挺久的。大概有十个人完成试音之后，我才看到了describe the city you live in，心想这个时间还是可以的。</p><hr><h1 id="阅读"><a href="#阅读" class="headerlink" title="阅读"></a>阅读</h1><p>直到考试，我才知道review的用法，点击之后，会出现一个表格，可以看到哪些题已经answered了。<br>阅读第一篇是关于研究者根据化石推断远古的自然环境，第二篇是什么记不太清了，好像也是历史相关的，第三篇是北美西海岸土著人的一些文化，还有配图。没有遇到加试。<br>总的来说，考场里效率还是比寝室要好一些的。我大概还有40分钟时做完了第一篇，到最后一篇时时间还有20多分钟，相对来说还是比较宽裕的。</p><hr><h1 id="听力"><a href="#听力" class="headerlink" title="听力"></a>听力</h1><p>接下来就是听力了，我的听力是加试，有3个section。第一个section的对话我考虑太久，导致最后答lecture三道题要在一分钟之内答完。当时也只能以这个section只有50%的概率计入成绩来安慰自己。<br>第二个section做得还行，一些笔记还是没记到要点上，还是得多练。遗憾的是，我听力有好几篇都没听懂听力开头“you will hear part of a lecture in a ……gy class”中学科具体是什么，如果能听懂的话肯定是有一定帮助的，词汇量还不够啊！<br><img src="/toefl20191019150438/我太难了.JPG" title="我太难了"><br>到了第三个section，我之前在考试教室门外抽的餐巾纸用完了，我只能忍着做题，结果第三个section的lecture的conversation中的男老师似乎有异国口音，说得很不清楚，在lecture部分我也走了一会神。同样的，我发现我不是很善于掌握1个conversation+1个lecture情况下的时间，lecture的时间又分配得不多。当时也只能又以这个section只有50%的概率计入成绩来安慰自己，好吧其实两个section都凉了。<br>考试前两天对自己的listening还是最自信的，现在看来还是得花真功夫才行。</p><hr><h1 id="休息"><a href="#休息" class="headerlink" title="休息"></a>休息</h1><p>终于休息了，我出门的时候拿到张纸条，上面提醒我11：04返场。我本来想喝口热水缓解一下我喉咙的疼痛，却被告知不能回储物室了。我这时候看到别的同学放在楼梯口的一个大桌子上的水和零食，心里真的拔凉拔凉的。我5块钱买的士力架啊！我的口语写作模板啊！不过好像大多数人都没怎么吃东西，要是我不生病的话应该也没什么问题。<br>考场外面的钟不是很准，我一直担心里面开始口语了我还没进去，后来发现开始第二部分的考试也是需要考官输入验证码的，因此完全不必担心。<br>什么都没带，我那十分钟也就上了个厕所并且补充了餐巾纸。</p><hr><h1 id="口语"><a href="#口语" class="headerlink" title="口语"></a>口语</h1><p>之前开始的晚，因此我休息的时间也差不多在大家的中间。口语部分一开始的continue就比较少了，又试了一次音。这回就没有人真正介绍自己的城市了，大家又当了一回复读机。可能由于感冒造成鼻音太重的问题，我试音的音量偏低，得说得用点劲才行。<br>我在这里也停顿得有点久，因为待会等大家都开始说了，我就可以偷偷混投入其中以掩盖我拙劣的口语哈哈。事实上，和大家一起说真的能说得更开更自信，当大部分人说完之后，我们考场里有一个女生还在说，我明显地听到她顿了一下，然后声音顿时小了很多。<br>之前超牛的老哥老早就进去了（他很早就完成了听力），我进去之后本想偷听他在说什么，因为口语题都一样，结果…天呐竟然跟不上他的语速！还好这时候有一个水平不高但的确可以帮到我的吞吞吐吐的小哥开始讲了，我听到他在说work什么的，自己在脑子里构思了一下便也开始答题了。<br>然而，我的提前构思反倒先入为主了。当题目放出来时，我花了好久才读清题目，因为跟我想的太不一样了。以后还是不能太期望于听到别人的答案。最后，第一部分比较凉。<br>其实整个口语都比较凉，因为我感冒鼻音简直太重了，就像蒙着几个口罩一样，特别是其中有个录音我还咳嗽了几声。<br><img src="/toefl20191019150438/啊啊啊啊啊.gif" title="啊啊啊啊啊"></p><hr><h1 id="写作"><a href="#写作" class="headerlink" title="写作"></a>写作</h1><p>最后两部分考试感觉时间飞快，既然口语凉了，也听说写作给分还可以，我就放飞自我开始写了。<br>在我听听力的时候，就听到劈里啪啦的打字声了。一直对自己打字速度很有自信的我还是小惊讶了一下。<br>两篇作文都不是很难，我第二篇大概只写了三百词出头一些，细节还是写的有点少，都是论述的，这点下回要改进。两篇作文都是到点自动保存提交的，没有整体检查拼写，打字速度还是得练，盲打还是得加强。考场的机械键盘相对来说比较扁平，跟笔记本手感还是比较相近的。</p><hr><h1 id="考完"><a href="#考完" class="headerlink" title="考完"></a>考完</h1><p>最后还有一个report成绩还是cancel的选项，考官明确说过这个不能提问，于是我看的很仔细。还好我的水平还是无压力看懂了，砸了两千块当然要report啦！出考场后还是有点不放心特地查了一下，发现还是有网友选cancel的，不过好像可以付额外的费用解决。<br>出考场才知道已经十二点半了，由于中途没补充零食，肚子也是饿得咕咕叫（写大作文的时候开始明显感到饿）。从储物柜拿手机的时候，不小心带了出来，掉在地上了，心里一惊，还好只在钢化膜上留下一条线，当时也觉得无所谓了。<br>考完也挺平静的，感觉托福考试也就这样，只可惜这次时运不济，命途多舛。以后考托福一定要在学期初考，并且好好准备，关键是要注意身体的健康！<br>早上起来的时候百度了一下自己的博客，发现已经被李彦宏爸爸的百度收录了，可以直接百度到我的博客和文章，也是今天比较开心的一件事吧，以后会好好做SEO的。</p><hr><h1 id="结果"><a href="#结果" class="headerlink" title="结果"></a>结果</h1><p>出成绩了，更新一下。<br>10月19号上午考完的试，31号凌晨3：51终于刷新的成绩。查询页面显示的是“2019年10月23日的MyBest Scores”，不知道是不是23号就批好了成绩。考完后一直关注贴吧和公众号，似乎我那周是最后一次最多两周出成绩的考试，以后托福的成绩好像都会考后10天就出结果。雅思更狠，马上跟着改成了6天出成绩，它们是不是也在竞争呢…<br>查分的时候还是很忐忑的，没想到这次首考的成绩能到90+，虽然完全不够，但还是比我想象得要好一些的。口语果然离20还是差了一点，或许有生病的影响，但的确能体现我的水平，还是得加强练习！别人口中的提分项——写作，我也没有取得高分，看来还是不能大意，平时需要熟能生巧。但愿二战能够取得一定的进步吧！<br>今天去听了我们学校的海外交流项目介绍的讲座，大体上的语言成绩要求是CET4&gt;550，CET6&gt;500，TOEFL&gt;80，IELTS&gt;6.0，否则要电话面试，但这些基本都是相对来说比较中规中矩的科研项目或者学分项目，还是得努力提高英语水平啊！</p><script type="text/javascript" src="/js/src/bai.js"></script><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      
      
        &lt;!-- build time:Tue Jan 28 2020 15:40:41 GMT+0800 (GMT+08:00) --&gt;&lt;p&gt;今天上午终于把自己开学以来耿耿于怀的托福考完了。目前来看是铁定要二战了，因此下午抽空把这次考试的经历总结一下，方便再战的时候可以吸取经验和教训。
      
    
    </summary>
    
    
      <category term="英语" scheme="https://gsy00517.github.io/categories/%E8%8B%B1%E8%AF%AD/"/>
    
    
      <category term="个人经历" scheme="https://gsy00517.github.io/tags/%E4%B8%AA%E4%BA%BA%E7%BB%8F%E5%8E%86/"/>
    
      <category term="托福" scheme="https://gsy00517.github.io/tags/%E6%89%98%E7%A6%8F/"/>
    
  </entry>
  
  <entry>
    <title>artificial intelligence笔记：吴恩达——阅读论文的建议</title>
    <link href="https://gsy00517.github.io/artificial-intelligence20191007232512/"/>
    <id>https://gsy00517.github.io/artificial-intelligence20191007232512/</id>
    <published>2019-10-07T15:25:12.000Z</published>
    <updated>2019-11-02T02:21:05.458Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Tue Jan 28 2020 15:40:41 GMT+0800 (GMT+08:00) --><p>接触科研，读paper是一件很头疼的事情。本文就来写一下吴恩达对于阅读ML、DL相关方面论文的建议，方便参考。</p><hr><h1 id="建议"><a href="#建议" class="headerlink" title="建议"></a>建议</h1><p>首先要说明的是，这里的建议不是我想出来的，仅仅是对吴恩达提供的建议做搬运及整理。<br>如果你读到这里，应该也知道这个领域的先驱+巨佬Andrew Ng的大名。吴恩达（Andrew Ng），著名的美籍华裔计算机科学家，曾担任百度首席科学家，任教于Stanford，大家刚入门的时候想必都了解过或者看过由吴恩达老师讲授的斯坦福的经典课程CS229机器学习、CS230深度学习，此外，Andrew Ng还特地在网易云上为中国学生提供了中文字幕的课程（Andrew Ng英语说得比中文溜好多了哈哈）。另外，他还是著名教育平台Coursera的创始人，那里的课程更新鲜更优质，而且还可以锻炼英语能力，旁听即可。<br><img src="/artificial-intelligence20191007232512/啊！.jpg" title="啊！"><br>呃放错了，不是上面那张，是这张。<br><img src="/artificial-intelligence20191007232512/吴恩达.jpg" title="吴恩达"><br>对于如何阅读论文，Andrew Ng的建议是：<br>不要从头读到尾。相反，需要多次遍历论文。<br>具体有如下几个注意点：</p><ol><li><h2 id="阅读文章标题、摘要和图"><a href="#阅读文章标题、摘要和图" class="headerlink" title="阅读文章标题、摘要和图"></a>阅读文章标题、摘要和图</h2>通过阅读文章标题、摘要、关键网络架构图，或许还有实验部分，你将能够对论文的概念有一个大致的了解。在深度学习中，有很多研究论文都是将整篇论文总结成一两个图形，而不需要费力地通读全文。尤其是在描述网络架构的时候，作者一般会采用比较通用的格式，读多了就会熟悉起来，比如下面DenseNet的结构：<img src="/artificial-intelligence20191007232512/DenseNet.jpg" title="DenseNet"></li><li><h2 id="读介绍、结论、图，略过其他"><a href="#读介绍、结论、图，略过其他" class="headerlink" title="读介绍、结论、图，略过其他"></a>读介绍、结论、图，略过其他</h2>介绍、结论和摘要是作者试图仔细总结自己工作的地方，以便向审稿人阐明为什么他们的论文应该被接受发表。<br>此外，略过相关的工作部分（如果可能的话），这部分的目的是突出其他人所做的工作，这些工作在某种程度上与作者的工作有关。因此，阅读它可能是有用的，但如果不熟悉这个主题，有时会很难理解。<img src="/artificial-intelligence20191007232512/大佬也不懂.JPG" title="大佬也不懂"></li><li><h2 id="通读全文，但跳过数学部分"><a href="#通读全文，但跳过数学部分" class="headerlink" title="通读全文，但跳过数学部分"></a>通读全文，但跳过数学部分</h2>这里我说一下我对于数学部分的处理：一般我会把重要的公式等略读一遍，然后参照着CSDN博客等网站上其他网友的解释与详解进行理解。<img src="/artificial-intelligence20191007232512/先用再说.PNG" title="先用再说"></li><li><h2 id="通读全文，但略过没有意义的部分"><a href="#通读全文，但略过没有意义的部分" class="headerlink" title="通读全文，但略过没有意义的部分"></a>通读全文，但略过没有意义的部分</h2>Andrew Ng还解释说，当你阅读论文时（即使是最有影响力的论文），你可能也会发现有些部分没什么用，或者没什么意义。因此，如果你读了一篇论文，其中一些内容没有意义（这并不罕见），那么你可以先略读。除非你想要掌握它，那就花更多的时间。确实，当我在阅读ILSVRC、COCO等顶级比赛许多获奖模型的论文时，其中都有对比赛情况的详细结果介绍，我觉得这些部分一定程度上是可以扫读和跳读的。</li></ol><hr><h1 id="分享"><a href="#分享" class="headerlink" title="分享"></a>分享</h1><p>关于论文，我之前也做过一些分享，详情可以看看我之前的文章。<br>在<a href="https://gsy00517.github.io/deep-learning20190915113859/" target="_blank">deep-learning笔记：开启深度学习热潮——AlexNet</a>一文中，我提到了刚开始阅读英文论文的比较有效的方法。<br>在<a href="https://gsy00517.github.io/deep-learning20191001184216/" target="_blank">deep-learning笔记：使网络能够更深——ResNet简介与pytorch实现</a>一文中，我也提供了许多经典模型论文的英文版、中文版、中英对照的链接。<br>最后要说明的是，本篇文章中Andrew Ng的建议有部分摘自公众号Datawhale的推送文章。我关注了不少这方面的公众号，删选了几个比较优质的，在今后也会一一放到博客中推荐。<br><img src="/artificial-intelligence20191007232512/Datawhale.JPG" title="Datawhale"></p><script type="text/javascript" src="/js/src/bai.js"></script><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      
      
        &lt;!-- build time:Tue Jan 28 2020 15:40:41 GMT+0800 (GMT+08:00) --&gt;&lt;p&gt;接触科研，读paper是一件很头疼的事情。本文就来写一下吴恩达对于阅读ML、DL相关方面论文的建议，方便参考。&lt;/p&gt;&lt;hr&gt;&lt;h1 id=&quot;建
      
    
    </summary>
    
    
      <category term="人工智能" scheme="https://gsy00517.github.io/categories/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/"/>
    
    
      <category term="机器学习" scheme="https://gsy00517.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="深度学习" scheme="https://gsy00517.github.io/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="论文分享" scheme="https://gsy00517.github.io/tags/%E8%AE%BA%E6%96%87%E5%88%86%E4%BA%AB/"/>
    
  </entry>
  
  <entry>
    <title>欢迎到访：写在前面</title>
    <link href="https://gsy00517.github.io/preface20191007202443/"/>
    <id>https://gsy00517.github.io/preface20191007202443/</id>
    <published>2019-10-07T12:24:43.000Z</published>
    <updated>2020-01-22T02:59:34.068Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Tue Jan 28 2020 15:40:41 GMT+0800 (GMT+08:00) --><p>不知不觉，建站已有不少日子了，无论是内容还是界面，都逐渐丰富了起来。觉得有必要补充一篇类似于导言的文字，今天抽出点时间写一下，日后继续完善。</p><hr><h1 id="关于我"><a href="#关于我" class="headerlink" title="关于我"></a>关于我</h1><p>我来自浙江，就读于华科，目前是一名电子信息工程专业的大二本科生。从大学开始真正比较全面地接触信息技术，一年下来，在课余接触并尝试过的方面主要有编程语言python与R、linux操作系统、前端设计、机器学习与深度学习。我的博客也主要围绕这几个方面展开，具体也可以看看<a href="https://gsy00517.github.io/tags/" target="_blank">标签</a>页，我也还在不断地探索与学习中。目前我主要学习的是计算机视觉方面的相关知识。<br>入门没多久，许多理解也还比较浅薄，博客内容主要是一些干货的搬运分享并结合自己积累的一些理解与经验，会有不足与疏漏，如果大家能给予指导，我将非常感激！今后我会尽量陆续加入更多深层次的内容。<br>对于这个博客网站，可以把它看作一个技术博客，而我更多的把它看成一个自己的空间，因此偶尔也会加入一些学习生活的元素，请别见怪！此外，我会尽我所能提升文章的质量，在发布后也会不断地查漏补缺，小幅修正与大幅补充结合，使每篇文章尽可能更规范易懂、更丰富充实。<br>或许在这个移动端主导的时代，搭网站写博客的价值有所降低。但当我写了一段时间之后，发现驱使我写博客的动力不减反增。最大的收获就是我每写一篇文章就会不断地想把相关的知识彻底搞懂，这就不断促使我去深究（哪怕钻牛角尖），因此总能收获好多写之前根本没想到的内容。</p><hr><h1 id="关于博客"><a href="#关于博客" class="headerlink" title="关于博客"></a>关于博客</h1><p>为了提高访问速度，我对我的博客进行了github+coding双线部署，url如下：<br>github page：<a href="https://gsy00517.github.io/">https://gsy00517.github.io/</a><br>coding page：<a href="http://gsy00517.coding.me/" target="_blank" rel="noopener">http://gsy00517.coding.me/</a><br>大家可以择优访问，事实上我并没有感到github的速度比coding慢。此外，由于我是同一个源文件双线部署，因此选择了把所有功能优先应用在github page上（比如文章打分、文章链接），但其实coding page也没有太大的区别。<br>由于我的文章主要是按时间顺序由新到老排列的，多了之后不方便查找和浏览，因此我新增了<a href="https://gsy00517.github.io/tags/" target="_blank">标签</a>和<a href="https://gsy00517.github.io/categories/" target="_blank">分类</a>，或许可以帮助你更快地查看想看的内容。此外，也推荐使用我页面上的搜索功能利用关键词查找，非常便捷。</p><blockquote><p>注：PC体验更佳~</p></blockquote><hr><h1 id="关于订阅"><a href="#关于订阅" class="headerlink" title="关于订阅"></a>关于订阅</h1><p>首先，我想说的是，如果你觉得我写的内容，或者方向对你有一点用处的话，非常欢迎收藏或订阅我的博客！如果你也在写博客的话，我们可以互相关注！<br>在侧栏，你可能会发现这样一个图标。点击之后，你会进入一个看不懂的atom.xml文件。<br><img src="/preface20191007202443/RSS.png" title="RSS"><br>其实，看不懂是正常的，因为这个是给电脑看的。一个便捷的办法就是使用chrome的扩展程序添加feed，然后打开网页时，就可以直接点击浏览器右上角的图标（会显示加号）进行订阅。这样以后每次更新了新的博文，你就可以收到提醒。<br><img src="/preface20191007202443/扩展程序.png" title="扩展程序"><br><img src="/preface20191007202443/消息提醒.png" title="消息提醒"><br>此外还可以像收藏其他网站一样进行收藏，这里不详述了。<br>欢迎大家常来踩踩，也欢迎大家留下评论。评论很简单，无需登录任何账号直接评论即可~<br>我目前也还在学习的过程中，欢迎大家和我交流，也欢迎各种批评与建议，我会努力改进！</p><hr><h1 id="Good-News"><a href="#Good-News" class="headerlink" title="Good News"></a>Good News</h1><p>好消息！好消息！本站已和百度、谷歌等世界知名搜索引擎达成战略合作关系！如果我写的文章有不足或疏漏之处、看完后有费解有困惑，都可以直接问度娘和谷哥就好啦~<br>哈哈哈皮这一下很开心。</p><script type="text/javascript" src="/js/src/bai.js"></script><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      
      
        &lt;!-- build time:Tue Jan 28 2020 15:40:41 GMT+0800 (GMT+08:00) --&gt;&lt;p&gt;不知不觉，建站已有不少日子了，无论是内容还是界面，都逐渐丰富了起来。觉得有必要补充一篇类似于导言的文字，今天抽出点时间写一下，日后继续完善。&lt;/
      
    
    </summary>
    
    
    
  </entry>
  
  <entry>
    <title>calculus笔记：分部积分表格法</title>
    <link href="https://gsy00517.github.io/calculus20191007184856/"/>
    <id>https://gsy00517.github.io/calculus20191007184856/</id>
    <published>2019-10-07T10:48:56.000Z</published>
    <updated>2019-11-03T05:44:34.547Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Tue Jan 28 2020 15:40:41 GMT+0800 (GMT+08:00) --><p>假期里想着不能让b站收藏夹里的学习资源一直吃灰，于是又刷了一遍b站的收藏夹。碰巧就看到了自己之前收藏的一种积分方法，那么这篇文章就来搬运一下这种方法的计算流程。</p><hr><h1 id="表格法"><a href="#表格法" class="headerlink" title="表格法"></a>表格法</h1><p>事实上，这种方法说白了还是分部积分法，但使用起来却要方便好多。我们直接看例子：<br>求解$ \int \left ( x^{2}+x \right )e^{x}dx $。</p><ol><li>画一个两行的表格。把多项式部分写在第一行，然后把剩余的部分写在第二行。<table border="1"><tr><td>$ x^{2}+x\ $</td><td></td></tr><tr><td>$ e^{x}\ $</td><td></td></tr></table></li><li>接下来，我们对第一行求导，直到导数为零为止。对第二行积分，直到与第一行的0对齐为止。<table border="1"><tr><td>$ x^{2}+x\ $</td><td>$ 2x+1\ $</td><td>2</td><td>0</td></tr><tr><td>$ e^{x}\ $</td><td>$ e^{x}\ $</td><td>$ e^{x}\ $</td><td>$ e^{x}\ $</td></tr></table></li><li>第三步就是交叉相乘，在本题即为第一行第一列与第二行第二列相乘，第一行第二列与第二行第三列相乘，第一行第三列与第二行第四列相乘。要注意的是，这里的交叉相乘还需要带符号，依次为正负正负正…以此类推。最后，将相乘结果相加，整理即可得到最终的解。<script type="math/tex;mode=display">+\left ( \left ( x^{2}+x \right )*e^{x} \right )-\left ( \left ( 2x+1 \right )*e^{x} \right )+\left ( 2*e^{x} \right )=\left ( x^{2}-x+1 \right )*e^{x}+C\</script><blockquote><p>注意：别忘了加上常数C。</p></blockquote></li></ol><p>下面再来看一个例子熟悉一下：<br>求解$ \int xsinxdx $。<br>画表格：<br><table border="1"><tr><td>$ x\ $</td><td>$ 1\ $</td><td>$ 0\ $</td></tr><tr><td>$ sinx\ $</td><td>$ -cosx\ $</td><td>$ -sinx\ $</td></tr></table><br>求解：</p><script type="math/tex;mode=display">+\left ( x*\left ( -cosx \right ) \right )-\left ( 1*\left ( -sinx \right ) \right )=-xcosx+sinx+C\</script><p>其实b站上还是有挺多这样的干货的，此生无悔入b站！<br><img src="/calculus20191007184856/b站大学.png" title="b站大学"></p><hr><h1 id="其它运算终止情况"><a href="#其它运算终止情况" class="headerlink" title="其它运算终止情况"></a>其它运算终止情况</h1><p>看完上面的部分，细心的你肯定会想到以上的方法并不普适，仅仅适用于导数能求导至零及含有多项式因式的情况。因此，为了能更灵活地运用分部积分表格法，下面补充其它两种运算可以终止的情况。</p><ol><li><h2 id="第一行出现零元素"><a href="#第一行出现零元素" class="headerlink" title="第一行出现零元素"></a>第一行出现零元素</h2>这就是上面所说的含多项式的情况，也一并列写在这里，方便总览归纳。</li><li><h2 id="某列函数的乘积（或它的常数倍）等于第一列"><a href="#某列函数的乘积（或它的常数倍）等于第一列" class="headerlink" title="某列函数的乘积（或它的常数倍）等于第一列"></a>某列函数的乘积（或它的常数倍）等于第一列</h2>按照分部积分的一般做法，当出现之后的某一项恰好是原来积分或者是原来积分的常数倍时，计算进入循环。这时就可以把两者移到等式的同一侧，计算出结果，这在表格法的分部积分中也是类似的。<br>来看看例子：求解$ \int e^{3x}sin2xdx $。<br><table border="1"><tr><td>$ e^{3x}\ $</td><td>$ 3e^{3x}\ $</td><td>$ 9e^{3x}\ $</td></tr><tr><td>$ sin2x\ $</td><td>$ -\frac{cos2x}{2}\ $</td><td>$ -\frac{sin2x}{4}\ $</td></tr></table><br>可见，第三列的乘积和第一列的乘积相差一个常数（这里是$ -\frac{9}{4} $），因此仿照之前的方法交叉相乘列出积分：<script type="math/tex;mode=display">\int e^{3x}sin2xdx=e^{3x}(-\frac{cos2x}{2})-3e^{3x}(-\frac{sin2x}{4})+9(-\frac{1}{4})\int e^{3x}sin2xdx\</script>移项化简可得：<script type="math/tex;mode=display">\int e^{3x}sin2xdx=\frac{1}{13}e^{3x}(3sin2x-2cos2x)+C\</script>即为所求。<br>看完这种情况，你一定会敏锐地发现，其实分部积分表格法本质上和一般的分部积分法一模一样，不过的确在使用上还是有一定的优势的。</li><li><h2 id="某列的两个函数乘积（记为-f-x-）是一个容易计算的积分"><a href="#某列的两个函数乘积（记为-f-x-）是一个容易计算的积分" class="headerlink" title="某列的两个函数乘积（记为$ f(x) $）是一个容易计算的积分"></a>某列的两个函数乘积（记为$ f(x) $）是一个容易计算的积分</h2>这种情况下，先把之前的项用之前的方法类似列出，再在结果后加上不定积分$ (-1)^{k-1}\int f(x)dx $。<br>来看例子：求解$ \int x^{2}arctanxdx $。<br><table border="1"><tr><td>$ arctanx\ $</td><td>$ \frac{1}{1+x^{2}}\ $</td></tr><tr><td>$ x^{2}\ $</td><td>$ \frac{1}{3}x^{3}\ $</td></tr></table><br>可得解：<script type="math/tex;mode=display">\int x^{2}arctanxdx=\frac{1}{3}x^{3}arctanx-\frac{1}{3}\int \frac{x^{3}}{1+x^{2}}dx=\frac{1}{3}\left \{ x^{3}arctanx-\frac{1}{2}[x^{2}-ln(1+x^{2})] \right \}+C\</script>另外，当表中的第一行的某列出现多项之和，而再求导无法改变该函数或者该函数中某一项的属性，则终止表格，后再重新组合，另建表格求解。这种情况一般不会出现在题目中，如遇到再做补充。</li></ol><script type="text/javascript" src="/js/src/bai.js"></script><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      
      
        &lt;!-- build time:Tue Jan 28 2020 15:40:41 GMT+0800 (GMT+08:00) --&gt;&lt;p&gt;假期里想着不能让b站收藏夹里的学习资源一直吃灰，于是又刷了一遍b站的收藏夹。碰巧就看到了自己之前收藏的一种积分方法，那么这篇文章就来搬运一下这种
      
    
    </summary>
    
    
      <category term="知识点与小技巧" scheme="https://gsy00517.github.io/categories/%E7%9F%A5%E8%AF%86%E7%82%B9%E4%B8%8E%E5%B0%8F%E6%8A%80%E5%B7%A7/"/>
    
    
      <category term="微积分" scheme="https://gsy00517.github.io/tags/%E5%BE%AE%E7%A7%AF%E5%88%86/"/>
    
  </entry>
  
  <entry>
    <title>game笔记：海贼王燃烧之血键盘操作</title>
    <link href="https://gsy00517.github.io/game20191007172952/"/>
    <id>https://gsy00517.github.io/game20191007172952/</id>
    <published>2019-10-07T09:29:52.000Z</published>
    <updated>2019-12-01T14:27:45.360Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Tue Jan 28 2020 15:40:41 GMT+0800 (GMT+08:00) --><p>这是一篇关于海贼王也关于游戏的文章，出于对海贼王狂热的喜爱，我决定还是在博客里添一篇这样的文章，感兴趣可以看看。</p><hr><h1 id="画展"><a href="#画展" class="headerlink" title="画展"></a>画展</h1><p>国庆期间，我留在了武汉。幸运的是，海贼王官方在大陆的首次巡展“路飞来了”正好此时也在武汉开展。作为一名海贼铁粉，我当然是毫不犹豫地买了票。其实根据我博客网站的icon以及我目前的个人头像，应该很容易看出我对海贼王的热爱哈哈。<br><img src="/game20191007172952/看展.JPG" title="看展"><br>去的时候快接近饭点了，人还是不少，都是真爱啊~不过像我这样我单身一人的占比不大，但也有。让我惊讶的是当我看着日文的原稿时竟能直接反应出中文，果然那么多年来全套漫画没白买。期间跟一位貌似是艺术生的小哥聊得挺开的，只可惜最后没留联系方式，有缘再见吧。<br>整个展看下来还是挺震撼的，尤其是刚进去的时候，激动地鸡皮疙瘩都起来了。不过跟我在东京塔下面的海贼王主题乐园激动得哭出来还是有一定差距的。看完展之后我买了几张原稿的复刻版，花了不少钱，但觉得挺值，珍藏了。我是一个漫画党，除了剧场版或特别篇之外我看的都是漫画（不过是通过动画入坑的，星空卫视司法岛，一代人的记忆哈哈）。说实话，尾田构思之精巧，漫画史上无人能及，感兴趣可以看看知乎上关于尾田构思的讨论，漫画真的埋了很多神一般的线索，这是动画里办不到的，细细看很有意思。<br><img src="/game20191007172952/哈哈哈路飞.JPG" title="哈哈哈路飞"></p><hr><h1 id="燃烧之血"><a href="#燃烧之血" class="headerlink" title="燃烧之血"></a>燃烧之血</h1><p>看完展，我心中对于海贼王的热血再一次得到激发，回学校就打开燃烧之血，回到海贼世界过把瘾。<br>海贼王燃烧之血（One Piece：Burning Blood）是16年发行的一款海贼王题材的格斗游戏，个人觉得其中的自然系元素化以及霸气设定真的太棒了！另外各种招式都还原得很全很细致，简直就是一边玩一边享受精彩的画面。文末提供了一些图，可以欣赏一下，真的很赞。<br>由于steam版价格原因（加上全部DLC需两三百rmb）以及原本这游戏好像是在游戏机上的（PC版是移植的），导致PC键盘操作方式的教程不是很全，因此本篇文章主要就是对该款游戏的按键操作做一个补充。</p><hr><h1 id="按键操作"><a href="#按键操作" class="headerlink" title="按键操作"></a>按键操作</h1><p>十几个小时玩下来，基本的按键摸得比较熟了，其实键盘操作也有键盘的优势，熟练就好。<br>首先是很普适的移动方式：<br>前进 W<br>后退 S<br>左行 A<br>右行 D<br>下面是一些基本的战斗操作：<br>攻击 K<br>重击 O<br>跳跃 L<br>防御 ；（分号键）<br>往后换人 E<br>往前换人 I<br>突破极限状态 右ctrl<br>必杀技（突破极限状态下） 右ctrl<br>如果要使用招式，那么按下Q，在战斗界面的左侧就会出现招式列表，即三个招式的名称及按键操作，按住Q不松，再配合对应按键，就可以使出对应的招式。<br>招式一 （招式列表情况下） K<br>招式二 （招式列表情况下） O<br>招式三 （招式列表情况下） ；<br>一般情况下，长按对应键不松可以延迟招式的释放时间（比如在对手倒地时可以尝试）。此外，一些招式延迟附带蓄力效果，可以打出更强的攻击（附带破防效果）。<br>接下来是一些组合按键的操作：<br>破防 K+L<br>重击破防 O+；<br>侧步闪躲 W、S、A、D+；<br>范围攻击 S+K<br>范围重击 S+O<br>跳跃攻击 L+K<br>跳跃重击 L+O<br>有些角色还拥有特殊的衍生技能，需通过一定的按键组合释放，这里举两个例子，别的可以参考收藏图鉴：<br>艾斯 神火•不知火 L+O<br>白胡子 垂直跳跃攻击 L+O<br>接下来就是非常有特色的能力啦，按住P键就可以开启自然系能力者的元素化，可以轻松躲掉普攻并适时反击。如果有霸气的话，按住P也可开启霸气，在期间进行攻击就可以造成更大伤害，也不用惧怕自然系了。此外，一些角色开启能力时还可以实现特定的能力，作为海贼迷真的感动到哭，下面举几个例子：<br>女帝 快速后闪 P+L<br>黄猿 瞬移 P+L+方向<br>大熊 瞬移 P+L<br>白胡子 双震 P+招式一<br><img src="/game20191007172952/双震.png" title="双震"><br>白胡子的双震是我最喜欢的技能，真的非常有打击感和冲击力，上图截自b站up主的操作教程，我的许多操作都是从那学来的，他在b站和爱奇艺上都有很详细的连招教程，同时配音也很逗，感兴趣的话可以去观摩一下。</p><hr><h1 id="角色特点"><a href="#角色特点" class="headerlink" title="角色特点"></a>角色特点</h1><p>这里补充一些目前我发现的角色的特点，也是只有海贼迷才懂的，可以说这游戏做得真的赞，后续我发现更多的话会继续补充。<br>1.众所周知，路飞无法被女帝石化。<br>2.山治对抗女性角色时，只能对女性示爱，因此只有挨打的份。</p><hr><h1 id="画面欣赏"><a href="#画面欣赏" class="headerlink" title="画面欣赏"></a>画面欣赏</h1><p>静态无声的画面比起动态有声的还是差多了，但依旧不影响其魅力，看着就觉得很兴奋啦~<br><img src="/game20191007172952/弟弟打哥哥了.jpg" title="弟弟打哥哥了"><br><img src="/game20191007172952/嘿嘿打不着.jpg" title="嘿嘿打不着"><br><img src="/game20191007172952/荒浪白线.jpg" title="荒浪白线"><br><img src="/game20191007172952/room.jpg" title="room"><br><img src="/game20191007172952/元素化.jpg" title="元素化"><br><img src="/game20191007172952/橡皮子弹.jpg" title="橡皮子弹"><br><img src="/game20191007172952/沙岚.jpg" title="沙岚"><br><img src="/game20191007172952/老沙.jpg" title="老沙"><br><img src="/game20191007172952/龙爪.jpg" title="龙爪"><br><img src="/game20191007172952/雷鸟.jpg" title="雷鸟"><br><img src="/game20191007172952/防护屏障.jpg" title="防护屏障"><br><img src="/game20191007172952/冲击屏障.jpg" title="冲击屏障"><br><img src="/game20191007172952/火枪.jpg" title="火枪"><br>当然，游戏仅是起娱乐作用，劳逸结合是关键。如果你也热爱海贼王的话，欢迎和我交流！</p><script type="text/javascript" src="/js/src/bai.js"></script><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      
      
        &lt;!-- build time:Tue Jan 28 2020 15:40:41 GMT+0800 (GMT+08:00) --&gt;&lt;p&gt;这是一篇关于海贼王也关于游戏的文章，出于对海贼王狂热的喜爱，我决定还是在博客里添一篇这样的文章，感兴趣可以看看。&lt;/p&gt;&lt;hr&gt;&lt;h1 id=&quot;
      
    
    </summary>
    
    
      <category term="游戏" scheme="https://gsy00517.github.io/categories/%E6%B8%B8%E6%88%8F/"/>
    
    
      <category term="海贼王" scheme="https://gsy00517.github.io/tags/%E6%B5%B7%E8%B4%BC%E7%8E%8B/"/>
    
      <category term="个人经历" scheme="https://gsy00517.github.io/tags/%E4%B8%AA%E4%BA%BA%E7%BB%8F%E5%8E%86/"/>
    
  </entry>
  
  <entry>
    <title>hexo笔记：ssh与https以及双线部署的一些注意点</title>
    <link href="https://gsy00517.github.io/hexo20191006232704/"/>
    <id>https://gsy00517.github.io/hexo20191006232704/</id>
    <published>2019-10-06T15:27:04.000Z</published>
    <updated>2019-11-15T23:32:44.709Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Tue Jan 28 2020 15:40:41 GMT+0800 (GMT+08:00) --><p>不久前，我对本博客网站做了一些优化，其中包括将网站同时部署到github和coding上。关于双线部署如何具体操作，网上有许多较为详尽的教程可以参考，如果有问题的话可以参考多篇不同的教程找出原因解决。在这篇文章中，我主要想讲讲我这期间遇到的一些小事项。</p><hr><h1 id="github与coding"><a href="#github与coding" class="headerlink" title="github与coding"></a>github与coding</h1><p>考虑到每次打开博客的加载速度问题，我前几天尝试了把博客部署到coding上，实现了coding+github双线部署。coding现已经被腾讯云收购，可以直接用微信登录。<br>部署完成后，为了看一看效果，我使用了<a href="http://tool.chinaz.com/" target="_blank">站长工具</a>分别对coding和github上的网站速度进行了测试。测试结果如下：<br><img src="/hexo20191006232704/github.png" title="github"><br><img src="/hexo20191006232704/coding.png" title="coding"><br>可见，部署在coding上确实能提高一点速度。不过事实上，在实际使用中，并没有感到coding更快，搜索之后发现似乎是coding的服务器也不在内地而在香港的原因。这里附上我的两个链接，可以看看效果，择优访问：<br>github page：<a href="https://gsy00517.github.io/">https://gsy00517.github.io/</a><br>coding page：<a href="https://gsy00517.coding.me/" target="_blank" rel="noopener">https://gsy00517.coding.me/</a></p><hr><h1 id="ssh与https"><a href="#ssh与https" class="headerlink" title="ssh与https"></a>ssh与https</h1><p>在网上的一个教程中，作者提到使用ssh比https更加稳定，尝试后暂时没有发现明显的区别，但是另一个直观的改变就是在push代码时，使用ssh url就不需要输入账号和密码。下面是我在hexo配置文件中的设置，也就是位于站点根目录下的_config.yml文件，其中后面注释中的<code>https://github.com/Gsy00517/Gsy00517.github.io.git</code>是原本的https url。<br><img src="/hexo20191006232704/使用ssh.png" title="使用ssh"><br>上面对应的ssh url一般可以从平台上直接复制获取，也可以参照我的格式进行设置。<br><img src="/hexo20191006232704/平台提供.png" title="平台提供"><br>这里简要说一说ssh与https的区别。<br>一般默认情况下使用的是https，除了需要在fetch和push时使用密码之外，使用https的配置比较方便。然而，使用ssh url却需要先配置和添加好ssh key，并且你必须是这个项目的拥有或者管理者，而https就没有这些要求。其实，配置ssh key也并没有那么繁琐，而且这是一劳永逸的，所以推荐还是使用ssh。<br>要注意的是，ssh key保存的默认位置或许会不同于网上的教程，不过可以自行更改。我的默认地址是在用户文件夹下的AppData\Roaming\SPB_16.6的ssh文件夹中。AppData文件夹默认是隐藏的，可以通过查看隐藏的项目打开。此外，如果需要经常清理temp文件的话，不妨取消这个文件夹的隐藏，这在释放windows空间中还是挺有效的，可以参见<a href="https://gsy00517.github.io/windows20190914091023/" target="_blank">windows笔记：释放空间</a>。<br><img src="/hexo20191006232704/SSHkey所在.png" title="SSHkey所在"><br>key所在的文件是上图所示的第二个publisher文件，然而似乎无法直接用office打开，选择打开方式为记事本即可。<br>当然，如果实在找不到key所在的文件，也可以直接使用文件资源管理器的搜索功能查找名为<code>.ssh</code>的文件夹即可。</p><blockquote><p>注：http与https的区别在于，http是明文传输的，而https是使用ssl加密的，更加安全。若要将连接提交百度站点验证，就需要使用https协议，这个在github和coding都有强制https访问的选项。</p></blockquote><hr><h1 id="双线部署注意事项"><a href="#双线部署注意事项" class="headerlink" title="双线部署注意事项"></a>双线部署注意事项</h1><ol><li><h2 id="LeanCloud"><a href="#LeanCloud" class="headerlink" title="LeanCloud"></a>LeanCloud</h2>这里主要针对hexo博客双线部署后可能会出现的几个问题说明一下注意点。<br>首先，如果之前使用的是LeanCloud来接受记录评论和统计阅读量的，那么为了共享数据，必须在LeanCloud控制台设置的安全中心中，添加新增的web安全域名，保存后即可解决问题。<img src="/hexo20191006232704/添加域名.png" title="添加域名"></li><li><h2 id="Widget"><a href="#Widget" class="headerlink" title="Widget"></a>Widget</h2>如果使用的是基于Widget的评分系统，那么必须更改Widget设置中的domain。我是免费使用Widget，只能同时添加一个domain。我继续使用github page的域名，因此只能在我的github page中看到评分系统。<img src="/hexo20191006232704/欢迎来评.png" title="欢迎来评"></li><li><h2 id="文内链接"><a href="#文内链接" class="headerlink" title="文内链接"></a>文内链接</h2>因为双线部署用的依旧还是同一份本地源码文件，因此在博文中提供的链接依旧是一致的。这里我也将继续使用github page的链接，也就是文内推荐的我本人的博文链接依旧还是指向github page的。事实上，这并无任何影响。</li></ol><script type="text/javascript" src="/js/src/bai.js"></script><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      
      
        &lt;!-- build time:Tue Jan 28 2020 15:40:41 GMT+0800 (GMT+08:00) --&gt;&lt;p&gt;不久前，我对本博客网站做了一些优化，其中包括将网站同时部署到github和coding上。关于双线部署如何具体操作，网上有许多较为详尽的教程可以
      
    
    </summary>
    
    
      <category term="操作和使用" scheme="https://gsy00517.github.io/categories/%E6%93%8D%E4%BD%9C%E5%92%8C%E4%BD%BF%E7%94%A8/"/>
    
    
      <category term="hexo" scheme="https://gsy00517.github.io/tags/hexo/"/>
    
      <category term="配置优化" scheme="https://gsy00517.github.io/tags/%E9%85%8D%E7%BD%AE%E4%BC%98%E5%8C%96/"/>
    
  </entry>
  
  <entry>
    <title>deep learning笔记：使网络能够更深——ResNet简介与pytorch实现</title>
    <link href="https://gsy00517.github.io/deep-learning20191001184216/"/>
    <id>https://gsy00517.github.io/deep-learning20191001184216/</id>
    <published>2019-10-01T10:42:16.000Z</published>
    <updated>2020-01-19T00:23:53.107Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Tue Jan 28 2020 15:40:41 GMT+0800 (GMT+08:00) --><p>之前我用pytorch把ResNet18实现了一下，但由于上周准备国家奖学金答辩没有时间来写我实现的过程与总结。今天是祖国70周年华诞，借着这股喜庆劲，把这篇文章补上。</p><p><strong>References</strong>：</p><p>电子文献：<br><a href="https://blog.csdn.net/weixin_43624538/article/details/85049699" target="_blank" rel="noopener">https://blog.csdn.net/weixin_43624538/article/details/85049699</a><br><a href="https://blog.csdn.net/u013289254/article/details/98785869" target="_blank" rel="noopener">https://blog.csdn.net/u013289254/article/details/98785869</a></p><p>参考文献：<br>[1]Deep Residual Learning for Image Recognition</p><hr><h1 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h1><p>ResNet残差网络是由何凯明等四名微软研究院的华人提出的，当初看到论文标题下面的中国名字还是挺高兴的。文章引入部分，作者就探讨了深度神经网络的优化是否就只是叠加层数、增加深度那么简单。显然这是不可能的，增加深度带来的首要问题就是梯度爆炸、消散的问题，这是由于随着层数的增多，在网络中反向传播的梯度会随着连乘变得不稳定，从而变得特别大或者特别小。其中以梯度消散更为常见。值得注意的是，论文中还提到深度更深的网络反而出现准确率下降并不是由过拟合所引起的。<br>为了解决这个问题，研究者们做出了很多思考与尝试，其中的代表有relu激活函数的使用，Batch Normalization的使用等。关于这两种方法，可以参考网上的资料以及我的博文<a href="https://gsy00517.github.io/deep-learning20190915113859/" target="_blank">deep-learning笔记：开启深度学习热潮——AlexNet</a>和<a href="https://gsy00517.github.io/deep-learning20191001151454/" target="_blank">deep-learning笔记：学习率衰减与批归一化</a>。<br>对于上面这个问题，ResNet作出的贡献是引入skip/identity connection。如下所示就是两个基本的残差模块。<br><img src="/deep-learning20191001184216/基本残差模块.png" title="基本残差模块"><br>上面这个block可表示为：$ F(X)=H(X)-x $。在这里，X为浅层输出，H(x)为深层的输出。当浅层的X代表的特征已经足够成熟，即当任何对于特征X的改变都会让loss变大时，F(X)会自动趋向于学习成为0，X则从恒等映射的路径继续传递。<br>这样，我们就可以在不增加计算成本的情况下使得在前向传递过程中，如果浅层的输出已经足够成熟（optimal），那么就让深层网络后面的层仅实现恒等映射的作用。<br>当X与F（X）通道数目不同时，作者尝试了两种identity mapping的方式。一种即对X缺失的通道直接补零从而使其能够对齐，这种方式比较简单直接，无需额外的参数；另一种则是通过使用1x1的conv来映射从而使通道也能达成一致。</p><hr><h1 id="论文"><a href="#论文" class="headerlink" title="论文"></a>论文</h1><p>老规矩，这里还是先呈上我用黄色荧光高亮出我认为比较重要的要点的论文原文，这里我只有<a href="1512.03385.pdf" target="_blank">英文版</a>。<br>如果需要没有被我标注过的原文，可以直接搜索，这里我仅提供一次，可以<a href="https://arxiv.org/abs/1512.03385" target="_blank">点击这里</a>下载。<br>不过，虽然没有pdf中文版，但其实深度学习CV方向一些比较经典的论文的英文、中文、中英对照都可以到<a href="https://github.com/SnailTyan/deep-learning-papers-translation" target="_blank">Deep Learning Papers Translation</a>上看到，非常方便。</p><hr><h1 id="自己实现"><a href="#自己实现" class="headerlink" title="自己实现"></a>自己实现</h1><p>在论文中，作者提到了如下几个ResNet的版本的结构。<br><img src="/deep-learning20191001184216/各版本ResNet.png" title="各版本ResNet"><br>这里我实现的是ResNet18。<br>由于这不是我第一次使用pytorch进行实现，一些基本的使用操作我就不加注释了，想看注释来理解的话可以参考我之前VGG的实现。<br>由于残差的引入，导致ResNet的结构比较复杂，而论文中并没有非常详细的阐述，在研究官方源码之后，我对它的结构才有了完整的了解，这里我画出来以便参考。<br><img src="/deep-learning20191001184216/基本结构.JPG" title="基本结构"></p><blockquote><p>注：此模块在2016年何大神的论文中给出了新的改进，可以参考我的博文<a href="https://gsy00517.github.io/deep-learning20200113174731/" target="_blank">deep-learning笔记：记首次ResNet实战</a>。</p></blockquote><p>ResNet18的每一layer包括了两个这样的basic block，其中1x1的卷积核仅在X与F（X）通道数目不一致时进行操作，在我的代码中，我定义shortcut函数来对应一切通道一致、无需处理的情况。<br></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">ResNet</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">        super(ResNet, self).__init__()</span><br><span class="line">        </span><br><span class="line">        self.conv1 = nn.Conv2d(in_channels = <span class="number">3</span>, out_channels = <span class="number">64</span>, kernel_size = <span class="number">7</span>, stride = <span class="number">2</span>, padding = <span class="number">3</span>, bias = <span class="literal">False</span>)</span><br><span class="line">        self.max = nn.MaxPool2d(kernel_size = <span class="number">3</span>, stride = <span class="number">2</span>, padding = <span class="number">1</span>)</span><br><span class="line">        </span><br><span class="line">        self.bn1 = nn.BatchNorm2d(<span class="number">64</span>)</span><br><span class="line">        self.bn2 = nn.BatchNorm2d(<span class="number">64</span>)</span><br><span class="line">        self.bn3 = nn.BatchNorm2d(<span class="number">128</span>)</span><br><span class="line">        self.bn4 = nn.BatchNorm2d(<span class="number">256</span>)</span><br><span class="line">        self.bn5 = nn.BatchNorm2d(<span class="number">512</span>)</span><br><span class="line">        </span><br><span class="line">        self.shortcut = nn.Sequential()</span><br><span class="line">        self.shortcut3 = nn.Sequential(nn.Conv2d(<span class="number">64</span>, <span class="number">128</span>, kernel_size = <span class="number">1</span>, stride = <span class="number">2</span>, bias = <span class="literal">False</span>), nn.BatchNorm2d(<span class="number">128</span>)) </span><br><span class="line">        self.shortcut4 = nn.Sequential(nn.Conv2d(<span class="number">128</span>, <span class="number">256</span>, kernel_size = <span class="number">1</span>, stride = <span class="number">2</span>, bias = <span class="literal">False</span>), nn.BatchNorm2d(<span class="number">256</span>))</span><br><span class="line">        self.shortcut5 = nn.Sequential(nn.Conv2d(<span class="number">256</span>, <span class="number">512</span>, kernel_size = <span class="number">1</span>, stride = <span class="number">2</span>, bias = <span class="literal">False</span>), nn.BatchNorm2d(<span class="number">512</span>))</span><br><span class="line">        </span><br><span class="line">        self.conv2 = nn.Conv2d(in_channels = <span class="number">64</span>, out_channels = <span class="number">64</span>, kernel_size = <span class="number">3</span>, stride = <span class="number">1</span>, padding = <span class="number">1</span>, bias = <span class="literal">False</span>)</span><br><span class="line">        </span><br><span class="line">        self.conv3_1 = nn.Conv2d(in_channels = <span class="number">64</span>, out_channels = <span class="number">128</span>, kernel_size = <span class="number">3</span>, stride = <span class="number">2</span>, padding = <span class="number">1</span>, bias = <span class="literal">False</span>)</span><br><span class="line">        self.conv3_2 = nn.Conv2d(in_channels = <span class="number">128</span>, out_channels = <span class="number">128</span>, kernel_size = <span class="number">3</span>, stride = <span class="number">1</span>, padding = <span class="number">1</span>, bias = <span class="literal">False</span>)</span><br><span class="line">        </span><br><span class="line">        self.conv4_1 = nn.Conv2d(in_channels = <span class="number">128</span>, out_channels = <span class="number">256</span>, kernel_size = <span class="number">3</span>, stride = <span class="number">2</span>, padding = <span class="number">1</span>, bias = <span class="literal">False</span>)</span><br><span class="line">        self.conv4_2 = nn.Conv2d(in_channels = <span class="number">256</span>, out_channels = <span class="number">256</span>, kernel_size = <span class="number">3</span>, stride = <span class="number">1</span>, padding = <span class="number">1</span>, bias = <span class="literal">False</span>)</span><br><span class="line">        </span><br><span class="line">        self.conv5_1 = nn.Conv2d(in_channels = <span class="number">256</span>, out_channels = <span class="number">512</span>, kernel_size = <span class="number">3</span>, stride = <span class="number">2</span>, padding = <span class="number">1</span>, bias = <span class="literal">False</span>)</span><br><span class="line">        self.conv5_2 = nn.Conv2d(in_channels = <span class="number">512</span>, out_channels = <span class="number">512</span>, kernel_size = <span class="number">3</span>, stride = <span class="number">1</span>, padding = <span class="number">1</span>, bias = <span class="literal">False</span>)</span><br><span class="line">        </span><br><span class="line">        self.avg = nn.AdaptiveAvgPool2d((<span class="number">1</span>, <span class="number">1</span>))</span><br><span class="line">        <span class="comment">#adaptive自适应，只给定输入和输出大小，让机器自行调整选择核尺寸和步长大小</span></span><br><span class="line">        </span><br><span class="line">        self.fc = nn.Linear(<span class="number">512</span>, <span class="number">1000</span>)</span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        x = F.relu(self.bn1(self.conv1(x)))</span><br><span class="line">        x1 = self.max(x)</span><br><span class="line">        </span><br><span class="line">        <span class="comment">#layer1</span></span><br><span class="line">        x = F.relu(self.bn2(self.conv2(x1)))</span><br><span class="line">        x = self.bn2(self.conv2(x))</span><br><span class="line">        x += self.shortcut(x1) <span class="comment">#pytorch0.4.0之后这里要改为x = x + self.shortcut(x1)</span></span><br><span class="line">        x2 = F.relu(x)</span><br><span class="line">        </span><br><span class="line">        x = F.relu(self.bn2(self.conv2(x2)))</span><br><span class="line">        x = self.bn2(self.conv2(x))</span><br><span class="line">        x += self.shortcut(x2)</span><br><span class="line">        x3 = F.relu(x)</span><br><span class="line">        </span><br><span class="line">        <span class="comment">#layer2</span></span><br><span class="line">        x = F.relu(self.bn3(self.conv3_1(x3)))</span><br><span class="line">        x = self.bn3(self.conv3_2(x))</span><br><span class="line">        x += self.shortcut3(x3)</span><br><span class="line">        x4 = F.relu(x)</span><br><span class="line">        </span><br><span class="line">        x = F.relu(self.bn3(self.conv3_2(x4)))</span><br><span class="line">        x = self.bn3(self.conv3_2(x))</span><br><span class="line">        x += self.shortcut(x4)</span><br><span class="line">        x5 = F.relu(x)</span><br><span class="line">        </span><br><span class="line">        <span class="comment">#layer3</span></span><br><span class="line">        x = F.relu(self.bn4(self.conv4_1(x5)))</span><br><span class="line">        x = self.bn4(self.conv4_2(x))</span><br><span class="line">        x += self.shortcut4(x5)</span><br><span class="line">        x6 = F.relu(x)</span><br><span class="line">        </span><br><span class="line">        x = F.relu(self.bn4(self.conv4_2(x6)))</span><br><span class="line">        x = self.bn4(self.conv4_2(x))</span><br><span class="line">        x += self.shortcut(x6)</span><br><span class="line">        x7 = F.relu(x)</span><br><span class="line">        </span><br><span class="line">        <span class="comment">#layer4</span></span><br><span class="line">        x = F.relu(self.bn5(self.conv5_1(x7)))</span><br><span class="line">        x = self.bn5(self.conv5_2(x))</span><br><span class="line">        x += self.shortcut5(x7)</span><br><span class="line">        x8 = F.relu(x)</span><br><span class="line">        </span><br><span class="line">        x = F.relu(self.bn5(self.conv5_2(x8)))</span><br><span class="line">        x = self.bn5(self.conv5_2(x))</span><br><span class="line">        x += self.shortcut(x8)</span><br><span class="line">        x = F.relu(x)</span><br><span class="line">        </span><br><span class="line">        <span class="comment">#ending</span></span><br><span class="line">        x = self.avg(x)</span><br><span class="line">        </span><br><span class="line">        <span class="comment">#变换维度，可以设其中一个尺寸为-1，表示机器内部自己计算，但同时只能有一个为-1</span></span><br><span class="line">        x = x.view(<span class="number">-1</span>, self.num_flat_features(x))</span><br><span class="line">        x = self.fc(x)</span><br><span class="line">        </span><br><span class="line">        x = F.softmax(x, dim = <span class="number">1</span>)</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">num_flat_features</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        size = x.size()[<span class="number">1</span>:]</span><br><span class="line">        num_features = <span class="number">1</span></span><br><span class="line">        <span class="keyword">for</span> s <span class="keyword">in</span> size:</span><br><span class="line">            num_features *= s</span><br><span class="line">        <span class="keyword">return</span> num_features</span><br><span class="line">        </span><br><span class="line">net = ResNet()</span><br></pre></td></tr></table></figure><p></p><p>同样的，我们可以随机生成一个张量来进行验证：<br></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">input = torch.randn(<span class="number">1</span>, <span class="number">3</span>, <span class="number">48</span>, <span class="number">48</span>)</span><br><span class="line">out = net(input)</span><br><span class="line">print(out)</span><br></pre></td></tr></table></figure><p></p><p>如果可以顺利地输出，那么模型基本上是没有问题的。</p><hr><h1 id="出现的问题"><a href="#出现的问题" class="headerlink" title="出现的问题"></a>出现的问题</h1><p>在这里我还是想把自己踩的一些简单的坑记下来，引以为戒。</p><ol><li><h2 id="softmax输出全为1"><a href="#softmax输出全为1" class="headerlink" title="softmax输出全为1"></a>softmax输出全为1</h2><p>当我使用F.softmax之后，出现了这样的一个问题：</p><img src="/deep-learning20191001184216/输出全为1.png" title="输出全为1"><p>查找资料后发现，我错误的把对每一行softmax当作了对每一列softmax。因为这个softmax语句是我从之前的自己做的一道kaggle题目写的代码中ctrl+C+V过来的，复制过来的是<code>x = F.softmax(x, dim = 0)</code>，在这里，dim = 0意味着我对张量的每一列进行softmax，这是因为我之前的场景中需要处理的张量是一维的，也就是tensor（）里面只有一对“[]”，此时它默认只有一列，我对列进行softmax自然就没有问题。<br>而放到这里，我再对列进行softmax时，每列上就只有一个元素。那么结果就都是1即100%了。解决的方法就是把dim设为1。<br>下面我在用一组代码直观地展示一下softmax的用法与区别。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line">x1= torch.Tensor( [ [<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>], [<span class="number">1</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>], [<span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>]])</span><br><span class="line">y11= F.softmax(x1, dim = <span class="number">0</span>) <span class="comment">#对每一列进行softmax</span></span><br><span class="line">y12 = F.softmax(x1, dim = <span class="number">1</span>) <span class="comment">#对每一行进行softmax</span></span><br><span class="line">x2 = torch.Tensor([<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>])</span><br><span class="line">y2 = F.softmax(x2, dim = <span class="number">0</span>)</span><br></pre></td></tr></table></figure><p>我们输出每个结果，可以看到：</p><img src="/deep-learning20191001184216/结果.png" title="结果"></li><li><h2 id="bias"><a href="#bias" class="headerlink" title="bias"></a>bias</h2>或许你可以发现，在我的代码中，每个卷积层中都设置了<code>bias = False</code>，这是我在参考官方源码之后补上的。那么，这个bias是什么，又有什么用呢？<br>我们在学深度学习的时候，最早接触到的神经网络应该是感知器，它的结构如下图所示。 <img src="/deep-learning20191001184216/感知器.jpg" title="感知器"> 要想激活这个感知器，就必须使<code>x1 * w1 + x2 * w2 + ... + xn * wn &gt; T</code>（T为一个阈值），而T越大，想激活这个感知器的难度越大。<br>考虑样本较多的情况，我不可能手动选择一个阈值，使得模型整体表现最佳，因此我们不如使得T变成可学习的，这样一来，T会自动学习到一个数，使得模型的整体表现最佳。当把T移动到左边，它就成了bias偏置，<code>x1 * w1 + x2 * w2 + ... + xn * wn - T &gt; 0</code>。显然，偏置的大小控制着激活这个感知器的难易程度。<br>在比感知器高级的神经网络中，也是如此。<br>但倘若我们要在卷积后面加上归一化操作，那么bias的作用就无法体现了。<br>我们以ResNet卷积层后的BN层为例。<br>可参考我的上一篇博文，BN处理过程中有这样一步： <img src="/deep-learning20191001184216/归一化.png" title="归一化"> 对于分子而言，无论有没有bias，对结果都没有影响；而对于下面分母而言，因为是方差操作，所以也没有影响。因此，在ResNet中，因为每次卷积之后都要进行BN操作，那就不需要启用bias，否则非但不起作用，还会消耗一定的显卡内存。</li></ol><hr><h1 id="官方源码"><a href="#官方源码" class="headerlink" title="官方源码"></a>官方源码</h1><p>如果你此时对ResNet的结构已经有了比较清晰的理解，那么可以尝试着来理解一下官方源码的思路。其实我觉得先看像我这样直观的代码实现再看官方源码更有助理解且更高效。<br></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br><span class="line">213</span><br><span class="line">214</span><br><span class="line">215</span><br><span class="line">216</span><br><span class="line">217</span><br><span class="line">218</span><br><span class="line">219</span><br><span class="line">220</span><br><span class="line">221</span><br><span class="line">222</span><br><span class="line">223</span><br><span class="line">224</span><br><span class="line">225</span><br><span class="line">226</span><br><span class="line">227</span><br><span class="line">228</span><br><span class="line">229</span><br><span class="line">230</span><br><span class="line">231</span><br><span class="line">232</span><br><span class="line">233</span><br><span class="line">234</span><br><span class="line">235</span><br><span class="line">236</span><br><span class="line">237</span><br><span class="line">238</span><br><span class="line">239</span><br><span class="line">240</span><br><span class="line">241</span><br><span class="line">242</span><br><span class="line">243</span><br><span class="line">244</span><br><span class="line">245</span><br><span class="line">246</span><br><span class="line">247</span><br><span class="line">248</span><br><span class="line">249</span><br><span class="line">250</span><br><span class="line">251</span><br><span class="line">252</span><br><span class="line">253</span><br><span class="line">254</span><br><span class="line">255</span><br><span class="line">256</span><br><span class="line">257</span><br><span class="line">258</span><br><span class="line">259</span><br><span class="line">260</span><br><span class="line">261</span><br><span class="line">262</span><br><span class="line">263</span><br><span class="line">264</span><br><span class="line">265</span><br><span class="line">266</span><br><span class="line">267</span><br><span class="line">268</span><br><span class="line">269</span><br><span class="line">270</span><br><span class="line">271</span><br><span class="line">272</span><br><span class="line">273</span><br><span class="line">274</span><br><span class="line">275</span><br><span class="line">276</span><br><span class="line">277</span><br><span class="line">278</span><br><span class="line">279</span><br><span class="line">280</span><br><span class="line">281</span><br><span class="line">282</span><br><span class="line">283</span><br><span class="line">284</span><br><span class="line">285</span><br><span class="line">286</span><br><span class="line">287</span><br><span class="line">288</span><br><span class="line">289</span><br><span class="line">290</span><br><span class="line">291</span><br><span class="line">292</span><br><span class="line">293</span><br><span class="line">294</span><br><span class="line">295</span><br><span class="line">296</span><br><span class="line">297</span><br><span class="line">298</span><br><span class="line">299</span><br><span class="line">300</span><br><span class="line">301</span><br><span class="line">302</span><br><span class="line">303</span><br><span class="line">304</span><br><span class="line">305</span><br><span class="line">306</span><br><span class="line">307</span><br><span class="line">308</span><br><span class="line">309</span><br><span class="line">310</span><br><span class="line">311</span><br><span class="line">312</span><br><span class="line">313</span><br><span class="line">314</span><br><span class="line">315</span><br><span class="line">316</span><br><span class="line">317</span><br><span class="line">318</span><br><span class="line">319</span><br><span class="line">320</span><br><span class="line">321</span><br><span class="line">322</span><br><span class="line">323</span><br><span class="line">324</span><br><span class="line">325</span><br><span class="line">326</span><br><span class="line">327</span><br><span class="line">328</span><br><span class="line">329</span><br><span class="line">330</span><br><span class="line">331</span><br><span class="line">332</span><br><span class="line">333</span><br><span class="line">334</span><br><span class="line">335</span><br><span class="line">336</span><br><span class="line">337</span><br><span class="line">338</span><br><span class="line">339</span><br><span class="line">340</span><br><span class="line">341</span><br><span class="line">342</span><br><span class="line">343</span><br><span class="line">344</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">from</span> .utils <span class="keyword">import</span> load_state_dict_from_url</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">__all__ = [<span class="string">'ResNet'</span>, <span class="string">'resnet18'</span>, <span class="string">'resnet34'</span>, <span class="string">'resnet50'</span>, <span class="string">'resnet101'</span>,</span><br><span class="line">           <span class="string">'resnet152'</span>, <span class="string">'resnext50_32x4d'</span>, <span class="string">'resnext101_32x8d'</span>,</span><br><span class="line">           <span class="string">'wide_resnet50_2'</span>, <span class="string">'wide_resnet101_2'</span>]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">model_urls = &#123;</span><br><span class="line">    <span class="string">'resnet18'</span>: <span class="string">'https://download.pytorch.org/models/resnet18-5c106cde.pth'</span>,</span><br><span class="line">    <span class="string">'resnet34'</span>: <span class="string">'https://download.pytorch.org/models/resnet34-333f7ec4.pth'</span>,</span><br><span class="line">    <span class="string">'resnet50'</span>: <span class="string">'https://download.pytorch.org/models/resnet50-19c8e357.pth'</span>,</span><br><span class="line">    <span class="string">'resnet101'</span>: <span class="string">'https://download.pytorch.org/models/resnet101-5d3b4d8f.pth'</span>,</span><br><span class="line">    <span class="string">'resnet152'</span>: <span class="string">'https://download.pytorch.org/models/resnet152-b121ed2d.pth'</span>,</span><br><span class="line">    <span class="string">'resnext50_32x4d'</span>: <span class="string">'https://download.pytorch.org/models/resnext50_32x4d-7cdf4587.pth'</span>,</span><br><span class="line">    <span class="string">'resnext101_32x8d'</span>: <span class="string">'https://download.pytorch.org/models/resnext101_32x8d-8ba56ff5.pth'</span>,</span><br><span class="line">    <span class="string">'wide_resnet50_2'</span>: <span class="string">'https://download.pytorch.org/models/wide_resnet50_2-95faca4d.pth'</span>,</span><br><span class="line">    <span class="string">'wide_resnet101_2'</span>: <span class="string">'https://download.pytorch.org/models/wide_resnet101_2-32ee1156.pth'</span>,</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">conv3x3</span><span class="params">(in_planes, out_planes, stride=<span class="number">1</span>, groups=<span class="number">1</span>, dilation=<span class="number">1</span>)</span>:</span></span><br><span class="line">    <span class="string">"""3x3 convolution with padding"""</span></span><br><span class="line">    <span class="keyword">return</span> nn.Conv2d(in_planes, out_planes, kernel_size=<span class="number">3</span>, stride=stride,</span><br><span class="line">                     padding=dilation, groups=groups, bias=<span class="literal">False</span>, dilation=dilation)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">conv1x1</span><span class="params">(in_planes, out_planes, stride=<span class="number">1</span>)</span>:</span></span><br><span class="line">    <span class="string">"""1x1 convolution"""</span></span><br><span class="line">    <span class="keyword">return</span> nn.Conv2d(in_planes, out_planes, kernel_size=<span class="number">1</span>, stride=stride, bias=<span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">BasicBlock</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    expansion = <span class="number">1</span></span><br><span class="line">    __constants__ = [<span class="string">'downsample'</span>]</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, inplanes, planes, stride=<span class="number">1</span>, downsample=None, groups=<span class="number">1</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">                 base_width=<span class="number">64</span>, dilation=<span class="number">1</span>, norm_layer=None)</span>:</span></span><br><span class="line">        super(BasicBlock, self).__init__()</span><br><span class="line">        <span class="keyword">if</span> norm_layer <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">            norm_layer = nn.BatchNorm2d</span><br><span class="line">        <span class="keyword">if</span> groups != <span class="number">1</span> <span class="keyword">or</span> base_width != <span class="number">64</span>:</span><br><span class="line">            <span class="keyword">raise</span> ValueError(<span class="string">'BasicBlock only supports groups=1 and base_width=64'</span>)</span><br><span class="line">        <span class="keyword">if</span> dilation &gt; <span class="number">1</span>:</span><br><span class="line">            <span class="keyword">raise</span> NotImplementedError(<span class="string">"Dilation &gt; 1 not supported in BasicBlock"</span>)</span><br><span class="line">        <span class="comment"># Both self.conv1 and self.downsample layers downsample the input when stride != 1</span></span><br><span class="line">        self.conv1 = conv3x3(inplanes, planes, stride)</span><br><span class="line">        self.bn1 = norm_layer(planes)</span><br><span class="line">        self.relu = nn.ReLU(inplace=<span class="literal">True</span>)</span><br><span class="line">        self.conv2 = conv3x3(planes, planes)</span><br><span class="line">        self.bn2 = norm_layer(planes)</span><br><span class="line">        self.downsample = downsample</span><br><span class="line">        self.stride = stride</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        identity = x</span><br><span class="line"></span><br><span class="line">        out = self.conv1(x)</span><br><span class="line">        out = self.bn1(out)</span><br><span class="line">        out = self.relu(out)</span><br><span class="line"></span><br><span class="line">        out = self.conv2(out)</span><br><span class="line">        out = self.bn2(out)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> self.downsample <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            identity = self.downsample(x)</span><br><span class="line"></span><br><span class="line">        out += identity</span><br><span class="line">        out = self.relu(out)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> out</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Bottleneck</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    expansion = <span class="number">4</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, inplanes, planes, stride=<span class="number">1</span>, downsample=None, groups=<span class="number">1</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">                 base_width=<span class="number">64</span>, dilation=<span class="number">1</span>, norm_layer=None)</span>:</span></span><br><span class="line">        super(Bottleneck, self).__init__()</span><br><span class="line">        <span class="keyword">if</span> norm_layer <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">            norm_layer = nn.BatchNorm2d</span><br><span class="line">        width = int(planes * (base_width / <span class="number">64.</span>)) * groups</span><br><span class="line">        <span class="comment"># Both self.conv2 and self.downsample layers downsample the input when stride != 1</span></span><br><span class="line">        self.conv1 = conv1x1(inplanes, width)</span><br><span class="line">        self.bn1 = norm_layer(width)</span><br><span class="line">        self.conv2 = conv3x3(width, width, stride, groups, dilation)</span><br><span class="line">        self.bn2 = norm_layer(width)</span><br><span class="line">        self.conv3 = conv1x1(width, planes * self.expansion)</span><br><span class="line">        self.bn3 = norm_layer(planes * self.expansion)</span><br><span class="line">        self.relu = nn.ReLU(inplace=<span class="literal">True</span>)</span><br><span class="line">        self.downsample = downsample</span><br><span class="line">        self.stride = stride</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        identity = x</span><br><span class="line"></span><br><span class="line">        out = self.conv1(x)</span><br><span class="line">        out = self.bn1(out)</span><br><span class="line">        out = self.relu(out)</span><br><span class="line"></span><br><span class="line">        out = self.conv2(out)</span><br><span class="line">        out = self.bn2(out)</span><br><span class="line">        out = self.relu(out)</span><br><span class="line"></span><br><span class="line">        out = self.conv3(out)</span><br><span class="line">        out = self.bn3(out)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> self.downsample <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            identity = self.downsample(x)</span><br><span class="line"></span><br><span class="line">        out += identity</span><br><span class="line">        out = self.relu(out)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> out</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">ResNet</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, block, layers, num_classes=<span class="number">1000</span>, zero_init_residual=False,</span></span></span><br><span class="line"><span class="function"><span class="params">                 groups=<span class="number">1</span>, width_per_group=<span class="number">64</span>, replace_stride_with_dilation=None,</span></span></span><br><span class="line"><span class="function"><span class="params">                 norm_layer=None)</span>:</span></span><br><span class="line">        super(ResNet, self).__init__()</span><br><span class="line">        <span class="keyword">if</span> norm_layer <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">            norm_layer = nn.BatchNorm2d</span><br><span class="line">        self._norm_layer = norm_layer</span><br><span class="line"></span><br><span class="line">        self.inplanes = <span class="number">64</span></span><br><span class="line">        self.dilation = <span class="number">1</span></span><br><span class="line">        <span class="keyword">if</span> replace_stride_with_dilation <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">            <span class="comment"># each element in the tuple indicates if we should replace</span></span><br><span class="line">            <span class="comment"># the 2x2 stride with a dilated convolution instead</span></span><br><span class="line">            replace_stride_with_dilation = [<span class="literal">False</span>, <span class="literal">False</span>, <span class="literal">False</span>]</span><br><span class="line">        <span class="keyword">if</span> len(replace_stride_with_dilation) != <span class="number">3</span>:</span><br><span class="line">            <span class="keyword">raise</span> ValueError(<span class="string">"replace_stride_with_dilation should be None "</span></span><br><span class="line">                             <span class="string">"or a 3-element tuple, got &#123;&#125;"</span>.format(replace_stride_with_dilation))</span><br><span class="line">        self.groups = groups</span><br><span class="line">        self.base_width = width_per_group</span><br><span class="line">        self.conv1 = nn.Conv2d(<span class="number">3</span>, self.inplanes, kernel_size=<span class="number">7</span>, stride=<span class="number">2</span>, padding=<span class="number">3</span>,</span><br><span class="line">                               bias=<span class="literal">False</span>)</span><br><span class="line">        self.bn1 = norm_layer(self.inplanes)</span><br><span class="line">        self.relu = nn.ReLU(inplace=<span class="literal">True</span>)</span><br><span class="line">        self.maxpool = nn.MaxPool2d(kernel_size=<span class="number">3</span>, stride=<span class="number">2</span>, padding=<span class="number">1</span>)</span><br><span class="line">        self.layer1 = self._make_layer(block, <span class="number">64</span>, layers[<span class="number">0</span>])</span><br><span class="line">        self.layer2 = self._make_layer(block, <span class="number">128</span>, layers[<span class="number">1</span>], stride=<span class="number">2</span>,</span><br><span class="line">                                       dilate=replace_stride_with_dilation[<span class="number">0</span>])</span><br><span class="line">        self.layer3 = self._make_layer(block, <span class="number">256</span>, layers[<span class="number">2</span>], stride=<span class="number">2</span>,</span><br><span class="line">                                       dilate=replace_stride_with_dilation[<span class="number">1</span>])</span><br><span class="line">        self.layer4 = self._make_layer(block, <span class="number">512</span>, layers[<span class="number">3</span>], stride=<span class="number">2</span>,</span><br><span class="line">                                       dilate=replace_stride_with_dilation[<span class="number">2</span>])</span><br><span class="line">        self.avgpool = nn.AdaptiveAvgPool2d((<span class="number">1</span>, <span class="number">1</span>))</span><br><span class="line">        self.fc = nn.Linear(<span class="number">512</span> * block.expansion, num_classes)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> m <span class="keyword">in</span> self.modules():</span><br><span class="line">            <span class="keyword">if</span> isinstance(m, nn.Conv2d):</span><br><span class="line">                nn.init.kaiming_normal_(m.weight, mode=<span class="string">'fan_out'</span>, nonlinearity=<span class="string">'relu'</span>)</span><br><span class="line">            <span class="keyword">elif</span> isinstance(m, (nn.BatchNorm2d, nn.GroupNorm)):</span><br><span class="line">                nn.init.constant_(m.weight, <span class="number">1</span>)</span><br><span class="line">                nn.init.constant_(m.bias, <span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Zero-initialize the last BN in each residual branch,</span></span><br><span class="line">        <span class="comment"># so that the residual branch starts with zeros, and each residual block behaves like an identity.</span></span><br><span class="line">        <span class="comment"># This improves the model by 0.2~0.3% according to https://arxiv.org/abs/1706.02677</span></span><br><span class="line">        <span class="keyword">if</span> zero_init_residual:</span><br><span class="line">            <span class="keyword">for</span> m <span class="keyword">in</span> self.modules():</span><br><span class="line">                <span class="keyword">if</span> isinstance(m, Bottleneck):</span><br><span class="line">                    nn.init.constant_(m.bn3.weight, <span class="number">0</span>)</span><br><span class="line">                <span class="keyword">elif</span> isinstance(m, BasicBlock):</span><br><span class="line">                    nn.init.constant_(m.bn2.weight, <span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_make_layer</span><span class="params">(self, block, planes, blocks, stride=<span class="number">1</span>, dilate=False)</span>:</span></span><br><span class="line">        norm_layer = self._norm_layer</span><br><span class="line">        downsample = <span class="literal">None</span></span><br><span class="line">        previous_dilation = self.dilation</span><br><span class="line">        <span class="keyword">if</span> dilate:</span><br><span class="line">            self.dilation *= stride</span><br><span class="line">            stride = <span class="number">1</span></span><br><span class="line">        <span class="keyword">if</span> stride != <span class="number">1</span> <span class="keyword">or</span> self.inplanes != planes * block.expansion:</span><br><span class="line">            downsample = nn.Sequential(</span><br><span class="line">                conv1x1(self.inplanes, planes * block.expansion, stride),</span><br><span class="line">                norm_layer(planes * block.expansion),</span><br><span class="line">            )</span><br><span class="line"></span><br><span class="line">        layers = []</span><br><span class="line">        layers.append(block(self.inplanes, planes, stride, downsample, self.groups,</span><br><span class="line">                            self.base_width, previous_dilation, norm_layer))</span><br><span class="line">        self.inplanes = planes * block.expansion</span><br><span class="line">        <span class="keyword">for</span> _ <span class="keyword">in</span> range(<span class="number">1</span>, blocks):</span><br><span class="line">            layers.append(block(self.inplanes, planes, groups=self.groups,</span><br><span class="line">                                base_width=self.base_width, dilation=self.dilation,</span><br><span class="line">                                norm_layer=norm_layer))</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> nn.Sequential(*layers)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        x = self.conv1(x)</span><br><span class="line">        x = self.bn1(x)</span><br><span class="line">        x = self.relu(x)</span><br><span class="line">        x = self.maxpool(x)</span><br><span class="line"></span><br><span class="line">        x = self.layer1(x)</span><br><span class="line">        x = self.layer2(x)</span><br><span class="line">        x = self.layer3(x)</span><br><span class="line">        x = self.layer4(x)</span><br><span class="line"></span><br><span class="line">        x = self.avgpool(x)</span><br><span class="line">        x = torch.flatten(x, <span class="number">1</span>)</span><br><span class="line">        x = self.fc(x)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">_resnet</span><span class="params">(arch, block, layers, pretrained, progress, **kwargs)</span>:</span></span><br><span class="line">    model = ResNet(block, layers, **kwargs)</span><br><span class="line">    <span class="keyword">if</span> pretrained:</span><br><span class="line">        state_dict = load_state_dict_from_url(model_urls[arch],</span><br><span class="line">                                              progress=progress)</span><br><span class="line">        model.load_state_dict(state_dict)</span><br><span class="line">    <span class="keyword">return</span> model</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">resnet18</span><span class="params">(pretrained=False, progress=True, **kwargs)</span>:</span></span><br><span class="line">    <span class="string">r"""ResNet-18 model from</span></span><br><span class="line"><span class="string">    `"Deep Residual Learning for Image Recognition" &lt;https://arxiv.org/pdf/1512.03385.pdf&gt;`_</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        pretrained (bool): If True, returns a model pre-trained on ImageNet</span></span><br><span class="line"><span class="string">        progress (bool): If True, displays a progress bar of the download to stderr</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="keyword">return</span> _resnet(<span class="string">'resnet18'</span>, BasicBlock, [<span class="number">2</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">2</span>], pretrained, progress,</span><br><span class="line">                   **kwargs)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">resnet34</span><span class="params">(pretrained=False, progress=True, **kwargs)</span>:</span></span><br><span class="line">    <span class="string">r"""ResNet-34 model from</span></span><br><span class="line"><span class="string">    `"Deep Residual Learning for Image Recognition" &lt;https://arxiv.org/pdf/1512.03385.pdf&gt;`_</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        pretrained (bool): If True, returns a model pre-trained on ImageNet</span></span><br><span class="line"><span class="string">        progress (bool): If True, displays a progress bar of the download to stderr</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="keyword">return</span> _resnet(<span class="string">'resnet34'</span>, BasicBlock, [<span class="number">3</span>, <span class="number">4</span>, <span class="number">6</span>, <span class="number">3</span>], pretrained, progress,</span><br><span class="line">                   **kwargs)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">resnet50</span><span class="params">(pretrained=False, progress=True, **kwargs)</span>:</span></span><br><span class="line">    <span class="string">r"""ResNet-50 model from</span></span><br><span class="line"><span class="string">    `"Deep Residual Learning for Image Recognition" &lt;https://arxiv.org/pdf/1512.03385.pdf&gt;`_</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        pretrained (bool): If True, returns a model pre-trained on ImageNet</span></span><br><span class="line"><span class="string">        progress (bool): If True, displays a progress bar of the download to stderr</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="keyword">return</span> _resnet(<span class="string">'resnet50'</span>, Bottleneck, [<span class="number">3</span>, <span class="number">4</span>, <span class="number">6</span>, <span class="number">3</span>], pretrained, progress,</span><br><span class="line">                   **kwargs)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">resnet101</span><span class="params">(pretrained=False, progress=True, **kwargs)</span>:</span></span><br><span class="line">    <span class="string">r"""ResNet-101 model from</span></span><br><span class="line"><span class="string">    `"Deep Residual Learning for Image Recognition" &lt;https://arxiv.org/pdf/1512.03385.pdf&gt;`_</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        pretrained (bool): If True, returns a model pre-trained on ImageNet</span></span><br><span class="line"><span class="string">        progress (bool): If True, displays a progress bar of the download to stderr</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="keyword">return</span> _resnet(<span class="string">'resnet101'</span>, Bottleneck, [<span class="number">3</span>, <span class="number">4</span>, <span class="number">23</span>, <span class="number">3</span>], pretrained, progress,</span><br><span class="line">                   **kwargs)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">resnet152</span><span class="params">(pretrained=False, progress=True, **kwargs)</span>:</span></span><br><span class="line">    <span class="string">r"""ResNet-152 model from</span></span><br><span class="line"><span class="string">    `"Deep Residual Learning for Image Recognition" &lt;https://arxiv.org/pdf/1512.03385.pdf&gt;`_</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        pretrained (bool): If True, returns a model pre-trained on ImageNet</span></span><br><span class="line"><span class="string">        progress (bool): If True, displays a progress bar of the download to stderr</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="keyword">return</span> _resnet(<span class="string">'resnet152'</span>, Bottleneck, [<span class="number">3</span>, <span class="number">8</span>, <span class="number">36</span>, <span class="number">3</span>], pretrained, progress,</span><br><span class="line">                   **kwargs)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">resnext50_32x4d</span><span class="params">(pretrained=False, progress=True, **kwargs)</span>:</span></span><br><span class="line">    <span class="string">r"""ResNeXt-50 32x4d model from</span></span><br><span class="line"><span class="string">    `"Aggregated Residual Transformation for Deep Neural Networks" &lt;https://arxiv.org/pdf/1611.05431.pdf&gt;`_</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        pretrained (bool): If True, returns a model pre-trained on ImageNet</span></span><br><span class="line"><span class="string">        progress (bool): If True, displays a progress bar of the download to stderr</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    kwargs[<span class="string">'groups'</span>] = <span class="number">32</span></span><br><span class="line">    kwargs[<span class="string">'width_per_group'</span>] = <span class="number">4</span></span><br><span class="line">    <span class="keyword">return</span> _resnet(<span class="string">'resnext50_32x4d'</span>, Bottleneck, [<span class="number">3</span>, <span class="number">4</span>, <span class="number">6</span>, <span class="number">3</span>],</span><br><span class="line">                   pretrained, progress, **kwargs)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">resnext101_32x8d</span><span class="params">(pretrained=False, progress=True, **kwargs)</span>:</span></span><br><span class="line">    <span class="string">r"""ResNeXt-101 32x8d model from</span></span><br><span class="line"><span class="string">    `"Aggregated Residual Transformation for Deep Neural Networks" &lt;https://arxiv.org/pdf/1611.05431.pdf&gt;`_</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        pretrained (bool): If True, returns a model pre-trained on ImageNet</span></span><br><span class="line"><span class="string">        progress (bool): If True, displays a progress bar of the download to stderr</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    kwargs[<span class="string">'groups'</span>] = <span class="number">32</span></span><br><span class="line">    kwargs[<span class="string">'width_per_group'</span>] = <span class="number">8</span></span><br><span class="line">    <span class="keyword">return</span> _resnet(<span class="string">'resnext101_32x8d'</span>, Bottleneck, [<span class="number">3</span>, <span class="number">4</span>, <span class="number">23</span>, <span class="number">3</span>],</span><br><span class="line">                   pretrained, progress, **kwargs)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">wide_resnet50_2</span><span class="params">(pretrained=False, progress=True, **kwargs)</span>:</span></span><br><span class="line">    <span class="string">r"""Wide ResNet-50-2 model from</span></span><br><span class="line"><span class="string">    `"Wide Residual Networks" &lt;https://arxiv.org/pdf/1605.07146.pdf&gt;`_</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    The model is the same as ResNet except for the bottleneck number of channels</span></span><br><span class="line"><span class="string">    which is twice larger in every block. The number of channels in outer 1x1</span></span><br><span class="line"><span class="string">    convolutions is the same, e.g. last block in ResNet-50 has 2048-512-2048</span></span><br><span class="line"><span class="string">    channels, and in Wide ResNet-50-2 has 2048-1024-2048.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        pretrained (bool): If True, returns a model pre-trained on ImageNet</span></span><br><span class="line"><span class="string">        progress (bool): If True, displays a progress bar of the download to stderr</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    kwargs[<span class="string">'width_per_group'</span>] = <span class="number">64</span> * <span class="number">2</span></span><br><span class="line">    <span class="keyword">return</span> _resnet(<span class="string">'wide_resnet50_2'</span>, Bottleneck, [<span class="number">3</span>, <span class="number">4</span>, <span class="number">6</span>, <span class="number">3</span>],</span><br><span class="line">                   pretrained, progress, **kwargs)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">wide_resnet101_2</span><span class="params">(pretrained=False, progress=True, **kwargs)</span>:</span></span><br><span class="line">    <span class="string">r"""Wide ResNet-101-2 model from</span></span><br><span class="line"><span class="string">    `"Wide Residual Networks" &lt;https://arxiv.org/pdf/1605.07146.pdf&gt;`_</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    The model is the same as ResNet except for the bottleneck number of channels</span></span><br><span class="line"><span class="string">    which is twice larger in every block. The number of channels in outer 1x1</span></span><br><span class="line"><span class="string">    convolutions is the same, e.g. last block in ResNet-50 has 2048-512-2048</span></span><br><span class="line"><span class="string">    channels, and in Wide ResNet-50-2 has 2048-1024-2048.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        pretrained (bool): If True, returns a model pre-trained on ImageNet</span></span><br><span class="line"><span class="string">        progress (bool): If True, displays a progress bar of the download to stderr</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    kwargs[<span class="string">'width_per_group'</span>] = <span class="number">64</span> * <span class="number">2</span></span><br><span class="line">    <span class="keyword">return</span> _resnet(<span class="string">'wide_resnet101_2'</span>, Bottleneck, [<span class="number">3</span>, <span class="number">4</span>, <span class="number">23</span>, <span class="number">3</span>],</span><br><span class="line">                   pretrained, progress, **kwargs)</span><br></pre></td></tr></table></figure><p></p><hr><h1 id="pth文件"><a href="#pth文件" class="headerlink" title="pth文件"></a>pth文件</h1><p>在阅读官方源码时，我们会注意到官方提供了一系列版本的model_urls，其中，每一个url都是以.pth结尾的。<br>当我下载了对应的文件之后，并不知道如何处理，于是我通过搜索，简单的了解了pth文件的概念与使用方法。<br>简单来说，pth文件就是一个表示Python的模块搜索路径（module search path）的文本文件，在xxx.pth文件里面，会书写一些路径，一行一个。如果我们将xxx.pth文件放在特定位置，则可以让python在加载模块时，读取xxx.pth中指定的路径。<br>下面我使用pytorch对pth文件进行加载操作。<br>首先，我把<a href="https://download.pytorch.org/models/resnet18-5c106cde.pth" target="_blank">ResNet18对应的pth文件</a>下载到桌面。<br></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> torchvision.models <span class="keyword">as</span> models</span><br><span class="line"></span><br><span class="line"><span class="comment"># pretrained = True就可以使用预训练的模型</span></span><br><span class="line">net = models.resnet18(pretrained = <span class="literal">False</span>)</span><br><span class="line"><span class="comment">#注意，根据model的不同，这里models.xxx的内容也是不同的，比如models.squeezenet1_1</span></span><br><span class="line"></span><br><span class="line">pthfile = <span class="string">r'C:\Users\sheny\Desktop\resnet18-5c106cde.pth'</span><span class="comment">#pth文件所在路径</span></span><br><span class="line">net.load_state_dict(torch.load(pthfile))</span><br><span class="line">print(net)</span><br></pre></td></tr></table></figure><p></p><p>输出结果如下：<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br></pre></td><td class="code"><pre><span class="line">ResNet(</span><br><span class="line">  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)</span><br><span class="line">  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)</span><br><span class="line">  (relu): ReLU(inplace=True)</span><br><span class="line">  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)</span><br><span class="line">  (layer1): Sequential(</span><br><span class="line">    (0): BasicBlock(</span><br><span class="line">      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)</span><br><span class="line">      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)</span><br><span class="line">      (relu): ReLU(inplace=True)</span><br><span class="line">      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)</span><br><span class="line">      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)</span><br><span class="line">    )</span><br><span class="line">    (1): BasicBlock(</span><br><span class="line">      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)</span><br><span class="line">      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)</span><br><span class="line">      (relu): ReLU(inplace=True)</span><br><span class="line">      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)</span><br><span class="line">      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)</span><br><span class="line">    )</span><br><span class="line">  )</span><br><span class="line">  (layer2): Sequential(</span><br><span class="line">    (0): BasicBlock(</span><br><span class="line">      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)</span><br><span class="line">      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)</span><br><span class="line">      (relu): ReLU(inplace=True)</span><br><span class="line">      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)</span><br><span class="line">      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)</span><br><span class="line">      (downsample): Sequential(</span><br><span class="line">        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)</span><br><span class="line">        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)</span><br><span class="line">      )</span><br><span class="line">    )</span><br><span class="line">    (1): BasicBlock(</span><br><span class="line">      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)</span><br><span class="line">      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)</span><br><span class="line">      (relu): ReLU(inplace=True)</span><br><span class="line">      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)</span><br><span class="line">      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)</span><br><span class="line">    )</span><br><span class="line">  )</span><br><span class="line">  (layer3): Sequential(</span><br><span class="line">    (0): BasicBlock(</span><br><span class="line">      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)</span><br><span class="line">      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)</span><br><span class="line">      (relu): ReLU(inplace=True)</span><br><span class="line">      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)</span><br><span class="line">      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)</span><br><span class="line">      (downsample): Sequential(</span><br><span class="line">        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)</span><br><span class="line">        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)</span><br><span class="line">      )</span><br><span class="line">    )</span><br><span class="line">    (1): BasicBlock(</span><br><span class="line">      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)</span><br><span class="line">      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)</span><br><span class="line">      (relu): ReLU(inplace=True)</span><br><span class="line">      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)</span><br><span class="line">      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)</span><br><span class="line">    )</span><br><span class="line">  )</span><br><span class="line">  (layer4): Sequential(</span><br><span class="line">    (0): BasicBlock(</span><br><span class="line">      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)</span><br><span class="line">      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)</span><br><span class="line">      (relu): ReLU(inplace=True)</span><br><span class="line">      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)</span><br><span class="line">      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)</span><br><span class="line">      (downsample): Sequential(</span><br><span class="line">        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)</span><br><span class="line">        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)</span><br><span class="line">      )</span><br><span class="line">    )</span><br><span class="line">    (1): BasicBlock(</span><br><span class="line">      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)</span><br><span class="line">      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)</span><br><span class="line">      (relu): ReLU(inplace=True)</span><br><span class="line">      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)</span><br><span class="line">      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)</span><br><span class="line">    )</span><br><span class="line">  )</span><br><span class="line">  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))</span><br><span class="line">  (fc): Linear(in_features=512, out_features=1000, bias=True)</span><br><span class="line">)</span><br></pre></td></tr></table></figure><p></p><p>这样你就可以看到很详尽的参数设置了。<br>我们还可以加载所有的参数。<br></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line">pthfile = <span class="string">r'C:\Users\sheny\Desktop\resnet18-5c106cde.pth'</span></span><br><span class="line"></span><br><span class="line">net = torch.load(pthfile)</span><br><span class="line"></span><br><span class="line">print(net)</span><br></pre></td></tr></table></figure><p></p><p>输出如下：<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">OrderedDict([(&apos;conv1.weight&apos;, Parameter containing:</span><br><span class="line">tensor([[[[-1.0419e-02, -6.1356e-03, -1.8098e-03,  ...,  5.6615e-02,</span><br><span class="line">            1.7083e-02, -1.2694e-02],</span><br><span class="line">          [ 1.1083e-02,  9.5276e-03, -1.0993e-01,  ..., -2.7124e-01,</span><br><span class="line">           -1.2907e-01,  3.7424e-03],</span><br><span class="line">          [-6.9434e-03,  5.9089e-02,  2.9548e-01,  ...,  5.1972e-01,</span><br><span class="line">            2.5632e-01,  6.3573e-02],</span><br><span class="line">          ...,</span><br></pre></td></tr></table></figure><p></p><script type="text/javascript" src="/js/src/bai.js"></script><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      
      
        &lt;!-- build time:Tue Jan 28 2020 15:40:41 GMT+0800 (GMT+08:00) --&gt;&lt;p&gt;之前我用pytorch把ResNet18实现了一下，但由于上周准备国家奖学金答辩没有时间来写我实现的过程与总结。今天是祖国70周年华诞，借着这股
      
    
    </summary>
    
    
      <category term="人工智能" scheme="https://gsy00517.github.io/categories/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/"/>
    
    
      <category term="深度学习" scheme="https://gsy00517.github.io/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="论文分享" scheme="https://gsy00517.github.io/tags/%E8%AE%BA%E6%96%87%E5%88%86%E4%BA%AB/"/>
    
      <category term="踩坑血泪" scheme="https://gsy00517.github.io/tags/%E8%B8%A9%E5%9D%91%E8%A1%80%E6%B3%AA/"/>
    
      <category term="代码实现" scheme="https://gsy00517.github.io/tags/%E4%BB%A3%E7%A0%81%E5%AE%9E%E7%8E%B0/"/>
    
      <category term="pytorch" scheme="https://gsy00517.github.io/tags/pytorch/"/>
    
  </entry>
  
  <entry>
    <title>deep learning笔记：学习率衰减与批归一化</title>
    <link href="https://gsy00517.github.io/deep-learning20191001151454/"/>
    <id>https://gsy00517.github.io/deep-learning20191001151454/</id>
    <published>2019-10-01T07:14:54.000Z</published>
    <updated>2020-01-19T00:24:04.399Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Tue Jan 28 2020 15:40:41 GMT+0800 (GMT+08:00) --><p>一段时间之前，在一个深度学习交流群里看到一个群友发问：为什么他的训练误差最后疯狂上下抖动而不是一直降低。<br><img src="/deep-learning20191001151454/群友疑惑.PNG" title="群友疑惑"><br>作为一个入门小白，我当时也很疑惑。但后来我结合所学，仔细思考之后，发现这是一个挺容易犯的错误。</p><p><strong>References</strong>：</p><p>电子文献：<br><a href="https://blog.csdn.net/bestrivern/article/details/86301619" target="_blank" rel="noopener">https://blog.csdn.net/bestrivern/article/details/86301619</a><br><a href="https://www.jianshu.com/p/9643cba47655" target="_blank" rel="noopener">https://www.jianshu.com/p/9643cba47655</a><br><a href="https://www.cnblogs.com/eilearn/p/9780696.html" target="_blank" rel="noopener">https://www.cnblogs.com/eilearn/p/9780696.html</a><br><a href="https://blog.csdn.net/donkey_1993/article/details/81871132" target="_blank" rel="noopener">https://blog.csdn.net/donkey_1993/article/details/81871132</a><br><a href="https://www.pytorchtutorial.com/how-to-use-batchnorm/" target="_blank" rel="noopener">https://www.pytorchtutorial.com/how-to-use-batchnorm/</a></p><hr><h1 id="问题"><a href="#问题" class="headerlink" title="问题"></a>问题</h1><p>事实上，这是一个在机器学习中就有可能遇到的问题，当学习速率α设置得过大时，往往在模型训练的后期难以达到最优解，而是在最优解附近来回抖动。还有可能反而使损失函数越来越大，甚至达到无穷，如下图所示。<br><img src="/deep-learning20191001151454/损失函数.png" title="损失函数"><br>而在深度学习中，假设我们使用mini-batch梯度下降法，由于mini-batch的数量不大，大概64或者128个样本，在迭代过程中会有噪声。这个时候使用固定的学习率导致的结果就是虽然下降朝向最小值，但不会精确地收敛，只会在附近不断地波动（蓝色线）。<br><img src="/deep-learning20191001151454/不会真正收敛.png" title="不会真正收敛"><br>但如果慢慢减少学习率，在初期，学习还是相对较快地，但随着学习率的变小，步伐也会变慢变小，所以最后当开始收敛时，你的曲线（绿色线）会在最小值附近的一个较小区域之内摆动，而不是在训练过程中，大幅度地在最小值附近摆动。<br><img src="/deep-learning20191001151454/大幅度波动变小范围摆动.png" title="大幅度波动变小范围摆动"><br>对于这个问题，我目前收集了有下面这些解决办法。</p><hr><h1 id="直接修改学习率"><a href="#直接修改学习率" class="headerlink" title="直接修改学习率"></a>直接修改学习率</h1><p>在吴恩达的机器学习课程中，他介绍了一种人为选择学习率的规则：每三倍选择一个学习率。<br>比如：我们首先选择了0.1为学习率，那么当这个学习率过大时，我们修改成0.3。倘若还是偏大，我们继续改为0.01、0.003、0.001…以此类推，当学习率偏小是也是以三倍增加并尝试检验，最终选出比较合适的学习率。<br>但这种方法只适用于模型数量小的情况，且这种方法终究还是固定的学习率，依旧无法很好地权衡从而达到前期快速下降与后期稳定收敛的目的。</p><hr><h1 id="学习率动态衰减"><a href="#学习率动态衰减" class="headerlink" title="学习率动态衰减"></a>学习率动态衰减</h1><p>学习率衰减的本质在于，在学习初期，你能承受并且需要较大的步伐，但当开始收敛的时候，小一些的学习率能让你步伐小一些，从而更稳定地达到精确的最优解。<br>为此，我们另外增添衰减率超参数，构建函数使学习率能够在训练的过程中动态衰减。</p><script type="math/tex;mode=display">\alpha = \frac{1}{1+decayrate*epochnum}*\alpha _{0}\</script><p>其中decay rate称为衰减率，epoch num是代数，$ \alpha _{0} $是初始学习率。<br>此外还有下面这些构造方法：<br>指数衰减：$ \alpha =0.95^{epochnum}*\alpha _{0} $<br>其他常用方法：</p><script type="math/tex;mode=display">\alpha =\frac{k}{\sqrt{epochnum}}*\alpha _{0}\</script><script type="math/tex;mode=display">\alpha =\frac{k}{\sqrt{t}}\alpha _{0}\</script><p>其中k为mini-batch的数字。</p><hr><h1 id="几种衰减方法的实现"><a href="#几种衰减方法的实现" class="headerlink" title="几种衰减方法的实现"></a>几种衰减方法的实现</h1><p>在pytorch中，学习率调整主要有两种方式：<br>1.直接修改optimizer中的lr参数。<br>2.利用lr_scheduler()提供的几种衰减函数。即使用torch.optim.lr_scheduler，基于循环的次数提供了一些方法来调节学习率。<br>3.利用torch.optim.lr_scheduler.ReduceLROnPlateau，基于验证测量结果来设置不同的学习率.<br>下面提供几种实现方法：<br>准备（对下列通用）：<br></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch.optim <span class="keyword">import</span> * <span class="comment">#包含Adam，lr_scheduler等</span></span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"></span><br><span class="line"><span class="comment">#生成一个简单全连接神经网络</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">net</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">        super(net, self).__init__()</span><br><span class="line">        self.fc = nn.Linear(<span class="number">1</span>, <span class="number">10</span>)</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> self.fc(x)</span><br></pre></td></tr></table></figure><p></p><ol><li><h2 id="手动阶梯式衰减"><a href="#手动阶梯式衰减" class="headerlink" title="手动阶梯式衰减"></a>手动阶梯式衰减</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">model = net()</span><br><span class="line">LR = <span class="number">0.01</span></span><br><span class="line">optimizer = Adam(model.parameters(), lr = LR)</span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> range(<span class="number">100</span>):</span><br><span class="line">    <span class="keyword">if</span> epoch % <span class="number">5</span> == <span class="number">0</span>:</span><br><span class="line">        <span class="keyword">for</span> p <span class="keyword">in</span> optimizer.param_groups:</span><br><span class="line">            p[<span class="string">'lr'</span>] *= <span class="number">0.9</span> <span class="comment">#学习率超参的位置：optimizer.state_dict()['param_groups'][0]['lr']</span></span><br></pre></td></tr></table></figure><p>这里是每过5个epoch就进行一次衰减。</p><img src="/deep-learning20191001151454/手动阶梯式衰减.png" title="手动阶梯式衰减"></li><li><h2 id="lambda自定义衰减"><a href="#lambda自定义衰减" class="headerlink" title="lambda自定义衰减"></a>lambda自定义衰减</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np </span><br><span class="line">model = net()</span><br><span class="line">LR = <span class="number">0.01</span></span><br><span class="line">optimizer = Adam(model.parameters(), lr = LR)</span><br><span class="line">lambda1 = <span class="keyword">lambda</span> epoch: np.sin(epoch) / epoch</span><br><span class="line">scheduler = lr_scheduler.LambdaLR(optimizer, lr_lambda = lambda1)</span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> range(<span class="number">100</span>):</span><br><span class="line">    scheduler.step()</span><br></pre></td></tr></table></figure><p>lr_lambda会接收到一个int参数：epoch，然后根据epoch计算出对应的lr。如果设置多个lambda函数的话，会分别作用于optimizer中的不同的params_group。</p><img src="/deep-learning20191001151454/lambda自定义衰减.png" title="lambda自定义衰减"></li><li><h2 id="StepLR阶梯式衰减"><a href="#StepLR阶梯式衰减" class="headerlink" title="StepLR阶梯式衰减"></a>StepLR阶梯式衰减</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">model = net()</span><br><span class="line">LR = <span class="number">0.01</span></span><br><span class="line">optimizer = Adam(model.parameters(), lr = LR)</span><br><span class="line">scheduler = lr_scheduler.StepLR(optimizer, step_size = <span class="number">5</span>, gamma = <span class="number">0.8</span>)</span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> range(<span class="number">100</span>):</span><br><span class="line">    scheduler.step()</span><br></pre></td></tr></table></figure><p>每个epoch，lr会自动乘以gamma。</p><img src="/deep-learning20191001151454/StepLR阶梯式衰减.png" title="StepLR阶梯式衰减"></li><li><h2 id="三段式衰减"><a href="#三段式衰减" class="headerlink" title="三段式衰减"></a>三段式衰减</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">model = net()</span><br><span class="line">LR = <span class="number">0.01</span></span><br><span class="line">optimizer = Adam(model.parameters(), lr = LR)</span><br><span class="line">scheduler = lr_scheduler.MultiStepLR(optimizer, milestones = [<span class="number">20</span>,<span class="number">80</span>], gamma = <span class="number">0.9</span>)</span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> range(<span class="number">100</span>):</span><br><span class="line">    scheduler.step()</span><br></pre></td></tr></table></figure><p>这种方法就是，当epoch进入milestones范围内即乘以gamma，离开milestones范围之后再乘以gamma。<br>这种衰减方式也是在学术论文中最常见的方式，一般手动调整也会采用这种方法。</p><img src="/deep-learning20191001151454/三段式衰减.png" title="三段式衰减"></li><li><h2 id="连续衰减"><a href="#连续衰减" class="headerlink" title="连续衰减"></a>连续衰减</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">model = net()</span><br><span class="line">LR = <span class="number">0.01</span></span><br><span class="line">optimizer = Adam(model.parameters(), lr = LR)</span><br><span class="line">scheduler = lr_scheduler.ExponentialLR(optimizer, gamma = <span class="number">0.9</span>)</span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> range(<span class="number">100</span>):</span><br><span class="line">    scheduler.step()</span><br></pre></td></tr></table></figure><p>这种方法就是在每个epoch中lr都乘以gamma，从而达到连续衰减的效果。</p><img src="/deep-learning20191001151454/连续衰减.png" title="连续衰减"></li><li><h2 id="余弦式调整"><a href="#余弦式调整" class="headerlink" title="余弦式调整"></a>余弦式调整</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">model = net()</span><br><span class="line">LR = <span class="number">0.01</span></span><br><span class="line">optimizer = Adam(model.parameters(), lr = LR)</span><br><span class="line">scheduler = lr_scheduler.CosineAnnealingLR(optimizer, T_max = <span class="number">20</span>)</span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> range(<span class="number">100</span>):</span><br><span class="line">    scheduler.step()</span><br></pre></td></tr></table></figure><p>这里的T_max对应1/2个cos周期所对应的epoch数值。</p><img src="/deep-learning20191001151454/余弦式调整.png" title="余弦式调整"></li><li><h2 id="基于loss和accuracy"><a href="#基于loss和accuracy" class="headerlink" title="基于loss和accuracy"></a>基于loss和accuracy</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">model = net()</span><br><span class="line">LR = <span class="number">0.01</span></span><br><span class="line">optimizer = Adam(model.parameters(), lr = LR)</span><br><span class="line">scheduler = lr_scheduler.ReduceLROnPlateau(optimizer, mode = <span class="string">'min'</span>, factor = <span class="number">0.1</span>, patience = <span class="number">10</span>, verbose = <span class="literal">False</span>, threshold = <span class="number">0.0001</span>, threshold_mode = <span class="string">'rel'</span>, cooldown = <span class="number">0</span>, min_lr = <span class="number">0</span>, eps = <span class="number">1e-08</span>)</span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> range(<span class="number">100</span>):</span><br><span class="line">    scheduler.step()</span><br></pre></td></tr></table></figure><p>当发现loss不再降低或者accuracy不再提高之后，就降低学习率。</p><blockquote><p>注：上面代码中各参数意义如下：<br>mode：’min’模式检测metric是否不再减小，’max’模式检测metric是否不再增大；<br>factor：触发条件后lr*=factor；<br>patience：不再减小（或增大）的累计次数；<br>verbose：触发条件后print；<br>threshold：只关注超过阈值的显著变化；<br>threshold_mode：有rel和abs两种阈值计算模式，rel规则：max模式下如果超过best(1+threshold)为显著，min模式下如果低于best(1-threshold)为显著；abs规则：max模式下如果超过best+threshold为显著，min模式下如果低于best-threshold为显著；<br>cooldown：触发一次条件后，等待一定epoch再进行检测，避免lr下降过速；<br>min_lr：最小的允许lr；<br>eps：如果新旧lr之间的差异小与1e-8，则忽略此次更新。</p></blockquote></li></ol><p>这里非常感谢facebook的员工给我们提供了如此多的选择与便利！<br>对于上述方法如有任何疑惑，还请查阅<a href="https://ptorch.com/docs/1/optim" target="_blank">torch.optim文档</a>。</p><hr><h1 id="批归一化（Batch-Normalization）"><a href="#批归一化（Batch-Normalization）" class="headerlink" title="批归一化（Batch Normalization）"></a>批归一化（Batch Normalization）</h1><p>除了对学习率进行调整之外，Batch Normalization也可以有效地解决之前的问题。<br>我是在学习ResNet的时候第一次遇到批归一化这个概念的。随着深度神经网络深度的加深，训练越来越困难，收敛越来越慢。为此，很多论文都尝试解决这个问题，比如ReLU激活函数，再比如Residual Network，而BN本质上也是解释并从某个不同的角度来解决这个问题的。<br>通过使用Batch Normalization，我们可以加快网络的收敛速度，这样我们就可以使用较大的学习率来训练网络了。此外，BN还提高了网络的泛化能力。<br>BN的基本思想其实相当直观：<br>首先，因为深层神经网络在做非线性变换前的激活输入值（就是x=WU+B，U是输入）随着网络深度加深或者在训练过程中，其分布逐渐发生偏移或者变动，之所以训练收敛慢，一般是整体分布逐渐往非线性函数的取值区间的上下限两端靠近（对于Sigmoid函数来说，意味着激活输入值WU+B是大的负值或正值），这就导致了反向传播时低层神经网络的梯度消失，这是训练深层神经网络收敛越来越慢的本质原因。<br>事实上，神经网络学习过程本质上是为了学习数据的分布，而BN就是通过一定的规范化手段，把每层神经网络任意神经元这个输入值的分布强行拉回到均值为0、方差为1的标准正态分布，其实就是把越来越偏的分布强制拉回比较标准的分布，这样使得激活输入值落在非线性函数对输入比较敏感的区域，这样输入的小变化就会导致损失函数较大的变化，从而让梯度变大，避免梯度消失问题产生，而且梯度变大意味着学习收敛速度快，因此通过BN能大大加快训练速度。<br>下面来看看BN的具体操作过程：<br><img src="/deep-learning20191001151454/BN操作过程.png" title="BN操作过程"><br>即以下四个步骤：<br>1.计算样本均值。<br>2.计算样本方差。<br>3.对样本数据进行标准化处理。<br>4.进行平移和缩放处理。这里引入了γ和β两个参数。通过训练可学习重构的γ和β这两个参数，让我们的网络可以学习恢复出原始网络所要学习的特征分布。<br>下面是BN层的训练流程：<br><img src="/deep-learning20191001151454/BN训练流程.png" title="BN训练流程"><br>这里的详细过程如下：<br>输入：待进入激活函数的变量。<br>输出：<br>1.这里的K，在卷积网络中可以看作是卷积核个数，如网络中第n层有64个卷积核，就需要计算64次。</p><blockquote><p>注意：在正向传播时，会使用γ与β使得BN层输出与输入一样。</p></blockquote><p>2.在反向传播时利用γ与β求得梯度从而改变训练权值（变量）。<br>3.通过不断迭代直到训练结束，求得关于不同层的γ与β。<br>4.不断遍历训练集中的图片，取出每个batch_size中的γ与β，最后统计每层BN的γ与β各自的和除以图片数量得到平均值，并对其做无偏估计直作为每一层的E[x]与Var[x]。<br>5.在预测的正向传播时，对测试数据求取γ与β，并使用该层的E[x]与Var[x]，通过图中11：所表示的公式计算BN层输出。</p><blockquote><p>注意：在预测时，BN层的输出已经被改变，因此BN层在预测中的作用体现在此处。</p></blockquote><p>上面输入的是待进入激活函数的变量，在残差网络ResNet中，的确也是先经过BN层再用relu函数做非线性处理的。那么，为什么BN层一般用在线性层和卷积层的后面，而不是放在非线性单元即激活函数之后呢？<br>因为非线性单元的输出分布形状会在训练过程中变化，归一化无法消除他的方差偏移。相反的，全连接和卷积层的输出一般是一个对称、非稀疏的一个分布，更加类似高斯分布，对他们进行归一化会产生更加稳定的分布。<br>比如，我们对一个高斯分布的数据relu激活，那么小于0的直接就被抑制了，这样得到的结果很难是高斯分布了，这时候再添加一个BN层就很难达到所需的效果。<br>很多实验证明，BatchNorm只要用了就有效果，所以在一般情况下没有理由不用。但也有相反的情况，比如当每个batch里所有的sample都非常相似的时候，相似到mean和variance都基本为0时，最好不要用BatchNorm。此外如果batch size为1，从原理上来讲，此时用BatchNorm是没有任何意义的。</p><blockquote><p>注意：通常我们在进行Transfer Learning的时候，会冻结之前的网络权重，注意这时候往往也会冻结BatchNorm中训练好的moving averages值。这些moving averages值只适用于以前的旧的数据，对新数据不一定适用。所以最好的方法是在Transfer Learning的时候不要冻结BatchNorm层，让moving averages值重新从新的数据中学习。</p></blockquote><hr><h1 id="批归一化实现"><a href="#批归一化实现" class="headerlink" title="批归一化实现"></a>批归一化实现</h1><p>这里还是使用pytorch进行实现。<br>准备（对下列通用）：<br></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br></pre></td></tr></table></figure><p></p><ol><li><h2 id="2d或3d输入"><a href="#2d或3d输入" class="headerlink" title="2d或3d输入"></a>2d或3d输入</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 添加了可学习的仿射变换参数</span></span><br><span class="line">m = nn.BatchNorm1d(<span class="number">100</span>)</span><br><span class="line"><span class="comment"># 未添加可学习的仿射变换参数</span></span><br><span class="line">m = nn.BatchNorm1d(<span class="number">100</span>, affine = <span class="literal">False</span>)</span><br><span class="line">input = torch.autograd.Variable(torch.randn(<span class="number">20</span>, <span class="number">100</span>))</span><br><span class="line">output = m(input)</span><br></pre></td></tr></table></figure><p>我们查看m，可以看到有如下形式：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)</span><br></pre></td></tr></table></figure><p>这里解释一下涉及到的参数：<br>num_features：来自期望输入的特征数，该期望输入的大小为：<code>batch_size * num_features(* width)</code><br>eps：为保证数值稳定性（分母不能趋近或取0），给分母加上的值，默认为1e-5。<br>momentum：计算动态均值和动态方差并进行移动平均所使用的动量，默认为0.1。<br>affine：一个布尔值，当设为true时，就给该层添加可学习的仿射变换参数。仿射变换将在后文做简单介绍。<br>BatchNorm1d可以有两种输入输出：<br>1.输入（N，C），输出（N，C）。<br>2.输入（N，C，L），输出（N，C，L）。</p></li><li><h2 id="3d或4d输入"><a href="#3d或4d输入" class="headerlink" title="3d或4d输入"></a>3d或4d输入</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">m = nn.BatchNorm2d(<span class="number">100</span>)</span><br><span class="line"><span class="comment">#或者</span></span><br><span class="line">m = nn.BatchNorm2d(<span class="number">100</span>, affine = <span class="literal">False</span>)</span><br><span class="line">input = torch.autograd.Variable(torch.randn(<span class="number">20</span>, <span class="number">100</span>, <span class="number">35</span>, <span class="number">45</span>))</span><br><span class="line">output = m(input)</span><br></pre></td></tr></table></figure><p>BatchNorm2d也可以有两种输入输出：<br>1.输入（N，C，L），输出（N，C，L）。<br>2.输入（N，C，H，W），输出（N，C，H，W）。</p></li><li><h2 id="4d或5d输入"><a href="#4d或5d输入" class="headerlink" title="4d或5d输入"></a>4d或5d输入</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">m = nn.BatchNorm3d(<span class="number">100</span>)</span><br><span class="line"><span class="comment">#或者</span></span><br><span class="line">m = nn.BatchNorm3d(<span class="number">100</span>, affine=<span class="literal">False</span>)</span><br></pre></td></tr></table></figure><p>BatchNorm3d同样支持两种输入输出：<br>1.输入（N，C，H，W），输出（N，C，H，W）。<br>2.输入（N，C，D，H，W），输出（N，C，D，H，W）。</p></li></ol><hr><h1 id="仿射变换"><a href="#仿射变换" class="headerlink" title="仿射变换"></a>仿射变换</h1><p>这里我简单介绍一下仿射变换的概念，仿射变换（Affine Transformation或Affine Map）是一种二维坐标（x, y）到二维坐标（u, v）的变换，它是另外两种简单变换的叠加，一是线性变换，二是平移变换。同时，仿射变换保持了二维图形的“平直性”、“平行性”和“共线比例不变性”，非共线的三对对应点确定一个唯一的仿射变换。</p><blockquote><p>补充：<br>共线性：若几个点变换前在一条线上，则仿射变换后仍然在一条线上。<br>平行性：若两条线变换前平行，则变换后仍然平行。<br>共线比例不变性：变换前一条线上两条线段的比例，在变换后比例不变。</p></blockquote><p>在二维图像变换中，它的一般表达如下：<br><img src="/deep-learning20191001151454/仿射变换.png" title="仿射变换"><br>可以视为线性变换R和平移变换T的叠加。<br>另外，仿射变换可以通过一系列的原子变换的复合来实现，包括平移，缩放，翻转，旋转和剪切。因此我们可以将几种简单的变换矩阵相乘来实现仿射变换。</p><script type="text/javascript" src="/js/src/bai.js"></script><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      
      
        &lt;!-- build time:Tue Jan 28 2020 15:40:41 GMT+0800 (GMT+08:00) --&gt;&lt;p&gt;一段时间之前，在一个深度学习交流群里看到一个群友发问：为什么他的训练误差最后疯狂上下抖动而不是一直降低。&lt;br&gt;&lt;img src=&quot;/deep-
      
    
    </summary>
    
    
      <category term="人工智能" scheme="https://gsy00517.github.io/categories/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/"/>
    
    
      <category term="深度学习" scheme="https://gsy00517.github.io/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="代码实现" scheme="https://gsy00517.github.io/tags/%E4%BB%A3%E7%A0%81%E5%AE%9E%E7%8E%B0/"/>
    
      <category term="pytorch" scheme="https://gsy00517.github.io/tags/pytorch/"/>
    
  </entry>
  
  <entry>
    <title>windows笔记：一些快捷的操作</title>
    <link href="https://gsy00517.github.io/windows20191001130531/"/>
    <id>https://gsy00517.github.io/windows20191001130531/</id>
    <published>2019-10-01T05:05:31.000Z</published>
    <updated>2019-11-15T23:33:08.458Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Tue Jan 28 2020 15:40:41 GMT+0800 (GMT+08:00) --><p>之前在网上看到一个windows系统下的上帝模式，很好奇，尝试之后感觉不错，这里介绍一下创建的方法。除此之外附上一些类似的快捷操作。</p><hr><h1 id="上帝模式"><a href="#上帝模式" class="headerlink" title="上帝模式"></a>上帝模式</h1><p>上帝模式，即”God Mode”，或称为“完全控制面板”。它是windows系统中隐藏的一个简单的文件夹窗口，包含了几乎所有windows系统的设置，如控制面板的功能、界面个性化、辅助功能选项等控制设置，用户只需通过这一个窗口就能实现所有的操控，而不必再去为调整一个小小的系统设置细想半天究竟该在什么地方去打开设置。打开上帝模式后你将会看到如下界面：<br><img src="/windows20191001130531/上帝模式.png" title="上帝模式"><br>好吧我承认和想象中的上帝模式不太一样，不过下面我还是介绍一下这个略显简陋的上帝模式是怎么设置的。</p><ol><li><h2 id="方式一：添加桌面快捷方式"><a href="#方式一：添加桌面快捷方式" class="headerlink" title="方式一：添加桌面快捷方式"></a>方式一：添加桌面快捷方式</h2><ol><li>首先在桌面新建一个文件夹。</li><li>将新建的文件夹命名为：<code>GodMode.{ED7BA470-8E54-465E-825C-99712043E01C}</code>。</li><li>重命名完成后，你将看到一个类似于控制面板但没有名称的图标，双击打开，就可以看到之前所展示的上帝模式的界面了。<img src="/windows20191001130531/快捷方式.png" title="快捷方式"></li></ol></li><li><h2 id="方式二：添加到快捷菜单"><a href="#方式二：添加到快捷菜单" class="headerlink" title="方式二：添加到快捷菜单"></a>方式二：添加到快捷菜单</h2><ol><li>win+R运行，输入regedit打开注册表编辑器，允许更改。<img src="/windows20191001130531/注册表编辑器.png" title="注册表编辑器"></li><li>依次展开路径至HKEY_CLASSES_ROOT\DesktopBackground\Shell。<img src="/windows20191001130531/路径.jpg" title="路径"></li><li>点击shell后在右侧窗口鼠标右击，选择新建项。<img src="/windows20191001130531/新建项.jpg" title="新建项"></li><li>把新建的项重命名为“上帝模式”。<img src="/windows20191001130531/重命名上帝模式.jpg" title="重命名上帝模式"></li><li>点击上帝模式后，双击右侧窗口中的默认，在数值数据处输入上帝模式，点击确定。<img src="/windows20191001130531/输入上帝模式.jpg" title="输入上帝模式"></li><li>右击上帝模式，选择新建项。<img src="/windows20191001130531/再新建项.jpg" title="再新建项"></li><li>把新建的项重命名为“command”。</li><li>点击command后，双击右侧窗口中的默认，在数值数据处输入：<code>explorer shell:::{ED7BA470-8E54-465E-825C-99712043E01C}</code>，确定。<img src="/windows20191001130531/输入数值.jpg" title="输入数值"></li><li>这时候在桌面空白处右键打开快捷菜单，就可以看到上帝模式已成功添加。<img src="/windows20191001130531/快捷菜单.png" title="快捷菜单"></li></ol></li></ol><hr><h1 id="类似的操作"><a href="#类似的操作" class="headerlink" title="类似的操作"></a>类似的操作</h1><p>在上面我的快捷菜单中，可以看到还有关机、重启、锁屏等选项。其实它们添加的操作和添加上帝模式的步骤是一样的，只需把命名为“上帝模式”的地方修改成“关机”等文字，并且在上文中的第8步中，用对应的数值数据即可。<br><img src="/windows20191001130531/完成.jpg" title="完成"><br>这里提供四种功能对应的数值数据，其实这些和上面上帝模式的commmand命令都是可以直接在cmd中执行的：<br>关机<code>Shutdown -s -f -t 00</code><br>注销<code>Shutdown -l</code><br>重启<code>Shutdown -r -f -t 00</code><br>锁屏<code>Rundll32 User32.dll,LockWorkStation</code><br>事实上，锁屏功能可以直接使用win+L快捷键达到目的。除此之外，win还可以搭配其他的一些按键完成一些快捷操作，比如win+D可以快速最小化一切窗口回到桌面，想知道win有哪些搭配可以右键左下角的win图标查看。<br><img src="/windows20191001130531/可用快捷键.png" title="可用快捷键"></p><script type="text/javascript" src="/js/src/bai.js"></script><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      
      
        &lt;!-- build time:Tue Jan 28 2020 15:40:41 GMT+0800 (GMT+08:00) --&gt;&lt;p&gt;之前在网上看到一个windows系统下的上帝模式，很好奇，尝试之后感觉不错，这里介绍一下创建的方法。除此之外附上一些类似的快捷操作。&lt;/p&gt;&lt;h
      
    
    </summary>
    
    
      <category term="操作和使用" scheme="https://gsy00517.github.io/categories/%E6%93%8D%E4%BD%9C%E5%92%8C%E4%BD%BF%E7%94%A8/"/>
    
    
      <category term="命令操作" scheme="https://gsy00517.github.io/tags/%E5%91%BD%E4%BB%A4%E6%93%8D%E4%BD%9C/"/>
    
      <category term="配置优化" scheme="https://gsy00517.github.io/tags/%E9%85%8D%E7%BD%AE%E4%BC%98%E5%8C%96/"/>
    
      <category term="windows" scheme="https://gsy00517.github.io/tags/windows/"/>
    
  </entry>
  
  <entry>
    <title>machine learning笔记：过拟合与欠拟合</title>
    <link href="https://gsy00517.github.io/machine-learning20191001104538/"/>
    <id>https://gsy00517.github.io/machine-learning20191001104538/</id>
    <published>2019-10-01T02:45:38.000Z</published>
    <updated>2020-01-26T04:02:26.035Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Tue Jan 28 2020 15:40:41 GMT+0800 (GMT+08:00) --><p>本文介绍在模型评估可能会出现的过拟合与欠拟合两种现象，并对解决方法做一个总结。</p><hr><h1 id="解释"><a href="#解释" class="headerlink" title="解释"></a>解释</h1><p>我们先通过图片来直观地解释这两种现象：<br><img src="/machine-learning20191001104538/欠拟合与过拟合.jpg" title="欠拟合与过拟合"><br>在上图中，右边是过拟合的情况，它指的是模型对于训练数据拟合过度，反映到评估指标上，就是模型在训练集上的表现很好，但在测试集和新数据上的表现较差。这是因为在这种条件下，模型过于复杂，导致把噪声数据的特征也学习到了模型中，导致模型的泛化能力下降，从而在后期的应用过程中很容易输出错误的预测结果。<br>左边是欠拟合的情况，它指的是在训练和预测时的表现都不好，这样的模型没有很好地捕捉到数据地特征，从而不能够很好地拟合数据。<br>相比而言，中间是拟合适当的情况，这种模型在应用中就具有很好的鲁棒性。</p><hr><h1 id="解决方法"><a href="#解决方法" class="headerlink" title="解决方法"></a>解决方法</h1><ol><li><h2 id="针对过拟合"><a href="#针对过拟合" class="headerlink" title="针对过拟合"></a>针对过拟合</h2><ol><li><h3 id="获取更多数据"><a href="#获取更多数据" class="headerlink" title="获取更多数据"></a>获取更多数据</h3>更多的样本可以让模型学到更多有效的特征，从而减小噪声的影响。<br>当然，一般情况下直接增加数据是很困难的，因此我们需要通过一定的规则来扩充训练数据。比如，在图像分类问题上，我们可以使用数据增强的方法，通过对图像的平移、旋转、缩放等方式来扩充数据；更进一步地，可以使用生成式对抗网络来合成大量新的训练数据。</li><li><h3 id="降低模型复杂度"><a href="#降低模型复杂度" class="headerlink" title="降低模型复杂度"></a>降低模型复杂度</h3>模型复杂度过高是数据量较小时过拟合的主要原因。适当降低模型的复杂度可以避免模型拟合过多的噪声。比如，在神经网络模型中减少网络层数、神经元个数等；在决策树模型中降低树的深度、进行剪枝等。<blockquote><p>注意：网络深度增加引起的准确率退化不一定是过拟合引起的，这是因为深度造成的梯度消失、梯度爆炸等问题，这在ResNet的论文中有讨论，详细可以看我的博文<a href="https://gsy00517.github.io/deep-learning20191001184216/" target="_blank">deep-learning笔记：使网络能够更深——ResNet简介与pytorch实现</a>。</p></blockquote></li><li><h3 id="正则化方法"><a href="#正则化方法" class="headerlink" title="正则化方法"></a>正则化方法</h3>这里的方法主要是权重正则化法，具体说明可以参考<a href="https://gsy00517.github.io/machine-learning20190915150339/" target="_blank">machine-learning笔记：机器学习中正则化的理解</a>。</li><li><h3 id="交叉验证"><a href="#交叉验证" class="headerlink" title="交叉验证"></a>交叉验证</h3>交叉验证包括简单交叉验证（数据丰富时）、S折交叉验证（最常用）和留一交叉验证（数据匮乏时）。</li><li><h3 id="集成学习"><a href="#集成学习" class="headerlink" title="集成学习"></a>集成学习</h3>即把多个模型集成在一起，从而降低单一模型的过拟合风险。主要有Bagging（bootstrap aggregating）和Boosting（adaptive boosting）这两种集成学习方法。</li></ol></li><li><h2 id="针对欠拟合"><a href="#针对欠拟合" class="headerlink" title="针对欠拟合"></a>针对欠拟合</h2>解决欠拟合问题也可以参照解决过拟合问题的思路；<ol><li><h3 id="添加新特征"><a href="#添加新特征" class="headerlink" title="添加新特征"></a>添加新特征</h3>当特征不足或者现有特征与样本标签的相关性不强时，模型容易出现欠拟合。<br>因此，通过挖掘“上下文特征”、“组合特征”等新的特征，往往能够取得更好的效果。<br>在深度学习中，也有很多模型可以帮助完成特征工程，比如因此分解机、梯度提升决策树、Deep-crossing等都可以成为丰富特征的方法。</li><li><h3 id="增加模型复杂度"><a href="#增加模型复杂度" class="headerlink" title="增加模型复杂度"></a>增加模型复杂度</h3>当模型过于简单时，增加模型复杂度可以使模型拥有更强的拟合能力。比如，在线性模型中添加高次项，在神经网络模型中增加网络层数、神经元个数等。<br>对于模型的选择，我在文末补充了两种模型选择的准则供参考。</li><li><h3 id="减小正则化系数"><a href="#减小正则化系数" class="headerlink" title="减小正则化系数"></a>减小正则化系数</h3>正则化是用来防止过拟合的，但当模型出现欠拟合现象时，我们就应该有针对性地减小正则化系数。</li></ol></li></ol><hr><h1 id="模型选择准则"><a href="#模型选择准则" class="headerlink" title="模型选择准则"></a>模型选择准则</h1><p>模型选择的信息准则有很多，我这里介绍我知道的两个比较常用的模型选择准则：</p><ol><li><h2 id="AIC准则"><a href="#AIC准则" class="headerlink" title="AIC准则"></a>AIC准则</h2>赤池信息准则（Akaike Information Criterion，AIC）公式定义如下：<script type="math/tex;mode=display">AIC=2k-2ln(L)\</script>其中k表示模型参数个数（复杂度），L表示经验误差（似然函数）。<br>当需要从一组可供选择的模型中选择最佳模型时，通常选择AIC最小的模型。</li><li><h2 id="BIC准则"><a href="#BIC准则" class="headerlink" title="BIC准则"></a>BIC准则</h2>贝叶斯信息准则（Bayesian Information Criterion，BIC）是对AIC准则的改进，定义如下：<script type="math/tex;mode=display">BIC=kln(n)-2ln(L)\</script>与AIC不同，这里k的系数不再是常数。其中n代表的是样本量（数据量），这样，BIC准则就与样本量相关了。当样本量足够时，过拟合的风险变小，我们就可以允许模型复杂一些。<br>这里再次附上这张直观的图片，方便理解与体会。简析可参考<a href="https://gsy00517.github.io/machine-learning20190915150339/" target="_blank">machine-learning笔记：机器学习中正则化的理解</a>。<img src="/machine-learning20191001104538/复杂度与数据量对性能的影响.jpg" title="复杂度与数据量对性能的影响"></li></ol><script type="text/javascript" src="/js/src/bai.js"></script><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      
      
        &lt;!-- build time:Tue Jan 28 2020 15:40:41 GMT+0800 (GMT+08:00) --&gt;&lt;p&gt;本文介绍在模型评估可能会出现的过拟合与欠拟合两种现象，并对解决方法做一个总结。&lt;/p&gt;&lt;hr&gt;&lt;h1 id=&quot;解释&quot;&gt;&lt;a href=&quot;#解释&quot;
      
    
    </summary>
    
    
      <category term="人工智能" scheme="https://gsy00517.github.io/categories/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/"/>
    
    
      <category term="机器学习" scheme="https://gsy00517.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>artificial intelligence笔记：人工智能前沿发展情况分享</title>
    <link href="https://gsy00517.github.io/artificial-intelligence20191001101334/"/>
    <id>https://gsy00517.github.io/artificial-intelligence20191001101334/</id>
    <published>2019-10-01T02:13:34.000Z</published>
    <updated>2019-11-02T02:21:00.480Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Tue Jan 28 2020 15:40:41 GMT+0800 (GMT+08:00) --><p>这是我在一个相关的群里看到的一个论文，这篇论文比较新，看完之后觉得对目前AI发展状况的了解有一定价值，就放了上来。</p><hr><h1 id="论文"><a href="#论文" class="headerlink" title="论文"></a>论文</h1><p>这里直接提供图片形式的原文：<br><img src="/artificial-intelligence20191001101334/论文全文.JPG" title="论文全文"></p><hr><h1 id="其他"><a href="#其他" class="headerlink" title="其他"></a>其他</h1><p>难得有一篇以AI开篇的文章，由于在我不到一年前真正接触AI相关知识时，一直疑惑人工智能、机器学习与深度学习之间的关系。直到看了台大教授李宏毅的课才知道三者之间的包含关系，这里就把课件中的一张图片放上来，一目了然：<br><img src="/artificial-intelligence20191001101334/AI、ML、DL关系.jpg" title="AI、ML、DL关系"><br>最后，再补张和<a href="https://gsy00517.github.io/deep-learning20190914142553/" target="_blank">deep-learning笔记：一篇非常经典的论文——NatureDeepReview</a>文末对应的一张我觉得挺真实的图哈哈。<br><img src="/artificial-intelligence20191001101334/什么是机器学习.jpg" title="什么是机器学习"><br>不得不说，目前丰富的库和各种深度学习框架的确极大地方便了AI的学习与研究，许多轮子都已造好。学会运用这些工具还是很有帮助的！</p><script type="text/javascript" src="/js/src/bai.js"></script><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      
      
        &lt;!-- build time:Tue Jan 28 2020 15:40:41 GMT+0800 (GMT+08:00) --&gt;&lt;p&gt;这是我在一个相关的群里看到的一个论文，这篇论文比较新，看完之后觉得对目前AI发展状况的了解有一定价值，就放了上来。&lt;/p&gt;&lt;hr&gt;&lt;h1 id=
      
    
    </summary>
    
    
      <category term="人工智能" scheme="https://gsy00517.github.io/categories/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/"/>
    
    
      <category term="机器学习" scheme="https://gsy00517.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="深度学习" scheme="https://gsy00517.github.io/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="论文分享" scheme="https://gsy00517.github.io/tags/%E8%AE%BA%E6%96%87%E5%88%86%E4%BA%AB/"/>
    
  </entry>
  
  <entry>
    <title>machine learning笔记：一个支持向量机的问题</title>
    <link href="https://gsy00517.github.io/machine-learning20191001093428/"/>
    <id>https://gsy00517.github.io/machine-learning20191001093428/</id>
    <published>2019-10-01T01:34:28.000Z</published>
    <updated>2019-11-02T02:22:49.692Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Tue Jan 28 2020 15:40:41 GMT+0800 (GMT+08:00) --><p>在学习机器学习理论的过程中，支持向量机（SVM）应该是我们会遇到的第一个对数学要求比较高的概念。理解它的原理要花费了我不少时间，写这篇博文是因为我之前看到的一个有关SVM的问题，其解答需用到SVM的相关数学原理，可以促使我思考。支持向量机的具体原理以及推导网上有大量资源，我也会在文中提供一些供参考。</p><hr><h1 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h1><p>支持向量机是一种有监督的学习方法，主要思想是建立一个最优决策超平面，使得该平面两侧距离该平面最近的两类样本之间的距离最大化，从而对分类问题提供良好的泛化能力。<br>这里有个小故事，也是我第一次看SVM课程时老师提到的，可以通过这个小故事大致理解一下SVM在做什么。<br><img src="/machine-learning20191001093428/小故事.JPG" title="小故事"><br>它的优点主要有如下四点：<br>1.相对于其他训练分类算法，SVM不需要过多的样本。<br>2.SVM引入了核函数，可以处理高维的样本。<br>3.结构风险最小。也就是说，分类器对问题真实模型的逼近与真实解之间的累计误差最小。<br>4.由于SVM的非线性，它擅长应付线性不可分的问题。这主要是用松弛变量（惩罚变量）和核函数来实现的。<br>这里我附上我所知的三个SVM的常用软件工具包：<a href="http://svmlight.joachims.org/" target="_blank">SVMLight</a>、<a href="https://www.csie.ntu.edu.tw/~cjlin/libsvm/" target="_blank">LibSVM</a>、<a href="https://www.csie.ntu.edu.tw/~cjlin/liblinear/" target="_blank">Liblinear</a>。</p><hr><h1 id="问题"><a href="#问题" class="headerlink" title="问题"></a>问题</h1><p>下面就是我在文章开头提到的问题，直接搬运：<br><img src="/machine-learning20191001093428/问题.JPG" title="问题"><br>解析中提到的拉格朗日乘子法和KKT条件，也是我在看到这个问题后才尝试去理解的。能力有限，不能自己很好的解释，这里附上<a href="KKT.pdf" target="_blank">瑞典皇家理工学院（KTH）“统计学习基础”课程的KKT课件</a>，个人觉得讲的很直观且详细了。</p><script type="text/javascript" src="/js/src/bai.js"></script><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      
      
        &lt;!-- build time:Tue Jan 28 2020 15:40:41 GMT+0800 (GMT+08:00) --&gt;&lt;p&gt;在学习机器学习理论的过程中，支持向量机（SVM）应该是我们会遇到的第一个对数学要求比较高的概念。理解它的原理要花费了我不少时间，写这篇博文是因为
      
    
    </summary>
    
    
      <category term="人工智能" scheme="https://gsy00517.github.io/categories/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/"/>
    
    
      <category term="机器学习" scheme="https://gsy00517.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>database笔记：范式的理解</title>
    <link href="https://gsy00517.github.io/database20190921195840/"/>
    <id>https://gsy00517.github.io/database20190921195840/</id>
    <published>2019-09-21T11:58:40.000Z</published>
    <updated>2020-01-19T00:23:28.718Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Tue Jan 28 2020 15:40:41 GMT+0800 (GMT+08:00) --><p>今天终于完成了计算机三级数据库的考试，这也是本学期的第一门考试。听说计算机三级中要属计算机网络最简单，然而出于学到更多有用的知识的目的，我报了数据库。然而事实证明也没学到多少，毕竟这个计算机等级考试是给非计算机专业的人设置的，现在只求能过。不过两三天书看下来，还是有些收获，现在考完了有时间就在这里记一下，方便自己和别人今后有需要看。</p><p><strong>References</strong>：</p><p>电子文献：<br><a href="https://blog.csdn.net/he626shidizai/article/details/90707037" target="_blank" rel="noopener">https://blog.csdn.net/he626shidizai/article/details/90707037</a><br><a href="https://blog.csdn.net/u013011841/article/details/39023859" target="_blank" rel="noopener">https://blog.csdn.net/u013011841/article/details/39023859</a></p><hr><h1 id="范式"><a href="#范式" class="headerlink" title="范式"></a>范式</h1><blockquote><p>注意：本文中的范式指的是数据库范式。</p></blockquote><p>在设计数据库时，为了设计一个良好的逻辑关系，必须要使关系受一定条件的约束，这种约束逐渐成为一种规范，就是我们所说的范式。<br>目前关系数据库有六种范式：第一范式（1NF）、第二范式（2NF）、第三范式（3NF）、巴斯-科德范式（BCNF）、第四范式(4NF）和第五范式（5NF）。要求最低的是1NF，往后依次变得严格。其中最后的5NF又称完美范式。<br>数据库一般只需满足3NF，下面我就介绍一下前三种范式。</p><h1 id="第一范式"><a href="#第一范式" class="headerlink" title="第一范式"></a>第一范式</h1><p>数据库考试官方教程并没有对每个范式的定义进行讲解，另外因为文字定义比较晦涩难懂，我这里通过多方参考，用图片的形式来展示各个约束条件。<br>首先，1NF是所有关系型数据库最基本的要求，它的定义为：符合1NF的关系中的每个属性都不可再分。下图就是一个违反1NF的例子：<br><img src="/database20190921195840/不符1NF-1.png" title="不符1NF-1"><br>修改如下：<br><img src="/database20190921195840/符合1NF-1.png" title="符合1NF-1"><br>上面的情况就符合1NF了。<br>我们还可以把第一范式分成两点来理解：</p><ol><li><h2 id="每个字段都只能存放单一值"><a href="#每个字段都只能存放单一值" class="headerlink" title="每个字段都只能存放单一值"></a>每个字段都只能存放单一值</h2>还是上反例：<img src="/database20190921195840/不符1NF-2.png" title="不符1NF-2"> 上图中，第一行的课程有两个值，这就不符合第一范式了。因此要修改成这样：<img src="/database20190921195840/符合1NF-2.png" title="符合1NF-2"></li><li><h2 id="每笔记录都要能用一个唯一的主键识别"><a href="#每笔记录都要能用一个唯一的主键识别" class="headerlink" title="每笔记录都要能用一个唯一的主键识别"></a>每笔记录都要能用一个唯一的主键识别</h2><img src="/database20190921195840/不符1NF-3.png" title="不符1NF-3"> 这里出现了重复组，同样也不满足1NF，因为缺乏唯一的标识码。因此修改如下：<img src="/database20190921195840/符合1NF-3.png" title="符合1NF-3"></li></ol><h1 id="第二范式"><a href="#第二范式" class="headerlink" title="第二范式"></a>第二范式</h1><p>第二范式是建立在第一范式的基础上的，它的改进在于：消除了非主属性对于码的部分函数依赖。<br>第二范式消除了非主属性对于码的部分函数依赖，也就是说，第二范式中所有非主属性完全依赖于主键，即不能依赖于主键的一部分属性。<br>为了解释明白，还是通过实例的说明：<br><img src="/database20190921195840/不符2NF.png" title="不符2NF"><br>上表中，学号和课程号组合在一起是主键，但是姓名只由学号决定，这就违反了第二范式。同样的，课程名只由课程号决定，这也违反了第二范式。此外，只需要知道学号和课程号就能知道成绩。<br>为了满足第二范式，我们就需要对上表做如下拆分：<br><img src="/database20190921195840/符合2NF.png" title="符合2NF"></p><h1 id="第三范式"><a href="#第三范式" class="headerlink" title="第三范式"></a>第三范式</h1><p>同样的，第三范式建立在第二范式的基础上。不同之处在于，在第二范式的基础之上，第三范式中非主属性都不传递依赖于主键。<br>这是什么意思？还是看图说话：<br><img src="/database20190921195840/不符3NF.png" title="不符3NF"><br>上表中，主键是学号，且已满足第二范式。然而，学校的地址也可以根据学校名称来确定，第三范式就是在这里再做一个分解：<br><img src="/database20190921195840/符合3NF.png" title="符合3NF"></p><script type="text/javascript" src="/js/src/bai.js"></script><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      
      
        &lt;!-- build time:Tue Jan 28 2020 15:40:41 GMT+0800 (GMT+08:00) --&gt;&lt;p&gt;今天终于完成了计算机三级数据库的考试，这也是本学期的第一门考试。听说计算机三级中要属计算机网络最简单，然而出于学到更多有用的知识的目的，我报了数
      
    
    </summary>
    
    
      <category term="知识点与小技巧" scheme="https://gsy00517.github.io/categories/%E7%9F%A5%E8%AF%86%E7%82%B9%E4%B8%8E%E5%B0%8F%E6%8A%80%E5%B7%A7/"/>
    
    
      <category term="数据库" scheme="https://gsy00517.github.io/tags/%E6%95%B0%E6%8D%AE%E5%BA%93/"/>
    
  </entry>
  
  <entry>
    <title>hexo笔记：在linux（ubuntu）下安装使用</title>
    <link href="https://gsy00517.github.io/hexo20190917085649/"/>
    <id>https://gsy00517.github.io/hexo20190917085649/</id>
    <published>2019-09-17T00:56:49.000Z</published>
    <updated>2020-01-27T07:52:51.611Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Tue Jan 28 2020 15:40:41 GMT+0800 (GMT+08:00) --><p>之前一直在win10下使用hexo搭建部署博客，方法参见：<a href="https://gsy00517.github.io/hexo20190913153310/" target="_blank">hexo笔记：开始创建个人博客——方法及原因</a>。那么，如果想在linux环境下使用hexo，该如何操作呢？</p><p><strong>References</strong>：</p><p>电子文献：<br><a href="https://blog.csdn.net/y5492853/article/details/79529410" target="_blank" rel="noopener">https://blog.csdn.net/y5492853/article/details/79529410</a></p><hr><h1 id="原因"><a href="#原因" class="headerlink" title="原因"></a>原因</h1><p>由于在更改主题配置文件_config.yml时，长达八百多行的配置文件总是让我找得头晕目眩。由于VScode（好像）没有提供字符的快速查找匹配功能，我之前一直采用一种笨拙的办法，即在文件的空处输入想要查找的字符然后左键选中，这个时候文件中同样的字符也会被选中，这样快速拉动滚动条时就可以比较明显地发现想找的目标字符了。<br>然而对于这种办法，我觉得主要有两大问题：</p><ol><li><h2 id="忘记删除在空白处添加的文字"><a href="#忘记删除在空白处添加的文字" class="headerlink" title="忘记删除在空白处添加的文字"></a>忘记删除在空白处添加的文字</h2>我就犯过这样低级的错误，找到并更改之后没有删除自己添加的字符就直接快乐地ctrl+S了，于是就造成了网站能打开但是一片空白的bug。所以大家没事还是不要随意在主题配置文件中添加文字。</li><li><h2 id="不是长久之计"><a href="#不是长久之计" class="headerlink" title="不是长久之计"></a>不是长久之计</h2>虽然这个八百多行的文件已经让我够呛了，然后或许今后还会遇到更长的文件，那么这种方法就会变得极其低效（而且伤眼睛）。<br>基于这些因素，我脑子里的第一个反映就是vim编辑器中的对文件字符的查找定位的功能（关于vim的使用，等我多多尝试并熟练之后再做小结）。<br>好了，接下来就开始操作吧。</li></ol><hr><h1 id="更正"><a href="#更正" class="headerlink" title="更正"></a>更正</h1><p>最近突然发现VScode自带了搜索功能，可以直接在整个文件夹中搜索关键词。这里所给的快捷键是ctrl+shift+F，但win10用户可能会发现按了之后没有任何反应。事实上，反应还是有的，当你再次打字时，就会发现简体变成了繁体，再次按ctrl+shift+F即可恢复。<br><img src="/hexo20190917085649/搜索.png" title="搜索"><br>直接点击搜索图标即可便捷地进行搜索，为我之前眼瞎没有发现表示无奈，但下面还是写一下怎么安装。</p><hr><h1 id="安装"><a href="#安装" class="headerlink" title="安装"></a>安装</h1><p>首先安装node.js。这里就没windows下直接双击exe安装包那么easy啦，打开终端，老老实实输命令：<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">sudo apt-get install nodejs</span><br><span class="line">sudo apt install nodejs-legacy</span><br><span class="line">sudo apt install npm</span><br></pre></td></tr></table></figure><p></p><p>其实熟练之后觉得<code>apt</code>是真的好用。<br>由于ubuntu源中的node.js是旧版本，下面会出现问题，我在后文解释。<br>由于npm服务器在国外可能会影响下载速度，和windows下的步骤一样，我们换成淘宝镜像：<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo npm config set registry https://registry.npm.taobao.org</span><br></pre></td></tr></table></figure><p></p><p>这时候如果我们直接安装hexo，会出现如下错误：<br><img src="/hexo20190917085649/版本过低.png" title="版本过低"><br>因此，我们安装node升级工具n：<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo npm install n -g</span><br></pre></td></tr></table></figure><p></p><p>并且使用<code>sudo n stable</code>升级版本，若看到如下输出，说明升级成功：<br><img src="/hexo20190917085649/升级.png" title="升级"></p><blockquote><p>注意：fetch可能需要花费一点时间，这时候终端不会有任何输出，不要以为出错了，耐心等待即可，不要ctrl+C中止。</p></blockquote><p>最后，我们安装hexo：<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo npm install -g hexo</span><br></pre></td></tr></table></figure><p></p><blockquote><p>注：<code>-g</code>表示安装到全局环境。</p></blockquote><p>接下来的初始化操作跟windows下基本一样，可以参照我之前的博文。我继续对原来的博客进行编辑，所以无需初始化一个新的，直接把windows的对应文件夹整个copy过来就行了。</p><script type="text/javascript" src="/js/src/bai.js"></script><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      
      
        &lt;!-- build time:Tue Jan 28 2020 15:40:41 GMT+0800 (GMT+08:00) --&gt;&lt;p&gt;之前一直在win10下使用hexo搭建部署博客，方法参见：&lt;a href=&quot;https://gsy00517.github.io/hexo201
      
    
    </summary>
    
    
      <category term="操作和使用" scheme="https://gsy00517.github.io/categories/%E6%93%8D%E4%BD%9C%E5%92%8C%E4%BD%BF%E7%94%A8/"/>
    
    
      <category term="hexo" scheme="https://gsy00517.github.io/tags/hexo/"/>
    
      <category term="ubuntu" scheme="https://gsy00517.github.io/tags/ubuntu/"/>
    
      <category term="安装教程" scheme="https://gsy00517.github.io/tags/%E5%AE%89%E8%A3%85%E6%95%99%E7%A8%8B/"/>
    
  </entry>
  
  <entry>
    <title>machine learning笔记：机器学习中正则化的理解</title>
    <link href="https://gsy00517.github.io/machine-learning20190915150339/"/>
    <id>https://gsy00517.github.io/machine-learning20190915150339/</id>
    <published>2019-09-15T07:03:39.000Z</published>
    <updated>2020-01-26T04:13:01.839Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Tue Jan 28 2020 15:40:41 GMT+0800 (GMT+08:00) --><p>在接触了一些ml的知识后，大家一定会对正则化这个词不陌生，但是我感觉根据这个词的字面意思不能够直接地理解它的概念。因此我打算写一篇文章做个记录，方便以后回忆。</p><p><strong>References</strong>：</p><p>参考文献：<br>[1]统计学习方法（第2版）</p><hr><h1 id="线性代数中的正则化"><a href="#线性代数中的正则化" class="headerlink" title="线性代数中的正则化"></a>线性代数中的正则化</h1><p>如果直接搜索正则化这个名词，首先得到的一般是代数几何中的一个概念。<br>百度词条对它的解释是：给平面不可约代数曲线以某种形式的全纯参数表示。<br>怎么样？是不是觉得一头雾水。<br>这里我推荐使用谷歌或者维基百科来查询这些专业名词。</p><blockquote><p>对于不能科学上网的朋友，没关系，我这里提供了<a href="http://ac.scmor.com/" target="_blank">谷歌镜像</a>和<a href="https://www.wikiwand.com/" target="_blank">wikiwand</a>，大家可以在上面得到一样的搜索结果。</p></blockquote><p>我们直接到维基百科搜索regularization：<br>里面第一段是这样解释的：In mathematics, statistics, and computer science, particularly in machine learning and inverse problems, regularization is the process of adding information in order to solve an ill-posed problem or to prevent overfitting.<br>这就和我们在机器学习应用中的目的比较相近了。</p><hr><h1 id="机器学习中的正则化"><a href="#机器学习中的正则化" class="headerlink" title="机器学习中的正则化"></a>机器学习中的正则化</h1><p>在机器学习中，正则化是一种为了减小测试误差的行为（有时候会增加训练误差）。<br>我们在构造机器学习模型时，最终目的是让模型在面对新数据的时候，可以有很好的表现。当你用比较复杂的模型比如神经网络去拟合数据时，很容易出现过拟合现象（训练集表现很好，测试集表现较差），这会导致模型的泛化能力下降，这时候，我们就需要使用正则化，来降低模型的复杂度。<br>为了加深印象，我下面简单介绍几种常用的机器学习正则化方法：</p><ol><li><h2 id="早停法（Early-Stopping）"><a href="#早停法（Early-Stopping）" class="headerlink" title="早停法（Early Stopping）"></a>早停法（Early Stopping）</h2>早停法，就是当训练集的误差变好，但是验证集的误差变坏（即泛化效果变差）的时候停止训练。这种方法可以一定程度上有效地防止过拟合，同时这也说明了验证集在机器学习中的重要性。<img src="/machine-learning20190915150339/早停法.png" title="早停法"></li><li><h2 id="权重正则化法"><a href="#权重正则化法" class="headerlink" title="权重正则化法"></a>权重正则化法</h2><p>因为噪声相比于正常信号而言，通常会在某些点出现较大的峰值。所以，只要我们保证权重系数在绝对值意义上足够小，就能够保证噪声不会被过度响应，这也是奥卡姆剃刀原理的表现，即模型不应过度复杂，尤其是当数据量不大的时候。</p><img src="/machine-learning20190915150339/复杂度与数据量对性能的影响.jpg" title="复杂度与数据量对性能的影响"><p>上面是在一个网课上看到的、我觉得可以较好地呈现模型的复杂度与数据量对模型预测表现的影响的一张图片，其中向左的横轴表示数据量大小，向右的横轴表示模型复杂度，竖轴是预测表现。通过这张图，可以很明显地观察到：模型的复杂度提升需要大量的数据作为依托。<br>权重正则化主要有两种：</p><ul><li>L1正则：$ J=J_{0}+\lambda \left | w \right |_{1} $，其中J代表损失函数（也称代价函数），$ \left | w \right |_{1} $代表参数向量w的L1范数。</li><li>L2正则（weight decay）：$ J=J_{0}+\lambda \left | w \right |_{2} $，其中$ \left | w \right |_{2} $代表参数向量w的L2范数。<img src="/machine-learning20190915150339/权重正则化.png" title="权重正则化"> 这里就产生了<strong>Lasso回归</strong>与<strong>岭回归</strong>两大机器学习经典算法。其中Lasso回归是一种压缩估计，可以通过构造惩罚函数得到一个较为精炼的模型，使得它可以压缩一些系数，同时设定一些系数为0，从而达到特征选择的目的。基于Lasso回归这种可以选择特征并降维的特性，它主要有这些适用情况：</li></ul><ol><li>样本量比较小，但指标量非常多的时候（易于过拟合）。</li><li>进行高维统计时。</li><li>需要对特征进行选择时。<br>对于这些回归的详细解释，大家可以到网上搜集相关信息。<blockquote><p>补充：<br>L0范数：向量中非零元素的个数。<br>L1范数：向量中每个元素绝对值的和。<br>L2范数：向量元素绝对值的平方和再开方。</p></blockquote></li></ol><p>下面我再附上一组图，希望能帮助更好地理解权重正则化：<br>首先我们可视化一个损失函数。</p><img src="/machine-learning20190915150339/损失函数.png" title="损失函数"><p>下面我们看一看正则化项的图示，这里使用L1范数作为正则化项。</p><img src="/machine-learning20190915150339/正则化项.png" title="正则化项"><p>接着，我们将上面两者线性组合：</p><img src="/machine-learning20190915150339/叠加.png" title="叠加"><p>我们来看看结果：</p><img src="/machine-learning20190915150339/结果.png" title="结果"><p>可见，正则化项的引入排除了大量原本属于最优解的点，上图的情况中剩下一个唯一的局部最优解。<br>正则化项的引入，除了符合奥卡姆剃刀原理之外。同时从贝叶斯估计的角度看，正则化项对应于模型的先验概率，即相当于假设复杂的模型具有较小的先验概率，而简单的模型具有较大的先验概率。</p></li><li><h2 id="数据增强"><a href="#数据增强" class="headerlink" title="数据增强"></a>数据增强</h2>数据增强可以丰富图像数据集，有效防止过拟合。这种方法在AlexNet中有很好的应用，大家可以看看我的博文<a href="https://gsy00517.github.io/deep-learning20190915113859/" target="_blank">deep-learning笔记：开启深度学习热潮——AlexNet</a>。<img src="/machine-learning20190915150339/数据增强.jpg" title="数据增强"></li><li><h2 id="随机失活（dropout）"><a href="#随机失活（dropout）" class="headerlink" title="随机失活（dropout）"></a>随机失活（dropout）</h2>dropout即随机砍掉一部分神经元之间的连接，每次只更新一部分，这可以有效地增加它的鲁棒性，提高泛化能力。这个方法在AlexNet中也有详细的解释，推荐大家去看一下。<img src="/machine-learning20190915150339/dropout神经单元.png" title="dropout神经单元"> <img src="/machine-learning20190915150339/dropout神经网络.png" title="dropout神经网络"> 以上就是比较常规且流行的正则化方式，今后或许会有补充，也欢迎大家提供意见~</li></ol><hr><h1 id="奥卡姆剃刀原理"><a href="#奥卡姆剃刀原理" class="headerlink" title="奥卡姆剃刀原理"></a>奥卡姆剃刀原理</h1><p>上文在正则化一节提到了奥卡姆剃刀原理，这里就简单做个说明。<br>奥卡姆剃刀原理应用于模型选择时可以简单表述为如下思想：在所有可能选择的模型中，能够很好地解释已知数据并且十分简单的模型才是最好的模型，也就是我们应该选择的模型。</p><script type="text/javascript" src="/js/src/bai.js"></script><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      
      
        &lt;!-- build time:Tue Jan 28 2020 15:40:41 GMT+0800 (GMT+08:00) --&gt;&lt;p&gt;在接触了一些ml的知识后，大家一定会对正则化这个词不陌生，但是我感觉根据这个词的字面意思不能够直接地理解它的概念。因此我打算写一篇文章做个记录，
      
    
    </summary>
    
    
      <category term="人工智能" scheme="https://gsy00517.github.io/categories/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/"/>
    
    
      <category term="机器学习" scheme="https://gsy00517.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>deep learning笔记：开启深度学习热潮——AlexNet</title>
    <link href="https://gsy00517.github.io/deep-learning20190915113859/"/>
    <id>https://gsy00517.github.io/deep-learning20190915113859/</id>
    <published>2019-09-15T03:38:59.000Z</published>
    <updated>2020-01-19T02:40:34.663Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Tue Jan 28 2020 15:40:41 GMT+0800 (GMT+08:00) --><p>继之前那篇<a href="https://gsy00517.github.io/deep-learning20190915073809/" target="_blank">deep-learning笔记：着眼于深度——VGG简介与pytorch实现</a>，我觉得还是有必要提一下VGG的前辈——具有历史意义的AlexNet，于是就写了这篇文章简要介绍一下。</p><p><strong>References</strong>：</p><p>电子文献：<br><a href="https://blog.csdn.net/zym19941119/article/details/78982441" target="_blank" rel="noopener">https://blog.csdn.net/zym19941119/article/details/78982441</a></p><p>参考文献：<br>[1]ImageNet Classiﬁcation with Deep Convolutional Neural Networks</p><hr><h1 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h1><img src="/deep-learning20190915113859/AlexNet.jpg" title="AlexNet"><p>ALexNet是第一个运用大型深度卷积神经网络的模型，在ILSVRC中一下子比前一年把错误率降低了10%，这是非常惊人的，也很快引起了注意。于是，自2012年开始，深度学习热潮由此引发。<br><img src="/deep-learning20190915113859/AlexNet突破.jpg" title="AlexNet突破"><br>根据我之前听网课的笔记以及网上的其他文章，我把AlexNet主要的进步归纳如下：</p><ol><li>使用大型深度卷积神经网络。</li><li>分组卷积（groupconvolution）来充分利用GPU。</li><li>随机失活dropout：一种有效的正则化方法。关于正则化，可以看我的博文<a href="https://gsy00517.github.io/machine-learning20190915150339/" target="_blank">machine-learning笔记：机器学习中正则化的理解</a>。</li><li>数据增强data augumentation：增大数据集以减小过拟合问题。</li><li>relu激活函数：即max（0，x），至今还被广泛应用。</li></ol><hr><h1 id="个人思考"><a href="#个人思考" class="headerlink" title="个人思考"></a>个人思考</h1><p>这段时间也看了不少东西，对于如何提升神经网络的性能这个问题，我觉得主要有如下三个方面：</p><ol><li><h2 id="从网络本身入手"><a href="#从网络本身入手" class="headerlink" title="从网络本身入手"></a>从网络本身入手</h2><ol><li>增加深度。</li><li>增加宽度。</li><li>减少参数量。</li><li>防止过拟合。</li><li>解决梯度消失的问题。</li></ol></li><li><h2 id="从数据集入手"><a href="#从数据集入手" class="headerlink" title="从数据集入手"></a>从数据集入手</h2><ol><li>尽可能使用多的数据。</li></ol></li><li><h2 id="从硬件入手"><a href="#从硬件入手" class="headerlink" title="从硬件入手"></a>从硬件入手</h2><ol><li>提升GPU性能。</li><li>充分利用现有的GPU性能。<br>当你阅读完AlexNet的论文，你会发现它在这几个方面都有思考且做出了非常优秀的改进。</li></ol></li></ol><hr><h1 id="论文"><a href="#论文" class="headerlink" title="论文"></a>论文</h1><p>在放论文之前，我还是先贴一张流程图，方便在阅读论文的时候进行对照与理解。<br><img src="/deep-learning20190915113859/AlexNet流程图.png" title="AlexNet流程图"><br>下面奉上宝贵的论文：<br><a href="AlexNet.pdf" target="_blank">论文原版</a><br><a href="AlexNet中文.pdf" target="_blank">论文中文版</a><br>从introduction第一句开始，作者就开始了一段长长的吐槽：<br>Current approaches to object recognition make essential use of machine learning methods…<br>吐槽Yann LeCun大佬的论文被顶会拒收了仅仅因为Yann LeCun使用了神经网络。其实，那段时间之前，由于SVM等机器学习方法的兴起，神经网络是一种被许多ml大佬们看不起的算法模型。<br>在introduction的最后，作者留下了这样一句经典的话：<br>All of our experiments suggest that our results can be improved simply by waiting for faster GPUs and bigger datasets to become available.<br>有没有感觉到一种新世界大门被打开的感觉呢？<br>有关论文别的内容，我暂不多说了，大家可以自己看论文学习与体会。<br>附上推荐重点阅读的章节：3.1 ReLU Nonlinearity；3.5 Overall Architecture；4 Reducing Overfitting。</p><hr><h1 id="说明"><a href="#说明" class="headerlink" title="说明"></a>说明</h1><p>同我写VGG的那篇文章中一样，我在英文原版中用黄颜色高亮了我觉得重要的内容给自己和大家今后参考。<br>另外，我在这里推荐大家还是先尝试阅读英文原版。一方面由于一些公式、符号以及名词的原因，英文原版叙述更精准，中文翻译有缺漏、偏颇之处；另一方面更重要的，接触这些方面的知识仅参考中文是远远不够的。<br>在这里我推荐一个chrome英文pdf阅读插件，大家可以自己到chrome里面搜索安装：<br><img src="/deep-learning20190915113859/搜索插件.png" title="搜索插件"><br><img src="/deep-learning20190915113859/添加完成.png" title="添加完成"><br>有了这个插件，遇到不认识的单词，只需双击单词，就可以看到中文释义，一定程度上可以保证阅读的流畅性。但是如果想从根本上解决问题，只有好好背单词吧（我也在朝这个方向努力…）。<br>另外，iPad的上也有好多强大的app，在这里不一一推荐了。</p><blockquote><p>补充：最近又发现一款特别好用且美观的查词插件，功能非常强大，推荐一下：<a href="https://saladict.crimx.com/" target="_blank">沙拉查词</a>。</p></blockquote><script type="text/javascript" src="/js/src/bai.js"></script><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      
      
        &lt;!-- build time:Tue Jan 28 2020 15:40:41 GMT+0800 (GMT+08:00) --&gt;&lt;p&gt;继之前那篇&lt;a href=&quot;https://gsy00517.github.io/deep-learning20190915073809/&quot; t
      
    
    </summary>
    
    
      <category term="人工智能" scheme="https://gsy00517.github.io/categories/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/"/>
    
    
      <category term="深度学习" scheme="https://gsy00517.github.io/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="论文分享" scheme="https://gsy00517.github.io/tags/%E8%AE%BA%E6%96%87%E5%88%86%E4%BA%AB/"/>
    
  </entry>
  
  <entry>
    <title>markdown笔记：公式插入和代码高亮</title>
    <link href="https://gsy00517.github.io/markdown20190915095628/"/>
    <id>https://gsy00517.github.io/markdown20190915095628/</id>
    <published>2019-09-15T01:56:28.000Z</published>
    <updated>2020-01-20T06:40:33.806Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Tue Jan 28 2020 15:40:41 GMT+0800 (GMT+08:00) --><p>在上一篇文章<a href="https://gsy00517.github.io/deep-learning20190915073809/" target="_blank">deep-learning笔记：着眼于深度——VGG简介与pytorch实现</a>中，我用到了markdown其他的一些使用方法，因此我想在此对之前的一篇文章<a href="https://gsy00517.github.io/markdown20190913211144/" target="_blank">markdown笔记：markdown的基本使用</a>做一些补充。</p><p><strong>References</strong>：</p><p>电子文献：<br><a href="https://www.jianshu.com/p/25f0139637b7" target="_blank" rel="noopener">https://www.jianshu.com/p/25f0139637b7</a><br><a href="https://www.jianshu.com/p/fd97e1f8f699" target="_blank" rel="noopener">https://www.jianshu.com/p/fd97e1f8f699</a><br><a href="https://www.jianshu.com/p/68e6f82d88b7" target="_blank" rel="noopener">https://www.jianshu.com/p/68e6f82d88b7</a><br><a href="https://www.jianshu.com/p/7c02c112d532" target="_blank" rel="noopener">https://www.jianshu.com/p/7c02c112d532</a></p><hr><h1 id="公式插入"><a href="#公式插入" class="headerlink" title="公式插入"></a>公式插入</h1><p>无论是学习ml还是dl，我们总是离不开数学的，于是利用markdown插入数学公式就成了一个的需求。那么怎么在markdown中插入公式呢？<br>markdown中的公式分为两类，即行内公式与行间公式。它们对应的代码如下：<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ \Gamma(z) = \int_0^\infty t^&#123;z-1&#125;e^&#123;-t&#125;dt\,. $</span><br><span class="line">$$\Gamma(z) = \int_0^\infty t^&#123;z-1&#125;e^&#123;-t&#125;dt\,.$$</span><br></pre></td></tr></table></figure><p></p><p>让我们来看一下效果：<br>行内公式：$ \Gamma(z) = \int_0^\infty t^{z-1}e^{-t}dt\,. $<br>行间公式：</p><script type="math/tex;mode=display">\Gamma(z) = \int_0^\infty t^{z-1}e^{-t}dt\,.</script><blockquote><p>注意：使用单个<code>$</code>时，需要在公式两边与<code>$</code>之间空一格，我之前没空就一直转换不成公式的形式。</p></blockquote><p>如果你是使用hexo编写博客，那么默认的设置是无法转义markdown公式的，解决这个问题的配置方法可以参考本文顶部给出的第三个链接。<br>另外要注意，在使用公式时，对应文件需开启mathjax选项。<br><img src="/markdown20190915095628/开启mathjax.png" title="开启mathjax"><br>补充更新：在查看next主题配置文件时，我注意到next好像自带mathjax支持，设置如下，这样就无需在每个文件中添加开启mathjax的选项。<br><img src="/markdown20190915095628/mathjax支持.png" title="mathjax支持"><br>markdown公式的具体语法可以参照本文的第一个链接，你可以在typora中根据它的<a href="http://support.typora.io/Math/" target="_blank">官方文档</a>进行尝试。</p><blockquote><p>注意：在typora中，只需输入$或者$$就可直接进入公式编辑，无需输入一对。</p></blockquote><p>有机会我再对上面提到的语法进行搬运。下面介绍一种更简单省力的方法（也是我在用的方法）：</p><ol><li>打开<a href="https://www.codecogs.com/latex/eqneditor.php" target="_blank">在线LaTex公式编辑器</a>。</li><li>在上方的框框中输入你想要的公式：<img src="/markdown20190915095628/输入公式.png" title="输入公式"> 你可以在下方的GIF图中随时观察你的输入时候符合预期，如在书写word文档等类似文本时需要插入公式，也可以直接复制图片。</li><li>拷贝下方黄颜色方框中的代码到markdown文件。<img src="/markdown20190915095628/拷贝.png" title="拷贝"> 你可以选择去掉两边的“\”和方括号，否则你的公式两侧将会套有方括号，另外你还需要使用上文提到的$来确定公式显示方式。<br>这里我们这样输入：<code>$ x+y=z $</code>。<br>得到：$ x+y=z $。<br>以上就是使用LaTex给markdown添加公式的方法。<br>你也可以使用黄颜色框中的URL选项来添加代码，格式是<code>![](URL)</code>。<br>例如，输入：<code>![](https://latex.codecogs.com/gif.latex?x&amp;plus;y=z)</code><br>可以看到：<img src="https://latex.codecogs.com/gif.latex?x&plus;y=z" alt><br>这种方法就不需要文章顶部链接三中的配置了，也是一种推荐的方法。<blockquote><p>注：若在在线LaTex公式编辑器中找不到需要的元素或者符号的话，可以看一看<a href="https://www.cnblogs.com/echo-coding/p/8663676.html" target="_blank">LaTex常用公式整理</a>。</p></blockquote></li></ol><hr><h1 id="代码高亮"><a href="#代码高亮" class="headerlink" title="代码高亮"></a>代码高亮</h1><p>markdown中使代码高亮的格式如下：<br>三个反引号+语言名<br>代码…<br>三个反引号<br>例如，输入：<br><img src="/markdown20190915095628/输入.png" title="输入"><br>可以看到：<br></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">print(<span class="string">"hello world!"</span>)</span><br></pre></td></tr></table></figure><p></p><p>同样的，在typora中，你也不必输入成对的三个反引号。<br>这里我要提醒一个我以前用Rmarkdown时踩过的坑：<br><img src="/markdown20190915095628/坑.png" title="坑"><br>注意！他俩是不一样的！<br><img src="/markdown20190915095628/我们不一样.jpg" title="我们不一样"><br>真正的“`”在这里：<br><img src="/markdown20190915095628/在这.jpg" title="在这"></p><script type="text/javascript" src="/js/src/bai.js"></script><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      
      
        &lt;!-- build time:Tue Jan 28 2020 15:40:41 GMT+0800 (GMT+08:00) --&gt;&lt;p&gt;在上一篇文章&lt;a href=&quot;https://gsy00517.github.io/deep-learning20190915073809/&quot; 
      
    
    </summary>
    
    
      <category term="操作和使用" scheme="https://gsy00517.github.io/categories/%E6%93%8D%E4%BD%9C%E5%92%8C%E4%BD%BF%E7%94%A8/"/>
    
    
      <category term="markdown" scheme="https://gsy00517.github.io/tags/markdown/"/>
    
  </entry>
  
  <entry>
    <title>deep learning笔记：着眼于深度——VGG简介与pytorch实现</title>
    <link href="https://gsy00517.github.io/deep-learning20190915073809/"/>
    <id>https://gsy00517.github.io/deep-learning20190915073809/</id>
    <published>2019-09-14T23:38:09.000Z</published>
    <updated>2020-01-19T02:40:23.231Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Tue Jan 28 2020 15:40:41 GMT+0800 (GMT+08:00) --><p>VGG是我第一个自己编程实践的卷积神经网络，也是挺高兴的，下面我就对VGG在这篇文章中做一个分享。</p><p><strong>References</strong>：</p><p>电子文献：<br><a href="https://blog.csdn.net/xiaohuihui1994/article/details/89207534" target="_blank" rel="noopener">https://blog.csdn.net/xiaohuihui1994/article/details/89207534</a><br><a href="https://blog.csdn.net/sinat_33487968/article/details/83584289" target="_blank" rel="noopener">https://blog.csdn.net/sinat_33487968/article/details/83584289</a><br><a href="https://blog.csdn.net/qq_32172681/article/details/95971492" target="_blank" rel="noopener">https://blog.csdn.net/qq_32172681/article/details/95971492</a></p><p>参考文献：<br>[1]Very Deep Convolutional Networks For Large-scale Image Recognition</p><hr><h1 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h1><p>VGG模型在2014年取得了ILSVRC竞赛的第二名，第一名是GoogLeNet。但是VGG在多个迁移学习任务中的表现要优于googLeNet。<br><img src="/deep-learning20190915073809/ILSVRC历年winner表现.jpg" title="ILSVRC历年winner表现"><br>相比之前的神经网路，VGG主要有两大进步：其一是它增加了深度，其二是它使用了小的3x3的卷积核，这可以使它在增加深度的时候一定程度上防止了参数的增长。缺点是它的参数量比较庞大，但这并不意味着它不值得我们仔细研究。下图展示的是VGG的结构。<br><img src="/deep-learning20190915073809/VGG结构.png" title="VGG结构"><br>为了通过对比来对VGG的一些改进进行解释，VGG的作者在论文中提供了多个版本。<br><img src="/deep-learning20190915073809/各版本VGG.png" title="各版本VGG"></p><hr><h1 id="论文"><a href="#论文" class="headerlink" title="论文"></a>论文</h1><p>要详细分析VGG，我可能不能像网上写的那样好，更不可能像论文一样明白。那么我在这里就先附上论文。<br><a href="1409.1556.pdf" target="_blank">论文原版</a><br><a href="VggNet中文.pdf" target="_blank">论文中文版</a><br>我在英文原版中用黄颜色高亮了我觉得比较重要的内容，大家可以参考一下。<br>大家也可以自己到网上进行搜索，这类经典的网络网上有许多介绍与分析。<br>通过论文或者网上的资源对这个网络有一定理解之后，你可以看看我下面的代码实现。</p><hr><h1 id="结论"><a href="#结论" class="headerlink" title="结论"></a>结论</h1><p>此篇论文的得出了一些结论，总结如下：</p><ol><li>在一定范围内，通过增加网络深度能有效地提升网络性能。这在ResNet里更是得到了显著地体现，上面的ILSVRC历年winner表现统计图就是一个很好的证明，可参见<a href="https://gsy00517.github.io/deep-learning20191001184216/" target="_blank">deep-learning笔记：使网络能够更深——ResNet简介与pytorch实现</a>。</li><li>与AlexNet对比可知，多个小卷积核比单个大卷积核性能要好。</li><li>AlexNet中用到的LRN层（局部响应归一化层）并没有带来性能的提升，因此可以排除。</li><li>尺度抖动（scale jittering）即多尺度训练、多尺度测试有利于网络性能的提升。</li><li>最佳模型为VGG16，其从头到尾只用了3x3的卷积和2x2的池化。</li></ol><hr><h1 id="特点"><a href="#特点" class="headerlink" title="特点"></a>特点</h1><p>VGG的特点（创新点）主要有如下四个：</p><ol><li><h2 id="小卷积核"><a href="#小卷积核" class="headerlink" title="小卷积核"></a>小卷积核</h2>VGG使用多个小卷积核来代替大的，这样一方面可以减少参数，另一方面相当于进行了更多的非线性映射，可以增加网络的拟合、表达能力。</li><li><h2 id="小池化核"><a href="#小池化核" class="headerlink" title="小池化核"></a>小池化核</h2>相比AlexNet的3x3池化核，VGG一律采用了2x2的池化核。</li><li><h2 id="层数更深"><a href="#层数更深" class="headerlink" title="层数更深"></a>层数更深</h2>若仅计算conv、fc层的话，VGG中常用的网络层数达到了16、19层（VGG16效果最好），这相较于前几年的研究是一个深度的提升。</li><li><h2 id="conv替代fc"><a href="#conv替代fc" class="headerlink" title="conv替代fc"></a>conv替代fc</h2>在基本的CNN中，全连接层的作用是将经过多个卷积层和池化层的图像特征图中的特征进行整合，获取图像特征具有的高层含义，用于图像分类。<br>如果我们把全连接层的输出不再看成n个节点的集合，而是视作一个1x1xn的输出层，那么我们就可以用卷积层来替换全连接层了。并且从数学角度看，它和全连接层是一样的。<br>因为卷积层没有全连接层对输入的限制，因此使用卷积层代替全连接层可以接收任意宽或高的输入。<br>此外，相对于全连接层而言，使用卷积层不会破坏图像的空间结构。这也是谷歌的网络使用1x1的卷积层代替全连接层的重要原因。</li></ol><hr><h1 id="感受野"><a href="#感受野" class="headerlink" title="感受野"></a>感受野</h1><p>这里会涉及一个名为感受野（Receptive Field）的概念，它指的是卷积神经网络每一层输出的特征图（feature map）上的像素点在输入图片上映射的区域大小。简而言之，就是特征图上的一点跟原有图上有关系的点的区域。一般取一个pixel为单位，而输入的感受野就是1即只对应其自身的那个像素。画图易知，两层3x3的卷积层所得到的感受野与一层5x5的卷积层的感受野相同，这也是VGG使用3x3小卷积核来代替的原理之一。<br>感受野是CNN中一个比较重要的概念，一些目标检测的流行算法如SSD、Faster Rcnn等中的prior box和anchor box的设计都是以感受野为依据的。</p><hr><h1 id="1x1卷积核"><a href="#1x1卷积核" class="headerlink" title="1x1卷积核"></a>1x1卷积核</h1><p>虽然VGG所用的是2x2的卷积核，但是在上文提到了谷歌使用1x1的卷积层代替全连接层，那么顺便就对1x1卷积核的作用做一个总结。</p><ol><li>如上文所述，使用卷积层就没有全连接层对输入尺寸的限制，这也方便了许多。</li><li>全连接层会改变网络的空间结构，卷积层不会破坏图像的空间结构。</li><li>可以用于为决策增加非线性因素。</li><li>一些模型用1x1的全连接层来调整网络维度。比如MobileNet使用1x1的卷积核来扩维，ResNet使用1x1的卷积核来降维。</li></ol><hr><h1 id="自己实现"><a href="#自己实现" class="headerlink" title="自己实现"></a>自己实现</h1><p>这里我使用pytorch框架来实现VGG。pytorch是一个相对较新的框架，但热度上升很快。根据网上的介绍，pytorch是一个非常适合于学习与科研的深度学习框架。我尝试了之后，也发现上手很快。<br>在pytorch中，神经网络可以通过torch.nn包来构建。这里我不一一介绍了，大家可以参考<a href="http://pytorch123.com/SecondSection/neural_networks/" target="_blank">pytorch官方中文教程</a>来学习，照着文档自己动手敲一遍之后，基本上就知道了pytorch如何使用了。<br>为了方便直观的理解，我先提供一个VGG16版本的流程图。<br><img src="/deep-learning20190915073809/VGG流程图.png" title="VGG流程图"><br>下面是我实现VGG19版本的代码：<br>首先，我们import所需的包。<br></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br></pre></td></tr></table></figure><p></p><p>接下来，我们定义神经网络。<br></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">VGG</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, num_classes = <span class="number">1000</span>)</span>:</span> <span class="comment">#imagenet图像库总共1000个类</span></span><br><span class="line">        super(VGG, self).__init__() <span class="comment">#先运行父类nn.Module初始化函数</span></span><br><span class="line">        </span><br><span class="line">        self.conv1_1 = nn.Conv2d(in_channels = <span class="number">3</span>, out_channels = <span class="number">64</span>, kernel_size = <span class="number">3</span>, padding = <span class="number">1</span>)</span><br><span class="line">        <span class="comment">#定义图像卷积函数：输入为图像（3个频道，即RGB图），输出为64张特征图，卷积核为3x3正方形，为保留原空间分辨率，卷积层的空间填充为1即padding等于1，也就是防止每次卷积尺寸缩小过快导致无法使用更多的卷积层</span></span><br><span class="line">        self.conv1_2 = nn.Conv2d(in_channels = <span class="number">64</span>, out_channels = <span class="number">64</span>, kernel_size = <span class="number">3</span>, padding = <span class="number">1</span>)</span><br><span class="line">        </span><br><span class="line">        self.conv2_1 = nn.Conv2d(in_channels = <span class="number">64</span>, out_channels = <span class="number">128</span>, kernel_size = <span class="number">3</span>, padding = <span class="number">1</span>)</span><br><span class="line">        self.conv2_2 = nn.Conv2d(in_channels = <span class="number">128</span>, out_channels = <span class="number">128</span>, kernel_size = <span class="number">3</span>, padding = <span class="number">1</span>)</span><br><span class="line">        </span><br><span class="line">        self.conv3_1 = nn.Conv2d(in_channels = <span class="number">128</span>, out_channels = <span class="number">256</span>, kernel_size = <span class="number">3</span>, padding = <span class="number">1</span>)</span><br><span class="line">        self.conv3_2 = nn.Conv2d(in_channels = <span class="number">256</span>, out_channels = <span class="number">256</span>, kernel_size = <span class="number">3</span>, padding = <span class="number">1</span>)</span><br><span class="line">        self.conv3_3 = nn.Conv2d(in_channels = <span class="number">256</span>, out_channels = <span class="number">256</span>, kernel_size = <span class="number">3</span>, padding = <span class="number">1</span>)</span><br><span class="line">        self.conv3_4 = nn.Conv2d(in_channels = <span class="number">256</span>, out_channels = <span class="number">256</span>, kernel_size = <span class="number">3</span>, padding = <span class="number">1</span>)</span><br><span class="line">        </span><br><span class="line">        self.conv4_1 = nn.Conv2d(in_channels = <span class="number">256</span>, out_channels = <span class="number">512</span>, kernel_size = <span class="number">3</span>, padding = <span class="number">1</span>)</span><br><span class="line">        self.conv4_2 = nn.Conv2d(in_channels = <span class="number">512</span>, out_channels = <span class="number">512</span>, kernel_size = <span class="number">3</span>, padding = <span class="number">1</span>)</span><br><span class="line">        self.conv4_3 = nn.Conv2d(in_channels = <span class="number">512</span>, out_channels = <span class="number">512</span>, kernel_size = <span class="number">3</span>, padding = <span class="number">1</span>)</span><br><span class="line">        self.conv4_4 = nn.Conv2d(in_channels = <span class="number">512</span>, out_channels = <span class="number">512</span>, kernel_size = <span class="number">3</span>, padding = <span class="number">1</span>)</span><br><span class="line">        </span><br><span class="line">        self.conv5_1 = nn.Conv2d(in_channels = <span class="number">512</span>, out_channels = <span class="number">512</span>, kernel_size = <span class="number">3</span>, padding = <span class="number">1</span>)</span><br><span class="line">        self.conv5_2 = nn.Conv2d(in_channels = <span class="number">512</span>, out_channels = <span class="number">512</span>, kernel_size = <span class="number">3</span>, padding = <span class="number">1</span>)</span><br><span class="line">        self.conv5_3 = nn.Conv2d(in_channels = <span class="number">512</span>, out_channels = <span class="number">512</span>, kernel_size = <span class="number">3</span>, padding = <span class="number">1</span>)</span><br><span class="line">        self.conv5_4 = nn.Conv2d(in_channels = <span class="number">512</span>, out_channels = <span class="number">512</span>, kernel_size = <span class="number">3</span>, padding = <span class="number">1</span>)</span><br><span class="line">        </span><br><span class="line">        self.relu = nn.ReLU(inplace = <span class="literal">True</span>) <span class="comment">#inplace=TRUE表示原地操作</span></span><br><span class="line">        self.max = nn.MaxPool2d(kernel_size = <span class="number">2</span>, stride = <span class="number">2</span>)</span><br><span class="line">        </span><br><span class="line">        self.fc1 = nn.Linear(<span class="number">512</span> * <span class="number">7</span> * <span class="number">7</span>, <span class="number">4096</span>) <span class="comment">#定义全连接函数1为线性函数:y = Wx + b，并将512*7*7个节点连接到4096个节点上</span></span><br><span class="line">        self.fc2 = nn.Linear(<span class="number">4096</span>, <span class="number">4096</span>)</span><br><span class="line">        self.fc3 = nn.Linear(<span class="number">4096</span>, num_classes)</span><br><span class="line">        <span class="comment">#定义全连接函数3为线性函数:y = Wx + b，并将4096个节点连接到num_classes个节点上，然后可用softmax进行处理</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment">#定义该神经网络的向前传播函数，该函数必须定义，一旦定义成功，向后传播函数也会自动生成（autograd）</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        </span><br><span class="line">        x = self.relu(self.conv1_1(x))</span><br><span class="line">        x = self.relu(self.conv1_2(x))</span><br><span class="line">        x = self.max(x) </span><br><span class="line">        <span class="comment">#输入x经过卷积之后，经过激活函数ReLU，循环两次，最后使用2x2的窗口进行最大池化Max pooling，然后更新到x</span></span><br><span class="line">        </span><br><span class="line">        x = self.relu(self.conv2_1(x))</span><br><span class="line">        x = self.relu(self.conv2_2(x))</span><br><span class="line">        x = self.max(x)</span><br><span class="line">        </span><br><span class="line">        x = self.relu(self.conv3_1(x))</span><br><span class="line">        x = self.relu(self.conv3_2(x))</span><br><span class="line">        x = self.relu(self.conv3_3(x))</span><br><span class="line">        x = self.relu(self.conv3_4(x))</span><br><span class="line">        x = self.max(x)</span><br><span class="line">        </span><br><span class="line">        x = self.relu(self.conv4_1(x))</span><br><span class="line">        x = self.relu(self.conv4_2(x))</span><br><span class="line">        x = self.relu(self.conv4_3(x))</span><br><span class="line">        x = self.relu(self.conv4_4(x))</span><br><span class="line">        x = self.max(x)</span><br><span class="line">        </span><br><span class="line">        x = self.relu(self.conv5_1(x))</span><br><span class="line">        x = self.relu(self.conv5_2(x))</span><br><span class="line">        x = self.relu(self.conv5_3(x))</span><br><span class="line">        x = self.relu(self.conv5_4(x))</span><br><span class="line">        x = self.max(x)</span><br><span class="line">        </span><br><span class="line">        x = x.view(<span class="number">-1</span>, self.num_flat_features(x)) <span class="comment">#view函数将张量x变形成一维的向量形式,总特征数并不改变,为接下来的全连接作准备</span></span><br><span class="line">        </span><br><span class="line">        x = self.fc1(x) <span class="comment">#输入x经过全连接1，然后更新x</span></span><br><span class="line">        x = self.fc2(x)</span><br><span class="line">        x = self.fc3(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">num_flat_features</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        size = x.size()[<span class="number">1</span>:]  <span class="comment">#all dimensions except the batch dimension</span></span><br><span class="line">        num_features = <span class="number">1</span></span><br><span class="line">        <span class="keyword">for</span> s <span class="keyword">in</span> size:</span><br><span class="line">            num_features *= s</span><br><span class="line">        <span class="keyword">return</span> num_features</span><br><span class="line"></span><br><span class="line">vgg = VGG()</span><br><span class="line">print(vgg)</span><br></pre></td></tr></table></figure><p></p><p>我们print网络，可以看到输出如下：<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">VGG(</span><br><span class="line">  (conv1_1): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))</span><br><span class="line">  (conv1_2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))</span><br><span class="line">  (conv2_1): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))</span><br><span class="line">  (conv2_2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))</span><br><span class="line">  (conv3_1): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))</span><br><span class="line">  (conv3_2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))</span><br><span class="line">  (conv3_3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))</span><br><span class="line">  (conv3_4): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))</span><br><span class="line">  (conv4_1): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))</span><br><span class="line">  (conv4_2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))</span><br><span class="line">  (conv4_3): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))</span><br><span class="line">  (conv4_4): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))</span><br><span class="line">  (conv5_1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))</span><br><span class="line">  (conv5_2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))</span><br><span class="line">  (conv5_3): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))</span><br><span class="line">  (conv5_4): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))</span><br><span class="line">  (relu): ReLU(inplace=True)</span><br><span class="line">  (max): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)</span><br><span class="line">  (fc1): Linear(in_features=25088, out_features=4096, bias=True)</span><br><span class="line">  (fc2): Linear(in_features=4096, out_features=4096, bias=True)</span><br><span class="line">  (fc3): Linear(in_features=4096, out_features=1000, bias=True)</span><br><span class="line">)</span><br></pre></td></tr></table></figure><p></p><p>最后我们随机生成一个张量来进行验证。<br></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">input = torch.randn(<span class="number">1</span>, <span class="number">3</span>, <span class="number">224</span>, <span class="number">224</span>)</span><br><span class="line">out = vgg(input)</span><br><span class="line">print(out)</span><br></pre></td></tr></table></figure><p></p><p>其中(1, 3, 224, 224)表示1个3x224x224的矩阵，这是因为VGG输入的是固定尺寸的224x224的RGB（三通道）图像。<br>如果没有报错，那么就说明你的神经网路可以运行通过了。<br>我们也可以使用torch.nn.functional来实现激活函数与池化层，这样的话，你需要还需要多引入一个包：<br></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F <span class="comment">#新增</span></span><br></pre></td></tr></table></figure><p></p><p>同时，你不需要在init中实例化激活函数与最大池化层，相应的，你需要对forward前馈函数进行更改：<br></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        </span><br><span class="line">        x = F.relu(self.conv1_1(x))</span><br><span class="line">        x = F.relu(self.conv1_2(x))</span><br><span class="line">        x = F.max_pool2d(x, kernel_size = <span class="number">2</span>, stride = <span class="number">2</span>) </span><br><span class="line">        <span class="comment">#输入x经过卷积之后，经过激活函数ReLU，循环两次，最后使用2x2的窗口进行最大池化Max pooling，然后更新到x</span></span><br><span class="line">        </span><br><span class="line">        x = F.relu(self.conv2_1(x))</span><br><span class="line">        x = F.relu(self.conv2_2(x))</span><br><span class="line">        x = F.max_pool2d(x, kernel_size = <span class="number">2</span>, stride = <span class="number">2</span>)</span><br><span class="line">        </span><br><span class="line">        x = F.relu(self.conv3_1(x))</span><br><span class="line">        x = F.relu(self.conv3_2(x))</span><br><span class="line">        x = F.relu(self.conv3_3(x))</span><br><span class="line">        x = F.relu(self.conv3_4(x))</span><br><span class="line">        x = F.max_pool2d(x, kernel_size = <span class="number">2</span>, stride = <span class="number">2</span>)</span><br><span class="line">        </span><br><span class="line">        x = F.relu(self.conv4_1(x))</span><br><span class="line">        x = F.relu(self.conv4_2(x))</span><br><span class="line">        x = F.relu(self.conv4_3(x))</span><br><span class="line">        x = F.relu(self.conv4_4(x))</span><br><span class="line">        x = F.max_pool2d(x, kernel_size = <span class="number">2</span>, stride = <span class="number">2</span>)</span><br><span class="line">        </span><br><span class="line">        x = F.relu(self.conv5_1(x))</span><br><span class="line">        x = F.relu(self.conv5_2(x))</span><br><span class="line">        x = F.relu(self.conv5_3(x))</span><br><span class="line">        x = F.relu(self.conv5_4(x))</span><br><span class="line">        x = F.max_pool2d(x, kernel_size = <span class="number">2</span>, stride = <span class="number">2</span>)</span><br><span class="line">        </span><br><span class="line">        x = x.view(<span class="number">-1</span>, self.num_flat_features(x)) <span class="comment">#view函数将张量x变形成一维的向量形式,总特征数并不改变,为接下来的全连接作准备</span></span><br><span class="line">        </span><br><span class="line">        x = self.fc1(x) <span class="comment">#输入x经过全连接1，然后更新x</span></span><br><span class="line">        x = self.fc2(x)</span><br><span class="line">        x = self.fc3(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br></pre></td></tr></table></figure><p></p><p>如果你之前运行通过的话，那么这里也是没有问题的。<br>这里我想说明一下torch.nn与torch.nn.functional的区别。<br>这两个包中有许多类似的激活函数与损失函数，但是它们又有如下不同：<br>首先，在定义函数层（继承nn.Module）时，init函数中应该用torch.nn，例如torch.nn.ReLU，torch.nn.Dropout2d，而forward中应该用torch.nn.functionl，例如torch.nn.functional.relu，不过请注意，init里面定义的是标准的网络层。只有torch.nn定义的才会进行训练。torch.nn.functional定义的需要自己手动设置参数。所以通常，激活函数或者卷积之类的都用torch.nn定义。<br>另外，torch.nn是类，必须要先在init中实例化，然后在forward中使用，而torch.nn.functional可以直接在forward中使用。<br>大家还可以通过官方文档<a href="https://pytorch-cn.readthedocs.io/zh/latest/package_references/functional/" target="_blank">torch.nn.functional</a>与<a href="https://pytorch-cn.readthedocs.io/zh/latest/package_references/torch-nn/" target="_blank">torch.nn</a>来进一步了解两者的区别。<br>大家或许发现，我的代码中有大量的重复性工作。是的，你将在文章后面的官方实现中看到优化的代码，但是相对来说，我的代码更加直观些，完全是按照网络的结构顺序从上到下编写的，可以方便初学者（including myself）的理解。</p><hr><h1 id="出现的问题"><a href="#出现的问题" class="headerlink" title="出现的问题"></a>出现的问题</h1><p>虽然我的代码比较简单直白，但是过程中并不是一帆风顺的，出现了两次报错：</p><ol><li><h2 id="输入输出不匹配"><a href="#输入输出不匹配" class="headerlink" title="输入输出不匹配"></a>输入输出不匹配</h2>当我第一遍运行时，出现了一个RuntimeError：<img src="/deep-learning20190915073809/报错1.png" title="报错1"> 这是一个超级低级的错误，经学长提醒后我才发现，我两个卷积层之间输出输入的channel数并不匹配：<img src="/deep-learning20190915073809/错误原因.png" title="错误原因"> 唉又是ctrl+C+V惹的祸，改正后的网络可以参见上文。<br>在这里，我想提醒我自己和大家注意一下卷积层输入输出的维度公式：<br>假设输入的宽、高记为W、H。<br>超参数中，卷积核的维度是F，stride步长是S，padding是P。<br>那么输出的宽X与高Y可用如下公式表示：<script type="math/tex;mode=display">X=\frac{W+2P-F}{S}+1\</script><script type="math/tex;mode=display">Y=\frac{H+2P-F}{S}+1\</script>然而，当我在计算ResNet的维度的时候，发现套用这个公式是除不尽的。于是我搜索到了如下规则：<br>1.对卷积层操作，除不尽时，向下取整。<br>2.对池化层操作，除不尽时，向上取整。</li><li><h2 id="没有把张量转化成一维向量"><a href="#没有把张量转化成一维向量" class="headerlink" title="没有把张量转化成一维向量"></a>没有把张量转化成一维向量</h2>上面的问题解决了，结果还有错误：<img src="/deep-learning20190915073809/报错2.png" title="报错2"> 根据报错，可以发现3584x7是等于25088的，结合pytorch官方文档，我意识到我在把张量输入全连接层时，没有把它拍扁成一维。因此，我按照官方文档添加了如下代码：<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">x = x.view(<span class="number">-1</span>, self.num_flat_features(x))</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">num_flat_features</span><span class="params">(self, x)</span>:</span></span><br><span class="line">    size = x.size()[<span class="number">1</span>:]  <span class="comment">#all dimensions except the batch dimension</span></span><br><span class="line">    num_features = <span class="number">1</span></span><br><span class="line">    <span class="keyword">for</span> s <span class="keyword">in</span> size:</span><br><span class="line">        num_features *= s</span><br><span class="line">    <span class="keyword">return</span> num_features</span><br></pre></td></tr></table></figure></li></ol><p>再运行，问题解决。<br>另外，我原本写的代码中，在卷积层之间的对应位置都加上了relu激活函数与池化层。后来我才意识到，由于它们不具有任何需要学习的参数，我可以直接把它们拿出来单独定义：<br></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">self.relu = nn.ReLU(inplace = <span class="literal">True</span>)</span><br><span class="line">self.max = nn.MaxPool2d(kernel_size = <span class="number">2</span>, stride = <span class="number">2</span>)</span><br></pre></td></tr></table></figure><p></p><p>虽然是一些很低级的坑，但我还是想写下来供我自己和大家今后参考。</p><hr><h1 id="官方源码"><a href="#官方源码" class="headerlink" title="官方源码"></a>官方源码</h1><p>由于VGG的结构设计非常有规律，因此官方源码给出了更简洁的版本：<br></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> math</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">VGG</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, features, num_classes=<span class="number">1000</span>, init_weights=True)</span>:</span></span><br><span class="line">        super(VGG, self).__init__()</span><br><span class="line">        self.features = features</span><br><span class="line">        self.classifier = nn.Sequential(</span><br><span class="line">            nn.Linear(<span class="number">512</span> * <span class="number">7</span> * <span class="number">7</span>, <span class="number">4096</span>),</span><br><span class="line">            nn.ReLU(<span class="literal">True</span>),</span><br><span class="line">            nn.Dropout(),</span><br><span class="line">            nn.Linear(<span class="number">4096</span>, <span class="number">4096</span>),</span><br><span class="line">            nn.ReLU(<span class="literal">True</span>),</span><br><span class="line">            nn.Dropout(),</span><br><span class="line">            nn.Linear(<span class="number">4096</span>, num_classes),</span><br><span class="line">        )</span><br><span class="line">        <span class="keyword">if</span> init_weights:</span><br><span class="line">            self._initialize_weights()</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        x = self.features(x)</span><br><span class="line">        x = x.view(x.size(<span class="number">0</span>), <span class="number">-1</span>)</span><br><span class="line">        x = self.classifier(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_initialize_weights</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">for</span> m <span class="keyword">in</span> self.modules():</span><br><span class="line">            <span class="keyword">if</span> isinstance(m, nn.Conv2d):</span><br><span class="line">                n = m.kernel_size[<span class="number">0</span>] * m.kernel_size[<span class="number">1</span>] * m.out_channels</span><br><span class="line">                m.weight.data.normal_(<span class="number">0</span>, math.sqrt(<span class="number">2.</span> / n))</span><br><span class="line">                <span class="keyword">if</span> m.bias <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">                    m.bias.data.zero_()</span><br><span class="line">            <span class="keyword">elif</span> isinstance(m, nn.BatchNorm2d):</span><br><span class="line">                m.weight.data.fill_(<span class="number">1</span>)</span><br><span class="line">                m.bias.data.zero_()</span><br><span class="line">            <span class="keyword">elif</span> isinstance(m, nn.Linear):</span><br><span class="line">                m.weight.data.normal_(<span class="number">0</span>, <span class="number">0.01</span>)</span><br><span class="line">                m.bias.data.zero_()</span><br></pre></td></tr></table></figure><p></p><p>因为VGG中卷积层的重复性比较高，所以官方使用一个函数来循环产生卷积层：<br></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">make_layers</span><span class="params">(cfg, batch_norm=False)</span>:</span></span><br><span class="line">    layers = []</span><br><span class="line">    in_channels = <span class="number">3</span></span><br><span class="line">    <span class="keyword">for</span> v <span class="keyword">in</span> cfg:</span><br><span class="line">        <span class="keyword">if</span> v == <span class="string">'M'</span>:</span><br><span class="line">            layers += [nn.MaxPool2d(kernel_size=<span class="number">2</span>, stride=<span class="number">2</span>)]</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            conv2d = nn.Conv2d(in_channels, v, kernel_size=<span class="number">3</span>, padding=<span class="number">1</span>)</span><br><span class="line">            <span class="keyword">if</span> batch_norm:</span><br><span class="line">                layers += [conv2d, nn.BatchNorm2d(v), nn.ReLU(inplace=<span class="literal">True</span>)]</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                layers += [conv2d, nn.ReLU(inplace=<span class="literal">True</span>)]</span><br><span class="line">            in_channels = v</span><br><span class="line">    <span class="keyword">return</span> nn.Sequential(*layers)</span><br></pre></td></tr></table></figure><p></p><p>接下来定义各个版本的卷积层（可参考上文中对论文的截图），这里的“M”表示的是最大池化层。<br></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line">cfg = &#123;</span><br><span class="line">    <span class="string">'A'</span>: [<span class="number">64</span>, <span class="string">'M'</span>, <span class="number">128</span>, <span class="string">'M'</span>, <span class="number">256</span>, <span class="number">256</span>, <span class="string">'M'</span>, <span class="number">512</span>, <span class="number">512</span>, <span class="string">'M'</span>, <span class="number">512</span>, <span class="number">512</span>, <span class="string">'M'</span>],</span><br><span class="line">    <span class="string">'B'</span>: [<span class="number">64</span>, <span class="number">64</span>, <span class="string">'M'</span>, <span class="number">128</span>, <span class="number">128</span>, <span class="string">'M'</span>, <span class="number">256</span>, <span class="number">256</span>, <span class="string">'M'</span>, <span class="number">512</span>, <span class="number">512</span>, <span class="string">'M'</span>, <span class="number">512</span>, <span class="number">512</span>, <span class="string">'M'</span>],</span><br><span class="line">    <span class="string">'D'</span>: [<span class="number">64</span>, <span class="number">64</span>, <span class="string">'M'</span>, <span class="number">128</span>, <span class="number">128</span>, <span class="string">'M'</span>, <span class="number">256</span>, <span class="number">256</span>, <span class="number">256</span>, <span class="string">'M'</span>, <span class="number">512</span>, <span class="number">512</span>, <span class="number">512</span>, <span class="string">'M'</span>, <span class="number">512</span>, <span class="number">512</span>, <span class="number">512</span>, <span class="string">'M'</span>],</span><br><span class="line">    <span class="string">'E'</span>: [<span class="number">64</span>, <span class="number">64</span>, <span class="string">'M'</span>, <span class="number">128</span>, <span class="number">128</span>, <span class="string">'M'</span>, <span class="number">256</span>, <span class="number">256</span>, <span class="number">256</span>, <span class="number">256</span>, <span class="string">'M'</span>, <span class="number">512</span>, <span class="number">512</span>, <span class="number">512</span>, <span class="number">512</span>, <span class="string">'M'</span>, <span class="number">512</span>, <span class="number">512</span>, <span class="number">512</span>, <span class="number">512</span>, <span class="string">'M'</span>],</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">vgg11</span><span class="params">(**kwargs)</span>:</span></span><br><span class="line">    model = VGG(make_layers(cfg[<span class="string">'A'</span>]), **kwargs)</span><br><span class="line">    <span class="keyword">return</span> model</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">vgg11_bn</span><span class="params">(**kwargs)</span>:</span></span><br><span class="line">    model = VGG(make_layers(cfg[<span class="string">'A'</span>], batch_norm=<span class="literal">True</span>), **kwargs)</span><br><span class="line">    <span class="keyword">return</span> model</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">vgg13</span><span class="params">(**kwargs)</span>:</span></span><br><span class="line">    model = VGG(make_layers(cfg[<span class="string">'B'</span>]), **kwargs)</span><br><span class="line">    <span class="keyword">return</span> model</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">vgg13_bn</span><span class="params">(**kwargs)</span>:</span></span><br><span class="line">    model = VGG(make_layers(cfg[<span class="string">'B'</span>], batch_norm=<span class="literal">True</span>), **kwargs)</span><br><span class="line">    <span class="keyword">return</span> model</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">vgg16</span><span class="params">(**kwargs)</span>:</span></span><br><span class="line">    model = VGG(make_layers(cfg[<span class="string">'D'</span>]), **kwargs)</span><br><span class="line">    <span class="keyword">return</span> model</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">vgg16_bn</span><span class="params">(**kwargs)</span>:</span></span><br><span class="line">    model = VGG(make_layers(cfg[<span class="string">'D'</span>], batch_norm=<span class="literal">True</span>), **kwargs)</span><br><span class="line">    <span class="keyword">return</span> model</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">vgg19</span><span class="params">(**kwargs)</span>:</span></span><br><span class="line">    model = VGG(make_layers(cfg[<span class="string">'E'</span>]), **kwargs)</span><br><span class="line">    <span class="keyword">return</span> model</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">vgg19_bn</span><span class="params">(**kwargs)</span>:</span></span><br><span class="line">    model = VGG(make_layers(cfg[<span class="string">'E'</span>], batch_norm=<span class="literal">True</span>), **kwargs)</span><br><span class="line">    <span class="keyword">return</span> model</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">    <span class="comment"># 'VGG', 'vgg11', 'vgg11_bn', 'vgg13', 'vgg13_bn', 'vgg16', 'vgg16_bn', 'vgg19_bn', 'vgg19'</span></span><br><span class="line">    <span class="comment"># Example</span></span><br><span class="line">    net11 = vgg11()</span><br><span class="line">    print(net11)</span><br></pre></td></tr></table></figure><p></p><p>附上pytorch官方源码<a href="https://github.com/pytorch/vision" target="_blank">链接</a>。可以在vision/torchvision/models/下找到一系列用pytorch实现的经典神经网路模型。<br>好了，以上就是VGG的介绍与实现，如有不足之处欢迎大家补充！</p><script type="text/javascript" src="/js/src/bai.js"></script><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      
      
        &lt;!-- build time:Tue Jan 28 2020 15:40:41 GMT+0800 (GMT+08:00) --&gt;&lt;p&gt;VGG是我第一个自己编程实践的卷积神经网络，也是挺高兴的，下面我就对VGG在这篇文章中做一个分享。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Referenc
      
    
    </summary>
    
    
      <category term="人工智能" scheme="https://gsy00517.github.io/categories/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/"/>
    
    
      <category term="深度学习" scheme="https://gsy00517.github.io/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="论文分享" scheme="https://gsy00517.github.io/tags/%E8%AE%BA%E6%96%87%E5%88%86%E4%BA%AB/"/>
    
      <category term="踩坑血泪" scheme="https://gsy00517.github.io/tags/%E8%B8%A9%E5%9D%91%E8%A1%80%E6%B3%AA/"/>
    
      <category term="代码实现" scheme="https://gsy00517.github.io/tags/%E4%BB%A3%E7%A0%81%E5%AE%9E%E7%8E%B0/"/>
    
      <category term="pytorch" scheme="https://gsy00517.github.io/tags/pytorch/"/>
    
  </entry>
  
  <entry>
    <title>deep learning笔记：一篇非常经典的论文——NatureDeepReview</title>
    <link href="https://gsy00517.github.io/deep-learning20190914142553/"/>
    <id>https://gsy00517.github.io/deep-learning20190914142553/</id>
    <published>2019-09-14T06:25:53.000Z</published>
    <updated>2019-11-02T02:21:47.317Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Tue Jan 28 2020 15:40:41 GMT+0800 (GMT+08:00) --><p>这是一篇非常经典的有关深度学习的论文，最近在看一个网课的时候又被提到了，因此特地找了pdf文档放在这里和大家分享。</p><hr><h1 id="简述"><a href="#简述" class="headerlink" title="简述"></a>简述</h1><p>这篇文章首先介绍了深度学习的基本前期储备知识、发展背景，并对机器学习范畴内一个重要方向——监督学习进行完整介绍，然后介绍了反向传播算法和微积分链式法则等深度学习基础内容。<br>文章的接下来重点介绍了卷积神经网络CNN的实现过程、几个非常重要的经典卷积神经网络以及深度卷积神经网络对于视觉任务理解的应用。<br>文章最后探讨了分布表示和语言模型，循环神经网络RNN原理以及对未来的展望和现实的实现。<br>总而言之，我觉得这是一篇值得逐字逐句反复阅读咀嚼的文章，读完这篇文章，大概就相当于打开了深度学习的大门了吧。<br>这篇文章的个人理解与感悟或许我以后会补上，在接触还不深的情况下我不说废话啦，先附上原文，其中黄色高亮部分是一些比较重要的内容，大家有时间的话可以认真看一下。<br>下面附上<a href="NatureDeepReview.pdf" target="_blank">原文链接</a>。<br>最后贴一张我觉得挺搞笑的图。<br><img src="/deep-learning20190914142553/什么是深度学习.jpg" title="什么是深度学习"><br>这张图片还有张兄弟图，可以看看我的另一篇论文分享<a href="https://gsy00517.github.io/artificial-intelligence20191001101334/" target="_blank">artificial-intelligence笔记：人工智能前沿发展情况分享</a>。</p><script type="text/javascript" src="/js/src/bai.js"></script><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      
      
        &lt;!-- build time:Tue Jan 28 2020 15:40:41 GMT+0800 (GMT+08:00) --&gt;&lt;p&gt;这是一篇非常经典的有关深度学习的论文，最近在看一个网课的时候又被提到了，因此特地找了pdf文档放在这里和大家分享。&lt;/p&gt;&lt;hr&gt;&lt;h1 id=
      
    
    </summary>
    
    
      <category term="人工智能" scheme="https://gsy00517.github.io/categories/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/"/>
    
    
      <category term="深度学习" scheme="https://gsy00517.github.io/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="论文分享" scheme="https://gsy00517.github.io/tags/%E8%AE%BA%E6%96%87%E5%88%86%E4%BA%AB/"/>
    
  </entry>
  
  <entry>
    <title>ubuntu笔记：重装ubuntu——记一段辛酸血泪史</title>
    <link href="https://gsy00517.github.io/ubuntu20190914100050/"/>
    <id>https://gsy00517.github.io/ubuntu20190914100050/</id>
    <published>2019-09-14T02:00:50.000Z</published>
    <updated>2020-01-27T09:16:15.584Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Tue Jan 28 2020 15:40:41 GMT+0800 (GMT+08:00) --><p>这是不久前我踩过的一个巨坑，在这里我想先强调一下：<br><strong>不要升级linux发行版！！！重要的事情说三遍！！！</strong><br><strong>不要升级linux发行版！！！重要的事情说三遍！！！</strong><br><strong>不要升级linux发行版！！！重要的事情说三遍！！！</strong><br>为什么？不要问我为什么。我按系统提示升级ubuntu到18.04LTS后，就再也进不去系统了。不信你可以尝试一下，你将会看到如下界面：<br><img src="/ubuntu20190914100050/一点都不OK！.jpg" title="一点都不OK！"></p><blockquote><p>注：图片来自网络，我就不再为了截图而踩一次坑了。</p></blockquote><p>不仅是图形界面，命令行界面也进不去了（据说可以在重启时选择recovery mode并且狂按回车强行进入界面，但我失败了）。<br>不过如果你真的尝试了并且掉坑里了的话，没关系，你获得了一个很好的重装系统的锻炼机会。下面我们就按步骤锻炼一下。<br>截止本文最后一次更新前，我已用此方法安装过3遍系统（2遍16.04LTS，1遍18.04LTS），可以放心食用。由于我目前使用的是18.04LTS版本，而ubuntu18.04的社区也日渐活跃，因此本文将会涉及一些18.04版的安装步骤，基本上是一致的。</p><blockquote><p>注：LTS表示的是长期支持版本，一般<code>.04</code>都是LTS的，每两年发布一次，支持期好像是5年。</p></blockquote><p>本文同时适用于ubuntu16.04LTS和ubuntu18.04LTS。</p><p><strong>References</strong>：</p><p>电子文献：<br><a href="https://blog.csdn.net/Spacegene/article/details/86659349" target="_blank" rel="noopener">https://blog.csdn.net/Spacegene/article/details/86659349</a></p><hr><h1 id="准备"><a href="#准备" class="headerlink" title="准备"></a>准备</h1><ol><li><h2 id="U盘"><a href="#U盘" class="headerlink" title="U盘"></a>U盘</h2>准备一个2G以上的无用的U盘，或者备份好里面的文件。然后将其格式化。</li><li><h2 id="镜像"><a href="#镜像" class="headerlink" title="镜像"></a>镜像</h2>下载<a href="http://releases.ubuntu.com/16.04/ubuntu-16.04.6-desktop-amd64.iso" target="_blank">ubuntu16.04LTS镜像</a>或者<a href="http://releases.ubuntu.com/18.04/ubuntu-18.04.3-desktop-amd64.iso" target="_blank">ubuntu18.04LTS镜像</a>到本地。也可使用<a href="https://mirrors.tuna.tsinghua.edu.cn/ubuntu-releases/" target="_blank">清华源镜像</a>或者<a href="https://mirrors.aliyun.com/ubuntu-releases/" target="_blank">阿里云镜像</a>更快下载。</li><li><h2 id="删除分区"><a href="#删除分区" class="headerlink" title="删除分区"></a>删除分区</h2><p>你可以通过控制面板中的创建并格式化硬盘分区来看到你的windows与ubuntu分区的情况。（以下操作都是针对重装，不再重新分区，需要的可以自行上网查找教程）<br>在重新安装ubuntu16.04之前我们需要删除原先Ubuntu的EFI分区及启动引导项，这里推荐直接使用windows下的diskpart来删除。<br>使用win+R输入diskpart打开diskpart.exe，允许其对设备进行更改。</p><img src="/ubuntu20190914100050/diskpart.png" title="diskpart"><p>接下来使用<code>list dick</code>，我的笔记本当时只有一块SSD，两个系统都装在上面，故<code>select disk 0</code>进入disk 0。然后就可以输入<code>list partition</code>来查看具体的分区信息。</p><img src="/ubuntu20190914100050/查看分区信息.png" title="查看分区信息"><p>其中类型未知的便是分给ubuntu的分区，我这里有一块8G的swap分区和60G的/分区。<br>接下来执行如下命令：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">select partition 7</span><br><span class="line">delete partition override #删除该分区</span><br><span class="line">select partition 8</span><br><span class="line">delete partition override #删除该分区</span><br></pre></td></tr></table></figure><blockquote><p>注意：以上命令是针对我的情况，具体请按照对应ubuntu分区的序号删除。</p></blockquote><p>现在你可以在控制面板中的创建并格式化硬盘分区中看到你删除的分区已经合并成一块未分配的空间，这也意味着你与你原来ubuntu上的数据彻底说再见了。</p></li><li><h2 id="删除ubuntu启动引导项"><a href="#删除ubuntu启动引导项" class="headerlink" title="删除ubuntu启动引导项"></a>删除ubuntu启动引导项</h2>首先下载<a href="https://www.easyuefi.com/index-us.html" target="_blank">EasyUEFI</a>，使用免费试用版EasyUEFI Trial即可。如果试用期过了的话可以到网上找破解版来下。<br>下载完成后安装，打开EasyUEFI如图：<img src="/ubuntu20190914100050/EasyUEFI.png" title="EasyUEFI"> 选择管理EFI启动选项Manage EFI Boot Option，然后选择ubuntu启动引导项，点击中间的删除按钮来删除该引导项。<img src="/ubuntu20190914100050/删除引导.png" title="删除引导"> 现在重新启动，你会发现已经没有让你选择系统的引导界面，而是直接进入windows系统。</li><li><h2 id="制作启动U盘"><a href="#制作启动U盘" class="headerlink" title="制作启动U盘"></a>制作启动U盘</h2>首先我们下载一个免费的U盘制作工具<a href="https://rufus.ie/" target="_blank">rufus</a>。<br>此时插入已经格式化的U盘，打开rufus，一般情况下它会自动选择U盘，你也可以在device选项下手动选择或确认。<br>点击select，选择之前下载好的镜像文件。<br>其他设置保留默认即可，不放心的话可以比对下图：<img src="/ubuntu20190914100050/rufus.png" title="rufus"> 然后start开始制作。如果此时rufus提示需要下载一些其它文件，选择Yes继续即可。<br>没有问题的话制作完的U盘会如图所示：<img src="/ubuntu20190914100050/此时的U盘.png" title="此时的U盘"> 在下面的步骤中，请一直插着U盘不要拔。</li></ol><hr><h1 id="安装"><a href="#安装" class="headerlink" title="安装"></a>安装</h1><p>现在重新启动电脑，开机的过程中不停地快按F12进入bios界面（我的是戴尔的电脑，不同电脑按键或有不同，自行百度；如果快按不行的话再次重启试一试长按，因为网上有些教程说的是长按，而我是长按不行而快按可以）。<br>随后选择U盘启动（不同电脑这个界面也可能不一样，具体可以百度，我选择的是UEFI BOOT中UEFI：U盘名那项）。<br>接下来就进入了紫红色的GNU GRUB界面，选择install ubuntu。如果在这里迟疑了一下，会自动进入trying without install，这也没关系，你也可以进入后在图形界面中双击安装，安装之后可以继续试用，直到重启。<br>随后就是些比较简单的安装过程，基本上可以按默认进行，因为是重装，好像不需要联网安装且有汉化包。</p><blockquote><p>注意：若是安装18.04LTS，这里会出现一个“正常安装”还是“最小安装”的选择，一般无脑选择正常安装即可，但请事先对安装时间做好心里准备（我大概花了一个多小时）且安装完后有些软件还是比较多余的，可以手动卸载。</p></blockquote><p>接下来是比较重要的部分：<br>进入安装类型installation type界面后，选择其他选项something else，这样我们就可以自己配置ubuntu分区了，点击继续。<br>接下来会进入一个磁盘分区的界面，选中之前清出来的未分配分区（名为“空闲”，也可以通过大小来判断），点击下方+号，新建一个swap交换分区，大小为8G左右（一般和电脑的内存相当即可，不分这个区会有警告，也可不分之后再加）。<br>再次双击空闲分区，挂载点下拉，选择/（相当于windows的C盘）。<br>在安装启动引导器的设备选项中，选择Windows Boot Manager。<br>结果可以参考下图：<br><img src="/ubuntu20190914100050/ubuntu分区.jpg" title="ubuntu分区"><br>确认无误后点击现在安装，然后就一路默认直到安装完成。<br>这里会有一个设置用户名和计算机名的界面，建议设置得短一些比较好，否则在终端中每条键入的命令前都会有很长的一串“用户名@计算机名”。<br>安装完成后可以进行试用，此时一切操作都不会被保留。<br>如果无需试用，就重新启动系统，此时会提醒可以拔出installation medium即启动U盘。</p><hr><h1 id="后期"><a href="#后期" class="headerlink" title="后期"></a>后期</h1><p>别忘了把U盘格式化回来，可以继续使用，留着做纪念也行，说不定哪天又要重装。<br>下面我展示一下我目前的一部分美化效果，亲测发现这只会牺牲一点点儿CPU，所以并不用担心，大胆地美化就是，可能这也是使用linux发行版不多的几种乐趣之一吧。<br><img src="/ubuntu20190914100050/桌面.png" title="桌面"><br><img src="/ubuntu20190914100050/桌面立方体.png" title="桌面立方体"><br><img src="/ubuntu20190914100050/选择窗口.png" title="选择窗口"><br><img src="/ubuntu20190914100050/移动窗口.png" title="移动窗口"><br><img src="/ubuntu20190914100050/最小化窗口.png" title="最小化窗口"><br>以上就是重装ubuntu的全部内容，欢迎补充！我也会在新问题出现时及时更新。</p><script type="text/javascript" src="/js/src/bai.js"></script><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      
      
        &lt;!-- build time:Tue Jan 28 2020 15:40:41 GMT+0800 (GMT+08:00) --&gt;&lt;p&gt;这是不久前我踩过的一个巨坑，在这里我想先强调一下：&lt;br&gt;&lt;strong&gt;不要升级linux发行版！！！重要的事情说三遍！！！&lt;/strong&gt;
      
    
    </summary>
    
    
      <category term="操作和使用" scheme="https://gsy00517.github.io/categories/%E6%93%8D%E4%BD%9C%E5%92%8C%E4%BD%BF%E7%94%A8/"/>
    
    
      <category term="踩坑血泪" scheme="https://gsy00517.github.io/tags/%E8%B8%A9%E5%9D%91%E8%A1%80%E6%B3%AA/"/>
    
      <category term="个人经历" scheme="https://gsy00517.github.io/tags/%E4%B8%AA%E4%BA%BA%E7%BB%8F%E5%8E%86/"/>
    
      <category term="ubuntu" scheme="https://gsy00517.github.io/tags/ubuntu/"/>
    
      <category term="安装教程" scheme="https://gsy00517.github.io/tags/%E5%AE%89%E8%A3%85%E6%95%99%E7%A8%8B/"/>
    
  </entry>
  
  <entry>
    <title>ubuntu笔记：释放空间</title>
    <link href="https://gsy00517.github.io/ubuntu20190914094853/"/>
    <id>https://gsy00517.github.io/ubuntu20190914094853/</id>
    <published>2019-09-14T01:48:53.000Z</published>
    <updated>2020-01-26T02:02:25.357Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Tue Jan 28 2020 15:40:41 GMT+0800 (GMT+08:00) --><p>前一篇讲了如何清理windows下的空间，然而虽然ubuntu中垃圾文件没win10那么多，可是我给ubuntu分配的空间比win10少得多了，于是我又找了些清理ubuntu的方法。</p><p><strong>References</strong>：</p><p>电子文献：<br><a href="https://www.jb51.net/article/164589.htm" target="_blank" rel="noopener">https://www.jb51.net/article/164589.htm</a><br><a href="https://blog.csdn.net/m0_37407756/article/details/79903837" target="_blank" rel="noopener">https://blog.csdn.net/m0_37407756/article/details/79903837</a></p><hr><h1 id="查看"><a href="#查看" class="headerlink" title="查看"></a>查看</h1><p>我们可以在终端中使用<code>df</code>命令来查看磁盘的利用情况。<br>另外，可以加一个<code>-h</code>即“human reading”使显示的磁盘利用状况列表更加适合我们阅读（主要是转化了单位和列名）。</p><hr><h1 id="方法"><a href="#方法" class="headerlink" title="方法"></a>方法</h1><ol><li><h2 id="清理apt缓存文件"><a href="#清理apt缓存文件" class="headerlink" title="清理apt缓存文件"></a>清理apt缓存文件</h2><p>ubuntu在/var/cache/apt/archives目录中会保留deb软件包的缓冲文件。随着时间的推移，这些缓存可能会占有很多空间。<br>我们可以使用<code>sudo du -sh /var/cache/apt</code>来查看当前apt缓存文件的占用的大小。<br>我们可以直接在终端执行如下命令以清理过时的软件包：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo apt-get autoclean</span><br></pre></td></tr></table></figure><p>我们可以在终端中执行以下命令来移除所有apt缓存中的软件包：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo apt-get clean</span><br></pre></td></tr></table></figure><p>实践证明，这两条命令其实清理得不是非常干净（会剩下kb级的缓存），不过如果很久没清理的话，还是非常强力的。</p></li><li><h2 id="删除其他软件依赖的但现在已不用的软件包"><a href="#删除其他软件依赖的但现在已不用的软件包" class="headerlink" title="删除其他软件依赖的但现在已不用的软件包"></a>删除其他软件依赖的但现在已不用的软件包</h2><p>下面这条命令可以移除系统不再需要的依赖库和软件包。这些软件包是自动安装的，是当初为了使得某个安装的软件包满足依赖关系，而此时已不再需要。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo apt-get autoremove</span><br></pre></td></tr></table></figure><p>除了移除不再被系统需要的孤立软件包，这条命令也会移除安装在系统中的linux旧内核（有更精确的操作方法，有点专业，这里就不说了）。<br>注意，这条命令执行后，软件的配置文件还是会保留的。<br>可以使用<code>purge</code>选项来同时清除软件包和软件的配置文件。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo apt-get autoremove --purge</span><br></pre></td></tr></table></figure><blockquote><p>补充：这里扯点题外话，最近看到一个挺好用的命令<code>apt-get install -f</code>，其作用是修复依赖关系（depends），即假如系统上有某个package不满足依赖条件，这个命令就会自动安装那个package所依赖的package。</p></blockquote></li><li><h2 id="清除缩略图缓存"><a href="#清除缩略图缓存" class="headerlink" title="清除缩略图缓存"></a>清除缩略图缓存</h2><p>可以使用<code>du -sh ~/.cache/thumbnails/</code>查看缩略图缓存占用的空间。<br>其实如果不是摄影爱好者或者类似的使用者的话，这个缓存不会特别大，不过对缓存强迫症患者还是可以清一下的。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">rm -rf ~/.cache/thumbnails/*</span><br></pre></td></tr></table></figure></li><li><h2 id="清除残余配置文件"><a href="#清除残余配置文件" class="headerlink" title="清除残余配置文件"></a>清除残余配置文件</h2><p>可以使用<code>dpkg --list | grep &quot;^rc&quot;</code>查看残余的配置文件，如果没有的话，可以跳过后文。<br>这里的rc表示软件包已经删除（<strong>R</strong>emove），但配置文件（<strong>C</strong>onfig-file）还在的文件。<br>这里具体的介绍可以看一下我新写的文章<a href="https://gsy00517.github.io/ubuntu20200126083448/" target="_blank">ubuntu笔记：安装与卸载deb软件包</a>。<br>若有，咱们来删除：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">dpkg -l |grep ^rc|awk &apos;&#123;print $2&#125;&apos; |sudo xargs dpkg -P</span><br></pre></td></tr></table></figure><p>或者</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">dpkg --list | grep &quot;^rc&quot; | cut -d &quot; &quot; -f 3 | xargs sudo dpkg --purge</span><br></pre></td></tr></table></figure><p>这时候如果出现如下错误，那无需担心，因为已经不存在残余的配置文件了。</p><img src="/ubuntu20190914094853/已无残余文件.png" title="已无残余文件"></li></ol><p>可以把上面的命令按顺序执行一遍，就完成了对ubuntu系统的空间释放。</p><script type="text/javascript" src="/js/src/bai.js"></script><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      
      
        &lt;!-- build time:Tue Jan 28 2020 15:40:41 GMT+0800 (GMT+08:00) --&gt;&lt;p&gt;前一篇讲了如何清理windows下的空间，然而虽然ubuntu中垃圾文件没win10那么多，可是我给ubuntu分配的空间比win10少得多了，
      
    
    </summary>
    
    
      <category term="操作和使用" scheme="https://gsy00517.github.io/categories/%E6%93%8D%E4%BD%9C%E5%92%8C%E4%BD%BF%E7%94%A8/"/>
    
    
      <category term="命令操作" scheme="https://gsy00517.github.io/tags/%E5%91%BD%E4%BB%A4%E6%93%8D%E4%BD%9C/"/>
    
      <category term="ubuntu" scheme="https://gsy00517.github.io/tags/ubuntu/"/>
    
  </entry>
  
  <entry>
    <title>windows笔记：释放空间</title>
    <link href="https://gsy00517.github.io/windows20190914091023/"/>
    <id>https://gsy00517.github.io/windows20190914091023/</id>
    <published>2019-09-14T01:10:23.000Z</published>
    <updated>2020-01-19T00:27:28.382Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Tue Jan 28 2020 15:40:41 GMT+0800 (GMT+08:00) --><p>暑假里想跑CVPR中的代码，发现作者提供的环境配置都是基于linux终端的，这样windows的git bash就满足不了我了。二话不说我花了两天时间装了个ubuntu+windows双系统，好不容易装好了，却发现我的硬盘空间已经岌岌可危（理论上要留内存的三倍左右可以保证系统顺畅运行，我的内存是8G，也就是说我C盘应空出20G左右为宜）。于是我就找了些释放空间的办法，分享在这里。</p><p><strong>References</strong>：</p><p>电子文献：<br><a href="http://www.udaxia.com/wtjd/9147.html" target="_blank" rel="noopener">http://www.udaxia.com/wtjd/9147.html</a></p><hr><h1 id="利用磁盘属性进行清理"><a href="#利用磁盘属性进行清理" class="headerlink" title="利用磁盘属性进行清理"></a>利用磁盘属性进行清理</h1><p>这是最稳的方法，但释放的空间也相对较少，不过还是有效的。选择“此电脑”，右键C盘，属性，然后就可以在常规下面看到磁盘清理。一般按默认选择的进行清理，当然全点上勾也无所谓。<br><img src="/windows20190914091023/磁盘清理.png" title="磁盘清理"></p><blockquote><p>注意：千万不能选择压缩此驱动器以节约磁盘空间！</p></blockquote><p>另外在工具下面你可以看到一个优化的选项，一般系统会定期自动执行优化，如果你是强迫症，时时刻刻都容不得一点冗余的话，可以手动优化。<br><img src="/windows20190914091023/优化.png" title="优化"><br>据我们数据结构的老师说，由于数据在存储时大多是稀疏矩阵，存在许多的空间浪费，而磁盘碎片整理优化的就是这个。</p><hr><h1 id="清理系统文件"><a href="#清理系统文件" class="headerlink" title="清理系统文件"></a>清理系统文件</h1><p>在前面的磁盘清理界面中我们还可以看到清理系统文件这一选项，可以选择它进行进一步清理，这里面有一项是以前的windows安装文件，如果不打算回退的话清理无妨。<br><img src="/windows20190914091023/清理系统文件.png" title="清理系统文件"><br>有可能你还会注意到一个名为“系统错误内存转储文件”的选项，这个也可以大胆清除，对一般使用者（基本不需排查系统问题）这个文件完全没有任何作用。<br><img src="/windows20190914091023/系统错误内存转储文件.png" title="系统错误内存转储文件"><br>对于防止系统错误内存转储文件占用空间，还有一劳永逸直接禁止生成的办法，首先打开高级系统设置，来到如图所示选项卡。<br><img src="/windows20190914091023/高级系统设置.png" title="高级系统设置"><br>打开“启动和故障修复”设置窗口，在写入调试信息的下拉列表中选择“无”并确定即可。<br><img src="/windows20190914091023/禁止生成.png" title="禁止生成"><br>当然，如果你觉得你或许用得上系统错误内存转储文件，那么也可以选择这里的小内存转储或者核心内存转储，这样同样也能节省空间。<br>清理系统文件是给windows10瘦身最有效的办法之一，事实上，若无特殊需求，扫描结果中的文件均可以勾选清除。</p><hr><h1 id="删除临时文件"><a href="#删除临时文件" class="headerlink" title="删除临时文件"></a>删除临时文件</h1><p>这里有两个临时文件中的全部文件可以删除，一个是C:\Users\用户名\AppData\Local\Temp目录下的文件，这里是临时文件最多的地方，可以上到几个G；另一个是C:\Windows\Temp，这里文件大小相对较小，可以忽略不计。另外我也找到了一些其他的临时文件，但似乎它们的体积都是0，可能是系统自动清理了。</p><blockquote><p>注意：千万不要误删上一级目录！如果担心删除出错，可先放到回收站，重启之后看有无异常再做决定。亲测上述两个文件夹中的所有文件均可删除。</p></blockquote><hr><h1 id="删除冗余更新"><a href="#删除冗余更新" class="headerlink" title="删除冗余更新"></a>删除冗余更新</h1><p>众所周知，windows系统会自动更新，这也是许多人弃windows的一大原因，然而windows还是要用的，于是我找到了一种可以清理多余的windows更新文件的方法。</p><ol><li>首先右键左下角开始菜单，选择“Windows PowerShell（管理员）”<img src="/windows20190914091023/powershell.png" title="powershell"> 弹出是否允许进行更改选择“是”。</li><li>输入命令<code>dism.exe /Online /Cleanup-Image /AnalyzeComponentStore</code>并执行。</li><li>这时候会显示“推荐使用组件存储清理：是or否”，因为我前不久清过，所以这里显示为“否”，那么就别清理了。如果显示为“是”，那么进行第四步。<img src="/windows20190914091023/输出.png" title="输出"></li><li>输入命令<code>dism.exe /online /Cleanup-Image /StartComponentCleanup</code>并执行。<br>这样电脑就会开始清理啦。这个过程会比较长，不要着急和担心，在这期间你可以做些别的事情，比如看看我其他的几篇博客。</li></ol><hr><h1 id="重装系统"><a href="#重装系统" class="headerlink" title="重装系统"></a>重装系统</h1><p>俗话说得好“大力出奇迹”，重装系统无疑是最有效清理空间的一种方法。只要备份好数据，重装其实没有想象的那么困难。我本人这一年多来就重装过3次系统(两次是被迫的…)，其实装了几次就熟练了，我曾看到某linux大牛（忘了是谁）总共重装了19次linux。你可以在我的另一篇博文<a href="https://gsy00517.github.io/ubuntu20190914100050/" target="_blank">ubuntu笔记：重装ubuntu——记一次辛酸血泪史</a>中看到如何在双系统情况下重装ubuntu的过程。</p><script type="text/javascript" src="/js/src/bai.js"></script><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      
      
        &lt;!-- build time:Tue Jan 28 2020 15:40:41 GMT+0800 (GMT+08:00) --&gt;&lt;p&gt;暑假里想跑CVPR中的代码，发现作者提供的环境配置都是基于linux终端的，这样windows的git bash就满足不了我了。二话不说我花了两
      
    
    </summary>
    
    
      <category term="操作和使用" scheme="https://gsy00517.github.io/categories/%E6%93%8D%E4%BD%9C%E5%92%8C%E4%BD%BF%E7%94%A8/"/>
    
    
      <category term="命令操作" scheme="https://gsy00517.github.io/tags/%E5%91%BD%E4%BB%A4%E6%93%8D%E4%BD%9C/"/>
    
      <category term="windows" scheme="https://gsy00517.github.io/tags/windows/"/>
    
  </entry>
  
  <entry>
    <title>front end笔记：制作web时的一些小技巧与小问题</title>
    <link href="https://gsy00517.github.io/front-end20190914080224/"/>
    <id>https://gsy00517.github.io/front-end20190914080224/</id>
    <published>2019-09-14T00:02:24.000Z</published>
    <updated>2019-11-10T09:11:30.966Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Tue Jan 28 2020 15:40:41 GMT+0800 (GMT+08:00) --><p>大一有一段时间，我沉迷于web前端制作网页，比较熟练地掌握了html的语法，还根据需要接触了一些CSS以及js的内容。说白了，html只是一种标记语言（不属于编程语言），但是它简单易学，且很容易获得可视化的效果，对于培养兴趣而言我感觉是很有帮助的。油管up主，现哈佛在读学霸<a href="https://www.youtube.com/channel/UC5Gmg-VtFmnP8qLq8V7Pvtg" target="_blank">John Fish</a>（请科学上网）当初就是从html进入计算机世界的。下面贴一个我自己做的网页，是综合web三大语言编写的，大一的时候把自己需要的网站都放上面了，也有一种归属感吧。<br><img src="/front-end20190914080224/前端三大语言.jpg" title="前端三大语言"><br><img src="/front-end20190914080224/大一做的简单网页.png" title="大一做的简单网页"><br><img src="/front-end20190914080224/打开菜鸟教程.png" title="打开菜鸟教程"><br>主页上那个是python之禅，也是我很喜欢的一段文字，在python环境下import this就可以看到。由于上面的网页是我学web的时候边看书边编的，各种元素都尝试了一下，最后也没有美化一直到现在，所以大佬们勿喷哈。</p><hr><h1 id="小技巧"><a href="#小技巧" class="headerlink" title="小技巧"></a>小技巧</h1><p>我这里强烈推荐使用VScode写前端，它有很多强大的插件，我这里推荐其中一个吧。<br><img src="/front-end20190914080224/推荐插件.png" title="推荐插件"><br>如介绍所写，使用alt+B快捷键可以直接在默认浏览器下查看你写的网页，而shift+alt+B可以选浏览器查看，因为有些时候microsoft自带的edge浏览器无法实现你编写的效果（巨坑），推荐使用chrome打开浏览。使用这个插件能让你更快捷地预览你编写的效果并进行修改，大大提高了效率。<br>其他的插件网上有很多推荐，也等待着你自己去发现，这里就不一一列出了。<br>还有一个快捷的操作就是快速生成代码块，在VScode中是这样操作的（其他编辑器也应该类似）：</p><ol><li>输入一个！：<img src="/front-end20190914080224/输入！.png" title="输入！"></li><li>按tab键或者回车：<img src="/front-end20190914080224/快速生成.png" title="快速生成"> 这样就可以节省很多时间，非常方便。</li></ol><hr><h1 id="小问题"><a href="#小问题" class="headerlink" title="小问题"></a>小问题</h1><p>在我想使用web来打开我本地的txt文件时，我遇到过这样一个问题：打开的中文文档在浏览器中显示为乱码。在尝试其他浏览器后，我发现这不是浏览器的问题。最后我大致找到了两种解决办法：</p><ol><li>一种是找到head下面的meta charset，修改代码如下：<img src="/front-end20190914080224/解决方法1.png" title="解决方法1"></li><li>另一种是另存为文件时修改一下格式，这里我们修改成“UTF-8”。另外我室友在使用python导入文件的时候也因为格式导致报错，修改成ANSI后即可。<img src="/front-end20190914080224/解决方法2.png" title="解决方法2"> 希望通过上面两种方法的尝试能让你解决乱码问题，几种编码格式的区别在这里暂不说明，以后有空补上。</li></ol><script type="text/javascript" src="/js/src/bai.js"></script><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      
      
        &lt;!-- build time:Tue Jan 28 2020 15:40:41 GMT+0800 (GMT+08:00) --&gt;&lt;p&gt;大一有一段时间，我沉迷于web前端制作网页，比较熟练地掌握了html的语法，还根据需要接触了一些CSS以及js的内容。说白了，html只是一种标
      
    
    </summary>
    
    
      <category term="环境配置" scheme="https://gsy00517.github.io/categories/%E7%8E%AF%E5%A2%83%E9%85%8D%E7%BD%AE/"/>
    
    
      <category term="前端" scheme="https://gsy00517.github.io/tags/%E5%89%8D%E7%AB%AF/"/>
    
  </entry>
  
  <entry>
    <title>anaconda笔记：conda的各种命令行操作</title>
    <link href="https://gsy00517.github.io/anaconda20190913231748/"/>
    <id>https://gsy00517.github.io/anaconda20190913231748/</id>
    <published>2019-09-13T15:17:48.000Z</published>
    <updated>2020-01-08T13:35:40.872Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Tue Jan 28 2020 15:40:41 GMT+0800 (GMT+08:00) --><p>anaconda是一个开源的包、环境管理器，可以比较有效地配置多个虚拟环境，当python入门到一定程度时，安装anaconda是很必要的。前段时间室友学习python的时候问到过我一些相关的问题，我就在这里简单写一些我知道的以及我搜集到的知识。</p><hr><h1 id="环境变量"><a href="#环境变量" class="headerlink" title="环境变量"></a>环境变量</h1><p>安装anaconda过程中一个很重要的步骤就是配置环境变量，网上有很多手动添加环境变量的教程，其实很简单，只需添加三个路径，当然更简单的是直接在安装的时候添加到path（可以无视warning）。我想在这里写的是环境变量的概念问题，其实直到不久前帮同学安装我才明白。<br>环境变量是指在操作系统中用来指定操作系统运行环境的一些参数。当要求系统运行一个程序而没有告诉它程序所在的完整路径时，系统除了在当前目录下面寻找此程序外，还会到path中指定的路径去找。这就是为什么不添加C:\Users\用户名\Anaconda3\Scripts到path就无法执行conda命令，因为此时conda.exe无法被找到。</p><hr><h1 id="conda与pip"><a href="#conda与pip" class="headerlink" title="conda与pip"></a>conda与pip</h1><p>利用conda install与pip install命令来安装各种包的过程中，想必你也对两者之间的区别很疑惑，下面我就总结一下我搜集到的相关解答。<br>简而言之，pip是python包的通用管理器，而conda是一个与语言无关的跨平台环境管理器。对我们而言，最显着的区别可能是这样的：pip在任何环境中安装python包，conda安装在conda环境中装任何包。因此往往conda list的数量会大于pip list。<br>要注意的是，如果使用conda install多个环境时，对于同一个包只需要安装一次，有conda集中进行管理。<br>但是如果使用pip，因为每个环境安装使用的pip在不同的路径下，故会重复安装，而包会从缓存中取。<br>总的来说，我推荐尽早安装anaconda并且使用conda来管理python的各种包。</p><hr><h1 id="升级"><a href="#升级" class="headerlink" title="升级"></a>升级</h1><p>我们可以在命令行中或者anaconda prompt中执行命令进行操作。<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">conda update conda #升级conda</span><br><span class="line">conda update anaconda #升级anaconda前要先升级conda</span><br><span class="line">conda update --all #升级所有包</span><br></pre></td></tr></table></figure><p></p><p>在升级完成之后，我们可以使用命令来清理一些无用的包以释放一些空间：<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">conda clean -p #删除没有用的包</span><br><span class="line">conda clean -t #删除保存下来的压缩文件（.tar）</span><br></pre></td></tr></table></figure><p></p><hr><h1 id="虚拟环境"><a href="#虚拟环境" class="headerlink" title="虚拟环境"></a>虚拟环境</h1><p>conda list命令用于查看conda下的包，而conda env list命令可以用来查看conda创建的所有虚拟环境。<br><img src="/anaconda20190913231748/conda环境列表.png" title="conda环境列表"><br>下面就简述一下如何创建这些虚拟环境。<br>使用如下命令，可以创建一个新的环境：<br><code>conda create -n Python27 python=2.7</code><br>其中Python27是自定义的一个名称，而python=2.7是一个格式，可以变动等号右边的数字来改变python环境的kernel版本，这里我们安装的是python2.7版本（将于2020年停止维护）。<br>在anaconda prompt中，我们可以看到我们处在的是base环境下，也就是我安装的python3环境下，我们可以使用下面两个命令来切换环境：<br><img src="/anaconda20190913231748/虚拟环境切换.png" title="虚拟环境切换"><br>在创建环境的过程中，难免会不小心取了个难听的环境名，别担心，我们有方法来删除环境。<br><code>conda remove -n 难听的名字 --all</code><br>有时候一个环境已经配置好了，但我们想要重命名，这怎么办呢？可以这样办：<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">conda create -n 新名字 --clone 老名字</span><br><span class="line">conda remove -n 老名字 --all</span><br></pre></td></tr></table></figure><p></p><hr><h1 id="把环境添加到jupyter-notebook"><a href="#把环境添加到jupyter-notebook" class="headerlink" title="把环境添加到jupyter notebook"></a>把环境添加到jupyter notebook</h1><p>首先通过activate进入想要添加的环境中，然后安装ipykernel，接下来就可以进行添加了。<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">pip install ipykernel</span><br><span class="line">python -m ipykernel install --name Python27 #Python27可以取与环境名不一样的名字，但方便起见建议统一</span><br></pre></td></tr></table></figure><p></p><p>我们可以使用如下命令来查看已添加到jupyter notebook的kernel：<br><code>jupyter kernelspec list</code><br>显示如下：<br><img src="/anaconda20190913231748/查看kernel.png" title="查看kernel"><br>我们也可以在jupyter notebook中的new或者kernel下查看新环境是否成功添加。<br>若想删除某个指定的kernel，可以使用命令<code>jupyter kernelspec remove kernel_name</code>来完成。<br>在这里我想说明一下为什么要分开python的环境。<br>由于python是不向后兼容的，分开环境可以避免语法版本不一引起的错误，同时这也可以避免工具包安装与调用的混乱。</p><script type="text/javascript" src="/js/src/bai.js"></script><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      
      
        &lt;!-- build time:Tue Jan 28 2020 15:40:41 GMT+0800 (GMT+08:00) --&gt;&lt;p&gt;anaconda是一个开源的包、环境管理器，可以比较有效地配置多个虚拟环境，当python入门到一定程度时，安装anaconda是很必要的。前段
      
    
    </summary>
    
    
      <category term="环境配置" scheme="https://gsy00517.github.io/categories/%E7%8E%AF%E5%A2%83%E9%85%8D%E7%BD%AE/"/>
    
    
      <category term="命令操作" scheme="https://gsy00517.github.io/tags/%E5%91%BD%E4%BB%A4%E6%93%8D%E4%BD%9C/"/>
    
  </entry>
  
  <entry>
    <title>jupyter notebook笔记：一些细小的操作</title>
    <link href="https://gsy00517.github.io/jupyter-notebook20190913225022/"/>
    <id>https://gsy00517.github.io/jupyter-notebook20190913225022/</id>
    <published>2019-09-13T14:50:22.000Z</published>
    <updated>2019-11-10T09:11:43.074Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Tue Jan 28 2020 15:40:41 GMT+0800 (GMT+08:00) --><p>首先强烈安利jupyter notebook，它是一种交互式笔记本，安装anaconda的时候会一并安装，下载VS的时候也可以选择安装。<br>我是在初学机器学习的时候接触jupyter notebook的，立刻就被它便捷的交互与结果呈现方式所吸引，现在python编程基本不使用其他的软件。其实完全可以在初学python的时候使用jupyter notebook，可以立即得到反馈以及分析错误，可以进步很快！</p><hr><h1 id="打开操作"><a href="#打开操作" class="headerlink" title="打开操作"></a>打开操作</h1><p>当初安装好之后还不了解，每次打开jupyter notebook都会先弹出一个黑框框，这时候千万别关掉，等一会就能来到网页。另外打开之后也别关掉，使用的时候是一直需要的，因为只有开着才能访问本机web服务器发布的内容。<br><img src="/jupyter-notebook20190913225022/神秘的黑框框.png" title="神秘的黑框框"><br>另外你也可以不通过快捷方式，直接在命令行中直接输入jupyter notebook来打开它。<br>有些时候，当你插入硬盘或者需要直接在特定的目录下打开jupyter notebook（它的默认打开是在“usr/用户名/”路径下），那你可以在输入命令的后面加上你想要的路径。<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">jupyter notebook D:\ #打开D盘，于我是我的移动硬盘</span><br><span class="line">jupyter notebook E:\ #我的U盘</span><br><span class="line">jupyter notebook C:\Users\用户名\Desktop #在桌面打开</span><br></pre></td></tr></table></figure><p></p><hr><h1 id="快捷键操作"><a href="#快捷键操作" class="headerlink" title="快捷键操作"></a>快捷键操作</h1><p>在jupyter notebook中可以通过选中cell然后按h的方式查询快捷键。<br><img src="/jupyter-notebook20190913225022/快捷键查询.png" title="快捷键查询"></p><hr><h1 id="其他"><a href="#其他" class="headerlink" title="其他"></a>其他</h1><p>在jupyter notebook中可以直接使用markdown，这对学习可以起到很大的辅助作用，markdown的基本操作可以看我的另一篇博文<a href="https://gsy00517.github.io/markdown20190913211144/" target="_blank">markdown笔记：markdown的基本使用</a><br>此外，在我的博文<a href="https://gsy00517.github.io/anaconda20190913231748/" target="_blank">anaconda笔记：conda的各种命令行操作</a>中，也介绍了如何将python的虚拟环境添加到jupyter notebook中，欢迎阅读。<br>VScode前段时间也开始支持ipynb，喜欢高端暗黑科技风又懒得自己修改jupyter notebook的小伙伴可以试一试，不过我在kernel配置方面似乎还有一些小问题有待解决。</p><script type="text/javascript" src="/js/src/bai.js"></script><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      
      
        &lt;!-- build time:Tue Jan 28 2020 15:40:41 GMT+0800 (GMT+08:00) --&gt;&lt;p&gt;首先强烈安利jupyter notebook，它是一种交互式笔记本，安装anaconda的时候会一并安装，下载VS的时候也可以选择安装。&lt;br&gt;
      
    
    </summary>
    
    
      <category term="环境配置" scheme="https://gsy00517.github.io/categories/%E7%8E%AF%E5%A2%83%E9%85%8D%E7%BD%AE/"/>
    
    
      <category term="命令操作" scheme="https://gsy00517.github.io/tags/%E5%91%BD%E4%BB%A4%E6%93%8D%E4%BD%9C/"/>
    
  </entry>
  
  <entry>
    <title>markdown笔记：markdown的基本使用</title>
    <link href="https://gsy00517.github.io/markdown20190913211144/"/>
    <id>https://gsy00517.github.io/markdown20190913211144/</id>
    <published>2019-09-13T13:11:44.000Z</published>
    <updated>2020-01-27T05:19:43.743Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Tue Jan 28 2020 15:40:41 GMT+0800 (GMT+08:00) --><p>既然要写技术博客，那么markdown肯定是必备的了，这篇文章就来介绍一下markdown的基本使用操作。</p><p><strong>References</strong>：</p><p>电子文献：<br><a href="https://www.jianshu.com/p/191d1e21f7ed" target="_blank" rel="noopener">https://www.jianshu.com/p/191d1e21f7ed</a></p><hr><h1 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a>介绍</h1><p>Markdown是一种可以使用普通文本编辑器编写的标记语言，其功能比纯文本更强，因此许多程序员用它来写blog。在这里我先推荐一款markdown编辑器——<a href="https://www.typora.io/" target="_blank">typora</a>，大家可以免费下载使用。</p><hr><h1 id="注意"><a href="#注意" class="headerlink" title="注意"></a>注意</h1><p>在我刚开始使用markdown的时候总是跳进这个坑，在这里提上来提醒一下，在使用markdown标记后要添加文字时，需要在相应标记后空一格，否则标记也会被当作文本来处理，例如我输入“#####错误”时：</p><h5 id="错误"><a href="#错误" class="headerlink" title="错误"></a>错误</h5><p>正确的做法是输入“##### 正确”：</p><h5 id="正确"><a href="#正确" class="headerlink" title="正确"></a>正确</h5><p>一种简单的判别方法就是使用IDE，这样对应的标记就会有语法高亮。</p><hr><h1 id="使用"><a href="#使用" class="headerlink" title="使用"></a>使用</h1><ol><li><h2 id="标题"><a href="#标题" class="headerlink" title="标题"></a>标题</h2><p>话不多说，直接示范：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"># 这是一级标题</span><br><span class="line">## 这是二级标题</span><br><span class="line">### 这是三级标题</span><br><span class="line">#### 这是四级标题</span><br><span class="line">##### 这是五级标题</span><br><span class="line">###### 这是六级标题</span><br></pre></td></tr></table></figure><p>效果如下：</p><h1 id="这是一级标题"><a href="#这是一级标题" class="headerlink" title="这是一级标题"></a>这是一级标题</h1><h2 id="这是二级标题"><a href="#这是二级标题" class="headerlink" title="这是二级标题"></a>这是二级标题</h2><h3 id="这是三级标题"><a href="#这是三级标题" class="headerlink" title="这是三级标题"></a>这是三级标题</h3><h4 id="这是四级标题"><a href="#这是四级标题" class="headerlink" title="这是四级标题"></a>这是四级标题</h4><h5 id="这是五级标题"><a href="#这是五级标题" class="headerlink" title="这是五级标题"></a>这是五级标题</h5><h6 id="这是六级标题"><a href="#这是六级标题" class="headerlink" title="这是六级标题"></a>这是六级标题</h6></li><li><h2 id="字体"><a href="#字体" class="headerlink" title="字体"></a>字体</h2><p>还是直接示范：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">**这是加粗的文字**</span><br><span class="line">*这是倾斜的文字*`</span><br><span class="line">***这是斜体加粗的文字***</span><br><span class="line">~~这是加删除线的文字~~</span><br></pre></td></tr></table></figure><p><strong>这是加粗的文字</strong><br><em>这是倾斜的文字</em><br><strong><em>这是斜体加粗的文字</em></strong><br><del>这是加删除线的文字</del></p></li><li><h2 id="引用"><a href="#引用" class="headerlink" title="引用"></a>引用</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">&gt;我引用</span><br><span class="line">&gt;&gt;我还引用</span><br><span class="line">&gt;&gt;&gt;我再引用</span><br><span class="line">&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;扶我起来，我还能继续引用！</span><br></pre></td></tr></table></figure><blockquote><p>我引用</p><blockquote><p>我还引用</p><blockquote><p>我再引用</p><blockquote><blockquote><blockquote><blockquote><blockquote><blockquote><blockquote><blockquote><blockquote><blockquote><p>扶我起来，我还能继续引用！</p></blockquote></blockquote></blockquote></blockquote></blockquote></blockquote></blockquote></blockquote></blockquote></blockquote></blockquote></blockquote></blockquote><p>引用是可以嵌套的，可以加很多层，我一般使用一个&gt;来表示额外的需要注意的内容。另外，如果想让下一段文字不被引用，需要空一行。</p></li><li><h2 id="分割线"><a href="#分割线" class="headerlink" title="分割线"></a>分割线</h2><p>分割线使用三个及以上的<code>-</code>或<code>*</code>就可以。<br>有时候用<code>---</code>会造成别的文字的格式变化，因此我在使用VScode编辑时，如果看到<code>---</code>被高亮（分割线正常其作用时应该不高亮），就会改用<code>***</code>。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">---</span><br><span class="line">***</span><br></pre></td></tr></table></figure><p>效果如下：</p><hr><hr></li><li><h2 id="图片"><a href="#图片" class="headerlink" title="图片"></a>图片</h2><p>markdown中添加图片的语法是这样的：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">![显示在图片下方的文字](图片地址 &quot;图片title&quot;)</span><br></pre></td></tr></table></figure><p>其中title可加可不加，它就是鼠标移动到图片上时显示的文字。<br>然而我在使用hexo搭建我的个人博客的过程中，遇到了使用上述语法图片却无法显示的情况，因此我改用了下列标签插件：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&#123;% asset_img xxxxx.xxx 图片下方的名字 %&#125;</span><br></pre></td></tr></table></figure><p>其中xxxxx.xxx只需直接输入图片名称以及格式即可，因为我使用了hexo-asset-image插件，它可以在_posts文件中创建与博文名称相同的对应的文件夹，只需把图片移入即可。注意，这里的图片名中间不能有空格，否则会加载失败（它会以为图片名称到第一个空格为止）。<br>其安装命令：<code>npm install hexo-asset-image --save</code><br>也可用cnpm更快地安装：<code>cnpm install hexo-asset-image --save</code></p><blockquote><p>补充：后来发现在<a href="https://gsy00517.github.io/about/" target="_blank">关于本人</a>中无法使用上述asset_img标签插件来对图片进行插入，故又尝试了<code>![显示在图片下方的文字](图片地址 &quot;图片title&quot;)</code>的方法，发现可行！原因可能是之前误用了中文括号导致的。可以参考一下<a href="https://blog.csdn.net/Fitz1318/article/details/86548129" target="_blank">Hexo文章中插入图片的方法</a>。</p></blockquote><p>在插入图片的后面，会留有一小段空白区，看着不舒服的话可以不要回车，即直接在插入图片的语句后面跟进下一段的文字或者图片等，这样行间隙就会小很多。<br>其实在hexo中可以直接使用img标签，它会自行处理，并且这样还更方便调整高度和宽度。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&lt;img src=&quot;&quot; width=&quot;50%&quot; height=&quot;50%&quot;&gt;</span><br></pre></td></tr></table></figure></li><li><h2 id="超链接"><a href="#超链接" class="headerlink" title="超链接"></a>超链接</h2><p>由于我希望在新的页面打开链接，而似乎markdown本身的语法不支持在新标签页打开链接，因此我推荐直接使用html语言来代替。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&lt;a href=&quot;超链接地址&quot; target=&quot;_blank&quot;&gt;超链接名&lt;/a&gt;</span><br></pre></td></tr></table></figure></li><li><h2 id="列表"><a href="#列表" class="headerlink" title="列表"></a>列表</h2><ul><li><p>无序列表</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">- 列表内容</span><br><span class="line">+ 列表内容</span><br><span class="line">* 列表内容</span><br></pre></td></tr></table></figure><ul><li>列表内容</li></ul><ul><li>列表内容</li></ul><ul><li>列表内容</li></ul></li><li><p>有序列表</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">1. 列表内容</span><br><span class="line">2. 列表内容</span><br><span class="line">3. 列表内容</span><br></pre></td></tr></table></figure><ol><li>列表内容</li><li>列表内容</li><li>列表内容<br>可以看到，上面显示的列表是有嵌套的，方法就是敲三个空格缩进。</li></ol></li></ul></li><li><h2 id="表格"><a href="#表格" class="headerlink" title="表格"></a>表格</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">表头|表头|表头</span><br><span class="line">---|:--:|---:</span><br><span class="line">内容|内容|内容</span><br><span class="line">内容|内容|内容</span><br></pre></td></tr></table></figure><p>其中第二行的作用分割表头和内容，<code>-</code>有一个就行，为了对齐可多加几个。<br>此外文字默认居左，有两种改变方法：<br>两边加：表示文字居中。<br>右边加：表示文字居右。<br>然而我在hexo使用表格时，出现了无法正常转换的问题，因此我改用了如下HTML的表格形式。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">&lt;table border=&quot;1&quot;&gt;</span><br><span class="line">&lt;tr&gt;</span><br><span class="line">&lt;td&gt;第一行第一列&lt;/td&gt;</span><br><span class="line">&lt;td&gt;第一行第二列&lt;/td&gt;</span><br><span class="line">&lt;/tr&gt;</span><br><span class="line">&lt;tr&gt;</span><br><span class="line">&lt;td&gt;第二行第一列&lt;/td&gt;</span><br><span class="line">&lt;td&gt;第三行第二列&lt;/td&gt;</span><br><span class="line">&lt;/tr&gt;</span><br><span class="line">&lt;/table&gt;</span><br></pre></td></tr></table></figure><p>效果如下：</p><table border="1"><tr><td>第一行第一列</td><td>第一行第二列</td></tr><tr><td>第二行第一列</td><td>第二行第二列</td></tr></table></li><li><h2 id="代码"><a href="#代码" class="headerlink" title="代码"></a>代码</h2><p>最后的最后，是我最喜欢ctrl+C+V的代码了。<br>单行或句中代码输入方式：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">`来复制我呀`</span><br></pre></td></tr></table></figure><p>显示：<br><code>来复制我呀</code><br>其中`在键盘的左上角，我当初找了好久。<br>多行代码块的写法就是用上下两对```围住。<br>好了于是你现在就可以自由的复制粘贴啦。</p><img src="/markdown20190913211144/如何写代码.JPG" title="如何写代码"></li></ol><script type="text/javascript" src="/js/src/bai.js"></script><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      
      
        &lt;!-- build time:Tue Jan 28 2020 15:40:41 GMT+0800 (GMT+08:00) --&gt;&lt;p&gt;既然要写技术博客，那么markdown肯定是必备的了，这篇文章就来介绍一下markdown的基本使用操作。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Refe
      
    
    </summary>
    
    
      <category term="操作和使用" scheme="https://gsy00517.github.io/categories/%E6%93%8D%E4%BD%9C%E5%92%8C%E4%BD%BF%E7%94%A8/"/>
    
    
      <category term="markdown" scheme="https://gsy00517.github.io/tags/markdown/"/>
    
  </entry>
  
  <entry>
    <title>hexo笔记：开始创建个人博客——方法及原因</title>
    <link href="https://gsy00517.github.io/hexo20190913153310/"/>
    <id>https://gsy00517.github.io/hexo20190913153310/</id>
    <published>2019-09-13T07:33:10.000Z</published>
    <updated>2019-11-15T23:32:23.143Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Tue Jan 28 2020 15:40:41 GMT+0800 (GMT+08:00) --><p>大家好，这是我的第一篇博文，这也是我的第一个自己搭建的网站，既然搭了，那第一篇就讲讲我搭建的过程吧。</p><hr><h1 id="安装步骤"><a href="#安装步骤" class="headerlink" title="安装步骤"></a>安装步骤</h1><ol><li><h2 id="安装node-js"><a href="#安装node-js" class="headerlink" title="安装node.js"></a>安装node.js</h2>进入<a href="https://nodejs.org" target="_blank">官网</a>。<br>选择对应系统（我这里用win10），选择LTS（长期支持版本）安装，安装步骤中一直选择next即可。<br>安装完后就可以把安装包删除了。</li><li><h2 id="安装git"><a href="#安装git" class="headerlink" title="安装git"></a>安装git</h2>进入<a href="https://git-scm.com/downloads" target="_blank">官网</a>。<br>选择对应系统的版本下载，同样也是按默认安装。<br>安装成功后，你会在开始菜单中看到git文件夹。 <img src="/hexo20190913153310/成功安装git.png" title="成功安装git"> 其中Git Bash与linux和mac中的终端类似，它是git自带的程序，提供了linux风格的shell，我们可以在这里面执行相应的命令。<blockquote><p>注意：bash中的复制粘贴操作与linux中类似，ctrl+C用于终止进程，可以用鼠标中键进行粘贴操作。不嫌麻烦的话可以使用ctrl+shift+C和ctrl+shift+V进行复制粘贴操作。</p></blockquote></li><li><h2 id="安装hexo"><a href="#安装hexo" class="headerlink" title="安装hexo"></a>安装hexo</h2>hexo是一个快速、简洁且高效的博客框架，在这里我们使用hexo来搭建博客。<br>首先，新建一个名为“blog”的空文件夹，以后我们的操作都在这个文件夹里进行，可以在bash中使用pwd命令查看当前所处位置。<br>创建这个文件夹的目的是万一因为创建的博客出现问题或者不满意想重来等原因可以直接简单地把文件夹删除，也方便了对整个网站本地内容的移动。<br>打开新建的文件夹，右键空白处，选择Git Bash Here。 <img src="/hexo20190913153310/打开bash.png" title="打开bash"> 接下来我们输入两行命令来验证node.js是否安装成功。 <img src="/hexo20190913153310/成功安装node.js.png" title="成功安装node.js"> 如出现如图所示结果，则表明安装成功。<br>为了提高以后的下载速度，我们需要安装cnpm。cnpm是淘宝的国内镜像，因为npm的服务器位于国外有时可能会影响安装。<br>继续在bash中输入如下命令安装cnpm：<br><code>npm install -g cnpm --registry=https://registry.npm.taobao.org</code><br>检验安装是否成功，输入<code>cnpm</code>： <img src="/hexo20190913153310/成功安装cnpm.png" title="成功安装cnpm"> 接下来我们安装hexo，输入命令：<br><code>cnpm install -g hexo-cli</code><br>和上面一样，我们可以用<code>hexo -v</code>来验证是否成功安装hexo，这里就不贴图了。<br>接下来我们输入如下命令来建立整个项目：<br><code>hexo init</code><br>你会发现你的文件夹中多了许多文件，你也可以用ls -l命令来看到新增的文件。 <img src="/hexo20190913153310/新增文件.png" title="新增文件"></li><li><h2 id="完成本地环境的搭建"><a href="#完成本地环境的搭建" class="headerlink" title="完成本地环境的搭建"></a>完成本地环境的搭建</h2><p>至此，我们已经完成了本地环境的搭建，在这里，我想先介绍hexo中常用的命令。<br><code>hexo n &quot;文章标题&quot;</code>用于创建新的博文（欲删除文章，直接删除md文件并用下面的命令更新即可）。<br><code>hexo s</code>hexo会监视文件变动并自动更新，通过所给的localhost:4000/就可以直接在本地预览更新后的网站了。<br>部署到远端服务器三步曲：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">hexo clean #清除缓存，网页正常情况下可以忽略此条命令，执行该指令后，会删掉站点根目录下的public文件夹。</span><br><span class="line">hexo g #generate静态网页（静态网页这里指没有前端后端的网页而不是静止），该命令把md编译为html并存到public文件目录下。</span><br><span class="line">hexo d #将本地的更改部署到远端服务器（需要一点时间，请过一会再刷新网页）。</span><br></pre></td></tr></table></figure><p>此外，上面最后两步也可以使用<code>hexo g -d</code>直接代替。<br>如果出现ERROR Deployer not found: git报错，可以使用<code>npm install --save hexo-deployer-git</code>命令解决。</p><blockquote><p>注意：由于部署到远端输入密码时密码不可见，有时候会导致部署失败，只有出现INFO Deploy done: git的结果才表明部署成功，否则再次部署重输密码即可。</p></blockquote><p>现在我们在bash中运行<code>hexo s</code>，打开浏览器，输入<code>localhost:4000/</code>，就可以看到hexo默认创建的页面了。</p></li><li><h2 id="部署到远端服务器"><a href="#部署到远端服务器" class="headerlink" title="部署到远端服务器"></a>部署到远端服务器</h2>为了让别人能访问到你搭建的网站，我们需要部署到远端服务器。<br>这里有两种选择，一种是部署到github上，新建一个repository，然后创建一个xxxxx.github.io域名（这里xxxxx必须为你的github用户名）。<br>另一种选择是部署到国内的coding，这是考虑到访问速度的问题，不过我选择的是前者，亲测并没感觉有速度的困扰。<br>个人比较推荐用github pages创建个人博客。<br>部署这块网上有许多教程，这里不详细解释了，以后有机会补上。<br>在部署的时候涉及到对主题配置文件的操作，linux和mac用户可以使用vim进行编辑，不过也可以使用VScode、sublime等代码编辑器进行操作。<blockquote><p>注：为了国内的访问速度，我最后添加了coding/github双线部署，两者的操作方式大同小异。值得注意的是，如果使用的是leancloud的第三方阅读量与评论统计系统，那么还得在leancloud的安全中心中添加coding的web域名。</p></blockquote></li></ol><hr><h1 id="创建原因"><a href="#创建原因" class="headerlink" title="创建原因"></a>创建原因</h1><p>首先说明，我只是一个刚起步的入门级小白，懂得不多，别喷我哈~<br>步入大二，虽然我是大学才算真正接触编程，但一年多下来我也接触并且学习了不少技术知识。接触的多了、遇到的问题也复杂了起来，导致每次百度到的答案不一定能够解决我遇到的问题。此外，之前在学习编程语言、操作系统、ml、dl等知识的时候，为方便起见利用文本记了些笔记。然而笔记分散在四处，不方便管理与查看，因此就萌生了写博客的想法。由于个人比较喜欢自由DIY，所以没有使用CSDN、博客园等知名技术博客网站。<br>最后还是非常感谢我们华科的校友<a href="https://www.codesheep.cn/" target="_blank">程序羊</a>在b站和其他站点上分享的各种经验，我就是通过他的<a href="https://www.bilibili.com/video/av44544186" target="_blank">视频</a>来搭建起自己的第一个博客网站的。他的其他视频也给了我很多启迪。<br>最后，最关键的原因，还是因为今天中秋节有空闲的时间哈哈，祝大家节日快乐！</p><script type="text/javascript" src="/js/src/bai.js"></script><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      
      
        &lt;!-- build time:Tue Jan 28 2020 15:40:41 GMT+0800 (GMT+08:00) --&gt;&lt;p&gt;大家好，这是我的第一篇博文，这也是我的第一个自己搭建的网站，既然搭了，那第一篇就讲讲我搭建的过程吧。&lt;/p&gt;&lt;hr&gt;&lt;h1 id=&quot;安装步骤&quot;&gt;
      
    
    </summary>
    
    
      <category term="操作和使用" scheme="https://gsy00517.github.io/categories/%E6%93%8D%E4%BD%9C%E5%92%8C%E4%BD%BF%E7%94%A8/"/>
    
    
      <category term="hexo" scheme="https://gsy00517.github.io/tags/hexo/"/>
    
      <category term="安装教程" scheme="https://gsy00517.github.io/tags/%E5%AE%89%E8%A3%85%E6%95%99%E7%A8%8B/"/>
    
  </entry>
  
</feed>
