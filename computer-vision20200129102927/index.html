<!-- build time:Thu Feb 20 2020 15:54:48 GMT+0800 (GMT+08:00) --><!DOCTYPE html><html class="theme-next gemini use-motion" lang="zh-Hans"><head><meta name="generator" content="Hexo 3.9.0"><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width,initial-scale=1,maximum-scale=1"><meta name="theme-color" content="#222"><meta name="google-site-verification" content="YV24rdmIIf8GuLLOBH5IYEWm0Z3TGAqiLS-LLlspD7w"><script src="/lib/pace/pace.min.js?v=1.0.2"></script><link href="/lib/pace/pace-theme-big-counter.min.css?v=1.0.2" rel="stylesheet"><meta http-equiv="Cache-Control" content="no-transform"><meta http-equiv="Cache-Control" content="no-siteapp"><meta name="baidu-site-verification" content="true"><link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css"><link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css"><link href="/css/main.css?v=5.1.4" rel="stylesheet" type="text/css"><link rel="apple-touch-icon" sizes="180x180" href="/images/haizei.ico?v=5.1.4"><link rel="icon" type="image/png" sizes="32x32" href="/images/haizei.ico?v=5.1.4"><link rel="icon" type="image/png" sizes="16x16" href="/images/haizei.ico?v=5.1.4"><link rel="mask-icon" href="/images/haizei.ico?v=5.1.4" color="#222"><meta name="keywords" content="深度学习,计算机视觉,目标检测,"><link rel="alternate" href="/atom.xml" title="高深远的博客" type="application/atom+xml"><meta name="description" content="最近在看SiamRPN系列，结果看着看着就看到Faster RCNN上面去了。尽管这个模型已经有一段时间了，我还是想把通过这两天学习的理解写下来。RPN全称是Region Proposal Network，这里Region Proposal翻译为“区域选取”，也就是“提取候选框”的意思，所以RPN就是用来提取候选框的网络。在Faster RCNN这个结构中，RPN专门用来提取候选框，在RCNN和F"><meta name="keywords" content="深度学习,计算机视觉,目标检测"><meta property="og:type" content="article"><meta property="og:title" content="computer vision笔记：RPN与Faster RCNN"><meta property="og:url" content="https://gsy00517.github.io/computer-vision20200129102927/index.html"><meta property="og:site_name" content="高深远的博客"><meta property="og:description" content="最近在看SiamRPN系列，结果看着看着就看到Faster RCNN上面去了。尽管这个模型已经有一段时间了，我还是想把通过这两天学习的理解写下来。RPN全称是Region Proposal Network，这里Region Proposal翻译为“区域选取”，也就是“提取候选框”的意思，所以RPN就是用来提取候选框的网络。在Faster RCNN这个结构中，RPN专门用来提取候选框，在RCNN和F"><meta property="og:locale" content="zh-Hans"><meta property="og:image" content="https://gsy00517.github.io/computer-vision20200129102927/RCNN.png"><meta property="og:image" content="https://gsy00517.github.io/computer-vision20200129102927/Fast_RCNN.png"><meta property="og:image" content="https://gsy00517.github.io/computer-vision20200129102927/基本结构.jpg"><meta property="og:image" content="https://gsy00517.github.io/computer-vision20200129102927/图像预处理.png"><meta property="og:image" content="https://gsy00517.github.io/computer-vision20200129102927/使用VGG16的Fast_RCNN.jpg"><meta property="og:image" content="https://gsy00517.github.io/computer-vision20200129102927/卷积示意图.jpg"><meta property="og:image" content="https://gsy00517.github.io/computer-vision20200129102927/流程.png"><meta property="og:image" content="https://gsy00517.github.io/computer-vision20200129102927/训练过程.jpg"><meta property="og:image" content="https://gsy00517.github.io/computer-vision20200129102927/使用VGG16的Fast_RCNN.jpg"><meta property="og:image" content="https://gsy00517.github.io/computer-vision20200129102927/生成anchor.jpg"><meta property="og:image" content="https://gsy00517.github.io/computer-vision20200129102927/使用anchor.jpg"><meta property="og:image" content="https://gsy00517.github.io/computer-vision20200129102927/使用VGG16的Fast_RCNN.jpg"><meta property="og:image" content="https://gsy00517.github.io/computer-vision20200129102927/不准.jpg"><meta property="og:image" content="https://gsy00517.github.io/computer-vision20200129102927/微调.jpg"><meta property="og:image" content="https://gsy00517.github.io/computer-vision20200129102927/不影响.png"><meta property="og:image" content="https://gsy00517.github.io/computer-vision20200129102927/proposal_layer.png"><meta property="og:image" content="https://gsy00517.github.io/computer-vision20200129102927/传统方法.jpg"><meta property="og:image" content="https://gsy00517.github.io/computer-vision20200129102927/RoI示意图.jpg"><meta property="og:image" content="https://gsy00517.github.io/computer-vision20200129102927/最新进展.png"><meta property="og:image" content="https://gsy00517.github.io/computer-vision20200129102927/最后再次定位和回归.jpg"><meta property="og:updated_time" content="2020-02-15T14:08:24.171Z"><meta name="twitter:card" content="summary"><meta name="twitter:title" content="computer vision笔记：RPN与Faster RCNN"><meta name="twitter:description" content="最近在看SiamRPN系列，结果看着看着就看到Faster RCNN上面去了。尽管这个模型已经有一段时间了，我还是想把通过这两天学习的理解写下来。RPN全称是Region Proposal Network，这里Region Proposal翻译为“区域选取”，也就是“提取候选框”的意思，所以RPN就是用来提取候选框的网络。在Faster RCNN这个结构中，RPN专门用来提取候选框，在RCNN和F"><meta name="twitter:image" content="https://gsy00517.github.io/computer-vision20200129102927/RCNN.png"><script type="text/javascript" id="hexo.configurations">var NexT=window.NexT||{},CONFIG={root:"/",scheme:"Gemini",version:"5.1.4",sidebar:{position:"left",display:"post",offset:12,b2t:!1,scrollpercent:!0,onmobile:!1},fancybox:!0,tabs:!0,motion:{enable:!0,async:!1,transition:{post_block:"fadeIn",post_header:"slideDownIn",post_body:"slideDownIn",coll_header:"slideLeftIn",sidebar:"slideUpIn"}},duoshuo:{userId:"0",author:"博主"},algolia:{applicationID:"",apiKey:"",indexName:"",hits:{per_page:10},labels:{input_placeholder:"Search for Posts",hits_empty:"We didn't find any results for the search: ${query}",hits_stats:"${hits} results found in ${time} ms"}}}</script><link rel="canonical" href="https://gsy00517.github.io/computer-vision20200129102927/"><meta name="baidu-site-verification" content="o5QfpvLBz5"><title>computer vision笔记：RPN与Faster RCNN | 高深远的博客</title></head><body itemscope itemtype="http://schema.org/WebPage" lang="zh-Hans"><div class="container sidebar-position-left page-post-detail"><div class="headband"></div><header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="header-inner"><div class="site-brand-wrapper"><div class="site-meta"><div class="custom-logo-site-title"><a href="/" class="brand" rel="start"><span class="logo-line-before"><i></i></span> <span class="site-title">高深远的博客</span> <span class="logo-line-after"><i></i></span></a></div><h1 class="site-subtitle" itemprop="description"></h1></div><div class="site-nav-toggle"><button><span class="btn-bar"></span> <span class="btn-bar"></span> <span class="btn-bar"></span></button></div></div><nav class="site-nav"><ul id="menu" class="menu"><li class="menu-item menu-item-home"><a href="/" rel="section"><i class="menu-item-icon fa fa-fw fa-home"></i><br>首页</a></li><li class="menu-item menu-item-about"><a href="/about/" rel="section"><i class="menu-item-icon fa fa-fw fa-user"></i><br>关于</a></li><li class="menu-item menu-item-tags"><a href="/tags/" rel="section"><i class="menu-item-icon fa fa-fw fa-tags"></i><br>标签</a></li><li class="menu-item menu-item-categories"><a href="/categories/" rel="section"><i class="menu-item-icon fa fa-fw fa-th"></i><br>分类</a></li><li class="menu-item menu-item-archives"><a href="/archives/" rel="section"><i class="menu-item-icon fa fa-fw fa-archive"></i><br>最新发布</a></li><li class="menu-item menu-item-new"><a href="/new/" rel="section"><i class="menu-item-icon fa fa-fw fa-history"></i><br>最近阅读</a></li><li class="menu-item menu-item-rank"><a href="/rank/" rel="section"><i class="menu-item-icon fa fa-fw fa-signal"></i><br>热度排名</a></li><li class="menu-item menu-item-search"><a href="javascript:;" class="popup-trigger"><i class="menu-item-icon fa fa-search fa-fw"></i><br>站内搜索</a></li></ul><div class="site-search"><div class="popup search-popup local-search-popup"><div class="local-search-header clearfix"><span class="search-icon"><i class="fa fa-search"></i> </span><span class="popup-btn-close"><i class="fa fa-times-circle"></i></span><div class="local-search-input-wrapper"><input autocomplete="off" placeholder="搜索..." spellcheck="false" type="text" id="local-search-input"></div></div><div id="local-search-result"></div></div></div><div style="Text-align:center;width:100%"><div style="margin:0 auto"><canvas id="canvas" style="width:60%">当前浏览器不支持canvas，请更换浏览器后再试</canvas></div><script>!function(){function t(t,e){for(var a=0;a<l[e].length;a++)for(var n=0;n<l[e][a].length;n++)1==l[e][a][n]&&(h.beginPath(),h.arc(14*(g+2)*t+2*n*(g+1)+(g+1),2*a*(g+1)+(g+1),g,0,2*Math.PI),h.closePath(),h.fill())}function e(){var t=[],e=/(\d)(\d):(\d)(\d):(\d)(\d)/.exec(new Date),a=[];a.push(e[1],e[2],10,e[3],e[4],10,e[5],e[6]);for(var r=c.length-1;r>=0;r--)a[r]!==c[r]&&t.push(r+"_"+(Number(c[r])+1)%10);for(var r=0;r<t.length;r++)n.apply(this,t[r].split("_"));c=a.concat()}function a(){for(var t=0;t<d.length;t++)d[t].stepY+=d[t].disY,d[t].x+=d[t].stepX,d[t].y+=d[t].stepY,(d[t].x>i+g||d[t].y>f+g)&&(d.splice(t,1),t--)}function n(t,e){for(var a=[1,2,3],n=["#3BE","#09C","#A6C","#93C","#9C0","#690","#FB3","#F80","#F44","#C00"],r=0;r<l[e].length;r++)for(var o=0;o<l[e][r].length;o++)if(1==l[e][r][o]){var h={x:14*(g+2)*t+2*o*(g+1)+(g+1),y:2*r*(g+1)+(g+1),stepX:Math.floor(4*Math.random()-2),stepY:-2*a[Math.floor(Math.random()*a.length)],color:n[Math.floor(Math.random()*n.length)],disY:1};d.push(h)}}function r(){o.height=100;for(var e=0;e<c.length;e++)t(e,c[e]);for(var e=0;e<d.length;e++)h.beginPath(),h.arc(d[e].x,d[e].y,g,0,2*Math.PI),h.fillStyle=d[e].color,h.closePath(),h.fill()}var l=[[[0,0,1,1,1,0,0],[0,1,1,0,1,1,0],[1,1,0,0,0,1,1],[1,1,0,0,0,1,1],[1,1,0,0,0,1,1],[1,1,0,0,0,1,1],[1,1,0,0,0,1,1],[1,1,0,0,0,1,1],[0,1,1,0,1,1,0],[0,0,1,1,1,0,0]],[[0,0,0,1,1,0,0],[0,1,1,1,1,0,0],[0,0,0,1,1,0,0],[0,0,0,1,1,0,0],[0,0,0,1,1,0,0],[0,0,0,1,1,0,0],[0,0,0,1,1,0,0],[0,0,0,1,1,0,0],[0,0,0,1,1,0,0],[1,1,1,1,1,1,1]],[[0,1,1,1,1,1,0],[1,1,0,0,0,1,1],[0,0,0,0,0,1,1],[0,0,0,0,1,1,0],[0,0,0,1,1,0,0],[0,0,1,1,0,0,0],[0,1,1,0,0,0,0],[1,1,0,0,0,0,0],[1,1,0,0,0,1,1],[1,1,1,1,1,1,1]],[[1,1,1,1,1,1,1],[0,0,0,0,0,1,1],[0,0,0,0,1,1,0],[0,0,0,1,1,0,0],[0,0,1,1,1,0,0],[0,0,0,0,1,1,0],[0,0,0,0,0,1,1],[0,0,0,0,0,1,1],[1,1,0,0,0,1,1],[0,1,1,1,1,1,0]],[[0,0,0,0,1,1,0],[0,0,0,1,1,1,0],[0,0,1,1,1,1,0],[0,1,1,0,1,1,0],[1,1,0,0,1,1,0],[1,1,1,1,1,1,1],[0,0,0,0,1,1,0],[0,0,0,0,1,1,0],[0,0,0,0,1,1,0],[0,0,0,1,1,1,1]],[[1,1,1,1,1,1,1],[1,1,0,0,0,0,0],[1,1,0,0,0,0,0],[1,1,1,1,1,1,0],[0,0,0,0,0,1,1],[0,0,0,0,0,1,1],[0,0,0,0,0,1,1],[0,0,0,0,0,1,1],[1,1,0,0,0,1,1],[0,1,1,1,1,1,0]],[[0,0,0,0,1,1,0],[0,0,1,1,0,0,0],[0,1,1,0,0,0,0],[1,1,0,0,0,0,0],[1,1,0,1,1,1,0],[1,1,0,0,0,1,1],[1,1,0,0,0,1,1],[1,1,0,0,0,1,1],[1,1,0,0,0,1,1],[0,1,1,1,1,1,0]],[[1,1,1,1,1,1,1],[1,1,0,0,0,1,1],[0,0,0,0,1,1,0],[0,0,0,0,1,1,0],[0,0,0,1,1,0,0],[0,0,0,1,1,0,0],[0,0,1,1,0,0,0],[0,0,1,1,0,0,0],[0,0,1,1,0,0,0],[0,0,1,1,0,0,0]],[[0,1,1,1,1,1,0],[1,1,0,0,0,1,1],[1,1,0,0,0,1,1],[1,1,0,0,0,1,1],[0,1,1,1,1,1,0],[1,1,0,0,0,1,1],[1,1,0,0,0,1,1],[1,1,0,0,0,1,1],[1,1,0,0,0,1,1],[0,1,1,1,1,1,0]],[[0,1,1,1,1,1,0],[1,1,0,0,0,1,1],[1,1,0,0,0,1,1],[1,1,0,0,0,1,1],[0,1,1,1,0,1,1],[0,0,0,0,0,1,1],[0,0,0,0,0,1,1],[0,0,0,0,1,1,0],[0,0,0,1,1,0,0],[0,1,1,0,0,0,0]],[[0,0,0,0,0,0,0],[0,0,1,1,1,0,0],[0,0,1,1,1,0,0],[0,0,1,1,1,0,0],[0,0,0,0,0,0,0],[0,0,0,0,0,0,0],[0,0,1,1,1,0,0],[0,0,1,1,1,0,0],[0,0,1,1,1,0,0],[0,0,0,0,0,0,0]]],o=document.getElementById("canvas");if(o.getContext){var h=o.getContext("2d"),f=100,i=700;o.height=f,o.width=i,h.fillStyle="#f00",h.fillRect(10,10,50,50);var c=[],d=[],g=o.height/20-1;!function(){var t=/(\d)(\d):(\d)(\d):(\d)(\d)/.exec(new Date);c.push(t[1],t[2],10,t[3],t[4],10,t[5],t[6])}(),clearInterval(v);var v=setInterval(function(){e(),a(),r()},50)}}()</script></div></nav></div></header><main id="main" class="main"><div class="main-inner"><div class="content-wrap"><div id="content" class="content"><div id="posts" class="posts-expand"><article class="post post-type-normal" itemscope itemtype="http://schema.org/Article"><div class="post-block"><link itemprop="mainEntityOfPage" href="https://gsy00517.github.io/computer-vision20200129102927/"><span hidden itemprop="author" itemscope itemtype="http://schema.org/Person"><meta itemprop="name" content="高深远"><meta itemprop="description" content><meta itemprop="image" content="/images/lufei.jpg"></span><span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization"><meta itemprop="name" content="高深远的博客"></span><header class="post-header"><h2 class="post-title" itemprop="name headline">computer vision笔记：RPN与Faster RCNN</h2><div class="post-meta"><span class="post-time"><span class="post-meta-item-icon"><i class="fa fa-calendar-o"></i> </span><span class="post-meta-item-text">发表于</span> <time title="创建于" itemprop="dateCreated datePublished" datetime="2020-01-29T10:29:27+08:00">2020-01-29 </time><span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-calendar-check-o"></i> </span><span class="post-meta-item-text">更新于&#58;</span> <time title="更新于" itemprop="dateModified" datetime="2020-02-15T22:08:24+08:00">2020-02-15 </time></span><span class="post-category"><span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-folder-o"></i> </span><span class="post-meta-item-text">分类于</span> <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/人工智能/" itemprop="url" rel="index"><span itemprop="name">人工智能</span> </a></span></span><span class="post-comments-count"><span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-comment-o"></i> </span><a href="/computer-vision20200129102927/#comments" itemprop="discussionUrl"><span class="post-comments-count valine-comment-count" data-xid="/computer-vision20200129102927/" itemprop="commentCount"></span> </a></span><span id="/computer-vision20200129102927/" class="leancloud_visitors" data-flag-title="computer vision笔记：RPN与Faster RCNN"><span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-eye"></i> </span><span class="post-meta-item-text">阅读次数&#58;</span> <span class="leancloud-visitors-count"></span></span><div class="post-wordcount"><span class="post-meta-item-icon"><i class="fa fa-file-word-o"></i> </span><span class="post-meta-item-text">字数统计&#58;</span> <span title="字数统计">5k字 </span><span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-clock-o"></i> </span><span class="post-meta-item-text">阅读时长 &asymp;</span> <span title="阅读时长">18分钟</span></div></div></header><div class="post-body" itemprop="articleBody"><p>最近在看SiamRPN系列，结果看着看着就看到Faster RCNN上面去了。尽管这个模型已经有一段时间了，我还是想把通过这两天学习的理解写下来。<br>RPN全称是Region Proposal Network，这里Region Proposal翻译为“区域选取”，也就是“提取候选框”的意思，所以RPN就是用来提取候选框的网络。在Faster RCNN这个结构中，RPN专门用来提取候选框，在RCNN和Fast RCNN等物体检测架构中，用来提取候选框的方法通常是比较传统的方法，而且比较耗时。而RPN一方面耗时较少，另一方面可以很容易结合到Fast RCNN中，成为一个整体。我们可以认为Faster RCNN所做的创新与改进就是用RPN结合Fast RCNN。它们三者都是based on regional proposal，即预先提取出候选区域，再通过CNN对候选区域进行样本分类（two-stage），这会影响它的速度，达不到YOLO那样的实时性，但从另一方面也保证了它的定位精度。</p><p><strong>References</strong>：</p><p>电子文献：<br><a href="http://www.telesens.co/2018/03/11/object-detection-and-classification-using-r-cnns/" target="_blank" rel="noopener">http://www.telesens.co/2018/03/11/object-detection-and-classification-using-r-cnns/</a><br><a href="https://zhuanlan.zhihu.com/p/31426458" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/31426458</a><br><a href="https://blog.csdn.net/lanran2/article/details/54376126" target="_blank" rel="noopener">https://blog.csdn.net/lanran2/article/details/54376126</a></p><hr><h1 id="建议"><a href="#建议" class="headerlink" title="建议"></a>建议</h1><p>在正文开始之前，推荐可以先了解一下相关的概念，比如1x1卷积、bounding box、anchor box和NMS等。我之前写过几篇相关的文章，可以先速览一下以防概念不清。<br>Bbox和anchor：<a href="https://gsy00517.github.io/computer-vision20200128162333/" target="_blank">computer-vision笔记：anchor-box</a>。<br>非极大抑制：<a href="https://gsy00517.github.io/computer-vision20200128162422/" target="_blank">computer-vision笔记：non-max suppression</a>。<br>1x1卷积：<a href="https://gsy00517.github.io/deep-learning20190915073809/" target="_blank">deep-learning笔记：着眼于深度——VGG简介与pytorch实现</a>。<br>此外也推荐b站上的两个视频<a href="https://www.bilibili.com/video/av21846607?p=5" target="_blank">图解RCNN和FastRCNN</a>和<a href="https://www.bilibili.com/video/av21846607?p=6" target="_blank">图解FasterRCNN</a>，讲得挺不错的，就是声音有延迟，也可以直接看我的文章。视频中有地方讲得比较模糊，我在查阅资料之后更正了。<br>附faster RCNN的<a href="https://github.com/ruotianluo/pytorch-faster-rcnn" target="_blank">pytorch实现</a>。</p><hr><h1 id="RCNN和Fast-RCNN"><a href="#RCNN和Fast-RCNN" class="headerlink" title="RCNN和Fast RCNN"></a>RCNN和Fast RCNN</h1><p>它们两者是Faster RCNN的先驱和基础，在这里简单介绍一下。首先先说明，无论是RCNN，还是Fast RCNN，还是Faster RCNN，它们的目的都是一样的，就是对一个图片，找出其中的目标物体，并用bounding box框出。</p><h2 id="RCNN"><a href="#RCNN" class="headerlink" title="RCNN"></a>RCNN</h2><p>RCNN即Region CNN，可以说是利用深度学习进行目标检测的开山之作，它用CNN（AlexNet）代替了之前sliding window的方法。<br><img src="/computer-vision20200129102927/RCNN.png" title="RCNN"><br>上面是RCNN的基本结构（图源自上面推荐的视频）。首先我们通过Selective Search（选择性搜索）去生成大量的认为可能存在目标物体的小图块。然后将所有的这些图块通过预先训练得很完美的CNN进行特征提取，比如AlexNet、VGG等。然后我们再对这些convNet卷积网络的输出进行两个操作：（1）Bbox回归，确定Bbox框出的目标位置即用回归的方式确定Bbox的4个参数；（2）分类，即用SVM判断Bbox标注的目标是什么物体。<br>RCNN的主要缺点就是计算成本非常巨大，这里会用上千张小图块去通过一个同样的卷积网络，即进行约2000次左右的串行式前向传播，而在之后又要分别经过回归和支持向量机两个模型，也就是要重复地执行以上操作上千遍，这就严重影响了速度。<br>由于使用了AlexNet（或者VGG），需要每一个候选框统一成相同的227x227的尺寸（若使用VGG，则是224x224），这就严重影响了CNN提取特征的质量。由于RCNN中SVM需要单独训练，随着类别的增加训练SVM的个数也会增加，这也使得训练过程更加复杂。此外，Selective Search去生成这些图像块的过程也是非常expensive和slow的。</p><blockquote><p>补充：这里简单介绍一下Selective Search。<br>Selective Search类似于一种层次聚类算法，就是根据颜色、纹理、尺寸和空间交叠相似度，从众多小尺寸、细粒度的框中逐步选择、合并出大尺寸、粗粒度的约2000个候选框，用作随后的特征提取。</p></blockquote><p>考虑到RCNN提取特征的巨大花费和较低的质量，后来何恺明大神对此做出了最早的改进，提出了SPP（空间金字塔池化），使得候选区域特征的提取只需要执行一遍且能够使任意大小的RoI统一成相同尺寸，其思路类似于稍晚于它的Fast RCNN（不过从日后的表现来看Fast RCNN的RoI Pooling较好），直到后来更好的Faster RCNN被提出。</p><h2 id="Fast-RCNN"><a href="#Fast-RCNN" class="headerlink" title="Fast RCNN"></a>Fast RCNN</h2><p>Fast RCNN主要针对RCNN的问题进行了改进，即从逐个提取特征进步到了一次性提取特征。它首先直接对原始图像用卷积网络去提取特征，然后再在这张feature map上使用Selective Search。这样就使得只需要一张图像经过一遍卷积网络而不是将上千张图像去经过上千遍网络。然而Selective Search在这里还是没有得到改进，这是后面Faster RCNN使用RPN替换Selective Search的突破点。<br><img src="/computer-vision20200129102927/Fast_RCNN.png" title="Fast_RCNN"><br>然后通过RoI Pooling Layer使各个图像块的大小统一，以方便后面的回归和分类操作，这点后面还会讲到。最后Fast RCNN还做了一项改进就是使用两个并行的层代替了原本的SVM和Bbox回归两个模型，减少了模型的复杂度和参数量，同时这也将原本的分类、回归多任务损失统一在同一个框架中，相当于可以在训练的时候协同调整，使模型更加平衡，表现更好。</p><hr><h1 id="Faster-RCNN"><a href="#Faster-RCNN" class="headerlink" title="Faster RCNN"></a>Faster RCNN</h1><ol><li><h2 id="基本结构"><a href="#基本结构" class="headerlink" title="基本结构"></a>基本结构</h2><img src="/computer-vision20200129102927/基本结构.jpg" title="基本结构"> 如图所示，Faster RCNN可以分为4个主要部分：<ol><li><h3 id="conv-layers"><a href="#conv-layers" class="headerlink" title="conv layers"></a>conv layers</h3>作为一种CNN网络目标检测方法，Faster RCNN首先使用一组基础的conv卷积+relu激活+pooling池化提取image的feature maps。该feature maps被共享用于后续RPN层和全连接层。</li><li><h3 id="Region-Proposal-Network"><a href="#Region-Proposal-Network" class="headerlink" title="Region Proposal Network"></a>Region Proposal Network</h3>RPN网络用于生成region proposals。该层通过softmax判断anchors属于positive或者negative，再利用bounding box regression修正anchors获得精确的proposals。</li><li><h3 id="RoI-pooling"><a href="#RoI-pooling" class="headerlink" title="RoI pooling"></a>RoI pooling</h3>RoI即Region of Interest，RoI pooling是池化层的一种。该层收集输入的feature maps和proposals，综合这些信息后，提取proposal feature maps，送入后续全连接层判定目标类别。</li><li><h3 id="classifier"><a href="#classifier" class="headerlink" title="classifier"></a>classifier</h3>利用proposal feature maps计算proposal的类别，即确定是什么物体。同时再次进行bounding box regression以获得检测框最终的精确位置。</li></ol></li><li><h2 id="流程"><a href="#流程" class="headerlink" title="流程"></a>流程</h2><ol><li><h3 id="预处理"><a href="#预处理" class="headerlink" title="预处理"></a>预处理</h3>首先对输入的图像进行预处理（常规操作），即减去均值并缩放成固定大小MxN。这个预处理过程对training和inference都是identical的。注意，这里的mean指的是对于整个训练集/测试集的均值而不是单张图片本身。 <img src="/computer-vision20200129102927/图像预处理.png" title="图像预处理"></li><li><h3 id="特征提取"><a href="#特征提取" class="headerlink" title="特征提取"></a>特征提取</h3>接收了处理后固定大小的图像后，使用一个卷积网络去提取特征，这个网络包含conv，pooling，relu三种层。以使用VGG16的Faster RCNN版本的网络结构为例，如图所示。 <img src="/computer-vision20200129102927/使用VGG16的Fast_RCNN.jpg" title="使用VGG16的Fast_RCNN"> 这里的conv layers部分共有13个conv层，13个relu层和4个pooling层。这里的参数值得注意：所有的conv层都设置kernel size为3，padding为1，stride为1；而所有的pooling层都设置kernel size为2且stride为2。<br>这样设置有什么目的呢？可以结合下面的示意图来看，首先对所有的卷积都做了扩边处理（padding为1，即填充一圈0），使原图变为(M+2)x(N+2)的大小，然后再做3x3卷积，输出大小仍为MxN。正是这种设置，使得conv层不改变输入和输出矩阵大小。 <img src="/computer-vision20200129102927/卷积示意图.jpg" title="卷积示意图"> 再来看池化层，设kernel size为2且stride为2。这样每个经过pooling层的MxN矩阵，都会变为(M/2)x(N/2)大小。<br>综上所述，在通过的整个卷积网络中，conv层和relu层不改变输入输出大小，只有pooling层使输出长宽都变为输入的1/2。那么4个pooling层就使得MxN大小的矩阵经过特征提取后固定变为(M/16)x(N/16)的feature maps。</li><li><h3 id="提取候选框（RPN）"><a href="#提取候选框（RPN）" class="headerlink" title="提取候选框（RPN）"></a>提取候选框（RPN）</h3><p>接下来就是最重要的Region Proposal Network。经典的检测方法生成检测框都非常耗时，如OpenCV adaboost使用滑动窗口+图像金字塔生成检测框；或如RCNN使用SS（Selective Search）方法生成检测框。而Faster RCNN则抛弃了传统的滑动窗口和SS方法，直接使用RPN生成检测框，这也是Faster RCNN的巨大优势，能极大提升检测框的生成速度。<br>这里还是先借用一张图来明确一下Faster RCNN的流程，整个RPN网络其实相当于这里的Anchor Generation Layer和Region Proposal Layer。</p><img src="/computer-vision20200129102927/流程.png" title="流程"><p>这里的Anchor Target Layer是用于识别出一系列与ground truth box的分数达到一定阈值的较好的foreground anchors前景（物体）锚框和低于一定阈值的background anchors背景锚框，以及其对应的regression coefficients来训练RPN网络。这里的RPN Loss就是识别标记的foreground/background labels中的正确率与predicted和target regression coefficients之间的定义距离的组合。最后的Classification Loss也与RPN Loss定义类似，也是组合了正确率和系数距离。</p><blockquote><p>补充：这里我想先结合上面讲一下训练的过程，也可以跳过这一块继续看RPN。<br>上面所述的Anchor Target Layer的输出并不用于后面分类器的训练。用于后面分类器训练的是Proposal Target Layer的输出。也就是说RPN层和分类器是分开训练的，先用预训练好的模型（比如VGG、ResNet和作者论文中用的ZF）训练RPN，再把训练好的RPN放到Faster RCNN中走上面流程图中的另一条路径训练分类器也就是整个Fast RCNN网络。根据这种思路，实际的训练过程分为4步：<br>（1）在已经预训练好的model上，第一次训练RPN网络。<br>（2）第一次训练整个Fast RCNN网络。<br>（3）再第二次单独训练训练RPN网络。<br>（4）再次利用步骤3中训练好的RPN网络，收集proposals，并第二次训练Fast RCNN网络。</p><img src="/computer-vision20200129102927/训练过程.jpg" title="训练过程"><p>之所以只进行了类似的“循环”两次，是因为循环更多次并没有negligible improvements。</p></blockquote><p>好的还是先回到RPN模块。</p><img src="/computer-vision20200129102927/使用VGG16的Fast_RCNN.jpg" title="使用VGG16的Fast_RCNN"><p>还是参照上一节提到的使用VGG16的Faster RCNN版本的网络结构，可以看到RPN网络实际分为2条线，上面一条通过softmax分类anchors获得positive anchors（存在目标的，也就是foreground anchors）和negative anchors两类，下面一条用于计算对于anchors的bounding box regression偏移量，以获得精确的proposal。而最后的Proposal Layer则负责综合positive anchors和对应bounding box regression偏移量获取proposals，同时剔除太小和超出边界的proposals。其实整个网络到了Proposal Layer这里，就相当于完成了目标定位的功能。<br>下面更细地讲一下这里具体是怎么做的。</p><h4 id="anchor"><a href="#anchor" class="headerlink" title="anchor"></a>anchor</h4><p>作者是这样生成anchor box的：对输入的feature map上的每一个点（pixel），都设置9个不同尺度和形状的anchor box，如下图所示。</p><img src="/computer-vision20200129102927/生成anchor.jpg" title="生成anchor"><p>这样通过anchor box引入了目标检测中多尺度的方法，可以看到基本上整张图片上的各种尺度和形状都被cover到了。也可以事先通过对数据集聚类分析的方式来确定初始的anchor box的尺寸。<br>当然，这样做获得box很不准确，不用担心，后面还有2次bounding box regression可以修正检测框的位置。</p><blockquote><p>补充：下面是原论文中的一张图，在这里做一些简单的解释。</p><img src="/computer-vision20200129102927/使用anchor.jpg" title="使用anchor"><ol><li>这里的256-d指的是之前用于提取特征的卷积网络生成的feature maps的数量，其具体维度视使用的卷积网络而定。原文中使用的是ZF model中，其最后一层conv层输出维度为256，即生成256张feature maps，也相当于得到的一个feature map中每个点都是256维的。</li><li>结合前文中使用VGG16的Faster RCNN版本的网络结构，可以看到，在卷积网络提取出feature map之后，又做了3x3卷积且输出依旧是256维的，相当于每个点又融合了周围3x3的空间信息，也许这样做会是模型更鲁棒。</li><li>图中的k表示的不是千，而是每个点对应的anchor的个数，这里默认是9，而每个anhcor要分positive和negative，所以每个点cls分类需要两个分数，一个是前景（物体）分数，一个是背景分数，即图中所示2k scores；而每个anchor box又需要4个偏移量来定位，所以这里reg回归为4k coordinates。</li><li>在生成anchor的示意图中可以看到，显然anchors太多了，因此训练时会在合适的anchors中随机选取128个postive anchors与128个negative anchors进行训练。</li></ol></blockquote><h4 id="分类"><a href="#分类" class="headerlink" title="分类"></a>分类</h4><p>为了便于分析，我还是再把上面那张图拿下来。</p><img src="/computer-vision20200129102927/使用VGG16的Fast_RCNN.jpg" title="使用VGG16的Fast_RCNN"><p>在经过3x3的卷积之后，又经过18个1x1的卷积核，这里的卷积主要是为了改变维数。比如我们输入一张WxH的feature map，那么经过该卷积输出就为WxHx18大小（输出图像通道数总是等于卷积核数量）。那么为什么是18呢？<br>容易发现，18等于2（positive/negative）乘上9（anchors），也就是因为feature maps每一个点都有9个anchors，同时每个anchors又有可能是positive和negative，所以利用WxHx(9x2)大小的矩阵来保存这些信息。<br>这里的softmax就是用于分类获得positive anchors，也就相当于初步提取了检测目标候选区域的Bbox。<br>而softmax前后的两个reshape其实就是为了变换输入的张量以便于softmax分类（有点类似于一些网络在最后的卷积层和全连接层之间将张量拍扁成一维的），后面的reshape就是恢复原状的作用。<br>其实RPN在这里就是在原图尺度上，设置了密密麻麻的候选anchor box。然后判断哪些是里面有目标的positive anchor，哪些是没目标的negative anchor。</p><h4 id="回归"><a href="#回归" class="headerlink" title="回归"></a>回归</h4><p>接下来我们来看看RPN模块下面那一条路径在做什么。<br>如图所示，绿色框为事先标注的飞机的ground truth box（GT），红色框为前面提取的positive anchors，虽然红色框被分类器识别为飞机，但是由于红色框定位不准，依旧没有达到正确地检测出飞机的目标。所以我们希望采用一种方法对红色框进行微调，使得positive anchors和GT更加接近。</p><img src="/computer-vision20200129102927/不准.jpg" title="不准"><p>对于一个box，我们一般使用一个四维向量$\left ( x,y,w,h \right )$来表示，即标注中心点的坐标和box的宽度和高度。在下图中，红色框A代表原始的positive anchors，绿色框G代表目标的GT，我们的目标是寻找一种关系，使得输入原始的A经过映射得到一个跟真实窗口G更接近的回归窗口G’，即寻找一种变换$F$，s.t.$F\left ( A_{x},A_{y},A_{w},A_{h} \right )=\left ( G_{x}^{‘},G_{y}^{‘},G_{w}^{‘},G_{h}^{‘} \right )$且$\left ( G_{x}^{‘},G_{y}^{‘},G_{w}^{‘},G_{h}^{‘} \right )\approx \left ( G_{x},G_{y},G_{w},G_{h} \right )$。</p><img src="/computer-vision20200129102927/微调.jpg" title="微调"><p>那么这个变换$F$如何选择呢？一种简单的思路就是先做平移后做缩放，即：</p><script type="math/tex;mode=display">G_{x}^{'}=A_{w}\cdot d_{x}\left ( A \right )+A_{x}</script><script type="math/tex;mode=display">G_{y}^{'}=A_{h}\cdot d_{y}\left ( A \right )+A_{y}</script><script type="math/tex;mode=display">G_{w}^{'}=A_{w}\cdot exp\left ( d_{w}\left ( A \right ) \right )</script><script type="math/tex;mode=display">G_{h}^{'}=A_{h}\cdot exp\left ( d_{h}\left ( A \right ) \right )</script><p>因此我们需要学习的是$d_{x}\left ( A \right )$，$d_{y}\left ( A \right )$，$d_{w}\left ( A \right )$，$d_{h}\left ( A \right )$这四个变换。当输入的A与GT相差较小时，可以认为这种变换是一种线性变换，那么就可以用线性回归来进行微调。</p><blockquote><p>注：线性回归就是给定输入的特征向量$X$，学习一组参数$W$，使得经过线性回归后的值跟真实值$Y$非常接近，即$Y=WX$。</p></blockquote><p>对于该问题，输入$X$是cnn feature map，定义为$\phi$；同时还有训练传入A与GT之间的变换量，即$\left ( t_{x},t_{y},t_{w},t_{h} \right )$。输出是上面所说的四个变换。则目标函数可表示为：</p><script type="math/tex;mode=display">d_{\ast }\left ( A \right )=W_{\ast }^{T}\cdot \phi \left ( A \right )</script><p>为了让预测值$d_{\ast }\left ( A \right )$与真实值$t_{\ast }$差距最小，设计L1损失函数：</p><script type="math/tex;mode=display">Loss=\sum_{i}^{N}\left | t_{\ast }^{i}-W_{\ast }^{T}\cdot \phi \left ( A^{i} \right ) \right |</script><p>得到优化目标为：</p><script type="math/tex;mode=display">\widehat{W}_{\ast }=argmin_{W_{\ast }}\sum_{i}^{n}\left | t_{\ast }^{i}-W_{\ast }^{T}\cdot \phi \left ( A^{i} \right ) \right |+\lambda \left \| W_{\ast } \right \|</script><p>这里的$argmin$表示的是给定参数的表达式达到最小值。</p><blockquote><p>补充：这里positive anchor与ground truth之间的平移量$\left ( t_{x},t_{y} \right )$和尺度因子$\left ( t_{w},t_{h} \right )$定义如下：</p><script type="math/tex;mode=display">t_{x}=\frac{\left ( T_{x}-O_{x} \right )}{O_{w}}</script><script type="math/tex;mode=display">t_{y}=\frac{\left ( T_{y}-O_{y} \right )}{O_{h}}</script><script type="math/tex;mode=display">t_{w}=log\left ( \frac{T_{w}}{O_{w}} \right )</script><script type="math/tex;mode=display">t_{h}=log\left ( \frac{T_{h}}{O_{h}} \right )</script><p>请结合下图理解。之所以这样定义，是为了回归系数在图片进行仿射变换之后依旧能够保持不变。</p><img src="/computer-vision20200129102927/不影响.png" title="不影响"><p>有关仿射变换，可以看一下之前的文章<a href="https://gsy00517.github.io/linear-algebra20200116084728/" target="_blank">linear-algebra笔记：二维仿射变换</a>。</p></blockquote><h4 id="Proposal-Layer"><a href="#Proposal-Layer" class="headerlink" title="Proposal Layer"></a>Proposal Layer</h4><p>Proposal Layer有3个输入：positive anchors分类器结果，Bbox reg的变换量以及生成的anchor。如图所示，在选择最优的多个box，然后对这些box作NMS，结果作为proposals输出。</p><img src="/computer-vision20200129102927/proposal_layer.png" title="proposal_layer"><h4 id="RPN小结"><a href="#RPN小结" class="headerlink" title="RPN小结"></a>RPN小结</h4><p>以上就是RPN网络提取候选框的大致介绍，总结起来就是：首先，生成anchors；然后，用softmax分类器提取positvie anchors；接着，Bbox reg回归positive anchors；最后，通过Proposal Layer生成proposals。</p></li><li><h3 id="RoI-pooling-1"><a href="#RoI-pooling-1" class="headerlink" title="RoI pooling"></a>RoI pooling</h3>先来看一个问题：对于传统的CNN（如AlexNet和VGG），当网络训练好后输入的图像尺寸必须是固定值，同时网络输出也是固定大小的vector或者matrix。如果输入图像大小不定，这个问题就变得比较麻烦。有2种解决办法：从图像中crop一部分传入网络，或者将图像warp成需要的大小后传入网络。 <img src="/computer-vision20200129102927/传统方法.jpg" title="传统方法"> 问题是，无论采取那种办法都不是很好，要么crop后破坏了图像的完整结构，要么warp后破坏了图像原始形状信息。<br>于是，Fast RCNN就提出了RoI pooling来解决这个问题，其思路是与其wrap图像破坏信息，不如尝试着去wrap特征。<br>其步骤如下：<h4 id="Step1：Coordinate-on-image"><a href="#Step1：Coordinate-on-image" class="headerlink" title="Step1：Coordinate on image"></a>Step1：Coordinate on image</h4>由于proposal对应的尺度是MXN，所以首先将其映射回(M/16)X(N/16)尺度大小的feature map。（若不能整除，则向下取整，相当于丢弃小部分右侧和下侧的信息）<h4 id="Step2：Coordinate-on-feature-map"><a href="#Step2：Coordinate-on-feature-map" class="headerlink" title="Step2：Coordinate on feature map"></a>Step2：Coordinate on feature map</h4>再将每个proposal对应的feature map区域水平分为WxH的网格。<h4 id="Step3：Coordinate-on-RoI-feature"><a href="#Step3：Coordinate-on-RoI-feature" class="headerlink" title="Step3：Coordinate on RoI feature"></a>Step3：Coordinate on RoI feature</h4>接着对网格中的每一份都进行max pooling处理。如此就得到了WxH固定大小的输出。 <img src="/computer-vision20200129102927/RoI示意图.jpg" title="RoI示意图"> 由于RoI pooling这里采用了两次浮点数取证运算，这就使得池化之后的输出可能会于原图像的尺寸对不上，因此后来何恺明大神又提出了RoI Align，采用了双线性插值，作出了进一步改进。<blockquote><p>补充：2019年IoUNet又提出了PrRoI pooling，相比RoI Align，它不用预设N的个数，直接使用积分取均值。</p><img src="/computer-vision20200129102927/最新进展.png" title="最新进展"></blockquote></li><li><h3 id="分类器"><a href="#分类器" class="headerlink" title="分类器"></a>分类器</h3>最后的分类部分利用已经获得的proposal feature maps，通过full connect层与softmax计算每个proposal具体属于那个类别（如人，车，电视等），输出含有各个类别的概率向量；同时再次利用bounding box regression获得每个proposal的位置偏移量，用于回归更加精确的目标检测框。 <img src="/computer-vision20200129102927/最后再次定位和回归.jpg" title="最后再次定位和回归"></li></ol></li></ol><script type="text/javascript" src="/js/src/bai.js"></script></div><div><div><hr style="FILTER:progid:DXImageTransform.Microsoft.Shadow(color:#987cb9,direction:145,strength:15)" width="100%" color="#987cb9" size="1"><div style="text-align:center;color:#555;font-size:16px"><i class="fa fa-hand-o-up" aria-hidden="true"></i> 碰到底线咯 <i class="fa fa-handshake-o" aria-hidden="true"></i> 后面没有啦 <i class="fa fa-hand-o-down" aria-hidden="true"></i></div></div></div><div><div class="my_post_copyright"><script src="//cdn.bootcss.com/clipboard.js/1.5.10/clipboard.min.js"></script><script src="https://cdn.bootcss.com/jquery/2.0.0/jquery.min.js"></script><script src="https://unpkg.com/sweetalert/dist/sweetalert.min.js"></script><p><span>本文标题:</span><a href="/computer-vision20200129102927/">computer vision笔记：RPN与Faster RCNN</a></p><p><span>文章作者:</span><a href="/" title="访问 高深远 的个人博客">高深远</a></p><p><span>发布时间:</span>2020年01月29日 - 10:29</p><p><span>最后更新:</span>2020年02月15日 - 22:08</p><p><span>原始链接:</span><a href="/computer-vision20200129102927/" title="computer vision笔记：RPN与Faster RCNN">https://gsy00517.github.io/computer-vision20200129102927/</a> <span class="copy-path" title="点击复制文章链接"><i class="fa fa-clipboard" data-clipboard-text="https://gsy00517.github.io/computer-vision20200129102927/" aria-label="复制成功！"></i></span></p><p><span>许可协议:</span><i class="fa fa-creative-commons"></i> <a rel="license" href="https://creativecommons.org/licenses/by-nc-nd/4.0/" target="_blank" title="Attribution-NonCommercial-NoDerivatives 4.0 International (CC BY-NC-ND 4.0)">署名-非商业性使用-禁止演绎 4.0 国际</a> 转载请保留原文链接及作者。</p></div><script>var clipboard=new Clipboard(".fa-clipboard");$(".fa-clipboard").click(function(){clipboard.on("success",function(){swal({title:"复制成功",text:"感谢您的阅读与参考！欢迎留下任何建议噢！",icon:"success",showConfirmButton:!0})})})</script></div><footer class="post-footer"><div class="post-tags"><a href="/tags/深度学习/" rel="tag"><i class="fa fa-tag"></i> 深度学习</a> <a href="/tags/计算机视觉/" rel="tag"><i class="fa fa-tag"></i> 计算机视觉</a> <a href="/tags/目标检测/" rel="tag"><i class="fa fa-tag"></i> 目标检测</a></div><div class="post-widgets"><div class="wp_rating"><div id="wpac-rating"></div></div></div><div class="post-nav"><div class="post-nav-next post-nav-item"><a href="/computer-vision20200128162422/" rel="next" title="computer vision笔记：non-max suppression"><i class="fa fa-chevron-left"></i> computer vision笔记：non-max suppression</a></div><span class="post-nav-divider"></span><div class="post-nav-prev post-nav-item"><a href="/machine-learning20200130121842/" rel="prev" title="machine learning笔记：准确率和召回率">machine learning笔记：准确率和召回率 <i class="fa fa-chevron-right"></i></a></div></div></footer></div></article><div class="post-spread"></div></div></div><div class="comments" id="comments"></div></div><div class="sidebar-toggle"><div class="sidebar-toggle-line-wrap"><span class="sidebar-toggle-line sidebar-toggle-line-first"></span> <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span> <span class="sidebar-toggle-line sidebar-toggle-line-last"></span></div></div><aside id="sidebar" class="sidebar"><div class="sidebar-inner"><ul class="sidebar-nav motion-element"><li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap">文章目录</li><li class="sidebar-nav-overview" data-target="site-overview-wrap">站点概览</li></ul><section class="site-overview-wrap sidebar-panel"><div class="site-overview"><div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person"><img class="site-author-image" itemprop="image" src="/images/lufei.jpg" alt="高深远"><p class="site-author-name" itemprop="name">高深远</p><p class="site-description motion-element" itemprop="description"></p></div><nav class="site-state motion-element"><div class="site-state-item site-state-posts"><a href="/archives/"><span class="site-state-item-count">81</span> <span class="site-state-item-name">日志</span></a></div><div class="site-state-item site-state-categories"><a href="/categories/index.html"><span class="site-state-item-count">7</span> <span class="site-state-item-name">分类</span></a></div><div class="site-state-item site-state-tags"><a href="/tags/index.html"><span class="site-state-item-count">33</span> <span class="site-state-item-name">标签</span></a></div></nav><div class="feed-link motion-element"><a href="/atom.xml" rel="alternate"><i class="fa fa-rss"></i> RSS</a></div><div class="links-of-blogroll motion-element links-of-blogroll-inline"><div class="links-of-blogroll-title"><i class="fa fa-fw fa-link"></i> 网站收藏</div><ul class="links-of-blogroll-list"><li class="links-of-blogroll-item"><a href="https://www.runoob.com/" title="菜鸟教程" target="_blank">菜鸟教程</a></li><li class="links-of-blogroll-item"><a href="https://paperswithcode.com/" title="PaperWithCode" target="_blank">PaperWithCode</a></li><li class="links-of-blogroll-item"><a href="https://www.jiqizhixin.com/sota" title="机器之心" target="_blank">机器之心</a></li><li class="links-of-blogroll-item"><a href="http://pytorch123.com/" title="Pytorch中文文档" target="_blank">Pytorch中文文档</a></li></ul></div></div></section><section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active"><div class="post-toc"><div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#建议"><span class="nav-number">1.</span> <span class="nav-text">建议</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#RCNN和Fast-RCNN"><span class="nav-number">2.</span> <span class="nav-text">RCNN和Fast RCNN</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#RCNN"><span class="nav-number">2.1.</span> <span class="nav-text">RCNN</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Fast-RCNN"><span class="nav-number">2.2.</span> <span class="nav-text">Fast RCNN</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Faster-RCNN"><span class="nav-number">3.</span> <span class="nav-text">Faster RCNN</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#基本结构"><span class="nav-number">3.1.</span> <span class="nav-text">基本结构</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#conv-layers"><span class="nav-number">3.1.1.</span> <span class="nav-text">conv layers</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Region-Proposal-Network"><span class="nav-number">3.1.2.</span> <span class="nav-text">Region Proposal Network</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#RoI-pooling"><span class="nav-number">3.1.3.</span> <span class="nav-text">RoI pooling</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#classifier"><span class="nav-number">3.1.4.</span> <span class="nav-text">classifier</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#流程"><span class="nav-number">3.2.</span> <span class="nav-text">流程</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#预处理"><span class="nav-number">3.2.1.</span> <span class="nav-text">预处理</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#特征提取"><span class="nav-number">3.2.2.</span> <span class="nav-text">特征提取</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#提取候选框（RPN）"><span class="nav-number">3.2.3.</span> <span class="nav-text">提取候选框（RPN）</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#anchor"><span class="nav-number">3.2.3.1.</span> <span class="nav-text">anchor</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#分类"><span class="nav-number">3.2.3.2.</span> <span class="nav-text">分类</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#回归"><span class="nav-number">3.2.3.3.</span> <span class="nav-text">回归</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Proposal-Layer"><span class="nav-number">3.2.3.4.</span> <span class="nav-text">Proposal Layer</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#RPN小结"><span class="nav-number">3.2.3.5.</span> <span class="nav-text">RPN小结</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#RoI-pooling-1"><span class="nav-number">3.2.4.</span> <span class="nav-text">RoI pooling</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Step1：Coordinate-on-image"><span class="nav-number">3.2.4.1.</span> <span class="nav-text">Step1：Coordinate on image</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Step2：Coordinate-on-feature-map"><span class="nav-number">3.2.4.2.</span> <span class="nav-text">Step2：Coordinate on feature map</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Step3：Coordinate-on-RoI-feature"><span class="nav-number">3.2.4.3.</span> <span class="nav-text">Step3：Coordinate on RoI feature</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#分类器"><span class="nav-number">3.2.5.</span> <span class="nav-text">分类器</span></a></li></ol></li></ol></li></ol></div></div></section></div></aside></div></main><footer id="footer" class="footer"><div class="footer-inner"><div class="copyright">&copy; <span itemprop="copyrightYear">2020</span> <span class="with-love"><i class="fa fa-user"></i> </span><span class="author" itemprop="copyrightHolder">高深远</span> <span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-area-chart"></i> </span><span class="post-meta-item-text">Site words total count&#58;</span> <span title="Site words total count">151.5k</span></div><script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><span id="busuanzi_container_site_pv">访客已留下<span id="busuanzi_value_site_pv"></span>个脚印 </span><span id="busuanzi_container_site_uv">你是第<span id="busuanzi_value_site_uv"></span>位小伙伴</span></div></footer><div class="back-to-top"><i class="fa fa-arrow-up"></i> <span id="scrollpercent"><span>0</span>%</span></div></div><script type="text/javascript">"[object Function]"!==Object.prototype.toString.call(window.Promise)&&(window.Promise=null)</script><script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script><script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script><script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script><script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script><script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script><script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script><script type="text/javascript" src="/lib/three/three.min.js"></script><script type="text/javascript" src="/lib/three/three-waves.min.js"></script><script type="text/javascript" src="/js/src/utils.js?v=5.1.4"></script><script type="text/javascript" src="/js/src/motion.js?v=5.1.4"></script><script type="text/javascript" src="/js/src/affix.js?v=5.1.4"></script><script type="text/javascript" src="/js/src/schemes/pisces.js?v=5.1.4"></script><script type="text/javascript" src="/js/src/scrollspy.js?v=5.1.4"></script><script type="text/javascript" src="/js/src/post-details.js?v=5.1.4"></script><script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.4"></script><script src="//cdn1.lncld.net/static/js/3.0.4/av-min.js"></script><script src="//unpkg.com/valine/dist/Valine.min.js"></script><script type="text/javascript">var GUEST = ['nick','mail','link'];
    var guest = 'nick,mail,link';
    guest = guest.split(',').filter(item=>{
      return GUEST.indexOf(item)>-1;
    });
    new Valine({
        el: '#comments' ,
        verify: false,
        notify: true,
        appId: 'SVKPF4oiwJLMtpySQxvJLnLm-gzGzoHsz',
        appKey: '46OfvvEe65XMeUi79STU895I',
        placeholder: '动动手指，写下您的意见、疑惑或者鼓励吧！留下您的邮箱，这样就可以收到别人的回复啦！',
        avatar:'wavatar',
        guest_info:guest,
        pageSize:'10' || 10,
    });

    var infoEle = document.querySelector('#comments .info');
    if (infoEle && infoEle.childNodes && infoEle.childNodes.length > 0){
    infoEle.childNodes.forEach(function(item) {
    item.parentNode.removeChild(item);
    });
  }</script><script type="text/javascript">function proceedsearch(){$("body").append('<div class="search-popup-overlay local-search-pop-overlay"></div>').css("overflow","hidden"),$(".search-popup-overlay").click(onPopupClose),$(".popup").toggle();var t=$("#local-search-input");t.attr("autocapitalize","none"),t.attr("autocorrect","off"),t.focus()}var isfetched=!1,isXml=!0,search_path="search.xml";0===search_path.length?search_path="search.xml":/json$/i.test(search_path)&&(isXml=!1);var path="/"+search_path,onPopupClose=function(t){$(".popup").hide(),$("#local-search-input").val(""),$(".search-result-list").remove(),$("#no-result").remove(),$(".local-search-pop-overlay").remove(),$("body").css("overflow","")},searchFunc=function(t,e,o){"use strict";$("body").append('<div class="search-popup-overlay local-search-pop-overlay"><div id="search-loading-icon"><i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i></div></div>').css("overflow","hidden"),$("#search-loading-icon").css("margin","20% auto 0 auto").css("text-align","center"),$.ajax({url:t,dataType:isXml?"xml":"json",async:!0,success:function(t){isfetched=!0,$(".popup").detach().appendTo(".header-inner");var n=isXml?$("entry",t).map(function(){return{title:$("title",this).text(),content:$("content",this).text(),url:$("url",this).text()}}).get():t,r=document.getElementById(e),s=document.getElementById(o),a=function(){var t=r.value.trim().toLowerCase(),e=t.split(/[\s\-]+/);e.length>1&&e.push(t);var o=[];if(t.length>0&&n.forEach(function(n){function r(e,o,n,r){for(var s=r[r.length-1],a=s.position,i=s.word,l=[],h=0;a+i.length<=n&&0!=r.length;){i===t&&h++,l.push({position:a,length:i.length});var p=a+i.length;for(r.pop();0!=r.length&&(s=r[r.length-1],a=s.position,i=s.word,p>a);)r.pop()}return c+=h,{hits:l,start:o,end:n,searchTextCount:h}}function s(t,e){var o="",n=e.start;return e.hits.forEach(function(e){o+=t.substring(n,e.position);var r=e.position+e.length;o+='<b class="search-keyword">'+t.substring(e.position,r)+"</b>",n=r}),o+=t.substring(n,e.end)}var a=!1,i=0,c=0,l=n.title.trim(),h=l.toLowerCase(),p=n.content.trim().replace(/<[^>]+>/g,""),u=p.toLowerCase(),f=decodeURIComponent(n.url),d=[],g=[];if(""!=l&&(e.forEach(function(t){function e(t,e,o){var n=t.length;if(0===n)return[];var r=0,s=[],a=[];for(o||(e=e.toLowerCase(),t=t.toLowerCase());(s=e.indexOf(t,r))>-1;)a.push({position:s,word:t}),r=s+n;return a}d=d.concat(e(t,h,!1)),g=g.concat(e(t,u,!1))}),(d.length>0||g.length>0)&&(a=!0,i=d.length+g.length)),a){[d,g].forEach(function(t){t.sort(function(t,e){return e.position!==t.position?e.position-t.position:t.word.length-e.word.length})});var v=[];0!=d.length&&v.push(r(l,0,l.length,d));for(var $=[];0!=g.length;){var C=g[g.length-1],m=C.position,x=C.word,w=m-20,y=m+80;0>w&&(w=0),y<m+x.length&&(y=m+x.length),y>p.length&&(y=p.length),$.push(r(p,w,y,g))}$.sort(function(t,e){return t.searchTextCount!==e.searchTextCount?e.searchTextCount-t.searchTextCount:t.hits.length!==e.hits.length?e.hits.length-t.hits.length:t.start-e.start});var T=parseInt("1");T>=0&&($=$.slice(0,T));var b="";b+=0!=v.length?"<li><a href='"+f+"' class='search-result-title'>"+s(l,v[0])+"</a>":"<li><a href='"+f+"' class='search-result-title'>"+l+"</a>",$.forEach(function(t){b+="<a href='"+f+'\'><p class="search-result">'+s(p,t)+"...</p></a>"}),b+="</li>",o.push({item:b,searchTextCount:c,hitCount:i,id:o.length})}}),1===e.length&&""===e[0])s.innerHTML='<div id="no-result"><i class="fa fa-search fa-5x" /></div>';else if(0===o.length)s.innerHTML='<div id="no-result"><i class="fa fa-frown-o fa-5x" /></div>';else{o.sort(function(t,e){return t.searchTextCount!==e.searchTextCount?e.searchTextCount-t.searchTextCount:t.hitCount!==e.hitCount?e.hitCount-t.hitCount:e.id-t.id});var a='<ul class="search-result-list">';o.forEach(function(t){a+=t.item}),a+="</ul>",s.innerHTML=a}};r.addEventListener("input",a),$(".local-search-pop-overlay").remove(),$("body").css("overflow",""),proceedsearch()}})};$(".popup-trigger").click(function(t){t.stopPropagation(),isfetched===!1?searchFunc(path,"local-search-input","local-search-result"):proceedsearch()}),$(".popup-btn-close").click(onPopupClose),$(".popup").click(function(t){t.stopPropagation()}),$(document).on("keyup",function(t){var e=27===t.which&&$(".search-popup").is(":visible");e&&onPopupClose()})</script><script src="https://cdn1.lncld.net/static/js/av-core-mini-0.6.4.js"></script><script>AV.initialize("SVKPF4oiwJLMtpySQxvJLnLm-gzGzoHsz","46OfvvEe65XMeUi79STU895I")</script><script>function showTime(e){var t=new AV.Query(e),n=[],o=$(".leancloud_visitors");o.each(function(){n.push($(this).attr("id").trim())}),t.containedIn("url",n),t.find().done(function(e){var t=".leancloud-visitors-count";if(0===e.length)return void o.find(t).text(0);for(var i=0;i<e.length;i++){var r=e[i],s=r.get("url"),l=r.get("time"),c=document.getElementById(s);$(c).find(t).text(l)}for(var i=0;i<n.length;i++){var s=n[i],c=document.getElementById(s),u=$(c).find(t);""==u.text()&&u.text(0)}}).fail(function(e,t){console.log("Error: "+t.code+" "+t.message)})}function addCount(e){var t=$(".leancloud_visitors"),n=t.attr("id").trim(),o=t.attr("data-flag-title").trim(),i=new AV.Query(e);i.equalTo("url",n),i.find({success:function(t){if(t.length>0){var i=t[0];i.fetchWhenSave(!0),i.increment("time"),i.save(null,{success:function(e){var t=$(document.getElementById(n));t.find(".leancloud-visitors-count").text(e.get("time"))},error:function(e,t){console.log("Failed to save Visitor num, with error message: "+t.message)}})}else{var r=new e,s=new AV.ACL;s.setPublicReadAccess(!0),s.setPublicWriteAccess(!0),r.setACL(s),r.set("title",o),r.set("url",n),r.set("time",1),r.save(null,{success:function(e){var t=$(document.getElementById(n));t.find(".leancloud-visitors-count").text(e.get("time"))},error:function(e,t){console.log("Failed to create")}})}},error:function(e){console.log("Error:"+e.code+" "+e.message)}})}$(function(){var e=AV.Object.extend("Counter");1==$(".leancloud_visitors").length?addCount(e):$(".post-title-link").length>1&&showTime(e)})</script><script>!function(){var t=document.createElement("script"),s=window.location.protocol.split(":")[0];"https"===s?t.src="https://zz.bdstatic.com/linksubmit/push.js":t.src="http://push.zhanzhang.baidu.com/push.js";var e=document.getElementsByTagName("script")[0];e.parentNode.insertBefore(t,e)}()</script><script type="text/javascript">wpac_init=window.wpac_init||[],wpac_init.push({widget:"Rating",id:21228,el:"wpac-rating",color:"fc6423"}),function(){if(!("WIDGETPACK_LOADED"in window)){WIDGETPACK_LOADED=!0;var e=document.createElement("script");e.type="text/javascript",e.async=!0,e.src="//embed.widgetpack.com/widget.js";var t=document.getElementsByTagName("script")[0];t.parentNode.insertBefore(e,t.nextSibling)}}()</script><script type="text/x-mathjax-config">MathJax.Hub.Config({
        tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
          processEscapes: true,
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
        }
      });</script><script type="text/x-mathjax-config">MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for (i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
        }
      });</script><script type="text/javascript" src="//cdn.bootcss.com/mathjax/2.7.1/latest.js?config=TeX-AMS-MML_HTMLorMML"></script><canvas class="fireworks" style="position:fixed;left:0;top:0;z-index:1;pointer-events:none"></canvas><script type="text/javascript" src="//cdn.bootcss.com/animejs/2.2.0/anime.min.js"></script><script type="text/javascript" src="/js/src/fireworks.js"></script><script src="/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({pluginRootPath:"live2dw/",pluginJsPath:"lib/",pluginModelPath:"assets/",tagMode:!0,log:!1,model:{jsonPath:"/live2dw/assets/hijiki.model.json"},display:{position:"left",width:250,height:500},mobile:{show:!1},react:{opacity:.7}})</script></body></html><script type="text/javascript" src="/js/src/crash_cheat.js"></script><!-- rebuild by neat -->